title,company,location,link,description,skills,details
DATA ENGINEER (H/F),SFR,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-sfr-3879318123?position=2&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=02j4YeDzqfcHedAZ%2BJWgJw%3D%3D&trk=public_jobs_jserp-result_search-card,"En tant que Data Ingénieur expérimenté, vous occuperez un rôle essentiel dans notre équipe Data Science.
Vous serez responsable de la conception, du développement et de la maintenance des pipelines de données ainsi que de l'intégration de sources de données multiples.
Votre expertise sera cruciale pour garantir une gestion efficace des flux de données, ainsi que pour faciliter l'analyse et la visualisation des données en plus du support aux data scientists vos missions seront les suivantes :
Architecture projet des données
: Concevoir et développer des architectures projet de données robustes, évolutives et performantes pour intégrer et gérer de grandes quantités de données provenant de sources multiples. Assurer la fiabilité, l'évolutivité et la sécurité des flux de données entrant d’un projet Data Science.
Intégration des données
: Élaborer des pipelines de données efficaces pour l'extraction, la transformation et le chargement des données (via notre Framework ELT/ETL interne) provenant de différentes sources. Mettre en place des processus d'intégration automatisés et veiller à la qualité des données.
Gestion des bases de données
: Concevoir et optimiser des bases de données pour répondre aux besoins analytiques et de reporting. Assurer la performance, la disponibilité et la sécurité des bases de données, ainsi que la gestion efficace des requêtes.
Collaboration interfonctionnelle
: Support des Data Scientists, vous travaillerez avec les équipes business pour comprendre leurs besoins et fournir des conseils et des recommandations basés sur les données.
Optimisation des performances
: Surveiller et optimiser les performances des pipelines de données, des bases de données et des requêtes. Identifier les goulots d'étranglement et les points d'optimisation, et proposer des améliorations pour garantir des performances optimales.
Sécurité et conformité
: Veiller à ce que les données soient traitées et stockées conformément aux normes de sécurité et de confidentialité. Mettre en place des mécanismes de sécurité pour protéger les données sensibles et garantir la conformité aux réglementations en vigueur.
Votre profil :
Vous avez un
Diplôme universitaire en informatique, en génie logiciel, en science des données ou dans un domaine connexe et vous avez à minima 5 ans d'expérience en tant que Data Ingénieur.
Vous possédez également une solide maîtrise des technologies et des outils suivants :
Hadoop, Spark, SQL, Kafka, GCP BigQuery,
De plus vous avez une bonne compréhension des architectures, des modèles et des concepts de base de donnés avec une expérience avancée dans la mise en œuvre de pipelines ETL et dans la gestion de bases de données.
Vos connaissances en matière de sécurité des données, de conformité aux réglementations ainsi que vos compétences en programmation scripting et en développement logiciel seront un plus.
Vos excellentes compétences en communication seront des qualités appréciées et
un niveau d'anglais (appliquée au domaine technique) est un plus.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer - Lille,Capgemini,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-lille-at-capgemini-3914228495?position=3&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=1bNEMjrDx4lwdAM9LaPuxQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Capgemini en quelques mots
Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.
Vos missions
Vous maitrisez au minimum un langage de programmation appliqué à l’analyse de données
(Java, Python, Scala et les environnements Spark et / ou Hadoop).
Vous êtes passionné par le Big Data et le Machine Learning et l’analyse de données
Vous concevez et mettez en œuvre des stratégies sécurisées d'acquisition et d'intégration de données
Vous configurez des référentiels de données à la pointe de la technologie dans des environnements distribués
Vous construisez des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée
Votre profil
Diplômé(e) de Bac+5 en informatique
4 ans d’expérience
(au sein d’une ESN ou chez un intégrateur) en conseil clientèle
Une solide culture technologique
Un bon niveau d’anglais
3 raisons de nous rejoindre
Qualité de vie au travail :
accord de télétravail en France et à l’international, accord sur l’égalité
professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu :
certifications et formations en libre accès, accompagnement sur mesure avec
votre carreer manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE :
plan actionnariat, activités à tarifs préférentiels, remboursement partiel
vacances, remboursement de votre abonnement sportif ou culturel
Nos engagements et priorités
Le groupe Capgemini encourage une
culture inclusive dans un cadre multiculturel et handi-accueillant.
En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des
initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
Capgemini
est un
leader mondial
, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de
55 ans d’expérience,
nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le
cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning', 'Cloud'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Engineer (F/H),Thales,"Vélizy-Villacoublay, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-thales-3908228180?position=4&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=p249HvSiTp4FSVBFlTAldQ%3D%3D&trk=public_jobs_jserp-result_search-card,"QUI SOMMES-NOUS ?
Thales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.
Thales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.
QUI ETES-VOUS ?
Diplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.
CE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :
En tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.
VOS MISSIONS :
• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Hadoop.
• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.
• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.
• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.
• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.
• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.
• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.
Innovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Solutions Engineer (Data & AI),LVMH,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=5&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=eaNM3PFCRsmq1V%2BoOCrajg%3D%3D&trk=public_jobs_jserp-result_search-card,"LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.
We are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.
If you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,
If you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,
If you like building tech solutions having direct impacts on billion-dollar-valued businesses,
If you have good communication skills and like sharing your knowledge,
Apply now, and join us!
The mission
The Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.
Our team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.
You will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).
Main responsibilities
You will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).
You will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.
You will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.
Applying the quality and security standards. Making them evolve if necessary.
Producing realistic, understandable and documented solutions following the group guidelines.
Sharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.
Taking responsibility as member of the team on the product performances (delivery and long term usage)
Required expertise and knowledge
Ability to build technical solutions answering concrete usage (User Stories) and communicate them to the team.
Dimension and evaluate complexity for technical solution productions.
Extensive knowledge and experience with good learning and sharing abilities.
Evaluate quickly risks and opportunities about technical choices.
Solid oral, written, presentation and interpersonal communication and relationship skills.
Problem-solving skills on Data and AI, coding and software development
Tech lover
Feedback taker and giver
Team player
Key benefits to join our team
Attractive packages
Offices in the 8th arrondissement near the Champs Elysées
Flexibility on the working hours
Remote work possible (~40%)
7 weeks of holidays (congés payés + RTT)
LVMH brands exclusive private sales
Great employee committee and health insurance (CE, mutuelle)
Last generation MacBooks
Part of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!
You’re eligible if
You have a strong experience (3+ years) in cloud data architecting or consultancy.
You graduated from an engineering (or equivalent) with a master’s degree. Computer Science knowledge is mandatory.
Experience on data stacks and/or Google Cloud (built in components) is a huge plus.
French and English both written and oral (Maisons are all over the world)
You’re thrilled to support the #1 luxury group to get even better.
Hiring Process
Call with our HR partner dedicated to the Tech Team
Technical interview with the Solution Engineering Manager
Technical test
Interview with the Head of Engineering
Still here? Apply now!!
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Flexibility']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['40'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer – Grenoble,Capgemini,"Grenoble, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-grenoble-at-capgemini-3905836212?position=6&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=RnAhVxcz3cUG9tZh23ItbQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une communauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.
Vos missions :
En tant que Data Engineer au sein d'une équipe multidisciplinaire, vos responsabilités principales seront les suivantes :
Intervenir sur les différentes phases d'un projet dans un environnement Cloud et Agile.
Contribuer à la gestion de la qualité des données et extraction et analyse de celle-ci, ainsi qu’à la présentation des données dans leur forme raffinée.
Proposer des nouvelles lectures de données via un travail de fouille sur les gisements d’information, notamment client.
Adopter une posture de consultant : proposer de nouvelles solutions et accompagner le client dans ses choix.
Votre profil :
Titulaire d'un Bac+5 en école d’ingénieur ou en université.
Connaissances approfondies des ETL (Talend, Informatica ou SSIS), du traitement de données (Spark, Python, Scala) ainsi que des bases de données (Oracle, SQL Server, Postgres).
Faculté pour se montrer curieux, autonome et proactif dans la réalisation de ses tâches.
Capacité à faire preuve de rigueur et à travailler en équipe.
Bon niveau d’anglais (B2 minimum).
3 raisons de nous rejoindre :
Qualité de vie au travail
: accord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu
: certifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE
: plan actionnariat, tarif préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités
:
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
À propos de Capgemini :
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle', 'SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '55', '55', '55']}"
Data Engineer H/F,Thales,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-thales-3903089036?position=7&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=w99zOK4aMb4C6eulo3aUbA%3D%3D&trk=public_jobs_jserp-result_search-card,"📢 Nous recherchons un(e) Data Engineer, basé(e) à Lyon
👉Quelques mots sur les activités numériques de Thales Lyon :
Les activités numériques représentent une entité rattachée au groupe Thales, spécialisée dans l’IT et présente au national.
L’agence de Lyon adresse divers sujets d’expertise : ingénierie logiciels, cybersécurité, infogérance des infrastructures et transformation digitale.
🎯
Votre rôle et missions
En nous rejoignant, vous intégrerez le centre de compétences
Augmented data
,
spécialisé dans la conception, le développement et l’évolution d’applications data centrées. Vous y boosterez votre carrière en travaillant sur des technologies telles que
Spark, Elasticsearch, Kube ...
le plus souvent dans un environnement
Agile
.
Dans le cadre des projets que nous opérons aujourd’hui :
- Vous contribuerez à la conception, au maintien, à la scalabilité des plateformes d’analyse de données au travers de votre expertise sur les sujets data (base de données, gestion de flux, ETL …)
- Vous contribuerez à la conception et à la mise en production des pipelines d’analyses et de transformations de données en veillant à leur bonne adaptation aux besoins métiers et aux contraintes techniques du client
- Vous pourrez intervenir sur des sujets de visualisations, dans le but de notamment accompagnées nos clients sur la conception de Dashboard métier intelligent …
- Vous serez également amenées à échanger directement avec des DevOps/Datascientist pour la mise en place, l’intégration des pipelines et l’élaboration des algorithmes de traitements de données.
- A l’échelle du département, Vous serez un acteur majeur du développement de notre activité et du lancement de nouveaux projets de valorisation de données.
🙋‍♀️ 🙋‍♂️
Votre profil
De formation Bac +5 en informatique (école d’ingénieur, Master ou équivalent), vous justifiez d’une première expérience réussie sur un projet data ? Vous souhaitez participer à la conception et intervenir sur des solutions de récupération et d’exploitation de données métiers dans des contextes critiques et hautement sécurisés ?
Autonome, dynamique, organisé(e) et proactif(ve), vous souhaitez évoluer au sein d’équipes passionnées par l’exploration et l’intégration des technologies nouvelles au service des métiers de nos clients ?
Vous avez des compétences qui couvrent les domaines suivants :
Mise en place et gestion de base de données (SQL, Elasticsearch, Clickhouse ...)
Langages de programmations (Java, Python)
Gestion de flux (Kafka, flink, logstash …)
Environnements big Data (Spark/hadoop )
Principes et outils de type ETL
Vous êtes de plus intéressé(e):
Par les environnements containerisés (docker, kubernetes, helm ...)
Les concepts DevOps (Ansible, CI/CD...)
Les sujets de Datavisualisation (Vega, Kibana, python librairies...)
Vous aimez travailler en équipe ? Vous êtes reconnu(e) pour vos qualités relationnelles et vos capacités de vulgarisation ?
Alors notre poste d’Ingénieur(e) Data(H/F) est fait pour vous !
🙌
Votre carrière chez Thales
Différentes opportunités vous permettront de découvrir d'autres domaines ou sites. Vous pourrez évoluer et développer vos compétences dans différents domaines.
Explorez un espace attentif au développement personnel.
Développez vos talents dans un autre domaine du groupe Thales, en découvrant de nouveaux produits, de nouveaux clients, un nouveau pays ou en vous orientant vers une solution plus complexe.
Choisissez entre une expertise technique ou un parcours de leadership.
Vous travaillerez dans une entreprise résolument humaine avec des valeurs fortes comme la sécurité au travail, l’égalité Homme/Femme et l’équilibre vie personnelle/professionnelle (Accord Télétravail).
Rattaché(e) à la Convention métallurgie, vous bénéficierez aussi de ses multiples avantages (…)
Vous souhaitez en savoir plus ?
N’hésitez pas à contacter notre équipe de recrutement ou nos équipes directement.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible', 'Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'CI/CD'], 'FrSoftSkills': ['Leadership'], 'EnSoftSkils': ['Leadership']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer - Nantes,Capgemini,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=8&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=qE%2Ffs1Bymeu3P2TnbkpBiA%3D%3D&trk=public_jobs_jserp-result_search-card,"Capgemini
Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.
Vos missions :
Intégré(e) au sein d'une équipe projets intervenant pour des clients dans des secteurs d'activités variées, vous serez notamment en charge des missions suivantes :
Concevoir et mettre en oeuvre des stratégies sécurisées d'acquisition et d'intégration de données,
Configurer des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),
Construire des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée.
Votre profil :
Diplôme d’ingénieur ou équivalent universitaire
Minimum 3 ans d'expérience
Anglais courant
Maîtrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.
Expertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks…
3 raisons de nous rejoindre :
Qualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité
professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel
vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités :
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
Capgemini
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer (H/F),Technology & Strategy,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-technology-strategy-3881556102?position=9&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=MKZjIJGEG0En6bgmZVJg5g%3D%3D&trk=public_jobs_jserp-result_search-card,"Découvrez Novencia
:
Expert en Data et Intelligence Artificielle, nous aidons nos clients à exploiter et à valoriser leurs données sous toutes ses formes en les accompagnant sur des projets de Data Analyse, Data Gourvernance, Data Architecture, Data Science, et Data Engineering…
Vous avez une solide expérience de minimum 2 ans dans l'ingénierie des données et vous êtes à la recherche de nouveaux défis ? Bouclez votre ceinture, la suite est pour vous !
Type de contrat : CDI
Lieu : Lyon
En qualité de Data Engineer (H/F), votre rôle sera :
Concevoir et proposer les solutions de développement répondant aux besoins fonctionnels et techniques des projets big data.
Tu participes à la conception de solutions permettant le traitement de volumes importants de pipelines données.
Réaliser ces solutions par l’écriture de code, en respectant les méthodes et procédures qualités définies au sein du département Technique.
Mise à disposition sécurisé et lisible de la data.
S’assurer de la conformité fonctionnelle et technique de ces réalisations en effectuant les tests automatisés nécessaire et la mise en place de monitoring (système et qualité).
Assurer la maintenance des applicatifs / plateforme data science
Assurer une veille technologique
Vous disposez des compétences suivantes :
Maitrise des plateformes Cloud (AWS, GCP ou Azure), de Scala et de SQL.
Un.e touche à tout : possédant des compétences en langage Python/Spark, de bonnes capacités de modélisation, une forte appétence pour le Big Data
Fin.e connaisseur.euse : Data Engineer convaincu, très peu de secrets pour les clusters et pour les calculs parallèles
Explorateur.trice : découvre de nouvelles technos grâce à une veille régulière
Débrouillard.e : relève de nouveaux défis
Notre objectif commun est de co-construire votre carrière en fonction de vos aspirations et de vos compétences.
Contactez-moi en message privé ou par mail à s.ziki@technologyandstrategy.com !
Let's make it possible #together
*Nos postes sont ouverts aux personnes bénéficiant d’une Reconnaissance de la Qualité de Travailleur Handicapé (RQTH). T&S Groupe encourage la diversité et l’égalité sur le lieu de travail. Tous les candidats qualifiés H/F/* sont pris en considération pour un emploi sur un même pied d'égalité.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer,digiRocks recrute ✅,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-digirocks-recrute-%E2%9C%85-3903481080?position=10&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=M%2Bh%2FZxqWFn8O59EgZBx1GA%3D%3D&trk=public_jobs_jserp-result_search-card,"😎 Envie d'accompagner des organisations dans leurs stratégies, Fan de data?
Rejoins un jeune cabinet de conseil en stratégie spécialisé en data. Le cabinet a été créé il y a 4 ans pas des anciens de grands cabinets de conseil en stratégie qui ne se reconnaissaient plus dans ce qu'est devenu le ""consulting"". Cependant ils n'ont pas perdu espoir de pouvoir apporter du conseil à haute valeur ajoutée dans une ambiance friendly, façon start-up, sans sacrifier l'excellence.
Jean-Patrick recrute un(e) Consultant Data Engineer à Paris en CDI
✅ MISSION :
Vous serez responsable de la mise en œuvre de bout en bout de la pile de données, de la collecte au reporting, avec un accent sur l'infrastructure et les processus techniques. Vous travaillerez avec des Consultants en Stratégie & Data et les soutiendrez dans la résolution des défis liés aux données de leurs clients. Vous contribuerez à la définition des stratégies de données, à la mise en œuvre des systèmes de données et vous soutiendrez l'exploitation des données dans des projets transformationnels. En général, vous serez responsable de comprendre intimement les problèmes, de concevoir une stratégie technique pour les adresser et de faciliter une exécution technique de haute qualité.
✅ RÉSULTATS ATTENDUS :
🚀 Résultat 1: Unificateur de Données : Architecturer, assembler, assimiler, nettoyer et conformer de grands ensembles de données complexes pour livrer des insights commerciaux et alimenter les expériences de produits de données.
🚀 Résultat 2: Agent de Sécurité des Données : Concevoir et construire une infrastructure de données fiable et évolutive avec les techniques de confidentialité et de sécurité de pointe pour protéger les données.
🚀 Résultat 3: DataOps : Posséder la pile de données de bout en bout, y compris la collecte d'événements, la gouvernance des données, les intégrations de données et la modélisation.
🚀 Résultat 4: Gardien des Données : Assurer la cohérence et la qualité de l'environnement technique et de la structure des données à travers des métriques, de la documentation, des processus, des tests de données et de la formation.
Requirements
✅ PROFIL RECHERCHÉ :
Diplômé d'une Grande Ecole de Commerce ou d'ingénieur, avec une première expérience réussie comme Data Engineer, idéalement dans un contexte similaire au Conseil,
Connaissance des services de Data Warehouses Cloud. Expérience avec Google BigQuery, Snowflake, AWS Redshift/Athena, Looker, Azure SQL DWH, ou Azure Databricks est très souhaitable.
Connaissance des architectures de données relationnelles et de grandes données, de l'entreposage de données, de l'intégration de données, de la modélisation de données, de l'optimisation de données et des techniques d'analyse de données.
Expérience dans la construction de pipelines de données de bout en bout en utilisant des plateformes de données sur site ou basées sur le cloud.
Expérience pratique dans la livraison de solutions comprenant des bases de données, SQL avancé et développement logiciel dans des langues telles que Python.
Intéressé et connaissant les technologies Big Data et les technologies de l'écosystème Apache telles que Beam, Spark, Kafka, Airflow, bases de données, intégration, gestion des données de référence, assurance qualité, manipulation de données et technologies de gouvernance des données.
Expérience avec les plateformes cloud publiques et l'infrastructure cloud qui est essentielle.
Exposé aux outils ETL/ELT et de gouvernance.
Intéressé par les technologies et principes IA et ML.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Big Data Engineer - Spark & Python - F/H,Orange Business,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/big-data-engineer-spark-python-f-h-at-orange-business-3916552415?position=11&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=dierIvCJx0KVOvKwp%2FTLFA%3D%3D&trk=public_jobs_jserp-result_search-card,"Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?
Nous l’avons fait ! Notre alchimie nous positionne comme un
acteur unique
intervenant sur toutes les étapes du
voyage de la donnée.
Depuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’
ESN d’Orange Business
alliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …
Description du poste
Orange Business, recherche pour son site de Lille, son futur Ingénieur Big Data pour rejoindre sa team Data.
Votre quotidien ?
En intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :
Au démarrage du projet :
Recueillir et analyser les besoins du client
Rédiger les spécifications fonctionnelles et techniques
Estimer les charges
Pendant la phase de réalisation :
Modéliser des datawarehouses et datamart
Développer les procédures d’alimentation (ETL)
Développement SPARK
en batch et streaming
Développer les visualisations de données (DataViz)
Réaliser la recette et les tests
Suivre et mettre en production
En fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !
Qualifications
Vous possédez 5 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels.
Vous avez de solides compétences
Spark
(job, scripting, déploiement) ainsi que sur
Python.
Avoir des connaissances Kafka sera un plus également.
Envie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?
Outre l’aspect technique, c’est une personnalité qui est recherchée !
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '30', '30', '30']}"
Data Engineer H/F,Chantelle,"Cachan, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-chantelle-3909775663?position=12&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=dYJ3VWvHoYOq3%2Bgog2FcIg%3D%3D&trk=public_jobs_jserp-result_search-card,"La Direction des Systèmes d'Information et du Digital du groupe Chantelle recherche son/sa futur.e Data Engineer H/F, pour le lancement du grand chantier de rénovation de l'architecture Data : la bascule de l'intégralité de son Data Warehouse vers Google Big Query.
Nous souhaitons recruter un Data Engineer H/F confirmé.e, chargé.e de contribuer à la définition de la feuille de route de la Chantelle Data Plaform. En tant que Data Engineer vous travaillerez au sein de l'équipe Data Intégration en charge de la Chantelle Data Platform.
Vos Missions :
- Mettre en œuvre une infrastructure autour de Google Cloud Platform permettant de collecter (airbyte, API, ...) , transformer (dataform, Bigquery ...), exposer (dataviz, API, applications, ...) et historiser les données générées par l'entreprise.
- Travailler en étroite proximité avec les responsables des différents domaines fonctionnels (Référentiels, Supply Chain, Manufacturing, B2B, Retail & e-commerce, Finance, ...), avec notre équipe de Data Analysts ainsi qu'avec l'équipe technique en charge des infrastructures transverses
- Être force de proposition sur tous les sujets d'architecture et de modélisation (choix de mise en place de pipeline temps réels ou au contraire de flux de données en mode batch, ou bien encore stockage sur Big Query / Big Table en fonction des cas d'usage).
- Définir les éléments structurants, en justifiant vos choix, et les mettre en œuvre.
- Rationaliser et moderniser notre architecture d'intégration inter-applicative; se projeter sur la création d'un modèle de données de type Datamesh.
- Faire la refonte de la BI de nombreux use cases tels que le pilotage de nos stocks, personnalisation de nos sites e-commerce en temps réel en fonction de nos profils client, etc…
Stack technique : Google Cloud Platform, BigQuery, DataForm, DataFlow, PubSub, Airbyte, Github ...
Bonne maîtrise des langages Python et SQL
Pourquoi travailler chez Chantelle ?
Une flexibilité dans votre lieu de travail, selon la politique de télétravail de l'entreprise.
11 jours de RTT/an ainsi qu'un 13ème mois.
Une culture d'entreprise familiale basée sur des valeurs de respect, de créativité, de durabilité et de transparence
Une aventure dans laquelle vous pourrez vous épanouir, apprendre et entreprendre, avec une grande variété de missions et beaucoup d'autonomie
Des équipes ressources humaines et des managers à votre écoute pour vous accompagner dans votre parcours professionnel
Des réductions sur nos produits et des ventes au personnel
Des avantages dans votre qualité de vie au travail : une conciergerie complète proposant un large panel de services, des activités en interne, un CSE.
Vous souhaitez rejoindre un Groupe familial, innovant, engagé et leader dans son secteur en France comme à l'international et vous souhaitez apporter votre expertise et authenticité pour guider votre équipe vers le succès : postulez et rejoignez le Groupe Chantelle !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery', 'Big Query'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Créativité', 'Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer - Modélisation SQL - F/H,Orange Business,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-mod%C3%A9lisation-sql-f-h-at-orange-business-3916551577?position=13&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=7BVihBcm8eV8DMPwgcInOw%3D%3D&trk=public_jobs_jserp-result_search-card,"Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?
Nous l’avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les étapes du voyage de la donnée.
Depuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’ESN d’Orange Business alliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …
Description du poste
Orange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.
Votre quotidien ?
En intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :
Au démarrage du projet :
Recueillir et analyser les besoins du client
Rédiger les spécifications fonctionnelles et techniques
Estimer les charges
Pendant la phase de réalisation :
Modéliser des datawarehouses et datamart (intégration de flux et consolidation des données)
Développer les procédures d’alimentation (ETL)
Développer en SQL / PLSQL / Shell
Garantir la qualité des données et leur disponibilité
Réaliser la recette et les tests
Suivre et mettre en production
En fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !
Qualifications
Vous possédez 5 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels et en modélisation.
Vous avez de s
olides compétences en développement SQL
(job, scripting, déploiement) ainsi que sur Python.
Envie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?
Outre l’aspect technique, c’est une personnalité qui est recherchée !
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '30', '30', '30']}"
Data Engineer – SQL & GCP - F/H,Orange Business,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-sql-gcp-f-h-at-orange-business-3916557264?position=14&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=jq%2FJhC7d%2Fp5ECg%2Fs%2FlXO0g%3D%3D&trk=public_jobs_jserp-result_search-card,"Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?
Nous l’avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les étapes du voyage de la donnée.
Depuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’ESN d’Orange Business alliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …
Description du poste
Orange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.
Votre quotidien ?
En intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :
Au démarrage du projet :
Recueillir et analyser les besoins du client
Rédiger les spécifications fonctionnelles et techniques
Estimre les charges
Pendant la phase de réalisation :
Modéliser des datawarehouses et datamart (intégration de flux et consolidation des données)
Développer les procédures d’alimentation (ETL)
Développer en SQL
/ PLSQL / Shell
Garantir la qualité des données et leur disponibilité
Concevoir et développer des solutions frontend BI à des fins analytics & dashboarding
Réaliser la recette et les tests
Suivre et mettre en production
En fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !
Qualifications
Vous possédez 3 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels et ingénierie ou analyse data.
Vous avez de
solides compétences en développement SQL
(job, scripting, déploiement), vous avez l’habitude de travailler dans un
environnement Google Cloud Plateform
ainsi qu’avec
Power BI
.
Envie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?
Outre l’aspect technique, c’est une personnalité qui est recherchée !
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '30', '30', '30']}"
Data Engineer,eXalt Value,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=15&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=i%2Fl3myI%2Bj%2FSf1j4kvKCgug%3D%3D&trk=public_jobs_jserp-result_search-card,"eXalt
est un cabinet de conseil IT
Pure player Data
& IA basé à Paris.
Notre offre s’articule autour de 4 piliers réunis au sein d’une même communauté pour un accompagnement à 360° alliant une expertise technique et méthodologique à une approche conseil métier:
Data Gouvernance & Project
Data Engineering & Big Data
Data Performance & Analytics
Data Science & IA
Filiale du groupe eXalt, créé en 2018,
regroupant plus de
950 collaborateurs en France
(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)
et à l’international
(Colombie, Etats-Unis, Espagne, Belgique),
eXalt Value
apporte une
expertise approfondie
dans le domaine de la Data & IA et conseille les entreprises dans le déploiement de leurs stratégies data-driven.
Bénéficiant du support du groupe eXalt
(1er dans la catégorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value
est en pleine croissance et regroupe aujourd’hui une communauté d’expertise de plus de 60 collaborateurs en région parisienne.
Nos consultants interviennent sur d
es projets d’envergure
dans divers secteurs d’activité,
Banque & Assurance, Médias, Transports, Retail, Tourisme, etc.
Nous recherchons un
Data Engineer Confirmé H/F (minimum 4 ans d'expérience dans la fonction)
pour rejoindre notre communauté sur le
pilier Data Engineering & Big Data.
Vos missions:
Concevoir et développer des pipelines et des flux de données.
Intégrer et transformer des données provenant de différentes sources.
Développer et mettre en œuvre des algorithmes de traitement de données avancés.
Collaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.
Assurer la qualité et la fiabilité des solutions développées.
Conseiller les équipes clients sur les solutions à mettre en place.
Les Prérequis :
Titulaire d'un Bac+5, Ecole d'Ingénieur
Maîtrise d'un ou plusieurs langages de programmation (
Python, Scala, Spark, etc
.).
Expérience approfondie des technologies
Big Data (Hadoop, Spark, Kafka, Talend, etc.)
Expérience avérée
en
environnement Cloud (AWS, GCP, ou Azure)
.
Solides compétences en conception et en optimisation de pipelines de données.
Expérience de travail en
méthode Agile
Capacité à travailler de manière autonome et en équipe.
Excellentes compétences en communication et en résolution de problèmes.
Maîtrise de l’anglais (oral & écrit dans un contexte international professionnel).
Votre environnement eXalté:
Un environnement de travail Collaboratif
favorisant les initiatives et projets transverses à la Practice Data & IA (Lab IA, Data Hub, etc.).
Un collectif de consultants passionnés,
s’intéressant aux tendances innovantes du secteur.
Une Practice de proximité,
privilégiant la montée en compétence de ses collaborateurs (formations, coachings, mentorats, etc.)
Un suivi individualisé et de proximité
par un.e Data Sales Manager référent du compte client, un.e Chargé.e RH et un.e Practice Manager
Une équipe ouverte et dynamique,
qui privilégie les moments de partage et de convivialité (séminaires, eXaltemps, meet-up, déjeuners d’équipe, etc.)
Notre processus de recrutement :
Un entretien RH avec Estelle,
à la suite duquel vous saurez tout (ou presque) d’eXalt Value,
Un entretien technique avec un Manager assorti d’un test technique,
lors duquel vous aurez l’occasion de démontrer vos talents mais aussi d’apprendre avant même de dire oui,
Un entretien final avec la Directrice Associée ou le Directeur Opérationnel,
pour finir de vous convaincre de nous rejoindre 😊
Nous avons hâte de recevoir vos CV, et de faire votre connaissance!
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Résolution de problèmes'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Engineer,EarthDaily Agro,"Balma, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-at-earthdaily-agro-3883708013?position=16&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=i4cRUHDtp5y1qiyFUBzepw%3D%3D&trk=public_jobs_jserp-result_search-card,"About Us
EarthDaily Agro provides space age data and analytics to the organizations and people who feed the planet!
With 35 years of industry experience, EarthDaily Agro provides customers with the data, analysis and knowledge they need to make more efficient and effective decisions. B2B services range from global risk management and monitoring of agricultural commodities to the marketing of inputs and precision agriculture consulting, using the latest research in agronomy, information technologies and remote sensing.
EarthDaily Agro also develops highly customized business solutions for agricultural lenders, insurers, input suppliers and food companies, with easy-to-use analytics, that help reduce the daily risks of agriculture.
EarthDaily Agro is headquartered in Minneapolis, MN, USA, with offices in France, Brazil, Australia and Switzerland and is a division of EarthDaily Analytics Corp.
EarthDaily Analytics Corp., a vertically-integrated data processing and analytics company, is launching a new constellation of earth observation satellites. The EarthDaily satellite constellation will significantly enhance geospatial analytics capabilities in agriculture, forestry, environment, financial services, and intelligence, among many other verticals.
Main Job Tasks And Responsibilities
As a EarthDailyAgro Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and infrastructure specialized for geospatial and remote sensing applications. You will work closely with data scientists, geospatial analysts, remote sensing experts, software engineers, and DevOps teams to ensure the successful deployment and scaling of data pipeline to feed geospatial data machine learning models. Your role will be crucial in optimizing the geospatial machine learning ecosystem and ensuring the seamless integration of AI-driven geospatial solutions into real-world applications.
Your Responsibilities Include
Cloud-based data pipeline Conceptualization, Development and Scaling: Build up pipeline to ingest large volumes of geospatial data, pre-process them and meet data scientists’ requirements, in terms of accessibility, speed, format, quality.
Automation and CI/CD: Industrialization of pipeline deployment, orchestration, workflows, and versioning.
Cost & Speed Optimization: Collaborate with infrastructure team to develop, optimize, and fine-tune pipeline.
Cloud and Containerization: Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization tools.
Infrastructure Management: Utilize containerization technologies and cloud-based services to set up and manage infrastructure, enabling seamless deployment and scalability.
Monitoring and Anomaly Detection: Implement monitoring systems to track pipeline performance and identify anomalies.
Version Control and Data Version Control: Proficient with version control systems like Git and DVC.
Security and Compliance: Ensure the security and privacy of geospatial data, adhering to relevant data protection regulations and industry best practices.
Collaboration and Communication: Collaborate with interdisciplinary teams to integrate data pipeline into existing applications or develop new geospatial products.
Issue Resolution and Troubleshooting: Identify and resolve promptly technical issues related to geospatial data processing, performance, or infrastructure.
Education, Knowledge And Abilities
Requirements
Education: Master's degree in Computer Science, specialisation in Geomatics and/or Remote sensing would be a plus.
Experience: 3+ years experiences with data pipeline processes and deployment is a must-have. Proven hands-on experience in setting up pipelines and data processes with opensource tools (e.g., MLFlow, Argo, Kubeflow) is desirable.
Programming Skills: Proficiency in Python and with data manipulation frameworks (e.g., dataframe, numpy, pandas, xarray, rasterio) and librairies (e.g., Dask).
Problem-Solving Skills: Autonomous, and strong analytical and problem-solving abilities to address complex geospatial data and analysis challenges.
Communication Skills: Excellent communication and interpersonal skills to collaborate effectively with cross-functional teams and stakeholders.
French mandatory (job based in France). Fluent in English (oral and written): meetings with internal are mostly in English.
Preferred Additional Skills
Experience with Earth Observation (EO) data analysis and processing.
Experience with geospatial data formats (e.g., GeoTIFF, Shapefile, NetCDF).
Spatial Analysis Techniques: Understanding of spatial analysis techniques and algorithms commonly used in geospatial data manipulation.
Remote Sensing Integration: Knowledge of remote sensing data sources (e.g., STAC catalog, satellite imagery, LiDAR, SAR) integration into data pipelines for accurate and up-to-date geospatial analysis.
CONDITIONS
Full time job based in Balma, near Toulouse, France.
Fixed + Bonuses
TR / ""Family"" insurance / CSE
Powered by JazzHR
WrfSXQ5YJg
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['Pandas', 'NumPy', 'R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'ML', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Organization', 'Interpersonal Skills']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (F/H),Aubay,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=17&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=5udQna6j91DvsfDszC3TCA%3D%3D&trk=public_jobs_jserp-result_search-card,"Passionné par la Data, tu souhaites rejoindre une communauté d’experts dans le domaine afin de développer tes compétences en Data Engineering. Aubay renforce ses équipes Data et recherche des Data Engineers pour intégrer des dispositifs de projets pointus et variés.
Ton quotidien en tant que Data Engineer chez Aubay, :
Définition de la stratégie de stockage et mise en œuvre des technologie appropriées (base de données SQL, NoSQL, stockage distribué,…)
Ingestion des données (structurées, semi-structurées ou non-structurées) selon différentes fréquences : batch, micro-batch ou temps réel
Conception et mise en œuvre de pipelines de données afin de fournir des données prêtes à l’emploi aux consommateurs : uniformisation, mise en qualité, enrichissement, calcul d’indicateurs,…
Conception et développement d’API pour exposer les données auprès d’applications tierces
Appui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning
Préparation et animation d’ateliers de travail avec des interlocuteurs variés : recueil/approfondissement des besoins métiers, avancement/restitution des travaux, transfert de compétences,…
Ton profil :
Tu dispose d’une formation niveau BAC+5 (Master 2 ou école d’ingénieur) spécialisée en informatique
Tu as déjà une première expérience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data
Les technologies telles que Hadoop, Spark ou Kafka sont tes technologies de prédilection
La programmation n’a plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python
Tu maîtrises les tenants et aboutissants de la philosophie DevOps et des outils orientés CI/CD
Tu es soucieux de la qualité et de la performance de tes développements et tu t'intéresse à l’innovation frugale
Tu es un expert technique dans ton domaine sans pour autant oublier l’importance d’une communication orale et écrite de qualité et adaptée à chacun de tes interlocuteurs
Tu travaille au quotidien en mode agile et tu en maitrise les fondements
Ce qui nous caractérise :
Des missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs variés (Banque, Assurance, Telecom, Industrie,…) qui permettent à nos collaborateurs de monter en compétences et de devenir des experts Data reconnus
De l’apprentissage en continu avec des formations et des certifications sur les technologies Data d’aujourd’hui et de demain
Des experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets
Des communautés de savoir-faire Data proposant de manière régulière aux collaborateurs d’Aubay du contenu et des évènements de partage (webinar, meetup/afterwork, BBL,…) sur les thématiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,…
Aubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.
Ta carrière chez Aubay :
Tu auras la possibilité de développer et certifier tes compétences sur les dernières technologies Data avec un focus fort sur les plateformes Data Cloud telles qu’Azure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks
Tu pourras rejoindre la BU d’excellence Data et évoluer au sein d’un environnement humain et professionnel de haut niveau. Tu profiteras d’un management sur-mesure pour t'accompagner dans ta trajectoire de carrière
Au sein de la BU d’excellence, de multiples perspectives s’offriront à toi :
Rôle de « Lead » : Vous pourrez gagner en responsabilité sur le plan technologique et devenir un référent auprès de nos clients et des collaborateurs de la communauté Data Engineering
Rôle de « Champion » : Vous représenterez Aubay auprès d’un ou plusieurs de nos partenaires éditeurs stratégiques et vous participerez activement à l’animation de la relation sur le plan technologique
Rôle de « Head » : Vous pourrez prendre la responsabilité du savoir-faire Data Engineering et de ses offres et en assurer le développement au sens large (développement business, recrutement, management de collaborateurs, définition de la stratégie et animation de la communauté au sein du groupe Aubay,…)
Besoin d’en savoir plus sur le processus de recrutement ?
Un échange macro au niveau RH avec Doriane
Un entretien technique avec Marius ou Peter, deux de nos référents techniques
Un échange managérial avec le Directeur de la BU Modern BI & Data
A savoir que l’ordre des étapes peut varier selon tes envies (ex : échange managérial avec l’échange technique)
Aubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure', 'Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer / Développeur Big Data # H/F,Air France,"Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-d%C3%A9veloppeur-big-data-%23-h-f-at-air-france-3900080172?position=18&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=U0cfa8nHfasz5xn9vr%2BUqw%3D%3D&trk=public_jobs_jserp-result_search-card,"Description du poste
Intitulé du poste
Data Engineer / Développeur Big Data # H/F
Métier
Systèmes d'informations - Développement
Catégorie socio-professionnelle
Cadre
Présentation du contexte
Vous avez peut-être déjà voyagé avec nous, mais que connaissez-vous de nos métiers et de la richesse des données qu’ils génèrent au quotidien ? Comment le traitement et l’exploitation de ces données peut contribuer à notre stratégie de Revenue Management, ou encore aux multiples opérations à réaliser pour permettre à un vol de partir à l’heure ?
Air France-KLM fait rêver 104 millions de passagers par an, en les emmenant vers plus de 250 destinations, grâce à une flotte de plus de 500 appareils. Le Groupe emploie 80 000 collaborateurs partout dans le monde :les opportunités sont vastes pour mettre à profit ses compétences, apprendre et se développer !
Le département de développement DATA, OR & AI d’Air France, au sein de la direction des Systèmes d’Information, intervient dans toute la chaîne de captation et de traitement des données du groupe pour délivrer à nos métiers des solutions applicatives clés en main.
Le département est également en charge de l’ensemble des outils techniques (ETL, DataLakes, DataWarehouses, Data visualisation) et du développement des talents et compétences de Data Engineering.
Notre mission ? Transformer la donnée brute en décision intelligente, pour mieux optimiser les métiers d’Air France – KLM !
Pour cela, nous avons chacun un rôle essentiel à jouer, pourquoi le vôtre ne serait pas celui de Data Engineer et de développeur Big Data ?
Description de la mission
Au sein de notre département, vous travaillerez main dans la main avec d’autres Data Engineers et développeurs Big Data ainsi qu’avec des spécialistes des métiers.
Intégré au sein d’une product team agile passionnée et dynamique :
Vous participez à l’analyse des besoins métiers du commercial, des opérations aériennes, de l’exploitation sol en aéroport, de la maintenance aéronautique ou encore du Cargo.
Vous contribuez à la définition, au développement, à l’industrialisation et à la maintenance d’applications Big Data ou en Business Intelligence
Vous présentez la restitution de vos travaux et accompagnez les utilisateurs d’un point de vue fonctionnel ou méthodologique
Vous serez en contact avec les directions métier du groupe Air France KLM.
Nous attachons beaucoup d'importance au développement des compétences de nos collaborateurs ainsi qu’à leur offrir des conditions de travail favorables à l’autonomie et aux missions à forte valeur ajoutée. L'ouverture, le respect, la bienveillance et le partage sont des valeurs humaines portées par l'entreprise.
Profil recherché
Vous êtes diplômé de niveau Master ou Ingénieur dans les domaines informatiques, vous avez acquis une expérience professionnelle dans le développement d’applications.
Vous disposez d’une expérience du développement indispensable en Backend / Java
Vous maîtrisez les bases de données relationnelles et le langage SQL
En Complément, Vous Avez Une Connaissance Ou Une Expérience Dans Tout Ou Partie Des Concepts Ou Outils Suivants
Environnement Big Data (Spark, Hadoop, Elasticsearch, Kafka, ...)
Base de données noSQL (MongoDB, HBase, REDIS) ou Data Warehouse Teradata
Outil de Datavisualisation (Spotfire, PowerBI, Qlik ou Kibana)
Solutions de Cloud (GCP) et hybride (GCP / AZURE)
(Ces compétences complémentaires ou manquantes pouvant aussi s'acquérir à travers un parcours de reskilling et de formations aux outils du data engineering dispensé en interne).
Vous avez participé à des projets organisés en Scrum ou Kanban, et avez peut-être même œuvré comme Scrum-Master, ce qui vous permettra de vous intégrer aisément au sein d’une Product Team. Votre esprit de synthèse, votre force de conviction et votre maîtrise de la communication facilitent les décisions avec l’ensemble des collaborateurs de l’équipe, éventuellement en langue anglaise, à l’écrit comme à l’oral.
Vous êtes autonome, rigoureux(se), responsable et curieux(se), vous aimez travailler en équipe. Vous possédez de bonnes capacités d'écoute, d'analyse, de synthèse et de communication.
Et bien sûr, vous êtes passionné(e), enthousiaste et ingénieux(se)
Ce que nous vous offrons
De la création de valeur pour l’ensemble des métiers d’Air France KLM
Des challenges et problématiques complexes à résoudre
L’opportunité de déployer des solutions Data industrielles à l’échelle !
Une grande part de responsabilité dans une structure hiérarchique horizontale
Un important degré de liberté pour apprendre et développer son expertise au sein de l’équipe
On vous attend le plus rapidement possible ! Et pour une durée indéterminée ;)
Type de contrat
CDI
Temps partiel possible
Non
Type d'horaires
Administratif
Profil candidat
Niveau d'études min. requis
Bac + 5 et plus
Langue
Anglais (4 - Confirmé / C1)
Localisation du poste
Localisation du poste
France, Provence-Alpes-Côte d'Azur, Alpes Maritimes (06)
Site
Valbonne
Show more
Show less","{'ProgLanguage': ['Java', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'HBase', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Hybride', 'Confirmé'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Junior Data engineer,WA.Technology,"Crouseilles, Nouvelle-Aquitaine, France",https://fr.linkedin.com/jobs/view/junior-data-engineer-at-wa-technology-3908458326?position=19&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=EbGJHiVHkqLvwlLGyNZEhQ%3D%3D&trk=public_jobs_jserp-result_search-card,"WA.Technology
is a B2B supplier of iGaming solutions with rapidly growing clients and partnerships in emerging markets. We offer a state-of-the-art iGaming platform, turnkey solutions, and standalone products that empower operators to enter or expand into emerging markets quickly and easily.
The WA.Platform is a fully scalable and customisable solution, featuring over 75 game providers, 6,400+ games, and support for multiple currencies, along with access to over 80 payment methods. WA. Technology enables operators to build their own casino, sportsbook, lottery, fantasy, or poker business precisely as they envision it.
About The Role,
We are seeking a highly skilled and motivated
Data Engineer
to design, implement, and maintain efficient and scalable data pipelines on the Google Cloud Platform (GCP). In this role, you will be responsible for managing data from MariaDB and Kafka sources, ensuring seamless integration into BigQuery, our primary data destination. Collaboration with cross-functional teams is crucial to understanding and meeting diverse data needs.
In this role, you will need to:
Design, implement, and maintain robust data pipelines on Google Cloud Platform.
Integrate MariaDB and Kafka as primary data sources for seamless data flow into BigQuery.
Collaborate across departments to address unique data requirements aligned with organizational goals.
Utilize Dataflow and Dataform for efficient data processing and transformation.
Ensure data integrity through rigorous validation and cleansing processes.
Optimize cloud-based infrastructure for speed and scalability.
Implement monitoring tools for proactive system performance tracking and issue resolution.
Provide ongoing support for data integrity and availability.
Maintain comprehensive documentation of data architecture, updating regularly.
Stay informed about the latest data technology trends.
Evaluate and recommend new technologies/methodologies to enhance processing and analysis capabilities.
What are the key experience and personal attribute requirements?
Bachelor's degree in Computer Science, Information Technology, or a related field.
2+ Hands-on experience relational database
Proven experience in developing data pipelines and ETL processes.
Strong SQL skills.
Knowledge of data modeling and database design.
Excellent collaboration and communication skills.
Strong problem-solving and troubleshooting abilities.
Ability to work independently and as part of a team.
Continuous learner, keeping up with emerging trends in data engineering.
What are some of the benefits of working at WA Technology?
100% remote opportunity
Flexible work environment
Attractive remuneration package
Opportunity to work with well-connected industry leaders.
A leadership approach that fosters innovation, creativity, and trust.
Opportunity to experience the buzz of highly driven and motivated work colleagues.
Experience a start-up feel in a fast-paced growth-driven environment.
Show more
Show less","{'ProgLanguage': ['Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Communication', 'Leadership', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Leadership', 'Creativity', 'Collaboration', 'Organization']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),iPepperGroup,"Valbonne, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ipeppergroup-3894091360?position=20&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=MXiVBVIb6BxwB0mgaE66rg%3D%3D&trk=public_jobs_jserp-result_search-card,"iPepper recrute pour l'un de ses clients une PME éditeur de logiciel dans le domaine du voyage, un
Data Engineer (H/F)
passionné(e) et expérimenté(e) pour rejoindre une équipe dynamique.
En tant qu'Ingénieur(e) Data, vous serez en charge d'extraire et de transformer des données, de construire et d'optimiser des pipelines de données, ainsi que de concevoir des visualisations de données intuitives et informatives.
Responsabilités :
Concevoir, construire et maintenir des pipelines de données évolutifs et efficaces pour transférer des données entre des bases de données SQL et NoSQL.
Développer et mettre en œuvre des processus ETL pour extraire, transformer et charger des données à partir de différentes sources dans notre entrepôt de données.
Collaborer avec des équipes pluridisciplinaires pour comprendre les besoins en données et garantir la fourniture réussie de solutions de données.
Optimiser et ajuster les pipelines de données existants pour la performance et la fiabilité.
Concevoir et développer des visualisations de données et des tableaux de bord pour fournir des insights exploitables aux parties prenantes.
Surveiller et résoudre les problèmes de pipelines de données, en veillant à la qualité et à l'intégrité des données.
Profil recherché :
Diplôme universitaire en informatique, en ingénierie ou dans un domaine connexe.
Expérience avérée en tant que Data Engineer ou dans un rôle similaire, avec un accent particulier sur la construction de pipelines de données et de processus ETL.
Compréhension solide des bases de données
SQL
et
NoSQL
, y compris la modélisation des données et la conception de schémas.
Maîtrise des langages de programmation tels que
Python, Java ou Scala.
Expérience avec des outils de visualisation de données tels que
Tableau, Power BI.
Solides compétences en analyse et en résolution de problèmes, avec la capacité de traduire des données complexes en insights exploitables.
Excellentes compétences en communication et en collaboration, avec la capacité de travailler efficacement dans un environnement d'équipe pluridisciplinaire.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Résolution de problèmes', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer - Profils expérimentés H/F,LCL,"Villejuif, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-profils-exp%C3%A9riment%C3%A9s-h-f-at-lcl-3888403052?position=21&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=z4Vl6fiAzW4xQbNi93g%2B9A%3D%3D&trk=public_jobs_jserp-result_search-card,"🏦 LCL, c’est LA banque urbaine du Groupe Crédit Agricole - avec nous, accompagnez la transformation, le développement et le maintien technologiques de nos outils avec une vision business et de satisfaction de nos 6 millions de clients.
En tant qu’acteur majeur de la banque de détail, nous nous adaptons chaque jour aux nouveaux modes de consommation et les projets de nos de clients internes et externes tout en garantissant le besoin de sécurité et de développement technologique qu’impliquent nos activités.
💡Organisées en mode Agile, les 8 squads de la tribu DATA (6 squads Métier et 2 squads transverses) œuvrent au quotidien pour répondre à un enjeu majeur pour la banque : la collecte, le stockage, la gestion et l’usage de la donnée. En interaction permanente avec les autres tribus IT et les métiers, elles étudient et proposent les solutions et architectures à déployer pour répondre au mieux aux stratégies de développement et de pilotage de l’ensemble des métiers de la banque.
Rejoignez-nous si vous souhaitez participer aux réflexions et au développement de la trajectoire technique et DataCentric du SI LCL et plus largement du Groupe CA. Vous côtoierez et serez au cœur de l’implémentation de technologies variées telles que les plateformes Teradata, les solutions d’architecture applicative des technologies BigData ou IA, des environnements analytiques ou encore des solutions de datavisualisation. Vous assurerez le traitement de données en temps réel ou en batch et exposerez les données sous différentes formes.
Que vous souhaitiez devenir expert sur les socles technologiques ou relever le challenge de la gestion de projets Métier, nous vous aiderons à atteindre vos propres objectifs.
Vous rejoindrez une équipe pluridisciplinaire, clairement orientée vers le développement de ses collaborateurs à de nouvelles technologies !
🎯 En tant que Data Engineer :
· Vous aimez analyser les besoins avec les métiers, challenger, identifier les sources de données dans les différents univers technologiques, industrialiser des algorithmes, concevoir et développer des Datalab ou des Datamart sur les plateformes ? Vous saurez relever les challenges proposés par les squads métier !
· Vous préférez travailler à l’architecture et au déploiement de nouvelles plateformes, à la levée de la dette technologique ou encore réaliser de la veille au service de notre trajectoire ? La squad Socles Data est faite pour vous !
· Au-delà des projets que vous gérerez, garant du bon fonctionnement de votre parc applicatif, vous attacherez une grande attention à la mise en œuvre de solutions optimisées.
· La rigueur, la communication, l’esprit d’équipe mais aussi la curiosité et la créativité font partie de vos soft skills ! ils vous permettront de répondre aux enjeux de sécurité, de qualité, de transmission de la connaissance et contribueront à l’atteinte des objectifs de l’IT et plus largement de LCL, au service de ses clients.
💻 Voici les principales technologies utilisées au sein de la tribu, si certaines vous sont familières, nous vous aiderons à monter en compétence sur d’autres !
Langages utilisés : SQL, Python, Scala
SGBD : Teradata et utilitaires (TPT, BTEQ, …)
Streaming : Kafka
Search : ElasticSearch, SolR
Environnement : Unix
Solutions Big Data : Hadoop Cloudera, DataIku, HDFS, Hive, Impala,
Devops : GitLab, Jenkins, Nexus
Outils de visualisation : MS BI (SSIS, SSAS, SSRS) Qlik Sens, BO
Modélisation : MEGA
Outils collaboratifs : GIT, Jira, Confluence, Teams
⚡Si les nouveaux enjeux bancaires vous intéressent, que vous souhaitez intégrer une équipe Agile au service des métiers dans laquelle vous serez force de proposition et que vous aimez travailler dans un environnement motivant et dynamique, rejoignez-nous, cette offre est faite pour vous !
🔥 Les + de notre entreprise :
Accès au Plan d’épargne Groupe, intéressement et participation aux bénéfices de l’entreprise + abondement
Prix préférentiels bancaires et avantages CSE
Parcours évolutif dans l’entreprise et/ou dans le Groupe CA.S.A
Télétravail (jusqu'à 2 jours de télétravail par semaine)
De multiples commodités sur le campus (restaurants d'entreprise, salle de sport, crèche, centre médical, médiathèque...)
Forfait et avantages pratiques « mobilité durable » pour les velotafeurs
Des équipes aussi diversifiées que structurées dans une dynamique de transformation
LCL s’engage en faveur de la diversité et nous encourageons tout(e) candidat(e) ayant l’expérience requise à postuler à nos offres. Tous nos postes sont ouverts aux personnes en situation de handicap.
Nous avons encore de nombreuses raisons à vous présenter pour vous convaincre de nous rejoindre mais pour cela, il faudra postuler ici !
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Confluence', 'Teams'], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Créativité'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,Ramify,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-at-ramify-3896146641?position=22&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=JnSrc62bk8WQOsZhoPlmmg%3D%3D&trk=public_jobs_jserp-result_search-card,"ABOUT
Ramify’s mission is to help people achieve financial freedom, no matter who they are and how much they have. We are revolutionizing the private wealth management industry by making smart and affordable financial products for everyone.
No more standardized solutions, hidden fees and complicated words, Ramify enables everyone to become a smart investor. The team combines elements of its research with technology to design customized investment portfolios composed of world-class financial products.
The team comprises around 15 talented individuals. Ramify is looking for talented people in all sectors, who want to have a huge impact, move fast and deliver.
JOB DESCRIPTION
The Quantitative Investment Solutions (QIS) Team is dedicated to designing innovative investment portfolio models and developing cutting-edge investment features within our product suite. Moreover, the QIS Team is at the forefront of driving AI-based solutions for Ramify. This involves conceptualizing and implementing transformative AI solutions tailored to meet the diverse needs of various teams within Ramify.
As a Data Engineer on our QIS Team, you will play a pivotal role in shaping the future of investment strategies through data-driven insights and AI-based solutions. Collaborating closely with our talented team of quantitative researchers, and investment experts, your responsibilities will encompass architecting and implementing robust data pipelines. These pipelines will facilitate the seamless integration of diverse data sources, empowering Ramify teams to make informed decision-making.
Key Responsibilities:
Design, build and launch data pipelines at scale to move data across Ramify platform with SQL technologies.
Design and implement processes and tools for data onboarding and quality, helping to deliver an industry best-practice solution for managing the data lifecycle.
Produce stand-alone tools that can be used by other teams to automate data quality and discover faults.
Build analytical tools that provide insight into business metrics across Ramify.
Architect and lead the implementation of AI based solutions within Ramify
PREFERRED EXPERIENCE
We're looking for people who:
Want to make a difference. We are a small team effectively reshaping how people look at the industry. We need people who 'get it' and want to play an integral part in helping us accomplish this mission and are persistent in getting the job done.
Skills we're looking for:
Master’s or upper-year undergraduate-level coursework in either Computer Science, Management Information Systems, Business Information Systems, Mathematics or Finance related field.
2+ years of professional experience in data engineering.
2+ years of experience with one or more coding languages such as Python (is a must), Java.
Experience with data modeling and ETL design, implementation and maintenance.
Demonstrable mastery of industry best practices in the data lifecycle, including data quality automation and tooling.
Excellent written and verbal communication skills with ability to communicate complex designs and solutions to non-technical and highly technical audiences alike.
Good attention to detail.
Strong analytics and strategic thinking skills
Nice-to-haves :
Understanding of ML/ Generative AI technologies and their applications.
Possess a passion, curiosity, and energy for finance + investing. You understand the ins and outs of the wealth management, trading, and more importantly - know how to explain these concepts simply
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,Thales,"Ollioules, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-at-thales-3902424527?position=23&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=9D9Rc8Bs0%2FSs66SNI5xCGA%3D%3D&trk=public_jobs_jserp-result_search-card,"QUI SOMMES-NOUS ?
Thales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.
Thales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.
QUI ETES-VOUS ?
Diplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez a
u moins 3 ans d'expérience
dans les technologies Big Data.
Passionné par le
secteur de la Défense et du Naval.
CE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :
En tant que
Data Engineer,
vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.
VOS MISSIONS :
• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies
Hadoop, Spark, Scala
.
• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.
• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.
• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.
• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.
• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.
Innovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.
Show more
Show less","{'ProgLanguage': ['Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer H/F,Inetum,"St.-Ouen, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-inetum-3843966639?position=24&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=Q%2Bk6aPwFl6DvxvKp76rWTA%3D%3D&trk=public_jobs_jserp-result_search-card,"Détail de l'offre
Informations générales
Entité de rattachement
Inetum est un leader européen des services numériques. Pour les entreprises, les acteurs publics et la société dans son ensemble, les 28 000 consultants et spécialistes du groupe visent chaque jour l'impact digital des solutions qui contribuent à la performance, à l'innovation et au bien commun.
Présent dans 19 pays au plus près des territoires, et avec ses grands partenaires éditeurs de logiciels, Inetum répond aux enjeux de la transformation digitale avec proximité et flexibilité.
Porté par son ambition de croissance et d'industrialisation, Inetum a généré en 2023 un chiffre d'affaires de 2,5 milliards d'€.
Pour répondre à un marché en croissance continue depuis plus de 30ans, Inetum a fait le choix délibéré de se recentrer sur 4 métiers afin de gagner en puissance et proposer des solutions sur mesure, adaptées aux besoins spécifiques de ses clients le conseil (Inetum Consulting), la gestion des infrastructures et applications à façon (Inetum Technologies), l'implémentation de progiciels (Inetum Solutions) et sa propre activité d'éditeur de logiciels (Inetum Software). Inetum a conclu des partenariats stratégiques avec 4 grands éditeurs mondiaux - Salesforce, ServiceNow, Microsoft et SAP et poursuit une stratégie d'acquisitions dédiée afin d'entrer dans le top 5 européen sur ces technologies et proposer la meilleure expertise à ses clients.
Tous nos postes sont ouverts aux personnes en situation de handicap.
Description du poste
Métier
Applications Delivery - Software Development
Intitulé du poste
Data Engineer H/F
Contrat
CDI
Description De La Mission
Le pôle BFA de la branche Application Services du groupe INETUM, recherche plusieurs Data Engineers afin d'intervenir auprès de clients grands comptes au sein des marchés bancaires et de l'assurance.
Au sein de l'équipe Data, en tant que Data Engineer, vous participez à la réalisation de divers projets et vos missions sont
Apporter votre connaissance en Big Data permettant la manipulation des données
Concevoir les plateformes permettant de traiter des volumes de données importants
Mettre en place des bases de données
Préparer le pipeline de données pour que les données déployées soient sécurisées et claires afin d'être analysées et transformées.
Profil
De formation ingénieure en informatique Bac + 5 informatique ou scientifique
Bonne communication orale et écrite en français et niveau d’anglais professionnel
Savoir- être Bon esprit d'analyse et de synthèse, sens de l'organisation et de la qualité, force de proposition, rigueur, travail en équipe, adaptabilité.
Si vous vous reconnaissez, n'hésitez pas à postuler !
Localisation du poste
Localisation du poste
France
Ville
Saint-Ouen
Critères candidat
Niveau d'études min. requis
Bac+5
Niveau d'expérience min. requis
Plus de 2 ans
Compétences
SQL
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Communication', 'Adaptabilité', 'Organisation', 'Flexibilité'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '30', '30', '30']}"
Data Engineer (F/H),Renault Digital,"Boulogne-Billancourt, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-renault-digital-3911202728?position=25&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=xQaI%2FKhtHEWrp1eIaFQ%2Bxg%3D%3D&trk=public_jobs_jserp-result_search-card,"Contexte :
Dans le cadre de son programme Industrie 4.0, Renault développe depuis 2017 sa propre plateforme pour connecter et agréger les données industrielles des 22 sites du Groupe et de plus de 2500 machines.
Fort de partenariats stratégiques signés avec Google Cloud (stack data full GCP), Renault Digital est à la recherche d’un(e) Data Engineer au sein du Pôle Architecture et Data pour mettre en place des chaînes de traitement de données répondant à de nouveaux besoins métiers.
Vous collaborerez au jour le jour avec les équipes métiers ainsi qu’avec les autres fonctions du Pôle Architecture & Data (Data Analysts et Scientists, architectes, …), exploitant des téraoctets de données (événements en mode streaming, traitements en batch et temps réels et les appels aux APIs) afin entre autres d’alimenter des modèles de machine learning (segmentation clients, détection automatiquement des pannes des véhicules, …).
Responsabilités principales :
Vous participez aux phases de framing, MVP et release des produits, services et APIs orientés data ;
Vous argumentez les choix d’architecture des projets et de la plateforme datalake sur GCP ;
Vous contribuez à la valeur métier des produits orientés Data s’appuyant sur le Datalake, en mettant en place des chaînes bout en bout de traitement de la data, de l’ingestion à l’exposition d’APIs et à la visualisation des données et des solutions ML/DS ;
Vous êtes garant de la qualité des données transformées dans le Datalake, du bon fonctionnement des chaînes de traitement et de l’optimisation de l’utilisation des ressources des ressources cloud ;
Vous proposez des standards d’architecture et de développement ;
Vous êtes force de proposition, innovant(e) et bienveillant(e).
Environement technique :
Spark, Scala, Python, Java, Airflow, SQL, Google Cloud Platform (BigQuery, Cloud Storage, PubSub, Beam, Dataflow, Cloud ML, TensorFlow, Kubernetes), Git, Docker, JSON, Bash, Spotfire
Profil recherché :
Vous avez minimum 5 ans d’expérience en tant que Data Engineer ;
Vous disposez d’une expérience en développement Spark, Scala, Python et requêtage SQL sur des gros volumes de données ;
Vous avez une appétence pour la data : validation, transformation, analyse, valorisation ;
Vous possédez une expérience de développement et orchestration de chaines ETL complexes via Airflow ou équivalent ;
Vous pratiquez la méthodologie agile (Agile Scrum et/ou Kanban) ;
Vous utilisez les services cloud (préférablement GCP) ;
Vous êtes capable d’échanger en anglais technique écrit et oral.
Informations complémentaires :
Votre poste sera basé à Boulogne-Billancourt (France) en CDI (temps plein)
Vous bénéficiez de 2 à 3 jours de télétravail par semaine
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go', 'Bash'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': ['TensorFlow'], 'DataSerialization': ['Json'], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['ML', 'Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Temps plein', 'Full'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer BI - Nantes,Capgemini,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-bi-nantes-at-capgemini-3803963477?position=26&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=%2Fm%2B%2Bu0YrOQ7XM7%2FjP%2Fx0gg%3D%3D&trk=public_jobs_jserp-result_search-card,"Capgemini
Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.
Vos missions :
Intégré(e) au sein d’une équipe projet BI, Big Data ou Data Gouvernance pour des clients intervenant dans des secteurs d'activités divers, vous serez notamment en charge des missions suivantes :
Mener les analyses fonctionnelles destinées à traduire les besoins du client,
Mener les travaux de conception et de modélisation,
Diriger le développement de la solution / des traitements d'alimentation du DataWareHouse,
Organiser et préparer les travaux de recette utilisateurs,
Mettre en place les processus d'industrialisation et mener cette dernière.
Votre profil :
Diplôme d’ingénieur ou équivalent universitaire
Minimum 3 ans d'expérience
Anglais courant
Compétences en BI sur SAP BI (Hana, BW, BODS, BI 4), Microsoft BI (SQL Server, SSIS, SSAS, SSRS), Oracle (ODI, OBIEE), Teradata, Informatica (Powercenter), IBM (Datastage, Cognos, TM1), Talend, AB Initio
Maîtrise d'un ou de plusieurs outils de Dataviz : Microsoft Power BI, Tableau, Qlikview
Connaissances en Big Data (Ecosystème Hadoop (HIVE, PIG, Mahout…), Cloudera, Pivotal, Spark, HNX) ou en analytics (R, SAS, IBM SPSS)
3 raisons de nous rejoindre :
Qualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité
professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel
vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités :
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
Capgemini
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les
plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle', 'SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer (H/F),Beelix,"Antibes, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-beelix-3838611420?position=27&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=ueaRofuONErN9s66mmUeUA%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes-nous ?
Depuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants :
🚗Automobile
⚡Energie
📡Médias & Télécoms
👗Luxe & Retail
💶 Banque, Finance & Assurance
✈️Défense
Aujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.
Dans le cadre de notre développement, nous recherchons un Data Engineer (H/F) pour l'un de nos clients.
Quelles missions au quotidien ?
Être le leader de la brique Datalakehouse
Développer les scripts de transformations de données et les pipelines d’alimentation
Proposer des évolutions architecturales ou de fonctionnalités pour améliorer le socle technique
Être le back-up du leader technique sur la partie reporting (Power BI)
Orientation satisfaction client et résultat final forte mais également sensibilité au « comment »
Innovation et proposition de nouvelles pratiques pour améliorer l’environnement et les conditions de travail des équipes
A propos de vous ?
5 + années d'expérience en tant que Data Engineer
Maîtrise des technologies suivantes : Microsoft Azure, Microsoft Azure Synapse Analytics (Spark / Python / Pipeline / Serverless), fichiers parquet / delta, Microsoft Power BI, Microsoft SQL Server, langage SQL, Datawarehousing / Modélisation de données
Analyses et export de données
Connaissance de l’ensemble du processus depuis la collecte jusqu’à la mise à disposition des données en ayant comme point fort la maitrise de sa transformation et mise en forme
Vous avez un bon niveau d’anglais
Localisation : Biot et/ou Carros
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,Digital Waffle,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-digital-waffle-3913824888?position=28&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=IEl%2FhktPBzJjGw4t%2Fz%2BTrw%3D%3D&trk=public_jobs_jserp-result_search-card,"Digital Waffle is proud to have partnered with an innovative tech startup in Paris, who are looking for a talented Data Engineer to join their growing team!
They are made up of a powerhouse of experts, combining
data engineers, business process gurus, and Project managers
who leverage the most advanced solutions available; utilising process mining, automation tools, and smart execution systems.
Looking for an experienced Data Engineer (3-5 years)
What You'll Do:
This is a full-time,
hybrid role (Paris-based)
where you'll wear many hats: data exploration, system integration, data prep, data modeling, and implementing data solutions.
Experience:
Expertise in data engineering, data modeling, and ETL (Extract, Transform, Load) processes
Data warehousing and data analytics skills
Experience handling large, complex datasets
Proficiency in SQL and programming languages like Python or Java
Stellar problem-solving and analytical skills
Top-notch communication and collaboration abilities
Bachelor's or Master's in Computer Science, Information Systems, or a similar field (a plus for process mining or intelligent process automation experience)
If you are an experienced and driven Data Engineer, please apply here!
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),Scalian,"Valbonne, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-scalian-3819563847?position=29&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=zbaJOj29uyfFAKD4DOxKZA%3D%3D&trk=public_jobs_jserp-result_search-card,"Ingénieur DATA / Data engineer (H/F)
Valbonne/Sophia-Antipolis
Type : CDI
Lieu : Locaux Scalian Sophia-Antipolis
Télétravail : En fonction des possibilités
Date de prise de poste : immédiatement ou en fonction de votre préavis
Salaire : en fonction du profil - entre 40 et 48K Brut annuels (hors avantages Scalian)
Avantages Scalian : Accord d'entreprise télétravail, Tickets restaurants, Mutuelle groupe, accord aménagement temps de travail, compte épargne temps, accord de participation et intéressement groupe, programme cooptation et apports d'affaires, accompagnement parentalité, avantages CSE
Vous êtes data engineer ou vous souhaitez le devenir !
Quel sera votre rôle ?
La portée de la mission comprend (sans toutefois s'y limiter) :
Science des données
Ingénierie des données
Analyse des données
Génie logiciel
Ce que cette expérience va vous apporter
Vous êtes autonome, vous avez le sens du service et de l’analyse, vous êtes impliqué, nous vous offrons une ouverture sur des projets complexes et une rapide évolution de carrière. Vous rejoignez notre business unit à Sophia Antipolis composée d'environ 50 consultants, avec possibilité de télétravail en fonction des sujets.
Nous co-construisons votre trajectoire professionnelle et assurons votre montée en compétences.
Nous nous inscrivons ensemble dans la durée, nous assurons votre montée en compétences et disposons d'une variété de sujets passionnants.
Ce que nous recherchons chez vous
De formation supérieure (Bac+5, école ou université), vous possédez idéalement une première expérience réussie dans ce domaine (débutants acceptés), vous aimez le travail en équipe.
Compétences requises
:
Etape d’analyse : Comprendre l’architecture technique, les sources de données, les objectifs fonctionnels.
Etape de conception : Solution de conception avec un fort centrage sur les pipelines de données et les modèles ML et l’exposition des KPI via API
Mise en œuvre : Après les phases d’analyse et de conception, procéder à a mise en œuvre dans des technologies sélectionnées (Java,Scala,Python,Spark)
Créer un code testé et documenté
Techno : Linux, Shell, Hadoop, Scrum, Python, Spark, Scala
Pourquoi feriez-vous le grand saut ?
Parce que Scalian vous accompagne dans le développement de votre carrière :
Programme d'onboarding complet sur 1 an avec votre manager et votre RH
Programme de formation (Scalian Academy, e-learning, webinaires et formations externes)
Communautés techniques (Squads, Practices) afin de valoriser et développer votre expertise
Événements internes (Afterworks, Awards Dinner, Kick Off, Live Event du COMEX, Stand Up) et externes (participation à des salons et forums spécialisés dans nos domaines d’activités…)
Dispositif d’accélération d’accès à la mobilité interne et à des échanges internationaux type Erasmus
Parce que Scalian favorise la Qualité de Vie au Travail :
Certifications Great Place to Work® et Best Workplaces for Women®
Prime de cooptation, prime vacances, prise en charge par l’employeur de 60% des titres-restaurant, Accord télétravail (jusqu’à 2,5 jours par semaine indemnisés), RTT (dont une partie monétisable), CSE (activités ludiques, chèques-cadeaux, chèques vacances)
Berceaux en crèches inter-entreprises
Don ou réception de jours de congés en cas de difficultés personnelles
Parce que Scalian développe une politique RSE concrète et ambitieuse :
Mobilité durable (indemnité kilométrique vélo, leasing de vélos à assistance électrique)
Actions environnementales (Fresque du Climat, Reforest'Action, Clean Up Day, mécénat ONF)
Postes ouverts aux personnes en situation de Handicap
Diverses politiques de diversité, d’inclusion et d’intégration mises en place
Scalian c’est aussi :
Une entreprise en très forte croissance qui, créée en 1989, compte aujourd’hui plus de 5500 personnes
Des références clients à forte valeur ajoutée auprès de grands industriels français (du CAC40) et internationaux
Un terrain de jeu où l’expertise se conjugue avec audace, liberté d’entreprendre et convivialité
Si vous aspirez à un environnement de travail qui valorise autant votre bien-être que votre développement professionnel,
rejoignez-nous et exprimez pleinement votre talent !
Envie d'élargir le cadre ?
Je suis Liza Djehel, Talent Acquisition Officer.
Si votre CV est retenu, je vous contacte pour un premier échange téléphonique de 15 à 20 minutes.
Nous déterminons ensemble si ce poste est en adéquation avec vos compétences et surtout, avec vos attentes.
L'échange est positif ? Nous convenons d'un entretien de 1h (en présentiel ou en visio) avec Lucas Daunar, Business Manager à Sophia-Antipolis. Cet échange permet de revenir en détail sur vos compétences, vos attentes, de vous présenter le poste plus en détail, et d'évoquer d'autres opportunités.
Nous prévoyons ensuite un rendez-vous technique de 1h (en présentiel ou en visio) avec un de nos responsable technique.
Enfin, nous vous présentons notre proposition d'embauche.
Notre processus de recrutement dure entre 15 et 30 jours
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['40'], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
Data Engineer H/F,Ippon Technologies,Greater Nantes Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=30&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=AsgLBKPObp783Z2tkccsfQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Envie de rejoindre la communauté DATA la plus dynamique de France ?
Notre spécialité est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.
Membre de la Practice Data, le/la futur(e) Data Engineer sera intégré(e) à nos équipes de conseil et sera suivi(e) par un(e) mentor qui l’aidera à monter en compétences.
Votre champs d’expertise :
Intervenir sur les data platforms de nos clients pour développer de nouveaux pipelines de données (ingestion, traitement, exposition).
Travailler en collaboration avec les métiers et les data scientists pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship etc…)
Déployer des infrastructures cloud full
infra-as-code
(Terraform, CloudFormation).
Participer aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).
Capitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne.
Vos connaissances :
Un framework de calcul distribué tel que Spark, Storm, Flink.
Un ou plusieurs langages de programmation (Python, Scala, Java...)
Différents systèmes de stockage de données (SQL ou NoSQL) et bien sûr le langage SQL.
La connaissance de Snowflake est bienvenue ;-)
Un framework de streaming de données tel que Kafka ou Amazon Kinesis.
Une expérience sur les technologies Cloud : AWS, GCP, Azure
Le delivery et les projets en production faisant partie de notre ADN, vous êtes capable de livrer du code de qualité dans des environnements agiles.
De plus en plus de nos projets se font en remote avec des clients du monde entier, il devient nécessaire d’être à l’aise en Anglais.
Ippon technologies c’est aussi :
👍 Bénéficier d'un suivi de proximité réalisé par votre manager technique : points réguliers pour votre suivi en mission, votre formation et votre évolution de carrière
✌️ Rejoindre une entreprise où les valeurs du sport sont nos leitmotiv : dépassement de soi, travail en équipe, bienveillance.
🗒️ Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL
😁 Travailler en pair programming ou avec un.e mentor pour gravir les échelons !
💪 Pouvoir participer à une aventure humaine au sein de notre Fondation Ippon pour réduire la fracture numérique dans le monde !
🤝 Participer à nos apéros et divers évènements internes pour consolider la cohésion d’équipe
Et après ?
Et oui alors ? Que se passe-t-il une fois que vous êtes convaincu d’avoir lu l’offre d’emploi qui vous correspond bien ?
Nous vous proposons de prendre contact et de nous rencontrer !
Les Next Steps :
1 call RH
1 échange RH
1 échange Technique
Si le match est bon des deux côtés : Hadjimé ! Vous vous lancerez sur le tatami Ippon !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform', 'CloudFormation'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),ternair,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ternair-3915757963?position=31&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=DDZVC8nqVobqpHnmOgDD6g%3D%3D&trk=public_jobs_jserp-result_search-card,"👨‍🚀 MISSION : 👩‍🚀
En cohérence avec la stratégie d’entreprise et la roadmap data, vous aurez pour principales missions de :
En lien avec l’équipe DevOps, construire, maintenir et faire évoluer la plateforme de données;
Définir et piloter la cohérence de la collecte, la gestion et l’alimentation des données internes et externes, en différents modes : batch, streaming, API (architecture micro-services);
Préparer et mettre en qualité les données pour les rendre disponibles dans les différents environnements de travail (datalake, datawarehouse, datamart);
Vérifier la qualité des données, de leur bonne et régulière exécution ainsi que de leur utilisation adéquate (gestion des coûts);
Travailler en étroite collaboration avec les data analysts, scientists et data stewards et business de l’entreprise ;
En lien avec l’IT et la sécurité, veiller aux règles d'intégrité et de sécurité des données;
Veille technologique.
🧮 Les outils :
Plateforme data : Google Cloud Platform (Big Query, Airflow)
Développement : Github/GitLab, Docker, Terraform, Python
Analytiques : Qlik
Gestion de projet s: Jira, Confluence, Miro, Drive, Docs, Sheets, Slides
🤩 Profil recherché : 🤩
Expérience d'au moins 4-5 ans (après études) en data ingénierie (flux, modélisation, run)
A l’aise avec l’environnement Cloud et les infrastructures digitales
Communiquant, pédagogue et fortes capacités relationnelles
Anglais (à l’écrit)
Rémunération : 42-60 k€ en package selon expérience
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['Big Query'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': ['JIRA', 'Confluence'], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer,Shippeo,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-at-shippeo-3908268236?position=32&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=SDVf9YZXmr8b1pbHooAYoA%3D%3D&trk=public_jobs_jserp-result_search-card,"Founded in 2014, Shippeo is a French based SaaS company providing supply chain visibility. Shippeo has grown from 70 to 220 employees during the last two years and is continuing to rapidly scale after an additional $40M fundraising round in October 2022.
Shippeo is an exceptionally diverse company with colleagues from 27 different nationalities and speaking 29 languages. With offices throughout Europe, North America and recently Asia, Shippeo provides global coverage to all of our clients.
Our product is composed of a mission critical SaaS web platform (API everywhere), with high traffic inbound/outbound integrations.
Our mission is to anticipate problems and proactively alert end-customers so they can efficiently manage exceptions. We achieve this by collecting and matching millions of theoretical and real data from different stakeholders.
The Data Intelligence Tribe is responsible for leveraging Shippeo’s data from our large shipper and carrier base, to build data products that help our users (shippers and carriers alike) and ML models to provide predictive insights. This tribe’s typical responsibilities are to:
get accurately alerted in advance of any potential delays on their multimodal flows or anomalies so that they can proactively anticipate any resulting disruptions
extract the data they need, get direct access to it or analyze it directly on the platform to gain actionable insights that can help them increase their operational performance and the quality and compliance of their tracking
provide best-in-class data quality by implementing advanced cleansing & enhancement rules
As a Data Engineer at Shippeo, your objective is to ensure that data is available and exploitable by our Data Scientists and Analysts on our different data platforms. You will contribute to the construction and maintenance of Shippeo’s modern data stack that’s composed of different technology blocks:
Data Acquisition (Kafka, KafkaConnect, RabbitMQ),
Batch data transformation (Airflow, DBT),
Cloud Data Warehousing (Snowflake, BigQuery),
Stream/event data processing (Python, docker, Kubernetes) and all the underlying infrastructure that support these use cases.
Qualifications
Required:
You have a degree (MSc or equivalent) in Computer Science.
3+ years of experience as a Data Engineer.
Experience building, maintaining, testing and optimizing data pipelines and architectures
Programming skills in Python and experience with asynchronous event processing (asyncio).
Advanced working knowledge of SQL, experience working with relational databases and familiarity with a variety of databases.
Working knowledge of message queuing and stream processing.
Knowledge of Docker and Kubernetes.
Knowledge of a cloud platform (preferably GCP).
Experience working with workflow management systems such as Airflow.
Desired:
Experience with cloud based data warehouse solutions (BigQuery, Snowflake).
Experience with Kafka and KafkaConnect (Debezium).
Experience with Infrastructure as code (Terraform/Terragrunt).
Experience building and evolving CI/CD pipelines with Github Actions.
Monitoring and alerting on Grafana / Prometheus.
Experience working on Apache Nifi.
Informations supplémentaires
We are looking for talents who share our values:
🚀 Ambition
💙 Care
🎯 Deliver
🤝 Collaboration
Find out more about our values in
Our Culture Book
If you identify with our values and enjoy working in a fast-paced and international environment, Shippeo is just the place for you!
We are committed to fostering diversity and inclusion within our workplace as we value the unique perspectives and experiences that individuals from all backgrounds bring to our team. We are dedicated to providing equal employment opportunities to all candidates, regardless of their background or abilities, and our commitment to inclusion is reflected in our policies, practices, and workplace culture.
We understand that candidates may have unique needs or questions related to disability inclusion. To facilitate this, you can reach our dedicated Disability Advisor at
inclusion@shippeo.com
with any inquiries or requests for accommodations during the application process.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['ML', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data engineer - F / H,United Robotics Group,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=33&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=BOP09wArK5oohNUn%2FG9McA%3D%3D&trk=public_jobs_jserp-result_search-card,"Bienvenue chez
Aldebaran
, leader européen de la robotique
au sein du groupe
United Robotics Group
.
Nous concevons et industrialisons des robots innovants avec une vision sociétale ambitieuse pour façonner un monde plus humain. Depuis 2005, nous sommes à l'avant-garde de l'interaction homme-robot avec des produits emblématiques tels que NAO et Pepper.
Notre dernier-né,
Plato
,
incarne notre engagement envers la technologie de pointe et la sécurité,
fabriqué en France avec des composants européens.
Rejoignez nos équipes multiculturelles et dynamiques pour être au cœur de la révolution de la robotique.
Si vous êtes passionné.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer à façonner l'avenir, nous vous offrons une expérience enrichissante et stimulante.
En tant que membre de notre équipe, vous bénéficierez d'une culture d'entreprise axée sur le sens de ce que nous faisons et valorisant la responsabilité sociale et environnementale.
Chez Aldebaran, nous valorisons l'innovation, la diversité et l'égalité et encourageons chacun.e à être ouvert.e, authentique, courageux.se, responsable et engagé.e.
Finalité du poste
Au sein de l'équipe Cloud-Online Services, le Data engineer intégrera l'équipe Data, responsable du développement des produits destinés à la collecte, aux process et à l'exploitation des données de nos robots.
Il aura pour rôle de définir et d'implémenter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui gèrent les robots du groupe.
Missions principales
Le Data engineer aura pour responsabilités de :
évaluer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,
concevoir et développer des services Data en respectant la spécification fonctionnelle et la méthodologie agile,
agréger et stocker de grandes quantités de données,
mettre en place des solutions de data processing,
intégrer/développer des outils de visualisation de données et analyser les KPI,
développer, tester, sélectionner et mettre en production des algorithmes qui permettent de répondre aux besoins,
réaliser des analyses de données,
mettre en place des tests de charge et fonctionnels pour les solutions Data,
investiguer et corriger les bugs remontés par les utilisateurs,
contribuer à la mise en place de l'infrastructure et outil de déploiement (CI/CD)
Rejoignez-nous pour faire partie d'une aventure passionnante où Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !
Requirements
Pour la bonne exécution des missions confiées, vous témoignez d'au moins 6 ans d'expérience en tant que développeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.
Compétences demandées :
Bonne compréhension des technologies d'infrastructure et de déploiement,
Compétences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,
Bonne compréhension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,
Bonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)
Bonne connaissance et une expérience pratique de Scrum\Scrumban et des méthodes agiles,
Une certification AWS sera appréciée,
Un niveau de français et d'anglais courant est indispensable,
Des expériences dans des environnements fortement internationaux sont un plus
Benefits
Nos principaux avantages :
Une culture du bien-être en entreprise qui a fait ses preuves (budget célébration et moments de convivialité par équipes et directions, restauration collective de qualité, environnement de travail agréable)
Un engagement fort en matière de responsabilité sociale et environnementale (promotion de l'égalité professionnelle, performance de notre plan diversité et inclusion, référent handicap, fresque du numérique)
Une culture du télétravail encadrée de manière appropriée !
Tous nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
Data Engineer H/F,Neosoft,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-neosoft-3877878521?position=34&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=5ibIRVZnYxMQyNtqkit%2BIw%3D%3D&trk=public_jobs_jserp-result_search-card,"Tous nos postes sont ouverts au télétravail
Groupe indépendant de conseil en transformation digitale de près de 1800 collaborateurs, Néosoft s’est construit, depuis 2005, sur un modèle qui place l’excellence, le dépassement de soi et la RSE au cœur de sa stratégie.
En nous rejoignant, vous intégrez des communautés d’experts et de talents qui vous permettent de développer vos compétences et d’offrir à nos clients le meilleur accompagnement possible.
Notre savoir-faire s’articule autour de nos 6 domaines d’expertise :
Conseil & Agilité
Cybersécurité
Data
DevOps
Infrastructures & Cloud
Software Engineering
Nous recherchons pour intégrer notre
agence lilloise
un(e)
Data Engineer confirmé(e)
.
Nous aimerions vous voir rayonner au sein de notre communauté DATA (+100 collaborateurs) animée par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients à consolider un patrimoine Data responsable.
🎯
Vos missions :
Après une période d’intégration, en tant que
Data Engineer
, voici à quoi ressembleront vos activités dans des contextes clients Retail ou Banque / Assurance / Finance :
Analyser et s'approprier les cas d'usages
Analyser et valoriser les données du patrimoine
Mettre en place des flux de transformation de données
Réaliser les tests permettant de s'assurer la qualité du delivery
Continuer la mise au point de frameworks data
Créer et développer des modules de déploiement des solutions
Assurer l'industrialisation de moteurs basés sur l'IA
Assurer le niveau de performance des pipelines
Implémenter les outils de monitoring du socles de données
📝
Votre profil :
Nous vous imaginons avec au moins 4 ans d’expériences sur des projets autour de la
Data
, une maîtrise des
bases de données (SQL)
, des outils de transformation de la donnée
(Talend, BigQuery, Airflow)
, et un socle de compétences solides autours des langages
Python, Spark, Scala, Hadoop, Java.
👉
Votre carrière chez Néosoft
Depuis sa création, Néosoft place ses collaborateurs au cœur de sa stratégie. Notre culture pourrait se résumer en un mot : le collectif.
Nos communautés d’experts vous donnent la possibilité d’apprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.
Nous veillons à ce que chacun bénéficie d’un accompagnement de proximité et d’un suivi de carrière personnalisé auprès de votre manager dédié :
1 bilan d’activité trimestriel pour suivre le développement de vos compétences
1 entretien d’évaluation qui a lieu chaque année pour évaluer votre performance et déterminer vos nouveaux objectifs
1 entretien annuel auprès de votre RH dans le but de cartographier vos nouvelles compétences pour échanger sur vos projets professionnels et souhaits de formations
👉
Vos avantages
Formations et développement de l’expertise :
Vous disposez de temps alloué et rémunéré en contribuant au développement de votre expertise technique et de celle du groupe (Participations à des Tech days, animation d’une conférence à l’interne ou à l’externe, rédaction d’articles, rencontres avec nos candidats en processus de recrutement…)
Un abonnement illimité LinkedIn Learning offert
Bien-être au travail :
Un accord de télétravail flexible jusqu’à 100% de télétravail et personnalisable
Un partenariat avec Gymlib qui favorise le sport en entreprise
Des initiatives locales (afterworks, défis sportifs, team buildings, …)
Et bien plus encore :
Parce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive rémunérée dès l’arrivée du collaborateur
En plus de votre salaire : participation, compte épargne temps, actionnariat...
👉
Votre parcours candidat
Notre processus de recrutement se compose de deux étapes clés :
Un entretien de recrutement RH avec un Talent Acquisition Spécialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carrière possibles au sein de notre groupe
Un entretien d’évaluation technique pour réaliser un diagnostic de vos compétences techniques et identifier les compétences sur lesquels poursuivre votre évolution
Vous aurez également la possibilité de rencontrer pour compléter votre processus un acteur de notre pôle Business ou un pair de votre métier pour échanger sur son expérience collaborateur.
Nous avons hâte de vous rencontrer !
A bientôt,
L’équipe Néosoft 🖐
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': ['Salaire'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data & Cloud Engineer (H/F),fifty-five,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-cloud-engineer-h-f-at-fifty-five-3910028674?position=35&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=goi29nNPD0xfl2sj7aYFrg%3D%3D&trk=public_jobs_jserp-result_search-card,"Data & Cloud Engineer
fifty-five est une data-company d'un genre nouveau qui aide les marques à exploiter les données pour améliorer le marketing, les médias et l'expérience client grâce à une combinaison de services de conseil et de technologie spécialisés.
En tant que pilier data et marketing du Brandtech Group, nous offrons des services qui combinent le conseil en stratégie, les services de cloud, le conseil en média et l'expérience client.
fifty-five, c'est plus de 400 experts du numérique. Des digital consultants, des spécialistes du tracking et du média, des ingénieurs et des data scientists, travaillent tous en étroite collaboration pour fournir des conseils marketing de haut niveau et une assistance technique aux marques, dans tout type d'industrie, partout dans le monde.
Partenaire des annonceurs de la collecte à l'activation et l'exploitation des données, nous aidons les organisations à devenir de véritables entités omnicanales maîtrisant l'efficacité de leur écosystème digital et ses synergies avec le monde physique.
Basé à Paris, nous opérons sur 3 fuseaux horaires depuis nos 10 bureaux, situés à Paris, Londres, Genève, Milan, Shanghai, Hong Kong, Shenzhen, Taipei, Singapour et New York. fifty-five attache une importance particulière au bien-être de ses collaborateurs, ce qui lui a permis de figurer dans le classement Best Workplaces France en 2018.
Contexte :
L'équipe d'ingénierie développe et met en œuvre les solutions techniques permettant la réalisation de pipelines de données et l'implémentation de data platform pour nos clients : récupération de datas sur de multiples sources de données (APIs, files, etc.), data cleaning, data processing, automation et monitoring de l'ensemble. L'équipe s'appuie sur des technologies récentes (docker, kubernetes, terraform, notebooks, etc.) et met en place ses projets dans les différents clouds du marché (GCP, Azure, AWS...).
Mission :
Nous sommes à la recherche d'une personne capable de réaliser des projets techniques pour répondre aux besoins de nos clients (par exemple: système de recommandations de produits, détection d'anomalies, ranking). Les activités vont du chiffrage et du sizing technique à la mise en œuvre des architectures, en passant par la revue des spécifications fonctionnelles et la production de code. Le Data & Cloud Engineer sera épaulé par un Lead dans ses missions. Il sera également amené à participer à la R&D et à accompagner les équipes transverses dans la mise en place d'outils de travail internes (librairies pour les data scientists, environnement Notebooks pour les data analysts et data scientists, développement de frameworks sur différents cloud providers, etc.).
Nous souhaitons trouver la bonne personne pour faire évoluer ou créer de nouvelles solutions dans ce cadre. Les missions comprennent aussi bien du prototypage rapide pour des démonstrateurs, que de la production de code robuste qui tourne en production tous les jours.
Compétences et expériences :
2 ans d'expérience en tant que Data Engineer
Maîtrise de Python, SQL
Maîtrise des environnements Cloud. Idéalement certifié GCP, Azure ou AWS
Bonne connaissance de Docker/Kubernetes
Bonne connaissance d'au moins un data warehouse (BigQuery, Snowflake, etc)
Connaissance autour des Notebooks (Jupyter)
A l'aise avec des concepts liés aux APIs (OAuth, REST, etc.)
A l'aise avec les notions d'Infrastructure as Code (Terraform)
Au courant des pratiques GitOps et connaissances des concepts autour du CI/CD
La maîtrise d'un orchestrateur, comme Apache Airflow, est un plus
Esprit d'équipe (collaborer aux tests unitaires, revue de code, partage de code, sprints)
Bon niveau en français et en anglais
A déjà travaillé en mode projet avec des interlocuteurs variés (consultant, data analyst, data scientist)
Une expérience en marketing digital est un plus
Nous proposons :
un bureau au centre de Paris avec terrasse et jardin
un environnement multiculturel avec des collaborateurs aux nationalités multiples (France, Royaume-Uni, Etats-Unis, Chine, Tunisie, Italie et plus)
des projets avec nos bureaux à Londres, Hong Kong, New York, Shanghai, Genève, Shenzhen et Taipei
des TGIF et supers soirées
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer,AFD Technologies,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-afd-technologies-3899625641?position=36&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=0XV4X5qxaks0ljefO73%2FRA%3D%3D&trk=public_jobs_jserp-result_search-card,"AFD.TECH part of Accenture
est le spécialiste du conseil en transformation digitale des grandes entreprises 🚀.
A ce jour, le Groupe est composé de 2.000 talents répartis dans 3 pays (France, Belgique & Maroc) 🌎 pour un chiffre d’affaires annuel de 125M€ !
Nos Talents d’abord 😎:
Les Talents d’AFD.TECH part of Accenture sont au cœur de la stratégie du Groupe et il est primordial pour nous que chaque collaborateur trouve du sens dans son travail.
Au-delà de proposer une carrière ambitieuse et personnalisée à nos Talents, nous avons à cœur de leur offrir un environnement de travail flexible (remote), inclusif et épanouissant dans tous nos bureaux (Paris, Bruxelles, Rabat, Lyon, Strasbourg, Lille, Nantes, Toulouse, Marseille, Bordeaux et Rennes)🌍.
Avec 20% de croissance par an et plus de 20 ans d’existence, AFD.TECH part of Accenture est devenu l’acteur incontournable du marché des infrastructures informatiques, réseaux et télécoms.
Notre proposition de valeur ? Intervenir sur l’ensemble du cycle de vie de projets complexes, pour des clients grands comptes, venant de secteurs tels que la banque, le ferroviaire, les médias, télécoms, etc (comme la Société Générale, Bouygues Telecom, Orange, Thales et bien d’autres encore !)👩🏻‍💻.
Nous rejoindre est une formidable aventure humaine : nous vous proposons un poste un poste de
Data Engineer en CDI
, au sein de notre agence Lilloise.
Vos missions ✅:
En tant que Data Engineer pour l'un de nos clients grands comptes, votre rôle s’articulera autour de différents axes :
Appréhender le contexte et les enjeux métier du client.
Collaborer avec les équipes métier pour comprendre les exigences en matière de données.
Définir des architectures data.
Concevoir et mettre en place des pipelines de données.
Construire des flux de données complexes.
Vous travaillerez dans une mission à forte valeur ajoutée et de longue durée (minimum 1 an et demi).
Votre profil✅:
Vous maîtrisez le langage SQL, les ETL et les ELT.
Vous aimez automatiser, mettre en place vos data pipelines et maîtriser les technologies: CI/CD, Terraform, Github, Python, Kafka.
Vous possédez des compétences en data visualisation : Business Objects, Qlikview, Qlik Sense, PowerBI ou Data Studio.
Vous connaissez Google Cloud Platform (GCS, BigQuery).
Vous êtes diplômé(e) d’une formation BAC + 5.
Vous avez une première expérience significative dans la data engineering (
minimum 3 ans
).
Vous projetez votre carrière dans un cabinet de conseil exigent et successful, qui vous permettra de développer votre esprit entrepreneurial et de répondre à vos ambitions.
Ce que nous offrons chez AFD.TECH part of Accenture 🤗:
Une politique de flexibilité dans votre organisation et un bon équilibre de vie 🏃‍♂️.
Des avantages plus que compétitifs 💰.
Un accompagnement et un suivi régulier durant tout votre parcours chez AFD.TECH (Launchpad, Linkers, rookies, etc…).
Un état d’esprit familial et de la proximité entre tous 👨‍👩‍👧‍👦.
Des moments de convivialité toute l’année 🍾 (évent en équipe, séminaire annuel, sports collectifs etc.).
Un parcours d’évolution sur mesure 🔼.
A très bientôt chez AFD.TECH part of Accenture!
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': ['Organisation', 'Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': ['Remote'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '20', '20', '20']}"
Data engineer H/F,Extia,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-extia-3599188121?position=37&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=RpAHVod1lg1ca3LwgraXfw%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous souhaitez rejoindre une entreprise qui place l’humain au cœur de ses préoccupations ? On vous attend chez
Extia
!
Société de conseil spécialisée dans les métiers de l’IT, de l’ingénierie et du digital, Extia privilégie depuis sa création en 2007 une approche qui allie performance et bien-être au travail. Une vision de l’entreprise partagée aujourd’hui par plus de 2 500 Extiens en France et à l'international et récompensée par le label Great Place to Work® depuis 13 ans, notamment en
2024 où les Extiens se hissent à la première place du palmarès Best Workplaces France
!
Chez Extia, c’est « D’abord qui, ensuite quoi » alors, allons-y !
D'abord qui
Vous êtes habitué à travailler aussi bien avec des méta-données qu’avec des données non-structurées. A cet effet vous maitrisez un ou plusieurs des concepts comme l’ETL, le Data mining le Machine learning, les Big data ou encore la Théorie des graphes par exemple,
Vous maitrisez les bases de l’analyse statistique,
Vous êtes apte à rédiger des scripts en Python et/ou R, et une connaissance d'autres langages de programmation comme Java, Scala ou SAS est un plus,
Vous maitrisez Spark et Hadoop
Vous êtes familiarisé avec l’environnement Linux,
Une expérience avec les outils de Stockage de fichiers volumineux (HDFS, Data Lake, S3, stockage Blob), la connaissance des infrastructures cloud AWS ou GCP et des bases en streaming temps réel seront aussi de réels atouts.
Ensuite quoi
Vous aurez le rôle de support technique aux équipes d’analyse : structurer les données, réaliser des analyses « statistiques » ou « techniques » sur les données, développer des outils d’analyse…
Vous mènerez des études afin d’évaluer les nouvelles technologies dans le domaine du Big Data, Data Mining ou Machine Learning afin d’identifier les solutions les plus pertinentes.
Vous serez en charge de :
Participer à la définition des besoins et à la rédaction des User Stories,
Collaborer avec les Data Scientists au développement des modules d’analyse de donnée,
Concevoir et construire des architectures de données,
Intégrer des sources de données,
Vous assurez que les données sont facilement accessibles et que leur exploitation fonctionne comme demandé, même dans des circonstances hautement évolutives,
Exécuter des processus ETL (extraire / transformer / charger) à partir d'ensembles de données complexes et / ou volumineux
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning', 'Statistiques', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '13', '13', '13']}"
Ingénieur Data Spark (F/H),Thales,"Vélizy-Villacoublay, Île-de-France, France",https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-spark-f-h-at-thales-3890949531?position=38&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=LyzjWq%2FfgvJpebjvBp057g%3D%3D&trk=public_jobs_jserp-result_search-card,"QUI SOMMES-NOUS ?
Thales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.
Thales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.
QUI ETES-VOUS ?
Diplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.
CE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :
En tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.
VOS MISSIONS :
• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Spark.
• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.
• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.
• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.
• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.
• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.
• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.
Innovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Alternance - Data Engineer H/F,Hermès,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/alternance-data-engineer-h-f-at-herm%C3%A8s-3889716412?position=39&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=5VSk5nClTgZjL1xgLiAxew%3D%3D&trk=public_jobs_jserp-result_search-card,"Eléments de contexte
Hermès Digital Ventes et Services recherche pour sa direction Data & Performance :
Un Alternant Data Engineer (H/F)
Contrat d'alternance de 12 mois
A partir de Septembre 2024
Basé à Paris
Principales activités
Vous êtes rattaché au Data manager.
Vous avez pour principale mission d’accompagner l’équipe Data dans les tâches quotidiennes :
Reporting et statistiques de ventes et trafic (notamment via l’outil Google Analytics et Google BigQuery)
Analyse des leviers d’acquisition de traffic SEA/SEO/Referral
Création de Dashboard via l’outil Google Data Studio
Participation aux travaux de CRO (Conversion Rate Optimization) et d’AB testing
Mise en place d’étude prédictive sur les données des sites Ecommerce
Profil
Etudiant en école d’ingénieur possédant une forte culture Internet et une sensibilité aux problématiques digitales e-commerce, vous avez une première expérience en entreprise
Profil technique ou aisé avec la technique, une spécialisation en digital est en plus
Organisé, rigoureux, curieux, autonome, bonne expression écrite et aisance relationnelle
Maîtrise du Pack Office indispensable, ayant déjà utilisé Google Analytics
La connaissance d’outils de BI / Datavisualisation serait appréciée (Google Data Studio, Tableau Software, Bime ou QlikView/QlikSense, PowerBI), de Base de Données (SQL, MySQL, BigQuery)
Une appétence pour la Data, ses languages (Python, R) et ses technologies (Notebooks, modélisation statistique, Machine learning) est fortement appréciée.
Anglais courant souhaité
Sensible aux produits hauts de gamme, vous souhaitez vous investir dans un stage riche et formateur
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI'], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['MySQL', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Statistiques'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,Coders Connect,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-coders-connect-3870419202?position=40&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=va%2BZHBT8a5zkOnTMbPoLOQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Coders Connect and Sanofi are joining forces to bring an electrifying twist to the biopharmaceutical world!
Work with a rhythm that suits your style (2 days remote and 3 days onsite magic).
Language
: Proficiency in English is required for this role to ensure effective communication within our diverse, global team.
About Sanofi:
We're not just a company; we're a global movement, focused on human health and making a real difference. Our mission? To battle pain, ease suffering, and sprinkle a little bit of magic in the process by developing life-changing medicines and vaccines through breakthrough science and wizard-level technology.
Digital & Data: The Pulse of Our Mission
At the heart of our quest lies our digital and data powerhouse. Think of us as the digital healthcare platform of your dreams, where innovation meets speed, and technology shakes hands with medicine. With our scale, deep-rooted connections in health ecosystems worldwide, and a knack for pushing boundaries, we're here to revolutionise medicine, one digital solution at a time.
The Role: Data Engineering Virtuoso
As our Data Engineering Virtuoso, you're tasked with designing and orchestrating the data pipelines that power our ambitious data analytics initiatives. You'll ensure our data's integrity and accessibility, laying the groundwork for groundbreaking insights and innovations.
Requirements
Cloud Platforms: Proficient in AWS services, with Azure & GCP knowledge a plus. Your work involves leveraging cloud solutions for scalable data processing and storage.
Data Modeling & Query Performance: Expertise in crafting data models and optimizing SQL queries to enhance performance. Experience with Snowflake or similar data warehousing solutions is highly valued.
Integration Services: Skilled in utilizing Integration Services like IICS and Tibco, you facilitate seamless data flow and integration across various platforms.
Scripting & Development: Proficient in scripting languages such as Python and R, enabling you to automate tasks and manipulate data efficiently. Familiarity with GitHub for source code management underscores your commitment to collaborative development and version control.
Visualization & Reporting: Knowledgeable in creating insightful data visualizations using tools like PowerBI, Tableau, or similar, turning complex data into actionable insights.
Data Governance & Compliance: A keen understanding of data quality, security, and governance standards, especially in healthcare environments subject to regulations like GxP, SOX, and data privacy laws.
Real World Data & Standard Models: Experience with Real World Data (e.g., EHR, Claims) and familiarity with standard data models (e.g., OMOP, FHIR) enhance your ability to contribute to our healthcare objectives.
Pipeline Frameworks: Proficiency in using frameworks like Apache Airflow or Kedro for crafting efficient, reliable data pipelines that support our AI and ML initiatives.
The Reward:
A chance to play a crucial role in a collaboration that's redefining healthcare through digital transformation.
A seat at the round table of diversity and inclusion, where different backgrounds and experiences conjure the richness of our culture.
An endless horizon of professional growth, learning opportunities, and the chance to contribute to a future where better health is a global reality.
The Call to Adventure:
If you're ready to join a quest for better – better treatments, better outcomes, and better science – and believe in the magic of bringing diverse talents together to make miracles happen, we want you. Let's embark on this journey together and transform the future of healthcare.
Better is out there. Are you ready to find it with us?
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Initiative']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),Epsilon France,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-epsilon-france-3912808369?position=41&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=%2F%2F3ZhY4YKHf7xPyoxJs8AA%3D%3D&trk=public_jobs_jserp-result_search-card,"Cette offre d’emploi est fournie par Pôle emploi
Description
Dans le cadre du développement de notre pôle Data & IA (cadrage fonctionnel et technique, définition de use-cases, stratégie des moyens, accompagnement du changement, mise en œuvre, maintenance et commercialisation de solutions), nous recherchons un(e) Data Engineer qui aura pour missions : - Délivrer des projets Data Lake / Big Data (ingestion de sources, pipeline de traitements de données, modélisation, tests, déploiements) dans un contexte de plus en plus DevOps, - Comprendre les besoins des équipes digitales, principalement associées aux projets Data Science et leurs technologies / outils (Jupyter, Zeppelin, R, Python, .), - Être capable de faire le lien avec les contraintes techniques (IT, sécurité, accès, outils) d'une DSI, - Assurer la veille technologique sur les composants d'une plateforme Datalake, Cloud - Maintenir les environnements techniques et partager ses connaissances (capitalisation, séminaires, formations, KM en ligne), - Rédiger des documents projets (design, réalisation, déploiement, .), - Gérer l'évolution des solutions proposées, et possiblement en assurer la TMA. Qualifications Inscrit en Master 2 informatique ou dans un domaine technique connexe au titre des années universitaire 2022-2025, Admis dans le cursus d'un CFA (université ou école d'ingénieur), Capacité d'apprendre, de comprendre et de travailler avec des nouvelles technologies, méthodologies et des solutions émergentes dans l'environnement technologique d'ingénierie cloud/données, Excellentes compétences en communication, organisation, avec une attitude proactive et positive, Passion pour les nouvelles technologies et engagement à acquérir de nouvelles compétences. Informations supplémentaires CHOISISSEZ. - Notre expertise reconnue dans le domaine du décisionnel et du Big Data depuis 30 ans, un cadre méthodologique et une organisation des compétences animées constamment dans un souci de veille et de progression, - Nos projets innovants et nos missions de conseil en cours de réalisation ou réalisés autour des solutions BI, Big Data et DMP, - Notre diversité de projets et de clients (SNCF, Groupe BPCE, Fnac, La Banque Postale.), - Notre management de proximité et notre souci de développement des compétences. Localisation : Paris 11e (Bastille) Contrat : POEI avec l'ecole Simplon et le programme Skills Les + EPSILON France : - Accès Restaurant d'Entreprise (Campus Bastille) - Travail Hybride grâce à notre Accord Télétravail qui autorise jusqu'à 2 jours par semaine - Engagé avec le Forfait Mobilité Durable - Dispositif d'Epargne Salariale (Accord d'intéressement et de participation)
PROFIL SOUHAITÉ
Expérience
Débutant accepté
Savoir-être professionnels
Faire preuve de rigueur et de précision
Faire preuve de réactivité
Être à l'écoute, faire preuve d'empathie
Langue
Anglais
Source: Pole emploi (https://www.pole-emploi.fr)
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Empathie', 'Organisation'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Hybride'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '30', '30', '30']}"
Data Engineer,Airswift,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-airswift-3909165766?position=42&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=6otrrgAHMn%2B2bPRXEycTZg%3D%3D&trk=public_jobs_jserp-result_search-card,"Data Engineer
Location
: Paris (Hybrid)
Contract type
: 12 months +
Years of Experience
: 4+
Recruitment Partner:
Airswift
Key Words:
Project Management | Jira | Digiboard | Banking | Stakeholder Management | Architecture | Cloud | Payments/Credit |ServiceNow | PPM |
Responsibilities
Design, develop, and implement data pipelines to collect, process, and store structured and unstructured data from various sources.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and translate them into technical solutions.
Optimize and tune data pipelines for performance, scalability, and reliability.
Ensure data quality and integrity throughout the data lifecycle, implementing data validation and monitoring processes.
Evaluate and implement new tools and technologies to enhance our data infrastructure and capabilities.
Requirements
:
Extensive experience in Python.
Strong experience with data processing frameworks and tools such as Apache Spark.
Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform.
Solid understanding of data modelling, database design, and SQL
French and English speaking
Freelancing opportunity
The next step
We have an exceptional team in place, and we are pleased to be able to appoint a further person to our growing business. We are aware that you may not ‘tick all the boxes’, but if you believe you can genuinely offer some valuable skills and experience to our business, please in the first instance contact our recruitment partner Airswift.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA'], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
"Data Engineer – Antibes, France (H/F)",Astek,"Antibes, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-antibes-france-h-f-at-astek-3909192086?position=43&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=BxG7YTWicXtd5pO8ytQ3Sg%3D%3D&trk=public_jobs_jserp-result_search-card,"CDI
Antibes - France
Publiée il y a 2 semaines
Le Groupe Astek
Ce Que Nous Allons Accomplir Ensemble :
Intervenir dans la conception, le développement, les tests unitaires, la qualification, l’intégration continue et la mise en production d’évolutions sur les projets du pôle produits scoring (un pôle visant à développer des solutions permettant de générer des scores ou des segments d’information pertinents dans divers domaines, notamment : profiling TV, PUB, SAB, MMDM, Voscastview) chez l’un de nos partenaires spécialisé dans le secteur des télécoms.
Votre Mission, Si Vous L’acceptez :
En collaboration avec les autres membres de l’équipe, vous devrez prendre en charge le RUN des applications du pôle produit scoring.
Conception d’une solution se basant sur les développements existants et les besoins métiers remontés par le Product Owner.
Réalisation et développement de nouvelles fonctionnalités sur les composants des applications du pôle produits scoring et environnement CGP.
Votre Future Équipe :
Au sein d’un environnement riche et complexe, vous évoluerez avec des experts passionnés à la fois techniques et fonctionnels (Ingénieurs spécialisés, chef de projet, scrum master, product owner, analystes …).
Votre stack de jeu
D ans un environnement SAFE sous cloud GCP, Big Query, OnPrime, Grafana, Python et Ansible.
Vous ?
De formation Ingénieur, vous justifiez d’une première expérience sur un poste de Data engineer. Vous possédez des compétences d’autonomie et d’adaptabilité et vous avez une capacité à communiquer efficacement au sein d’une équipe.
Le Groupe Astek
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de
ses 7800 collaborateurs
qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.
Tous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.
Rencontrons-nous
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies, présent sur les 5 continents. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de ses 7800 collaborateurs qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde et ayant réalisé un chiffre d’affaires hors
acquisitions de 600M€ en 2023.
Tous les détails sur le Groupe sur le site
Nos Plus
Astek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo
Une politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)
Notre charte de la Diversité
Mots-clés :
ingénieur – ingénieure – consultant – consultante – Data engineer – Big Data
Caractéristiques de l'emploi
Catégorie Ingénieur
Job Industry Télécom / Média
Postuler en ligne
Nom *
Prénom *
Email *
Un email valide est requis.
Téléphone *
Un numéro de téléphone valide est requis.
Joindre un CV *
Mots-clés :
ingénieur – ingénieure – consultant – consultante – Data engineer – Big Data
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Big Query'], 'SoftBigDataProcessing': [], 'Automation': ['Ansible', 'Chef'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Adaptabilité', 'Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),Web Transition,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=44&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=4zF8cj0%2Bo0yeJlbJeYrhvA%3D%3D&trk=public_jobs_jserp-result_search-card,"Web Transition, c’est qui ?
Fondée en 2011,
Web transition
est une entreprise de services numériques opérant sur le marché de l’IT/Digital !
Constituant une part essentielle de
MoOngy Digital Lab
, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et également en Data !
Notre objectif : nous implanter comme un acteur principal sur le marché de la Transformation Digitale en accompagnant et valorisant les compétences de nos collaborateurs !
Nous sommes convaincus que le succès de MoOngy Digital Lab réside dans la somme des potentiels de nos équipes 🤝
Ton équipe : La tribu Data
Parce qu’il est indispensable que tu puisses partager tes connaissances mais aussi en acquérir de nouvelles, tu feras partie de l’une de nos tribus : celle de la Data. De plus, cela te permettra d’être acteur dans le développement et la stratégie de Web Transition. Ce système de co-réflexion et co-construction est un fondement essentiel chez nous !
Dans cette aventure, tu :
T’assures
de la maîtrise de la donnée et est garant de la qualité de son utilisation (référencement, normalisation, et qualification)
Travailles
à la compréhension et l'intégration des données en provenance des différents formats
des interfaces de flux
également à la définition de la politique de la donnée et à la structuration de son cycle de vie dans le respect des réglementations en vigueur
la supervision et l'intégration des données de diverse nature qui proviennent de ces sources multiples et vérifie la qualité des données qui entrent dans le Data Lake
Garantis
l'accès qualitatif aux sources de données
Facilites
l’accès aux données pour tes collègues (data scientists, data analysts…)
Assistes
les autres équipes dans l'accès et la compréhension des données des socles.
Rejoins-nous si tu as :
Expérience d’au-moins 4 ans dans la Data
Appétence à la qualité des données.
Connaissance familière des Datawarehouses.
Maitrise de Python, Oracle SQL, GCP/Power BI
Aisance avec les indicateurs, tu as une bonne capacité d'analyse et de rédaction.
Ton savoir-être :
Ouvert d’esprit
Rigoureux
Autonome
Respectueux des différences de chacun
Curieux
Proactif
Agile
Par où on commence ?
Un premier entretien RH d’1h pour comprendre ton parcours et tes aspirations
Un second entretien de 45 minutes avec l’un de nos Business Manager afin de valider tes compétences et qu’il se projette sur l’une des missions qu’il pourrait te proposer
Un troisième entretien de quelques minutes avec notre responsable d’agence pour te proposer d’intégrer notre superbe Team Web !
3 entretiens en peu de temps, si ton profil correspond tu intègreras très vite nos équipes 😉
Prêt pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant à cette offre ! Voici les avantages qui t’attendent en tant que Weber :
🤩 Des collègues incroyables
🏆 Certifiée Great Place to Work
🎮 Des bureaux sympas (où vous serez toujours les bienvenus)
🎉 Des teambuilding et évents tous les mois
💻 Des tributs métiers pour échanger entre Weber du même métier
Des missions chez le client qui sont accompagnées et coachées par ton manager
Un accompagnement dans ton plan de carrière et tes envies de re skilling
🤓 Un catalogue de formations certifiantes ouvert à tous les salariés
🍽️ Une carte tickets restaurant MyEdenred
❤️ Une mutuelle GrasSavoye
🚎 Une prise en charge des frais de transport à 100%
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data engineer H/F,Akkodis,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-akkodis-3890779946?position=45&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=vC%2BhZztWd%2B8k1rfzefcAEw%3D%3D&trk=public_jobs_jserp-result_search-card,"La ligne de service Consulting & Solutions d’Akkodis France renforce ses équipes en région Hauts-de-France et recrute un
Data engineer H/F
en
CDI
sur la
métropole lilloise
:
Description de la mission :
Concevoir, mettre en oeuvre et maintenir des pipelines de données efficaces et évolutifs dans un environnement cloud (comme AWS, Azure, Google Cloud Platform…)
Assurer la qualité des données et des modèles
Définir les bonnes pratiques de développement en implémentant des outils de CI/CD
Assurer une veille technologique sur les technologies Cloud
Capacité à interagir avec des parties prenantes diverses : business analyst, architecte, métier…
Veiller au bon fonctionnement des pipelines en production
Profil :
De formation
Bac +4/5 en informatique
ou issu d'une
école d'ingénieur
, vous possédez une expérience de
3 ans
minimum en tant que data engineer ainsi que les compétences suivantes :
Une bonne connaissance des écosystèmes liés à la data (Kafka, ETL, base de données…)
Une première expérience sur un cloud provider (AWS, Azure, GCP)
Une bonne maitrise de langages de programmation tels que SQL, Python, Scala
Akkodis accompagne ses clients dans la mondialisation de leurs projets, aussi un anglais courant est requis pour l’ensemble de nos collaborateurs.
Processus de recrutement :
Une chargée de recrutement vous contacte pour échanger sur votre projet professionnel
Vous échangez ensuite avec un.e manager sur les aspects techniques, les projets
Chez Akkodis nous sommes convaincus que de l’intelligence collective naît le succès. Il n’existe pas qu’un modèle, nous valorisons l’agilité et l’excellence, l’audace et la créativité.
Et si nous parlions ensemble de vos ambitions pour les prochaines années ?
Akkodis est une entreprise handi-engagée et inclusive. Tous nos postes sont ouverts aux handicaps et à la diversité. Tous différents, tous compétents !
Akkodis, est un acteur mondial de l’ingénierie et de l’IT et un leader dans la smart industrie. Nous accompagnons nos clients dans leurs projets de transformation digitale via 4 lignes de service : Consulting, Solutions, Talent et Academy. Akkodis est un partenaire technologique de confiance pour ses clients à l’échelle internationale. Nous co-créons et nous imaginons des solutions de pointe pour répondre aux défis majeurs de notre société, qu'il s'agisse d'accélérer la transition énergétique et de développer la mobilité verte, ou encore de construire des approches centrées sur les utilisateurs.
Dotés d’une forte culture de l’inclusion et de la diversité, nos 50 000 experts en IT et en ingénierie, présents dans 30 pays, allient les meilleures compétences technologiques à une connaissance transverse de toutes les industries pour façonner un futur plus durable. Nous sommes passionnés par l’idée d’inventer ensemble un avenir meilleur.
Akkodis en France, ce sont près de 9.000 experts en IT et en ingénierie répartis sur l'ensemble du territoire, des collaborateurs partageant des valeurs fortes d'honnêteté, de respect, d'équité et d'inclusion. Notre engagement : leur permettre au quotidien d'être eux-mêmes au travail, et acteurs de leur vie et de leur développement au sein d'Akkodis.
*Akkodis est une marque commerciale sous laquelle les entités AKKA et Modis opèrent
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': ['Créativité'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer F/H,Mobilize Financial Services – France,"Noisy-le-Grand, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-mobilize-financial-services-%E2%80%93-france-3869637982?position=46&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=QjY4WuD6jrMpRjEyvVP3dw%3D%3D&trk=public_jobs_jserp-result_search-card,"🚗 En route vers Mobilize !
A l’écoute de tous nos clients, nous créons des services financiers innovants pour construire une mobilité durable pour tous.
Rejoindre Mobilize Financial Services,
c’est d’abord choisir d’intégrer un groupe international
, filiale de Renault Group, une banque de financement solide, partenaire des constructeurs Renault–Nissan–Mitsubishi. Nos 4 000 collaborateurs présents dans 35 pays, agissent ensemble au service de nos clients.
Nous proposons à nos clients - particuliers comme professionnels - les financements et les services les plus adaptés pour les véhicules neufs et d'occasion.
Nous finançons également l'activité des réseaux de concessionnaires des marques de l'Alliance Renault-Nissan-Mitsubishi et nous veillons à faciliter leur gestion au quotidien pour leur permettre de développer leurs ventes et assurer leur pérennité financière.
Notre entreprise se ""MOBILIZE"" en faveur de la diversité culturelle, l'égalité hommes-femmes et l'intégration de personnes en situation de Handicap. Nous favorisons un environnement de travail où les différences individuelles sont reconnues, appréciées, respectées et valorisées, de façon à mettre à profit les talents et les forces de chacun.
🚘Prenez le volant ! Pas de routine, tous nos itinéraires sont différents !
Au sein de la DSI
,
votre
futur métier consistera à :
Accompagner l’équipe dans la transformation du domaine décisionnel construit sur une architecture type DWH et le porter sur la solution GCP (Google Cloud Platform) de Mobilize FS
Participer à la construction du projet de transformation vers GCP
Participer aux projets d’évolution de notre plateforme Suite Elastic (ELK - Kibana)
Piloter des projets en étroite collaboration avec les directions métier et en accord avec le TBA (Tableau de Bord des Actions).
Assurer la gestion du budget, du planning, de la tenue des jalons et du respect des engagements sur les projets en responsabilité
Assurer la qualité et le bon fonctionnement du chargement des données.
Assurer la mise à disposition des données et des outils de reportings à toutes les directions clientes dans le respect des contrats de service
Véritable tout-terrain, vous nous intéressez !
L’esprit d’équipe et le sens du service client pour atteindre ensemble les différents objectifs ambitieux et satisfaire les différentes parties avec un haut niveau de qualité.
Vous avez un bon relationnel, de l’écoute et une excellente communication afin d’interagir avec des interlocuteurs de différents niveaux (direction technique et métier) et de travailler en transverse.
Le sens de l’analyse et de bonnes capacités d’anticipation pour déceler les problèmes avant la naissance de ces derniers.
Force de proposition : avec vous il n'y a pas de problèmes, que des solutions
Vous avez un niveau d’anglais vous permettant de lire et de comprendre de la documentation technique
💻🖱 Environnement technique :
Maitrise des langages Python - SQL / NoSQL
Expérience significative sur Python
Expérience avec Git
Une expérience avec les outils Nifi, Airflow et GCP (BigQuery / Cloud Function / Cloud Storage …) serait un plus
Gestion de projet, maintenance, évolution, support
Appétence pour les sujets techniques et fonctionnels : outils de modélisation, exploration de données, IA, machine learning
Pourquoi nous rejoindre ?
Votre Pack confort
est composé de nombreux avantages 😀 :
Rejoindre Mobilize Financial Services c’est intégrer un grand groupe international qui offre des opportunités de carrière
.
Un environnement de travail moderne et convivial
: locaux agréables, salle de sport, terrasse, restaurant d’entreprise, parking avec un CSE dynamique avec de nombreuses offres voyages, sport, famille,
Nous sommes mobilisés pour développer la qualité de vie au travail de nos collaborateurs en faisant évoluer nos façons de travailler (méthodes, outils, organisation du travail…) et nous sommes fiers d’être certifiés ⭐
Great Place To Work ⭐
Possibilité de télétravailler 2 jours par semaine
Nous proposons une
rémunération selon profil + Participation + Intéressement
Locaux situés au pied du RER A – Noisy le Grand Mont d’Est
❗ Mobilize Financial Services déménage ❗ Les postes à pourvoir en région parisienne seront basés à Boulogne Billancourt à horizon 2026
Pour en savoir plus sur notre entreprise,
suivez-nous sur LinkedIn !
La route du recrutement ?
📞 Un rapide entretien téléphonique,
🛑
un premier échange
avec Marie DE CARLI, Responsable du département DATA
↪ et un dernier virage avec Agathe GROSBOIS, Responsale des Ressources Humaines
L’équipe Mobilize FS a hâte de vous recevoir !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Cloud'], 'FrSoftSkills': ['Communication', 'Collaboration', 'Organisation'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),MP DATA,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-mp-data-3908719610?position=47&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=ow485J5mRO5KVr5k5FP7Jg%3D%3D&trk=public_jobs_jserp-result_search-card,"MP DATA est une société spécialisée dans l’acquisition, le traitement, et la valorisation des données.
Depuis sa création en 2015, MP DATA accompagne ses clients, majoritairement industriels, dans le management de leur performance et l’exploitation de leurs données.
Les collaborateurs, tous issus de grandes écoles, incarnent au quotidien les valeurs d’Excellence, de Partage et d’Engagement.
Ils associent savoir-faire technique, méthodologie et passion et mettent leurs compétences au service de missions et projets au sein de grands groupes français.
MP DATA accompagne ses clients sur toute la chaine au travers de 3 pôles d’expertise : Conseil et Stratégie, Infrastructure & CloudOPS, Data Science.
Chez MP DATA, les équipes commerciales cherchent des missions en fonction des envies des collaborateurs et non pas l’inverse. Les consultants sont accompagnés dans tous leurs projets, de la mobilité géographique, au changement de secteur d’activité en passant par le développement de nouvelles compétences.
Rejoindre MP DATA, c’est la garantie de travailler sur des sujets passionnants avec un cadre technique fort.
Descriptif du poste :
Nous recherchons un Data Engineer expérimenté pour rejoindre notre équipe.
En tant que Data Engineer, vous serez responsable de la conception, du développement et de la mise en œuvre de pipelines de traitement de données en temps réel à grande échelle.
Vous travaillerez avec des technologies telles que Kafka, Flink, Kinesis et vous utiliserez les services du cloud AWS pour stocker et traiter les données.
Vos responsabilités :
Utiliser Kafka pour le traitement de flux de données en temps réel à grande échelle, en travaillant avec les producteurs, les consommateurs et les topics.
Mettre en œuvre des pipelines de traitement de données en streaming avec Flink, en appliquant des transformations complexes et en gérant les états.
Écrire du code efficace et maintenable en Java / Python pour manipuler et analyser les données en temps réel.
Utiliser Kubernetes pour déployer et gérer des applications conteneurisées à grande échelle, en assurant la résilience et l’évolutivité des services.
Utiliser les services AWS tels que Amazon S3, AWS Lambda, Elastic Kubernetes Service (EKS), Elastic Container Service (ECS) et Elastic Compute Cloud (EC2) pour le stockage, le traitement et le calcul des données en temps réel.
Suivre les meilleures pratiques pour une utilisation efficace du cloud, en assurant la gestion des coûts, la sécurité des données et la disponibilité des services.
Collaborer avec l’équipe de développement logiciel et la gestion de projets pour assurer un flux de développement fluide et une livraison efficace des fonctionnalités.
Bon à savoir :
CDI / ASAP / Toulouse
Profil recherché:
Nous recherchons un candidat diplômé d'une grande école d'Ingénieur avec une première expérience.
Compétences nécessaires :
Expérience significative dans un environnement industriel en mode DevOps, avec des outils tels que CICD, gitlab, Jenkins, Sonar, Nexus, XLdeploy, Camunda, etc.
Maîtrise des langages de programmation tels que Python, Java et expertise dans l’écriture et l’optimisation du code SQL
Maîtrise du français et bonne maitrise de l’anglais.
Capacité à travailler en équipe et esprit d’équipe.
Le processus de recrutement se déroule en 3 entretiens :
Prise de contact
1er entretien : Présentation et projet du candidat + présentation MP DATA
2ème entretien : Entretien de qualification technique
3ème entretien : Rencontre avec les équipes dans les locaux MP DATA + Proposition de collaboration
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data engineer python,FINAXYS,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-python-at-finaxys-3887107285?position=48&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=vr485KzJPECM8NQIBq1TcQ%3D%3D&trk=public_jobs_jserp-result_search-card,"LE CONTEXTE
Leader en
IT
, dans les domaines Banque
,
Finance
et
Assurance
,
Finaxys
est un cabinet de
conseil
créé en 2008. Nous accompagnons au quotidien les plus grandes banques du pays sur leur
transformation digitale
(BNP Paribas, Société Générale, Crédit Agricole, Natixis, etc.)
Nos clients bancaires travaillent également dans des contextes Big Data sur des applications centrales rattachées aux Datalakes.
LES MISSIONS
Développement et traitements sur des applications Big Data (Python)
Être force de proposition sur les choix techniques les plus pertinents
Maintenir la qualité des solutions, mesure de cette qualité, alerte sur les non-conformités et validation des solutions définitives.
Analyser des risques liés aux solutions envisagées et proposition des actions de remédiation.
Apporter des solutions IT répondant au mieux aux besoins du business porté par la/le Product Owner (Métiers/Fonctions) en cherchant toujours la maximisation de la valeur générée
Accompagner les équipes dans les migrations Cloud
ENVIRONNEMENT TECHNIQUE
Python
Pandas
Scirpting Big Data
Culture DevOps (Jenkins, Maven, Ansible)
PROFIL
Compétences Techniques et Fonctionnelles requises
Maitrise obligatoire de l’anglais
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': [], 'DataAnalytics': ['Pandas', 'R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data engineer,SEVETYS,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-sevetys-3905649273?position=49&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=s9WJPofDRfEwiXXMXYrTmg%3D%3D&trk=public_jobs_jserp-result_search-card,"Sevetys, premier groupe français de cliniques vétérinaires, est présent dans toute la France avec plus de 150 établissements. Créé en 2016, le groupe souhaite moderniser le métier et mettre la qualité des soins et la satisfaction client au cœur de son projet.
Le projet d’entreprise se caractérise par son hyper croissance et une culture de type start-up axée sur le collectif, la cohésion, et l’engagement.
Fort de son succès, Sevetys poursuit sa structuration et recrute un / une :
Data Engineer
​Le
Data Engineer
travaille en étroite collaboration avec une équipe Agile pluridisciplinaire pour construire des pipelines de données de haute qualité permettant de mettre en œuvre des solutions analytiques. Ces solutions génèreront des informations à partir de nos données collectées, permettant de faire progresser les capacités de prise de décision du management de l’entreprise. Ce rôle nécessite une compréhension approfondie de l'architecture des données, de l'ingénierie des données, de l'analyse des données, du reporting. Le candidat idéal est un ingénieur en données/logiciel ayant au moins une première expérience dans la création de produits de données soutenant des solutions analytiques.
Missions :
Conçoit, développe, optimise et maintient une architecture de données et des pipelines qui respectent les objectifs de l'entreprise ;
Résout des problèmes de données afin de fournir des informations qui aident notre entreprise à atteindre ses objectifs ;
Crée des jeux de données pour les membres de l'équipe d'analyse afin d'améliorer leur productivité ;
Favorise une culture du partage, de la réutilisation, de la stabilité de la conception à l'échelle et de l'efficacité opérationnelle des données et des solutions analytiques ;
Contribue à l'évaluation, la mise en œuvre et le déploiement d'outils et de processus émergents pour l'ingénierie des données analytiques afin d'améliorer notre productivité en tant qu'équipe ;
Élabore et met en œuvre des plans de communication/éducation sur les capacités, les normes et les processus d'ingénierie des données analytiques ;
Travaille en partenariat avec des analystes business et des architectes de solutions pour développer des architectures techniques pour les projets et initiatives stratégiques de l'entreprise.
Expertises techniques :
Expérience du développement de bases de données et d'une variété de technologies de bases de données relationnelles ;
Expérience des entrepôts de données ;
Expertise en SQL et en analyse de données ; maîtrise Python ;
Idéalement certifié des technologies BI du Cloud Azure (Synapse, Azure Data Factory, PurView) ;
Connaissance de l'intelligence artificielle, des statistiques et/ou des mathématiques appliquées ;
Expérience dans le développement de solutions sur des services et infrastructures de cloud computing dans le domaine des données et de l'analyse ;
Expérience du déploiement de Power BI ;
Expérience conceptuelle des données et de l'analyse, par exemple ETL, modélisation dimensionnelle, outils de reporting, gouvernance des données, entreposage des données, données structurées et non structurées, qualité de données ;
Connaissance CI/CD et GitLab fortement apprécié.
Expérience agile / Digitale / gouvernance :
Passionné(e) le développement basé sur les données, la fiabilité et l'expérimentation ;
Expérience souhaitée de travail au sein d'une équipe produit Agile collaborative ;
Connaissance de la gouvernance de la donnée.
Skills Individuels :
Motivé(e) et doté(e) de solides compétences en matière de résolution de problèmes et d'apprentissage ;
Flexibilité face aux changements d'orientation du travail au fur et à mesure de l'évolution du projet ;
Excellentes capacités de communication, d'écoute et de persuasion.
Attitude attendue :
Sens aigu des chiffres, curiosité intellectuelle et volonté d'adapter sa position sur la base d'informations complémentaires ;
Forte éthique de travail ; capacité à travailler à un niveau abstrait et à obtenir un consensus.
Informations supplémentaires :
Poste à pourvoir dès que possible ;
Remboursement des frais de transports + Mutuelle ;
Possibilité de télétravail jusqu'à un jour par semaine.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': ['Statistiques'], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Statistiques', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication', 'Résolution de problèmes', 'Collaboration', 'Flexibilité'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
DATA ENGINEER,Apside,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-at-apside-3909772916?position=50&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=j2LW3ztW%2Bq%2B5gYaNxbUPPQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Offre d'Emploi : DATA ENGINEER H/F chez Apside
Description du poste :
Nous sommes à la recherche d'un Data Engineer passionné pour rejoindre notre équipe dynamique. Si vous avez une expertise dans le Big Data, la Data Science, l'analyse de données et l'architecture de données, cette opportunité est faite pour vous. Intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.
Vos missions :
Développement des jobs Spark pour la collecte et la transformation des données comptables disponibles dans les bucket S3.
Optimisation des jobs Spark.
Développement des batchs Java et écriture des données au formats comptables.
Écriture et ordonnancement des DAGs Airflow.
Support du développement Spark Scala.
Maintenance applicative.
Production des événements dédiés à la plateforme de données.
.
Votre rôle, vos compétences :
Vous maîtrisez au minimum un langage de programmation appliqué à l’analyse de données (SQL, Scala, Python, Java).
Vous êtes passionné par le Big Data et le Machine Learning.
Vous concevez et mettez en œuvre des stratégies sécurisées d'acquisition et d'intégration de données.
Vous configurez des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks).
Environnement technique :
SQL
Python/Spark
Cloud AWS: AWS Glue, AWS Lambda (possibilité de vous former sur AWS)
Stockage objet (AWS S3)
Orchestration et scheduling de tâches (Apache Airflow)
Bases analytiques et bases NoSQL (ElasticSearch, AWS Athena)
Votre profil :
Fort de 4 années d’expérience en Data Engineer/ DATA ANALYST
Titulaire d’une formation supérieure IT.
Capacité à s’intégrer dans un cadre technique client tout en étant à même de proposer des pistes d’améliorations pertinentes.
Autonome dans la gestion des projets.
Curieux et impliqué, vous êtes bon communicant avec les clients et les acteurs de culture technique différente.
De bonnes raisons de rejoindre Apside ?
Un esprit start-up avec la stabilité d’un grand groupe, qui favorise l’agilité, le travail d’équipe et la proximité. Alors qu’Apside ne cesse d’agrandir sa famille déjà forte de plus de 3000 consultants, nous sommes à la recherche de nos nouveaux talents !
CDI + package salarial avantageux (Mutuelle offerte, RTT, Tickets Restaurant, Intéressement ...)
Participez et animez nos soirées techniques (Project Lab, Test Lab…),
Devenez speaker (Devoxx, DevFest, NCraft…),
Formez vous avec l’Academy By Apside (e-learning, formation, certification).
Développez votre réseau (Soirées trimestrielles, Afterwork, Soirées d’intégration…),
Intégrez notre Communautés d’Experts et testez les dernières innovations techniques sur notre Bac à Sable !
Apside s’engage en faveur de l’emploi des personnes en situation de handicap avec sa filiale Apsid’EA : 1ère entreprise adaptée totalement intégrée à une ESN !
Pour aller plus loin avec APSIDE !
https://www.apside.com/fr/nos-offres-emploi/?_joboffer-agency=17833&_paged=2
Ce poste de DATA ENGINEER est fait pour vous !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['17833'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer Senior,AXA en France,"Hauts-de-Seine, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-senior-at-axa-en-france-3884386043?position=51&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=sDPS1GxZYKSqOr7DXdMrjQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Environnement
En tant que
Senior Data Engineer F/H
, vous allez contribuer directement aux projets des directions métier (ex : fraude santé, multiéquipements, pricing IARD, optimisation du lead management, fragilité auto, …) d’AXA France et à la construction du socle technique Big Data.
Vous allez intégrer une équipe d'une dizaine de personne composée de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus métier de la Direction Transformation Digital Tech et DATA (DT2).
La Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :
- Une organisation agile en feature teams : tribus, guildes, squads
- Des projets sur des applications innovantes à fort trafic (web, mobile…)
- Des méthodologies craft (TDD, BDD, clean code, code review…) et DevOps
- Une communauté de partage de bonnes pratiques (BBL, dojo, meetup, conf…)
Votre rôle et vos missions
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le data lake
Les exploiter pour atteindre la finalité business (exposition de business view, réintégration des résultats dans le SI, service de scoring, …)
De travailler à la création du socle technique Big Data et industrialiser le cycle de développement de l'équipe
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Votre profil
Vous justifiez de plusieurs expériences significatives (+ de 5 ans) sur du
développement big data, en particulier sur du PySpark.
Compétences techniques :
Connaissances avancées en développement en
PySpark
(Spark avec le langage Python)
Maitrise de l'environnement
Microsoft Azure
Connaissances avancées d'outils de BI comme
PowerBI
Compétences transverses :
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Expérience en mode de delivery Agile (Scrum, Kanban, etc...)
Driver et accompagner des Data Engineer junior sur les aspects technique
Et Idéalement :
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA
Maitrise des Traitements Big Data en mode Streaming
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
Qui sommes nous ?
AXA est un des leaders de l’assurance et de la gestion d’actifs dans le monde.
Nous aidons nos 108 millions de clients à traverser les petites et grandes difficultés de la vie.
Chaque jour, nous agissons ensemble pour inventer la meilleure manière de les protéger et voulons donner à chacun les moyens de vivre une vie meilleure.
Un challenge qui donne le sourire et envie de se lever le matin !
Chez AXA, nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C’est pour cette raison que nous menons une politique RH engagée qui favorise la diversité, qui préserve l’équilibre vie privée-vie professionnelle et accélère le développement des compétences et des carrières.
Ainsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une véritable culture d’expertise, accélérant le développement des compétences de chacun et proposant une rémunération attractive.
Pourquoi nous rejoindre ?
Vous êtes porteur d’idées et d’initiatives innovantes ? Vous proposez des solutions et êtes au service du client ? Faites partie de notre grande famille en rejoignant
Un leader mondial offrant des opportunités de carrières intéressantes
Une entreprise qui donne une place de choix à l’innovation, à l’initiative et aux actions solidaires (notamment via l’association AXA Atout Cœur)
Un environnement inclusif à tous les niveaux (mixité, handicap, initiatives pour favoriser l’insertion des jeunes, orientation sexuelle, etc.)
Un accès à de multiples avantages (congés, temps partiel, télétravail, etc.)
Un cadre stimulant, qui permet de rencontrer des collaborateurs performants et d’enrichir ses compétences
Victime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Teams'], 'Other': ['DevOps', 'Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Junior', 'Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer,Beelix,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-beelix-3865239426?position=52&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=RbppPHyc%2Ba%2B6Xq1WS%2FNoKQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes-nous ?
Depuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants:
🚗Automobile
⚡Energie
📡Médias & Télécoms
👗Luxe & Retail
💶 Banque, Finance & Assurance
✈️Défense
Aujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.
Dans le cadre de notre développement, nous recherchons un Data Engineer en Île-de-France.
Quelles missions au quotidien ?
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le Datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le Datalake
Les exploiter pour atteindre la finalité business (exposition de Business View, réintégration des résultats dans le SI, service de scoring, …)
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Accompagner les Data Engineers sur son périmètre pour garantir la qualité des livrables
Expertise souhaitée
Expertise en SPARK et PySpark
Expertise sur Databricks
Une expérience sur un cloud provider public comme Azure (idéalement), AWS, ou GCP
Connaissances avancées d'outils de BI comme PowerBI (idéalement) ou Spotfire
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Etre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT
Maitrise des Traitements Big Data en mode Streaming
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
A propos de vous ?
Diplômé d'une école d'ingénieurs ou équivalent
Au moins 3 ans d'expérience en tant que Data Engineer
Expérience en mode de Delivery Agile (Scrum, Kanban, etc.…)
Vous avez un bon niveau d’anglais tant à l’écrit qu’à l’oral
Pourquoi nous rejoindre ?
Un suivi et un accompagnement au quotidien
Un organisme de formation certifié Qualiopi, un abonnement linkedin learning pour chaque salarié et des partenariats avec des spécialistes pour d’autres expertises
De nombreux événements : Afterworks, Communautés métiers, Happy talks…
une Expérience personnalisée basée sur vos besoins grâce au Prédictive Index
Notre package « unBeelievable » : 100% du titre de transport, Tickets restaurants, CSE, Prime de participation ...
Nombreux évènements (afterworks, sport, etc) et des communautés métiers dynamiques
Le processus de recrutement ?
Échange téléphonique (15 min)
Entretien 1 RH pour apprendre à vous connaître
Entretien 2 avec votre futur N+1 pour appréhender la relation managériale
Entretien 3 avec un Responsable commercial pour avoir la vision stratégique
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer,CGI,"Niort, Nouvelle-Aquitaine, France",https://fr.linkedin.com/jobs/view/data-engineer-at-cgi-3902057928?position=53&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=yvn5h2BSifUPF6%2FPZkJ7xg%3D%3D&trk=public_jobs_jserp-result_search-card,"Description de poste
Big Data, Data Science, Data analyse, Data architecture ... Ça n’a pas de secret pour vous ?
Que vous commenciez votre carrière professionnelle ou que vous soyez spécialiste de l’une de ces disciplines, intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.
Si vous souhaitez intégrer nos équipes à Niort et accompagner les plus grands acteurs du secteur des Assurances, cette annonce est susceptible de vous intéresser.
En tant que Data Engineer, vous serez responsable de la conception, du développement, de la gestion et de l'intégration des systèmes basés sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop. Ce rôle implique la mise en place d'architectures évolutives et hautement disponibles pour répondre aux besoins de traitement et de stockage de données de l'entreprise.
Fonctions et responsabilités
Vos responsabilités seront les suivantes:
-Maintenir et développer des solutions basées sur les services AWS pour le stockage, le traitement et l'analyse de données
-Utiliser les services AWS appropriés tels que Amazon EC2, S3, RDS, Lambda, etc., pour répondre aux exigences du projet.
-Créer et maintenir les configurations Terraform pour la gestion de l'infrastructure en tant que code (IaC) sur AWS
-Participer à la maintenance et à la mise en place d'environnements OpenShift pour l'hébergement d'applications et de services
-Gérer et administrer les clusters Kafka pour garantir la disponibilité, la performance et la sécurité du système de messagerie
Participer à l’assistance utilisateurs sur les briques de la plateforme Hadoop Cloudera Data
-Travailler avec les projets et les devOps pour assurer un traitement efficace des données
En rejoignant CGI, vous bénéficiez notamment d’une offre complète de formations (techniques, métiers, développement personnel,…), de flexibilité grâce à notre accord télétravail (jusqu’à 3 jours de télétravail par semaine), d’une politique de congés avantageuse (27 jours de congés payés, RTT, congés ancienneté et enfant malade,…) et d’un package d’avantages intéressant (régime d’achats d’actions, participation, CSE,...).
Qualités requises pour réussir dans ce rôle
Ayant une première expérience en tant que Data Engineer, vous avez une première expérience relative aux points suivants:
-Développement et intégration sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop
-Connaissance avancée de l'administration Kafka, y compris la configuration, la gestion et la résolution des problèmes
-Mise en œuvre de l'infrastructure en tant que code à l'aide de Terraform
-Bonne compréhension des bonnes pratiques de sécurité pour les systèmes cloud, les clusters Kafka et les plateformes Hadoop
CGI est un employeur inclusif et attentif aux candidatures des personnes en situation de handicap, à l’évolution de carrières des hommes et des femmes et au bien-être de nos salariés LGBT+. Dans un souci d’accessibilité et de clarté, le point médian n’est pas utilisé dans cette annonce. Tous les termes employés se réfèrent aussi bien au genre féminin que masculin.
Ensemble, en tant que propriétaires, mettons notre savoir-faire à l’œuvre.
La vie chez CGI est ancrée dans l’actionnariat, le travail d’équipe, le respect et un sentiment d’appartenance. Chez nous, vous pourrez exploiter votre plein potentiel parce que…
Nous vous invitons à devenir propriétaire dès le jour 1 alors que nous travaillons ensemble à faire de notre rêve une réalité. C’est pourquoi nous nous désignons comme associés de CGI, plutôt que comme employés. Nous tirons profit des retombées de notre succès collectif et contribuons activement à l’orientation et à la stratégie de notre entreprise.
Votre travail crée de la valeur. Vous élaborerez des solutions novatrices et développerez des relations durables avec vos collègues et clients, tout en ayant accès à des capacités mondiales pour concrétiser vos idées, saisir de nouvelles opportunités, et bénéficier d’une expertise sectorielle et technologique de pointe.
Vous ferez évoluer votre carrière en vous joignant à une entreprise bâtie pour croître et durer. Vous serez soutenus par des leaders qui ont votre santé et bien-être à cœur et qui vous permettront de saisir des occasions afin de parfaire vos compétences et élargir les horizons.
Joignez-vous à nous, l’une des plus importantes entreprises de conseil en technologie de l’information (TI) et en management au monde.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['OpenShift'], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['1'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,PROXIAD,Greater Nice Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-at-proxiad-3901014428?position=54&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=oOSHoRrWkL6odgpXOOrPXg%3D%3D&trk=public_jobs_jserp-result_search-card,"Contexte
En tant que Data Engineer, votre rôle consistera à réaliser la conception, le développement, les tests unitaires, la qualification, l'intégration continue et la mise en production d'évolutions sur les projets du pôle produits scoring.
Ces projets Big Data GCP ont pour objet de développer des traitements de croisement de données, exploration data en mode agile (scrum et Safe), industriel (respect de normes), sur l'environnement Google Cloud Platform.
1 : Conception
Spécification et conception d'une solution se basant sur les développements existants.
Mettre en question les choix techniques dans le but de concevoir un logiciel répondant au mieux à la demande au moindre coût et avec la qualité demandée.
Conception de l'expression de besoins, de la réponse à l'expression de besoins à l'aide des besoins métiers remontés par le Product Owner.
2 : Réalisation
Développement de nouvelles fonctionnalités sur les composants des applications du pôle produits scoring en environnement GCP (DataProc, GCS, BigQuery, Airflow...)
Tests des développements réalisés
Revue de code des développements des autres développeurs
Mise en production via CICD des développements
3 : Suivi du RUN applicatif
Prendre en charge avec les autres membres de l'équipe le RUN des applications du pôle produits scoring. Cela inclus les tâches de rapport quotidien, la gestion des problèmes applicatifs, le soutien aux utilisateurs.
Compétences attendues
Maîtrise opérationnelle :
Confluence
Implémentation de l’intégration continue (Utilisation de la chaine CI/CD existante )
Connaissance des principes DevOps
Jira
Anglais (lu, écrit)
Maîtrise avancée :
Elaborer un cahier de recette
Big Query
Spécifications technique et documentation
Développement :Python, SQL, Scala, Javascript, GitLab
Expertise
GCP : Expérience significative en tant que Data Engineer Cloud. Mise en pratique des produits GCP et en particulier Dataproc, Big Query, composer, workflow, PubSub
Développement : Java
Compréhension générale des travaux BigData et du profiling
Informations complémentaires :
Télétravail 2 jours par semaines
Rémunération aux alentours des 45K€
Expérience requise : 6 ans
Localisation : Mougins
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go', 'JavaScript'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery', 'Big Query'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Confluence'], 'Other': ['DevOps', 'Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
CDI - DATA ENGINEER SPARK SCALA JUNIOR - H/F,ITNOVEM.,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/cdi-data-engineer-spark-scala-junior-h-f-at-itnovem-3899544280?position=55&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=AJXOr6PxIG7iPzlY%2F0%2FWWA%3D%3D&trk=public_jobs_jserp-result_search-card,"ITNOVEM, qui sommes-nous ?
Filiale technologique du groupe SNCF, intégrée à la Direction du Digital et des Systèmes d’information, Itnovem
.
se positionne comme expert de l’Internet Industriel. Porteuse de grands projets de la révolution digitale, notre société est en constante recherche de profils pour rejoindre la grande aventure de l’Internet des objets, de la data science et de l’accompagnement des projets digitaux.
Qu’il s’agisse de maintenance prédictive, d’aide à la décision sur la maintenance des infrastructures, de gare 4.0, d’usine du futur, ou de sécurisation des assets, nos équipes font valoir à la fois une expérience métier et une expertise technique sans cesse renouvelée, dans le respect des valeurs du groupe :
Excellence
,
Innovation
,
Collectif
,
Agile
,
Engagement.
CONTEXTE
Au sein du pôle Factory Data & IA et dans le cadre de la montée en charge des projets, nous sommes à la recherche d'un·e data engineer Scala/Spark junior.
Rattaché·e aux équipes Data Engineering et en collaboration avec les membres de l’équipe, son rôle sera de contribuer aux projets data sur stack Scala/Spark et à l’amélioration de l’outillage et des process internes.
Le recrutement intervient dans le cadre de la création d’un plateau projet dédié à l’activité TGV sur Nantes.
MISSIONS
Participer au développement des projets data sur stack Scala/Spark
Etre acteur de la mise en place de bonnes pratiques de communication entre les plateaux nantais et parisiens
Avec l’appui de l’équipe, être impliqué·e dans la roadmap technologique (pratiques, outils) et de l’amélioration continue du périmètre Scala/Spark
Contribuer proactivement à la qualité et aux compétences des équipes Data Science et Engineering : veille techno, capitalisation…
LE PROFIL RECHERCHE
Compétences métiers & outils :
Expérience professionnelle (alternance, stage) ou académique sur le langage Scala et le développement d’applications Spark
Connaissances autour du SQL (principes, langage, modélisation)
Appétence sur les aspects fonctionnels et métiers d’un projet
Notions de CI/CD (notre stack : Maven, Gitlab, Jenkins, Artifactory, Ansible)
Compétences transverses :
Intérêt prononcé pour le software engineering
Aisance relationnelle
Proactivité et clarté dans la communication
Rigueur et organisation
Force de proposition
Bonne communication écrite et orale
Expériences et formations
Titulaire d’un bac+5 spécialisé génie logiciel / développement ou expérience équivalente.
Vous venez d’obtenir votre diplôme ou occupez déjà votre premier poste dans le domaine du développement de pipelines Data.
Localisation
Poste basé à Saint Denis, possiblement à Lyon
Télétravail jusqu’à 3 jours par semaine.
D’autres raisons de rejoindre ITNOVEM !
🚀 En tant que filiale SNCF, des opportunités de carrières internes vous sont offertes.
📚 ITNOVEM croit en la formation continue de ses collaborateurs et leur donne l’opportunité de s’inscrire à une formation par an minimum.
🚊 Vos titres de transport sont pris en charge à hauteur de 75%.
🍽️ Via la carte titres-restaurant Swile, vous bénéficiez de 9,25 € par jour dont 60% pris en charge par ITNOVEM.
💻 Chez ITNOVEM, vous bénéficiez jusqu’à 3 jours de télétravail par semaine.
🏖️ ITNOVEM vous permet de profiter de 28 congés et de 16 RTT pour les cadres et 10 pour les non-cadres. Par ailleurs, 2 des 3 jours de congés pour enfant malade sont rémunérés.
👫 La mise en œuvre de l’égalité professionnelle femmes/hommes est primordiale chez ITNOVEM. A chaque nouvelle embauche, l'entreprise s'engage à proposer une rémunération équivalente tant aux femmes qu'aux hommes.
♻️ ITNOVEM incite tous les collaborateurs à trier leurs déchets et les gobelets ont été bannis. Par ailleurs, chaque année, ITNOVEM participe à « La grande collecte », une initiative SNCF qui permet de collecter les PC devenus obsolètes en leur offrant une seconde vie
Show more
Show less","{'ProgLanguage': ['Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['CI/CD'], 'FrSoftSkills': ['Communication', 'Collaboration', 'Organisation'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Initiative']}","{'JobDetail': ['Junior'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
Junior Data Engineer (H/F/N),Ekimetrics,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/junior-data-engineer-h-f-n-at-ekimetrics-3903416527?position=56&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=hLcIMdf1paDtm%2BnDr8qtJg%3D%3D&trk=public_jobs_jserp-result_search-card,"Ekimetrics
est leader en data science et fournisseur de solutions AI. Depuis 2006, nous utilisons la data science au service de l’optimisation de performance marketing, business, et de la transition vers une performance plus durable.
Si vous êtes passionné.e de data, ou de technologie en général, et que vous avez envie d’être acteur.rice de votre avenir professionnel, votre place est sûrement chez Ekimetrics !
📊Ekimetrics, c’est:
• 400 expert.e.s en data science
• 1000 projets divers et variés pour plus de 350 clients
• 4 bureaux : Paris, Hong Kong, Londres et New York
• 1 milliard de $ de profits générés pour nos clients depuis 2006
• 7000 tonnes de CO2 évitées pour nos clients en 2022
🌱Chez Ekimetrics, nous avons l’ambition d’accompagner nos clients à repenser leur business model, en réconciliant performance économique, environnementale et sociale, grâce à la data science.
C’est pourquoi nous avons en interne toutes les compétences nous permettant de répondre aux besoins de nos clients: Product Managers, Product Designers, Data Architects, Data Engineers, DevOps Engineers, Data Scientists.
Pourquoi recrutons-nous ?
En tant que Data Engineer, vous serez impliqué dans des projets stimulants avec des clients internationaux de premier plan dans des industries diverses, en construisant des solutions analytiques sur mesure pour répondre aux enjeux de nos clients. Vous travaillerez en équipe, avec d'autres consultants Ekimetrics (data engineers, data scientists, software engineers) sur 1 ou 2 projets simultanément. Vous bénéficierez de nos partenariats technologiques et d’une offre de formation pour vous accompagner dans votre montée en compétences.
Plus particulièrement vos responsabilités seront de
:
• Concevoir et développer des solutions permettant de collecter et préparer la donnée ;
• Implémenter et industrialiser des pipelines de données dans des environnements Cloud (Azure, GCP, AWS, Databricks, Snowflake) ;
• Développer des outils destinés à faciliter l’exécution et le déploiementdes pipelines de données (CICD, DevOps, MLOps) ;
• Approfondir vos connaissances en GenAI, Machine Learning, MMO ;
• Participer aux activités de R&D. (Veille, formations, animation de Meetups, Hackathons, etc.)
Le profil et les compétences recherchées
:
• Bac+ 5 Ecole d'ingénieur ou Équivalent ;
• Première expérience sur des sujets Big Data (Projet ou expérience professionnelle) ;
• Connaissances avancées en base de données et en développement (Python, SQL, Spark) ;
• Expérience dans un environnement Cloud ;
• Connaissances avancées en acquisition de données ;
• Appétence pour la Data Science.
🤝 Pourquoi nous rejoindre ?
Rejoindre Ekimetrics, c’est intégrer une entreprise dont les valeurs s’appliquent au quotidien :
• Evoluer dans un environnement entrepreneurial et non traditionnel (
#curiosité)
• Être capable de donner et recevoir du feedback pour s’améliorer en continu (
#excellence
)
• Se former dès son arrivée et en continu grâce à une expérience apprenante unique, riche de nombreuses ressources (internes, externes, live et digital) alliant savoirs techniques, savoir-être et savoir-faire (
#transmission
)
• Faire partie d’une communauté accueillante et soudée(
#plaisir
)
• Imaginer des solutions inattendues & sortir de sa zone de confort (
#créativité
)
En 2023, Ekimetrics a obtenu le statut d’entreprise à mission qui témoigne de notre ambition forte en matière de RSE. Notre raison d’être: Faire de la data science et de l’intelligence artificielle l’accélérateur de la transformation durable des organisations.
Nous sommes également certifiés Great Place to Work© en France, au Royaume-Uni et aux Etats-Unis, et notre bureau de Hong Kong a reçu le prix Best Companies to Work for in Asia 2023©.
🤩 Vous aurez accès à…
• Au catalogue de formation Eki.Academy qui contient des programmes qui vous feront monter en compétences sur nos solutions et nos métiers, des parcours apprenants sur notre plateforme digitale ainsi que des programmes dédiés à nos enjeux prioritaires, dont la sensibilisation aux sujets environnementaux avec la Climate School ;
• Une vie sportive, artistique, musicale, ludique, caritative et engagée : de notre salle de sport privatisée à nos expositions d’art, en passant par des jeux vidéo et des concerts, ou encore les défis RSE sur la plateforme Vendredi. Toutes ces initiatives sont portées par nos Eki.People ;
• De nombreux évènements et séminaires pour rester proche de votre communauté ;
• Des locaux modernes dans un quartier dynamique au cœur de Paris (Grands boulevards) ;
• Une politique de télétravail flexible.
🔄Notre processus recrutement :
🔸Un entretien RH avec un.e recruteur.se
🔸Un test technique ou
peer-to-peer
interview selon profil
🔸Une étude de cas avec un.e consultant.e
🔸Un entretien final avec un.e Manager ou Partner
Nous serions ravi.e.s de vous donner de plus amples informations lors d’un entretien et attendons votre candidature avec impatience!
En tant qu’employeur, Ekimetrics offre à tous les mêmes opportunités d’accès à l’emploi sans distinction de genre, ethnicité, religion, orientation sexuelle, statut social, handicap et d’âge. Ekimetrics veille à développer un environnement de travail inclusif qui reflète la diversité dans ses équipes.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'ML', 'Machine Learning', 'Cloud'], 'FrSoftSkills': ['Créativité', 'Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer confirmé (H/F),BforBank,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-confirm%C3%A9-h-f-at-bforbank-3918327555?position=57&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=LptYbqbw0Nw1VKBD9c1ANw%3D%3D&trk=public_jobs_jserp-result_search-card,"Sur le modèle d'une
""Tech company"",
BforBank place
l'humain et le digital
au cœur de sa transformation. Notre mission,
offrir à nos clients une expérience bancaire incomparable
pour répondre à leurs besoins et usages mobile. 🌟 📱
Rejoindre BforBank c’est
rejoindre une équipe engagée
dans un
grand projet de développement stratégique en France et en Europe.
Nous sommes aujourd’hui 350 passionné(e)s et
recherchons nos talents pour construire la banque de demain
. 🚀
Nous croyons en la force du collectif, chaque jour rassemblés autour de nos valeurs, de simplicité, d'optimisme et d’engagement, encourageant chacun à oser, essayer et accepter d’échouer.
🎯 Au sein de la Direction Technologie, la Data Factory a pour objectifs de piloter, définir, déployer et opérer les meilleures solutions technologiques répondant aux cas d’usage data et d’automatisations de processus de la banque au travers de plateformes. Également, la Data Factory contribue au développement des produits, à la cristallisation et à la diffusion des pratiques au sein des Squads BforBank sur les usages data dans la banque.
Tu rejoindras une squad en charge de résoudre des problématiques métiers en créant des solutions applicatives utilisant les données, des data products, avec pour finalités la prise de décision via des moteurs de calcul ou des dashboards, la création de flux réglementaires, la création de data layer ou de reportings.
🚀 Tes missions principales sont les suivantes :
· Participer aux analyses, études d’impacts et cadrage techniques
· Concevoir des solutions en respectant les bonnes pratiques d’architecture data et développement
· Réaliser le développement de nouveaux data products et assurer la maintenance évolutive et corrective des data products existants
· Rédiger la documentation technique des data products
· Assurer un support aux testeurs
· Reporter de ton activité à ta Squad et travailler dans une démarche d’efficacité collective
Concrètement tu seras amené(e) à produire les livrables suivants :
· Réaliser du code applicatif à l’état de l’art sur notre nouvelle Data Platform
· Créer des data layer et des rapports sur notre outil de Data Visualisation
· Rédiger les documentations techniques liées à ta solution, incluant le modèle de données, les procédures, l’ordonnancement
Ce que tu maîtrises :
· Maitrise des services managés de GCP (BigQuery, dataproc, dataflow, CloudSQL …)
· Maitrise du langage Python, Pandas, Spark
· Maitrise de la modélisation de base de données et du langage SQL
· Maitrise d’une chaine CI/CD (GitLab…)
· Bonne connaissance de Kafka
· Bonne connaissance d’un outil d’intégration de données type ETL (Informatica…)
· Connaissance de l’infra as code (Terraform)
· Connaissance d’un outil de reporting (Looker, BO…)
🤝 Ce poste est fait pour toi si :
· Tu es passionné(e) par la Data et leurs usages
· Tu es orienté résolution de problème, est curieux(se) et force de proposition
· Tu apprécies le travail en équipe
· Tu as un bon relationnel et est rigoureux(se)
· Tu as une bonne capacité d’analyse et rédactionnelle
· Tu t’adaptes rapidement aux changements
🎓
Formation :
Tu es diplômé(e) d’un master en école de commerce, école d’ingénieur ou équivalent.
Chez BforBank nous recherchons avant tout des compétences. Tu ne disposes pas du diplôme requis mais as des expériences équivalentes ? N'hésite pas à postuler !
💼
Expérience :
Expérience confirmée de 3 ans en tant que Data Engineer.
En rejoignant BforBank tu trouveras…
· Un projet ambitieux de transformation digitale et culturelle à l’échelon européen, terrain d’innovation et d’ouverture d’esprit
· Une organisation apprenante, proposant un large choix de formations toute l’année, et qui favorise l’échange avec les autres marques du Groupe
· Une promo RSE multi-métiers qui fait évoluer en continu les actions de BforBank vers une banque plus responsable
· Une organisation du travail en mode Agile, impliquant un degré élevé de collaboration et d'autonomie tout en travaillant avec un groupe de pairs diversifiés.
· Une Direction Technologie en pleine expansion, porteuse de nombreux défis stratégiques
Mais aussi…
De 2 jours à 5 jours de télétravail modulables par semaine, dans la limite de 84 jours par an (frais de fonctionnement pris en charge)
25 jours de congés + 16 jours de RTT
80% du coût de la mutuelle d’entreprise pris en charge / couvert
Avantages collaborateurs Crédit Agricole : taux et tarifs préférentiels
Des frais de transports remboursés à 75%
Un restaurant d’entreprise
Des douches pour les sportifs et un tarif avantageux auprès d’une salle de sport toute proche
📍 Le poste est basé à La Défense, dans des locaux flambant neufs !
BforBank s'engage à garantir l'égalité des chances aux candidats car nous sommes convaincus de la richesse apportée par la diversité et l'inclusion dans nos équipes.
Rencontrons-nous !
Le processus de recrutement se déroule en 4 étapes :
🧑🏼‍💻
Call de 30 minutes avec notre équipe Talent Acquisition
Echange avec le Data Factory Manager et notre équipe Talent Acquisition (présentiel)
Echange avec une personne de l’équipe avec qui tu seras amené à travailler (visio)
Echange avec le CTO (visio ou présentiel)
Notre processus de recrutement dure en moyenne 3 semaines et l’équipe Talent Acquisition se tiendra à ta disposition pour te donner un maximum de visibilité sur l’avancée du process.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['Pandas', 'R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
(Senior) Data Engineer,Mirakl,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-mirakl-3904071960?position=58&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=bdr9XDUp3RnH9V7yglXQrg%3D%3D&trk=public_jobs_jserp-result_search-card,"Mirakl, leader et pionnier de l’économie de plateforme, propose aux entreprises une suite unique de solutions leur permettant de transformer significativement leur e-commerce afin d'accélérer de façon durable et rentable leur croissance. Depuis 2012, Mirakl accompagne les entreprises B2C et B2B avec la technologie la plus avancée, sécurisée et évolutive leur permettant de digitaliser leur activité et d'élargir leur offre via la marketplace ou le dropship, faciliter la gestion des catalogues et des paiements de leurs fournisseurs pour plus d'efficacité, offrir une expérience d'achat personnalisée à leurs clients, et augmenter leurs profits grâce au retail media. Basée à Paris et Boston, Mirakl est certifiée Great Place to Work.
A propos de Mirakl Labs
Nos équipes techniques et produits, nommées Mirakl Labs, sont principalement réparties entre nos 2 hubs situés à Paris et à Bordeaux. Elles collaborent au quotidien afin d'adresser les problématiques de nos clients et utilisateurs en répondant à différents challenges liés aux nouvelles fonctionnalités, à la scalabilité, la sécurité et l’ergonomie…
Elles opèrent en mode agile et s'organisent en Squads composées d'un Squad Lead, de 5 développeurs, d'un Product Manager et d'un QA. Chaque Squad est spécialisée sur un scope fonctionnel afin de concevoir et réaliser de nouvelles features, leurs évolutions et des APIs (avec un découpage en micro-services). Nos équipes Infrastructure, Architecture, Sécurité, Documentation, Product Design, Data et Support opèrent en transverse en apportant leur expertise et de la cohérence sur l’ensemble des produits.
Toutes les équipes sont responsables de leur périmètre et chacun des collaborateurs apporte son expérience et ses idées. Innovation, feedback et implication dans les prises de décision sont au cœur de notre philosophie.
Et pour favoriser ce partage avec d’autres passionnés, nous sommes sponsors, speakers, et hôtes de différents événements, meetups, et associations de la scène Tech en France. Au cours des dernières années, nous avons participé à des événements tels que Devoxx, ReactEurope, ProductConf et Flupa UX Days.
A propos du job
La solution SaaS Mirakl est le moteur des marketplaces des plus importants e-commerçants à travers le monde. Cette solution gère et produit de gros volumes de données qui présentent des challenges extrêmement intéressants pour les spécialistes de la donnée (produits, commandes, clients, niveaux de stock, prix, messages, appels API, données de navigation, séries temporelles, données géolocalisées etc.).
En tant que (Senior) Data Engineer au sein de l’équipe Data Mirakl, vos principales missions seront de :
contribuer à l'enrichissement de la Data Platform (ETL)
améliorer la robustesse de nos pipelines de production pour nos applications Machine Learning (inférence real time etc.)
Intégré(e) dans une équipe de spécialistes de la donnée (data engineers, machine learning engineers, data scientists, data analysts), vous êtes un des acteurs clés pour garantir la place de Mirakl comme solution dominante sur son marché.
Notre stack et nos outils
Apache Spark, Kafka, AWS, Databricks, Python, Airflow, Mlflow, Tensorflow, Delta lake, Superset, Kubernetes, Redshift, SQL, Terraform, Ansible
Au quotidien, vous allez :
Participer à la définition et à l’implémentation d’une architecture performante, robuste, scalable et aux coûts maîtrisés pour nos applications Spark ainsi que pour nos pipelines de production de Machine Learning (évaluation des feature stores, refactoring de DAG Airflow)
Accompagner les Data Scientists lors de leur mise en production (relecture de code, pair programming) et mettre en place les best practices
Optimiser et améliorer la CI/CD de l’équipe en collaboration avec l’équipe SRE
Assurer la montée en compétence des membres de l’équipe sur les sujets de MLOps et Data Engineering
Réfléchir à la meilleure façon d'intégrer les données Google Analytics dans la data platform
Partager ses connaissances et présenter les travaux devant toutes les équipes Labs
Ce qu’on peut vous apporter :
Des projets data driven, divers et variés (traitements massifs d’images, de textes, time series etc.) pour des produits différents de Mirakl
Une culture orientée sur la veille technologique
Des projets qui ont un vrai impact business devant être déployés sur des centaines de clients dans un contexte multilingue
Quelques exemples de sujets en cours :
Enrichissement des données produit à partir des images et des descriptions
Modération automatique des produits
Mapping automatique des données produit
Identification des produits à fort potentiels
Détection de comportements frauduleux
Sentiment analysis sur les messages échangés entre clients et vendeurs et dans les évaluations
Détermination de prix optimaux
Monitoring de la qualité de service des vendeurs
Des applications d’inférence en synchrone de nos modèles de ML
Vous aimerez ce job si :
Vous êtes passionné(e) par la data et les technologies modernes permettant d'en tirer partie
Vous vous intéressez à la data science et avez des connaissances générales sur les algorithmes de Machine Learning
Vous avez un background en développement et avez évolué dans un environnement Data
Vous avez a minima 4 ans d’expérience en environnement Machine Learning et/ou Data
Vous avez mis en production avec succès des applications Big Data faisant appel à du Machine Learning, du NLP, du traitement d’images dans des projets d'envergure, à fort volume de données
Votre maîtrisez Python, êtes un pro des frameworks data de la fondation Apache et êtes à l'aise dans un environnement AWS
Vous maîtrisez au moins un outil d’orchestration (Airflow, Data Pipeline ou tout autre outil similaire)
Vous présentez vos travaux de manière simple et accessible
Vous faîtes preuve d'un bon relationnel et vous aimez mentorer des collaborateurs
Vous parlez couramment anglais et français
Les plus pour le poste :
Vous avez une expérience significative dans le domaine du e-commerce
Vous avez déjà mis en place un Data Lake, Data Warehouse ou une Data Platform
Vous avez déployé des applicatifs en environnement Kubernetes
Vous avez mis en place des pipelines d'ingestion de données avec une approche CDC à l'aide de Debezium ou autre
Vous maîtrisez Java/Scala
Mirakl est engagée en faveur de la diversité, de l’égalité des chances et de l’inclusion. Nous célébrons nos différences car nous sommes convaincus que les qualités visibles et invisibles de chaque Mirakl Worker sont une source de force et d’innovation. Dans le cadre de cet engagement, nous étudions toutes les candidatures sans distinction de : genre, ethnicité, religion, orientation sexuelle, handicap, âge ou toute autre caractéristique protégée par la loi.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': ['TensorFlow'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible', 'Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Machine Learning', 'CI/CD'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Engineer | Python - Azure | IA & Machine Learning  | Paris ou Remote Partiel,Octopus IT - Expert du recrutement tech,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-python-azure-ia-machine-learning-paris-ou-remote-partiel-at-octopus-it-expert-du-recrutement-tech-3664568765?position=59&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=lfoaC1fMJXcvWcFxm77Xmg%3D%3D&trk=public_jobs_jserp-result_search-card,"La société
Créée il y a plus de 2 ans, cette startup est la première base de connaissance intelligente dédiée aux services clients. Leur mission ? En finir avec la frustration lorsque l'on contact un Help Desk.
Pour cela, elle propose aux entreprises la possibilité de délivrer une expérience client d'exception : rapide et de qualité. Grâce à leur moteur de recherche intelligent, cette entreprise est capable de centraliser toute la connaissance interne de l'entreprise (procédures, produits, modes opératoires, etc.) et la diffuse intelligemment dans les outils de production des conseillers de service client.
Résultat :
Plus besoin de chercher l'information
Des réponses instantanées et de meilleures qualitées
Une autonomie totale des collaborateurs
Après une croissance fulgurante, elle a su séduire à la fois de nombreuses scale up (Luko, OpenClassrooms, Japhy...) et grands groupes (BNP Paribas, La Poste, Fnac Darty...).
Après le recrutement de leur Lead Data (réaliser ensemble) et suite à l'annonce de leur levée de 2,5M€ pour tripler la taille de ses équipes, le but est maintenant de s'imposer très vite comme la base de connaissance de référence en France et en Europe. Pour ce faire, nous recherchons un Data Engineer.
Le poste
En travaillant main dans la main avec le Lead Data, ta mission sera de développer et de maintenir des flux de données complexes et robustes. La donnée étant au coeur de l' entreprise, dans le produit comme dans la stratégie, tu seras amené à travailler avec un panel d’interlocuteurs très variés :
Data Scientists sur des sujets comme le monitoring des modèles de production et l’enrichissement des données d’entrainement.
Product Team sur des sujets de performance et d’acheminement de données au service de fonctionnalités produit telles que le dashboard d’analytics à destination de nos clients.
Customer Success / Strategy sur des sujets de pilotage comme le suivi de l’utilisation de notre plateforme ou la mise en place de KPIs de performance.
Tu travailleras sur les problématiques suivantes :
Tu seras responsable de notre architecture de données et de son outillage, mais aussi de la mise en place de pipelines de données complexes et robustes.
Tu seras amené à mettre en place des outils de monitoring et d’alerting pour suivre de près nos nombreuses pipelines de donnée.
Tu seras garant de la qualité de nos données en assurant l’application des guidelines de code et des tests automatisés pour chacune de nos pipelines.
Tu seras amené à mettre en place des outils de reporting / insights à destination d’interlocuteurs variés (Data Science, Product, Customer Success, Clients, etc.).
Tu créeras et développeras des pipelines de données avec des outils de scheduling et d’orchestration.
La stack sur laquelle vous travaillerez :
Langage : Python, Javascript
Framework data : PyTorch, Transformers (Hugging Face), FastAPI
Database : PostgreSQL, MongoDB, ElasticSearch, Redis
Infrastructure : Azure, Docker, Kubernetes, Spark, RabbitMQ, Serverless, Terraform
Environnement / Test : PyTest, Gitlab (git + ci/cd)
BI : Metabase, Superset
Votre profil
Entre 1 et 3 ans d'expérience en CDI
Tu as une expérience significative sur des problématiques de Data engineering
Tu es quelqu'un de pragmatique
Un très bon niveau en Python et une très bonne rigueur dans le code
Bonne pratique de dev : clean code, TDD, BDD
Une bonne culture Ops
Une logique cloud (Aws, GCP ou Azure)
Le salaire & avantages
50-7O K€ selon expérience
RTT
Carte Swile & Mutuelle
2/3 jours de télétravail par semaine
Et plus encore…
Ce qu’on préfère
Être impliqué à fond dans une aventure avec de nombreux challenges techniques
Belles opportunités d'évolutions sur des postes d'Architecte, de Lead ou de Ml Ops
Beaucoup de workshops en interne et catalogue de formations à votre guise
Une opportunité de travailler sur un produit unique qui a déjà séduit de très beaux clients (BNP Paribas, Fnac Darty, Luko, OpenClassrooms)
La possibilité de travailler sur une stack très moderne, des problématiques complexes aussi bien en traitement de données, qu'en DevOps
Un plan de BSPCE (actions de l'entreprise) très intéressant et motivant !
Une culture d'entreprise fondée sur l'apprentissage, l'autonomie, la bienveillance et l'exigence
Le fait de travailler au quotidien avec des fondateurs passionnés par leur domaine d'expertise
Ce poste a été soigneusement choisi par votre coach. Powered by Octopus IT, cabinet d’Experts en Recrutement Tech (CDI et clients finaux uniquement) – Visitez nous pour plus d’opportunités :
www.octopusit.fr
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go', 'JavaScript'], 'DataBase': ['SQL', ' MongoDB', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': ['PyTorch'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['PostgreSQL'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['50'], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer (H/F) - Lille,Logic@l Conseils,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-lille-at-logic%40l-conseils-3811575649?position=60&pageNum=0&refId=b67kRduAUeEfaeVnXh9rCA%3D%3D&trackingId=7mvmlHBKdLio1X1VzQ1izQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Dans le cadre du développement de nos activités sur la métropole Lilloise, nous recherchons un
consultant data engineer
(H/F) pour intervenir chez l'un de nos grands comptes clients.
Vos missions :
Recueillir
les besoins métiers et des équipes data
Concevoir et mettre en place les
traitements de données
Réaliser les
tests de validation
Assurer
l’alimentation du dataware
Réaliser les
ordonnancements des traitements
Etre garant de la
mise en place
, du
suivi
et de l’
exploitation
des outils déployés
Assurer
une veille technologique
régulière
Environnement technique :
Développement :
Python, Scala, R, Java,
Framework :
Spark,
Hadoop,
Outils Big data :
Yarn, Pig, Hive, Kafka, Splunk
Bases de données :
MongoDB, HBase, Cassandra
ETL :
Talend, Stambia
Plateforme :
Hortonworks, Cloudera, Map Reduce
,
AWS, GCP, Azure
Votre profil :
Vous disposez d’une expérience
d’au moins 2 ans en tant que data engineer
ou dans le domaine de l’analyse et du traitement de données.
Véritable
passionné de la data
, vous êtes
force de proposition
sur les solutions techniques à mettre en œuvre. Vous maitrisez l’anglais dans un contexte professionnel.
Compétences requises :
Analyses qualitatives et quantitatives (Intermédiaire)
Anglais (Intermédiaire)
Architecture fonctionnelle SI (Débutant)
Développement d'ouvrages, produits ou événements (Débutant)
Gestion des contrôles, tests et diagnostics (Débutant)
Gestion des risques (Intermédiaire)
Maîtrise des logiciels (Intermédiaire)
Mise en exploitation / Production et maintenance (Débutant)
Nos valeurs
Nous avons décidé de renverser la pyramide du management pour placer nos collaborateurs en tête des priorités de l’entreprise.
En effet, attaché à des valeurs fortes, telles que la proximité, la sincérité, la fidélité, la confiance et le respect, nous sommes persuadés que la réussite réside dans le bien-être de nos collaborateurs.
Cela se traduit par un accompagnement de proximité, de la transparence sans langue de bois, des échanges réguliers avec les managers référents, un accompagnement dans le développement de carrière qui est construit et jalonné avec les formations et certifications nécessaires et les missions en adéquation, pour mener à bien l’évolution de carrière.
Pour vous convaincre de nous rejoindre, nos avantages salariés complémentaires :
Environnement bienveillant et stimulant au sein de 3 pôles d’expertises
Formations et Certifications à la demande
Tickets restaurants : 13€ par ticket
Remboursement à 100 % des abonnements de transports en commun
Mutuelle frais de santé avec de hautes garanties
Prise en charge à 100% de l’assurance Prévoyance
Chèque Cadeau Culture 120 €
Compte CSE avec une cagnotte de 390 €
Compte CE : billetterie, voyages, culture, sorties, à des tarifs préférentiels
Des évènements chaque mois : activités associatives, sportives, afterwork, séminaire,
Partenariat Losc (participation aux match dans la loge VIP logical conseils – (Une Vingtaine de match par an)
Possibilité de télétravail
En intégrant Logic@l Conseils, vous participez à une réelle aventure humaine, alors pour postuler, il suffit de cliquer ci-dessous !
Tous nos postes sont ouverts, à compétences égales, aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['Cassandra', 'HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer BI - Nantes,Capgemini,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-bi-nantes-at-capgemini-3803963477?position=1&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=7SpGJA5uenzj7a4NzhHATw%3D%3D&trk=public_jobs_jserp-result_search-card,"Capgemini
Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.
Vos missions :
Intégré(e) au sein d’une équipe projet BI, Big Data ou Data Gouvernance pour des clients intervenant dans des secteurs d'activités divers, vous serez notamment en charge des missions suivantes :
Mener les analyses fonctionnelles destinées à traduire les besoins du client,
Mener les travaux de conception et de modélisation,
Diriger le développement de la solution / des traitements d'alimentation du DataWareHouse,
Organiser et préparer les travaux de recette utilisateurs,
Mettre en place les processus d'industrialisation et mener cette dernière.
Votre profil :
Diplôme d’ingénieur ou équivalent universitaire
Minimum 3 ans d'expérience
Anglais courant
Compétences en BI sur SAP BI (Hana, BW, BODS, BI 4), Microsoft BI (SQL Server, SSIS, SSAS, SSRS), Oracle (ODI, OBIEE), Teradata, Informatica (Powercenter), IBM (Datastage, Cognos, TM1), Talend, AB Initio
Maîtrise d'un ou de plusieurs outils de Dataviz : Microsoft Power BI, Tableau, Qlikview
Connaissances en Big Data (Ecosystème Hadoop (HIVE, PIG, Mahout…), Cloudera, Pivotal, Spark, HNX) ou en analytics (R, SAS, IBM SPSS)
3 raisons de nous rejoindre :
Qualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité
professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel
vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités :
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
Capgemini
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les
plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle', 'SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer (H/F),Beelix,"Antibes, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-beelix-3838611420?position=2&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=sfIXeaz3U1LEHTkwKKru%2BQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes-nous ?
Depuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants :
🚗Automobile
⚡Energie
📡Médias & Télécoms
👗Luxe & Retail
💶 Banque, Finance & Assurance
✈️Défense
Aujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.
Dans le cadre de notre développement, nous recherchons un Data Engineer (H/F) pour l'un de nos clients.
Quelles missions au quotidien ?
Être le leader de la brique Datalakehouse
Développer les scripts de transformations de données et les pipelines d’alimentation
Proposer des évolutions architecturales ou de fonctionnalités pour améliorer le socle technique
Être le back-up du leader technique sur la partie reporting (Power BI)
Orientation satisfaction client et résultat final forte mais également sensibilité au « comment »
Innovation et proposition de nouvelles pratiques pour améliorer l’environnement et les conditions de travail des équipes
A propos de vous ?
5 + années d'expérience en tant que Data Engineer
Maîtrise des technologies suivantes : Microsoft Azure, Microsoft Azure Synapse Analytics (Spark / Python / Pipeline / Serverless), fichiers parquet / delta, Microsoft Power BI, Microsoft SQL Server, langage SQL, Datawarehousing / Modélisation de données
Analyses et export de données
Connaissance de l’ensemble du processus depuis la collecte jusqu’à la mise à disposition des données en ayant comme point fort la maitrise de sa transformation et mise en forme
Vous avez un bon niveau d’anglais
Localisation : Biot et/ou Carros
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,Digital Waffle,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-digital-waffle-3913824888?position=3&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=fba8MFee7aoxkZLGja%2BWZg%3D%3D&trk=public_jobs_jserp-result_search-card,"Digital Waffle is proud to have partnered with an innovative tech startup in Paris, who are looking for a talented Data Engineer to join their growing team!
They are made up of a powerhouse of experts, combining
data engineers, business process gurus, and Project managers
who leverage the most advanced solutions available; utilising process mining, automation tools, and smart execution systems.
Looking for an experienced Data Engineer (3-5 years)
What You'll Do:
This is a full-time,
hybrid role (Paris-based)
where you'll wear many hats: data exploration, system integration, data prep, data modeling, and implementing data solutions.
Experience:
Expertise in data engineering, data modeling, and ETL (Extract, Transform, Load) processes
Data warehousing and data analytics skills
Experience handling large, complex datasets
Proficiency in SQL and programming languages like Python or Java
Stellar problem-solving and analytical skills
Top-notch communication and collaboration abilities
Bachelor's or Master's in Computer Science, Information Systems, or a similar field (a plus for process mining or intelligent process automation experience)
If you are an experienced and driven Data Engineer, please apply here!
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),Scalian,"Valbonne, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-scalian-3819563847?position=4&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=8DPeW3FeRfB3CDPMsZLmIw%3D%3D&trk=public_jobs_jserp-result_search-card,"Ingénieur DATA / Data engineer (H/F)
Valbonne/Sophia-Antipolis
Type : CDI
Lieu : Locaux Scalian Sophia-Antipolis
Télétravail : En fonction des possibilités
Date de prise de poste : immédiatement ou en fonction de votre préavis
Salaire : en fonction du profil - entre 40 et 48K Brut annuels (hors avantages Scalian)
Avantages Scalian : Accord d'entreprise télétravail, Tickets restaurants, Mutuelle groupe, accord aménagement temps de travail, compte épargne temps, accord de participation et intéressement groupe, programme cooptation et apports d'affaires, accompagnement parentalité, avantages CSE
Vous êtes data engineer ou vous souhaitez le devenir !
Quel sera votre rôle ?
La portée de la mission comprend (sans toutefois s'y limiter) :
Science des données
Ingénierie des données
Analyse des données
Génie logiciel
Ce que cette expérience va vous apporter
Vous êtes autonome, vous avez le sens du service et de l’analyse, vous êtes impliqué, nous vous offrons une ouverture sur des projets complexes et une rapide évolution de carrière. Vous rejoignez notre business unit à Sophia Antipolis composée d'environ 50 consultants, avec possibilité de télétravail en fonction des sujets.
Nous co-construisons votre trajectoire professionnelle et assurons votre montée en compétences.
Nous nous inscrivons ensemble dans la durée, nous assurons votre montée en compétences et disposons d'une variété de sujets passionnants.
Ce que nous recherchons chez vous
De formation supérieure (Bac+5, école ou université), vous possédez idéalement une première expérience réussie dans ce domaine (débutants acceptés), vous aimez le travail en équipe.
Compétences requises
:
Etape d’analyse : Comprendre l’architecture technique, les sources de données, les objectifs fonctionnels.
Etape de conception : Solution de conception avec un fort centrage sur les pipelines de données et les modèles ML et l’exposition des KPI via API
Mise en œuvre : Après les phases d’analyse et de conception, procéder à a mise en œuvre dans des technologies sélectionnées (Java,Scala,Python,Spark)
Créer un code testé et documenté
Techno : Linux, Shell, Hadoop, Scrum, Python, Spark, Scala
Pourquoi feriez-vous le grand saut ?
Parce que Scalian vous accompagne dans le développement de votre carrière :
Programme d'onboarding complet sur 1 an avec votre manager et votre RH
Programme de formation (Scalian Academy, e-learning, webinaires et formations externes)
Communautés techniques (Squads, Practices) afin de valoriser et développer votre expertise
Événements internes (Afterworks, Awards Dinner, Kick Off, Live Event du COMEX, Stand Up) et externes (participation à des salons et forums spécialisés dans nos domaines d’activités…)
Dispositif d’accélération d’accès à la mobilité interne et à des échanges internationaux type Erasmus
Parce que Scalian favorise la Qualité de Vie au Travail :
Certifications Great Place to Work® et Best Workplaces for Women®
Prime de cooptation, prime vacances, prise en charge par l’employeur de 60% des titres-restaurant, Accord télétravail (jusqu’à 2,5 jours par semaine indemnisés), RTT (dont une partie monétisable), CSE (activités ludiques, chèques-cadeaux, chèques vacances)
Berceaux en crèches inter-entreprises
Don ou réception de jours de congés en cas de difficultés personnelles
Parce que Scalian développe une politique RSE concrète et ambitieuse :
Mobilité durable (indemnité kilométrique vélo, leasing de vélos à assistance électrique)
Actions environnementales (Fresque du Climat, Reforest'Action, Clean Up Day, mécénat ONF)
Postes ouverts aux personnes en situation de Handicap
Diverses politiques de diversité, d’inclusion et d’intégration mises en place
Scalian c’est aussi :
Une entreprise en très forte croissance qui, créée en 1989, compte aujourd’hui plus de 5500 personnes
Des références clients à forte valeur ajoutée auprès de grands industriels français (du CAC40) et internationaux
Un terrain de jeu où l’expertise se conjugue avec audace, liberté d’entreprendre et convivialité
Si vous aspirez à un environnement de travail qui valorise autant votre bien-être que votre développement professionnel,
rejoignez-nous et exprimez pleinement votre talent !
Envie d'élargir le cadre ?
Je suis Liza Djehel, Talent Acquisition Officer.
Si votre CV est retenu, je vous contacte pour un premier échange téléphonique de 15 à 20 minutes.
Nous déterminons ensemble si ce poste est en adéquation avec vos compétences et surtout, avec vos attentes.
L'échange est positif ? Nous convenons d'un entretien de 1h (en présentiel ou en visio) avec Lucas Daunar, Business Manager à Sophia-Antipolis. Cet échange permet de revenir en détail sur vos compétences, vos attentes, de vous présenter le poste plus en détail, et d'évoquer d'autres opportunités.
Nous prévoyons ensuite un rendez-vous technique de 1h (en présentiel ou en visio) avec un de nos responsable technique.
Enfin, nous vous présentons notre proposition d'embauche.
Notre processus de recrutement dure entre 15 et 30 jours
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['40'], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
Data Engineer H/F,Ippon Technologies,Greater Nantes Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=5&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=igIUAipWQogxha%2BnVVMT3g%3D%3D&trk=public_jobs_jserp-result_search-card,"Envie de rejoindre la communauté DATA la plus dynamique de France ?
Notre spécialité est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.
Membre de la Practice Data, le/la futur(e) Data Engineer sera intégré(e) à nos équipes de conseil et sera suivi(e) par un(e) mentor qui l’aidera à monter en compétences.
Votre champs d’expertise :
Intervenir sur les data platforms de nos clients pour développer de nouveaux pipelines de données (ingestion, traitement, exposition).
Travailler en collaboration avec les métiers et les data scientists pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship etc…)
Déployer des infrastructures cloud full
infra-as-code
(Terraform, CloudFormation).
Participer aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).
Capitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne.
Vos connaissances :
Un framework de calcul distribué tel que Spark, Storm, Flink.
Un ou plusieurs langages de programmation (Python, Scala, Java...)
Différents systèmes de stockage de données (SQL ou NoSQL) et bien sûr le langage SQL.
La connaissance de Snowflake est bienvenue ;-)
Un framework de streaming de données tel que Kafka ou Amazon Kinesis.
Une expérience sur les technologies Cloud : AWS, GCP, Azure
Le delivery et les projets en production faisant partie de notre ADN, vous êtes capable de livrer du code de qualité dans des environnements agiles.
De plus en plus de nos projets se font en remote avec des clients du monde entier, il devient nécessaire d’être à l’aise en Anglais.
Ippon technologies c’est aussi :
👍 Bénéficier d'un suivi de proximité réalisé par votre manager technique : points réguliers pour votre suivi en mission, votre formation et votre évolution de carrière
✌️ Rejoindre une entreprise où les valeurs du sport sont nos leitmotiv : dépassement de soi, travail en équipe, bienveillance.
🗒️ Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL
😁 Travailler en pair programming ou avec un.e mentor pour gravir les échelons !
💪 Pouvoir participer à une aventure humaine au sein de notre Fondation Ippon pour réduire la fracture numérique dans le monde !
🤝 Participer à nos apéros et divers évènements internes pour consolider la cohésion d’équipe
Et après ?
Et oui alors ? Que se passe-t-il une fois que vous êtes convaincu d’avoir lu l’offre d’emploi qui vous correspond bien ?
Nous vous proposons de prendre contact et de nous rencontrer !
Les Next Steps :
1 call RH
1 échange RH
1 échange Technique
Si le match est bon des deux côtés : Hadjimé ! Vous vous lancerez sur le tatami Ippon !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform', 'CloudFormation'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),ternair,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ternair-3915757963?position=6&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=A01wsYYAG%2FYIpqiFBcjvNA%3D%3D&trk=public_jobs_jserp-result_search-card,"👨‍🚀 MISSION : 👩‍🚀
En cohérence avec la stratégie d’entreprise et la roadmap data, vous aurez pour principales missions de :
En lien avec l’équipe DevOps, construire, maintenir et faire évoluer la plateforme de données;
Définir et piloter la cohérence de la collecte, la gestion et l’alimentation des données internes et externes, en différents modes : batch, streaming, API (architecture micro-services);
Préparer et mettre en qualité les données pour les rendre disponibles dans les différents environnements de travail (datalake, datawarehouse, datamart);
Vérifier la qualité des données, de leur bonne et régulière exécution ainsi que de leur utilisation adéquate (gestion des coûts);
Travailler en étroite collaboration avec les data analysts, scientists et data stewards et business de l’entreprise ;
En lien avec l’IT et la sécurité, veiller aux règles d'intégrité et de sécurité des données;
Veille technologique.
🧮 Les outils :
Plateforme data : Google Cloud Platform (Big Query, Airflow)
Développement : Github/GitLab, Docker, Terraform, Python
Analytiques : Qlik
Gestion de projet s: Jira, Confluence, Miro, Drive, Docs, Sheets, Slides
🤩 Profil recherché : 🤩
Expérience d'au moins 4-5 ans (après études) en data ingénierie (flux, modélisation, run)
A l’aise avec l’environnement Cloud et les infrastructures digitales
Communiquant, pédagogue et fortes capacités relationnelles
Anglais (à l’écrit)
Rémunération : 42-60 k€ en package selon expérience
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['Big Query'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': ['JIRA', 'Confluence'], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer,Shippeo,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-at-shippeo-3908268236?position=7&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=Qkp3nd%2B7kuHYXRYFNEMI%2Bg%3D%3D&trk=public_jobs_jserp-result_search-card,"Founded in 2014, Shippeo is a French based SaaS company providing supply chain visibility. Shippeo has grown from 70 to 220 employees during the last two years and is continuing to rapidly scale after an additional $40M fundraising round in October 2022.
Shippeo is an exceptionally diverse company with colleagues from 27 different nationalities and speaking 29 languages. With offices throughout Europe, North America and recently Asia, Shippeo provides global coverage to all of our clients.
Our product is composed of a mission critical SaaS web platform (API everywhere), with high traffic inbound/outbound integrations.
Our mission is to anticipate problems and proactively alert end-customers so they can efficiently manage exceptions. We achieve this by collecting and matching millions of theoretical and real data from different stakeholders.
The Data Intelligence Tribe is responsible for leveraging Shippeo’s data from our large shipper and carrier base, to build data products that help our users (shippers and carriers alike) and ML models to provide predictive insights. This tribe’s typical responsibilities are to:
get accurately alerted in advance of any potential delays on their multimodal flows or anomalies so that they can proactively anticipate any resulting disruptions
extract the data they need, get direct access to it or analyze it directly on the platform to gain actionable insights that can help them increase their operational performance and the quality and compliance of their tracking
provide best-in-class data quality by implementing advanced cleansing & enhancement rules
As a Data Engineer at Shippeo, your objective is to ensure that data is available and exploitable by our Data Scientists and Analysts on our different data platforms. You will contribute to the construction and maintenance of Shippeo’s modern data stack that’s composed of different technology blocks:
Data Acquisition (Kafka, KafkaConnect, RabbitMQ),
Batch data transformation (Airflow, DBT),
Cloud Data Warehousing (Snowflake, BigQuery),
Stream/event data processing (Python, docker, Kubernetes) and all the underlying infrastructure that support these use cases.
Qualifications
Required:
You have a degree (MSc or equivalent) in Computer Science.
3+ years of experience as a Data Engineer.
Experience building, maintaining, testing and optimizing data pipelines and architectures
Programming skills in Python and experience with asynchronous event processing (asyncio).
Advanced working knowledge of SQL, experience working with relational databases and familiarity with a variety of databases.
Working knowledge of message queuing and stream processing.
Knowledge of Docker and Kubernetes.
Knowledge of a cloud platform (preferably GCP).
Experience working with workflow management systems such as Airflow.
Desired:
Experience with cloud based data warehouse solutions (BigQuery, Snowflake).
Experience with Kafka and KafkaConnect (Debezium).
Experience with Infrastructure as code (Terraform/Terragrunt).
Experience building and evolving CI/CD pipelines with Github Actions.
Monitoring and alerting on Grafana / Prometheus.
Experience working on Apache Nifi.
Informations supplémentaires
We are looking for talents who share our values:
🚀 Ambition
💙 Care
🎯 Deliver
🤝 Collaboration
Find out more about our values in
Our Culture Book
If you identify with our values and enjoy working in a fast-paced and international environment, Shippeo is just the place for you!
We are committed to fostering diversity and inclusion within our workplace as we value the unique perspectives and experiences that individuals from all backgrounds bring to our team. We are dedicated to providing equal employment opportunities to all candidates, regardless of their background or abilities, and our commitment to inclusion is reflected in our policies, practices, and workplace culture.
We understand that candidates may have unique needs or questions related to disability inclusion. To facilitate this, you can reach our dedicated Disability Advisor at
inclusion@shippeo.com
with any inquiries or requests for accommodations during the application process.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['ML', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data engineer - F / H,United Robotics Group,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=8&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=oVJtK1t11uimHlb7BlJr8w%3D%3D&trk=public_jobs_jserp-result_search-card,"Bienvenue chez
Aldebaran
, leader européen de la robotique
au sein du groupe
United Robotics Group
.
Nous concevons et industrialisons des robots innovants avec une vision sociétale ambitieuse pour façonner un monde plus humain. Depuis 2005, nous sommes à l'avant-garde de l'interaction homme-robot avec des produits emblématiques tels que NAO et Pepper.
Notre dernier-né,
Plato
,
incarne notre engagement envers la technologie de pointe et la sécurité,
fabriqué en France avec des composants européens.
Rejoignez nos équipes multiculturelles et dynamiques pour être au cœur de la révolution de la robotique.
Si vous êtes passionné.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer à façonner l'avenir, nous vous offrons une expérience enrichissante et stimulante.
En tant que membre de notre équipe, vous bénéficierez d'une culture d'entreprise axée sur le sens de ce que nous faisons et valorisant la responsabilité sociale et environnementale.
Chez Aldebaran, nous valorisons l'innovation, la diversité et l'égalité et encourageons chacun.e à être ouvert.e, authentique, courageux.se, responsable et engagé.e.
Finalité du poste
Au sein de l'équipe Cloud-Online Services, le Data engineer intégrera l'équipe Data, responsable du développement des produits destinés à la collecte, aux process et à l'exploitation des données de nos robots.
Il aura pour rôle de définir et d'implémenter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui gèrent les robots du groupe.
Missions principales
Le Data engineer aura pour responsabilités de :
évaluer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,
concevoir et développer des services Data en respectant la spécification fonctionnelle et la méthodologie agile,
agréger et stocker de grandes quantités de données,
mettre en place des solutions de data processing,
intégrer/développer des outils de visualisation de données et analyser les KPI,
développer, tester, sélectionner et mettre en production des algorithmes qui permettent de répondre aux besoins,
réaliser des analyses de données,
mettre en place des tests de charge et fonctionnels pour les solutions Data,
investiguer et corriger les bugs remontés par les utilisateurs,
contribuer à la mise en place de l'infrastructure et outil de déploiement (CI/CD)
Rejoignez-nous pour faire partie d'une aventure passionnante où Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !
Requirements
Pour la bonne exécution des missions confiées, vous témoignez d'au moins 6 ans d'expérience en tant que développeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.
Compétences demandées :
Bonne compréhension des technologies d'infrastructure et de déploiement,
Compétences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,
Bonne compréhension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,
Bonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)
Bonne connaissance et une expérience pratique de Scrum\Scrumban et des méthodes agiles,
Une certification AWS sera appréciée,
Un niveau de français et d'anglais courant est indispensable,
Des expériences dans des environnements fortement internationaux sont un plus
Benefits
Nos principaux avantages :
Une culture du bien-être en entreprise qui a fait ses preuves (budget célébration et moments de convivialité par équipes et directions, restauration collective de qualité, environnement de travail agréable)
Un engagement fort en matière de responsabilité sociale et environnementale (promotion de l'égalité professionnelle, performance de notre plan diversité et inclusion, référent handicap, fresque du numérique)
Une culture du télétravail encadrée de manière appropriée !
Tous nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
Data Engineer H/F,Neosoft,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-neosoft-3877878521?position=9&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=E0uvNqbx3ExrbNpYFJTgWA%3D%3D&trk=public_jobs_jserp-result_search-card,"Tous nos postes sont ouverts au télétravail
Groupe indépendant de conseil en transformation digitale de près de 1800 collaborateurs, Néosoft s’est construit, depuis 2005, sur un modèle qui place l’excellence, le dépassement de soi et la RSE au cœur de sa stratégie.
En nous rejoignant, vous intégrez des communautés d’experts et de talents qui vous permettent de développer vos compétences et d’offrir à nos clients le meilleur accompagnement possible.
Notre savoir-faire s’articule autour de nos 6 domaines d’expertise :
Conseil & Agilité
Cybersécurité
Data
DevOps
Infrastructures & Cloud
Software Engineering
Nous recherchons pour intégrer notre
agence lilloise
un(e)
Data Engineer confirmé(e)
.
Nous aimerions vous voir rayonner au sein de notre communauté DATA (+100 collaborateurs) animée par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients à consolider un patrimoine Data responsable.
🎯
Vos missions :
Après une période d’intégration, en tant que
Data Engineer
, voici à quoi ressembleront vos activités dans des contextes clients Retail ou Banque / Assurance / Finance :
Analyser et s'approprier les cas d'usages
Analyser et valoriser les données du patrimoine
Mettre en place des flux de transformation de données
Réaliser les tests permettant de s'assurer la qualité du delivery
Continuer la mise au point de frameworks data
Créer et développer des modules de déploiement des solutions
Assurer l'industrialisation de moteurs basés sur l'IA
Assurer le niveau de performance des pipelines
Implémenter les outils de monitoring du socles de données
📝
Votre profil :
Nous vous imaginons avec au moins 4 ans d’expériences sur des projets autour de la
Data
, une maîtrise des
bases de données (SQL)
, des outils de transformation de la donnée
(Talend, BigQuery, Airflow)
, et un socle de compétences solides autours des langages
Python, Spark, Scala, Hadoop, Java.
👉
Votre carrière chez Néosoft
Depuis sa création, Néosoft place ses collaborateurs au cœur de sa stratégie. Notre culture pourrait se résumer en un mot : le collectif.
Nos communautés d’experts vous donnent la possibilité d’apprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.
Nous veillons à ce que chacun bénéficie d’un accompagnement de proximité et d’un suivi de carrière personnalisé auprès de votre manager dédié :
1 bilan d’activité trimestriel pour suivre le développement de vos compétences
1 entretien d’évaluation qui a lieu chaque année pour évaluer votre performance et déterminer vos nouveaux objectifs
1 entretien annuel auprès de votre RH dans le but de cartographier vos nouvelles compétences pour échanger sur vos projets professionnels et souhaits de formations
👉
Vos avantages
Formations et développement de l’expertise :
Vous disposez de temps alloué et rémunéré en contribuant au développement de votre expertise technique et de celle du groupe (Participations à des Tech days, animation d’une conférence à l’interne ou à l’externe, rédaction d’articles, rencontres avec nos candidats en processus de recrutement…)
Un abonnement illimité LinkedIn Learning offert
Bien-être au travail :
Un accord de télétravail flexible jusqu’à 100% de télétravail et personnalisable
Un partenariat avec Gymlib qui favorise le sport en entreprise
Des initiatives locales (afterworks, défis sportifs, team buildings, …)
Et bien plus encore :
Parce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive rémunérée dès l’arrivée du collaborateur
En plus de votre salaire : participation, compte épargne temps, actionnariat...
👉
Votre parcours candidat
Notre processus de recrutement se compose de deux étapes clés :
Un entretien de recrutement RH avec un Talent Acquisition Spécialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carrière possibles au sein de notre groupe
Un entretien d’évaluation technique pour réaliser un diagnostic de vos compétences techniques et identifier les compétences sur lesquels poursuivre votre évolution
Vous aurez également la possibilité de rencontrer pour compléter votre processus un acteur de notre pôle Business ou un pair de votre métier pour échanger sur son expérience collaborateur.
Nous avons hâte de vous rencontrer !
A bientôt,
L’équipe Néosoft 🖐
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': ['Salaire'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data & Cloud Engineer (H/F),fifty-five,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-cloud-engineer-h-f-at-fifty-five-3910028674?position=10&pageNum=2&refId=5U0uFfx%2F5Nb5dx3Ih9znKA%3D%3D&trackingId=o9htNm8KQ7fNkBtS4SPmyQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Data & Cloud Engineer
fifty-five est une data-company d'un genre nouveau qui aide les marques à exploiter les données pour améliorer le marketing, les médias et l'expérience client grâce à une combinaison de services de conseil et de technologie spécialisés.
En tant que pilier data et marketing du Brandtech Group, nous offrons des services qui combinent le conseil en stratégie, les services de cloud, le conseil en média et l'expérience client.
fifty-five, c'est plus de 400 experts du numérique. Des digital consultants, des spécialistes du tracking et du média, des ingénieurs et des data scientists, travaillent tous en étroite collaboration pour fournir des conseils marketing de haut niveau et une assistance technique aux marques, dans tout type d'industrie, partout dans le monde.
Partenaire des annonceurs de la collecte à l'activation et l'exploitation des données, nous aidons les organisations à devenir de véritables entités omnicanales maîtrisant l'efficacité de leur écosystème digital et ses synergies avec le monde physique.
Basé à Paris, nous opérons sur 3 fuseaux horaires depuis nos 10 bureaux, situés à Paris, Londres, Genève, Milan, Shanghai, Hong Kong, Shenzhen, Taipei, Singapour et New York. fifty-five attache une importance particulière au bien-être de ses collaborateurs, ce qui lui a permis de figurer dans le classement Best Workplaces France en 2018.
Contexte :
L'équipe d'ingénierie développe et met en œuvre les solutions techniques permettant la réalisation de pipelines de données et l'implémentation de data platform pour nos clients : récupération de datas sur de multiples sources de données (APIs, files, etc.), data cleaning, data processing, automation et monitoring de l'ensemble. L'équipe s'appuie sur des technologies récentes (docker, kubernetes, terraform, notebooks, etc.) et met en place ses projets dans les différents clouds du marché (GCP, Azure, AWS...).
Mission :
Nous sommes à la recherche d'une personne capable de réaliser des projets techniques pour répondre aux besoins de nos clients (par exemple: système de recommandations de produits, détection d'anomalies, ranking). Les activités vont du chiffrage et du sizing technique à la mise en œuvre des architectures, en passant par la revue des spécifications fonctionnelles et la production de code. Le Data & Cloud Engineer sera épaulé par un Lead dans ses missions. Il sera également amené à participer à la R&D et à accompagner les équipes transverses dans la mise en place d'outils de travail internes (librairies pour les data scientists, environnement Notebooks pour les data analysts et data scientists, développement de frameworks sur différents cloud providers, etc.).
Nous souhaitons trouver la bonne personne pour faire évoluer ou créer de nouvelles solutions dans ce cadre. Les missions comprennent aussi bien du prototypage rapide pour des démonstrateurs, que de la production de code robuste qui tourne en production tous les jours.
Compétences et expériences :
2 ans d'expérience en tant que Data Engineer
Maîtrise de Python, SQL
Maîtrise des environnements Cloud. Idéalement certifié GCP, Azure ou AWS
Bonne connaissance de Docker/Kubernetes
Bonne connaissance d'au moins un data warehouse (BigQuery, Snowflake, etc)
Connaissance autour des Notebooks (Jupyter)
A l'aise avec des concepts liés aux APIs (OAuth, REST, etc.)
A l'aise avec les notions d'Infrastructure as Code (Terraform)
Au courant des pratiques GitOps et connaissances des concepts autour du CI/CD
La maîtrise d'un orchestrateur, comme Apache Airflow, est un plus
Esprit d'équipe (collaborer aux tests unitaires, revue de code, partage de code, sprints)
Bon niveau en français et en anglais
A déjà travaillé en mode projet avec des interlocuteurs variés (consultant, data analyst, data scientist)
Une expérience en marketing digital est un plus
Nous proposons :
un bureau au centre de Paris avec terrasse et jardin
un environnement multiculturel avec des collaborateurs aux nationalités multiples (France, Royaume-Uni, Etats-Unis, Chine, Tunisie, Italie et plus)
des projets avec nos bureaux à Londres, Hong Kong, New York, Shanghai, Genève, Shenzhen et Taipei
des TGIF et supers soirées
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer Senior,AXA en France,"Hauts-de-Seine, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-senior-at-axa-en-france-3884386043?position=1&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=bodYRty8CHMU9PuiSRGKTA%3D%3D&trk=public_jobs_jserp-result_search-card,"Environnement
En tant que
Senior Data Engineer F/H
, vous allez contribuer directement aux projets des directions métier (ex : fraude santé, multiéquipements, pricing IARD, optimisation du lead management, fragilité auto, …) d’AXA France et à la construction du socle technique Big Data.
Vous allez intégrer une équipe d'une dizaine de personne composée de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus métier de la Direction Transformation Digital Tech et DATA (DT2).
La Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :
- Une organisation agile en feature teams : tribus, guildes, squads
- Des projets sur des applications innovantes à fort trafic (web, mobile…)
- Des méthodologies craft (TDD, BDD, clean code, code review…) et DevOps
- Une communauté de partage de bonnes pratiques (BBL, dojo, meetup, conf…)
Votre rôle et vos missions
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le data lake
Les exploiter pour atteindre la finalité business (exposition de business view, réintégration des résultats dans le SI, service de scoring, …)
De travailler à la création du socle technique Big Data et industrialiser le cycle de développement de l'équipe
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Votre profil
Vous justifiez de plusieurs expériences significatives (+ de 5 ans) sur du
développement big data, en particulier sur du PySpark.
Compétences techniques :
Connaissances avancées en développement en
PySpark
(Spark avec le langage Python)
Maitrise de l'environnement
Microsoft Azure
Connaissances avancées d'outils de BI comme
PowerBI
Compétences transverses :
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Expérience en mode de delivery Agile (Scrum, Kanban, etc...)
Driver et accompagner des Data Engineer junior sur les aspects technique
Et Idéalement :
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA
Maitrise des Traitements Big Data en mode Streaming
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
Qui sommes nous ?
AXA est un des leaders de l’assurance et de la gestion d’actifs dans le monde.
Nous aidons nos 108 millions de clients à traverser les petites et grandes difficultés de la vie.
Chaque jour, nous agissons ensemble pour inventer la meilleure manière de les protéger et voulons donner à chacun les moyens de vivre une vie meilleure.
Un challenge qui donne le sourire et envie de se lever le matin !
Chez AXA, nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C’est pour cette raison que nous menons une politique RH engagée qui favorise la diversité, qui préserve l’équilibre vie privée-vie professionnelle et accélère le développement des compétences et des carrières.
Ainsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une véritable culture d’expertise, accélérant le développement des compétences de chacun et proposant une rémunération attractive.
Pourquoi nous rejoindre ?
Vous êtes porteur d’idées et d’initiatives innovantes ? Vous proposez des solutions et êtes au service du client ? Faites partie de notre grande famille en rejoignant
Un leader mondial offrant des opportunités de carrières intéressantes
Une entreprise qui donne une place de choix à l’innovation, à l’initiative et aux actions solidaires (notamment via l’association AXA Atout Cœur)
Un environnement inclusif à tous les niveaux (mixité, handicap, initiatives pour favoriser l’insertion des jeunes, orientation sexuelle, etc.)
Un accès à de multiples avantages (congés, temps partiel, télétravail, etc.)
Un cadre stimulant, qui permet de rencontrer des collaborateurs performants et d’enrichir ses compétences
Victime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Teams'], 'Other': ['DevOps', 'Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Junior', 'Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer,Beelix,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-beelix-3865239426?position=2&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=At6zkzF%2BwCiC%2FjDzU74g7g%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes-nous ?
Depuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants:
🚗Automobile
⚡Energie
📡Médias & Télécoms
👗Luxe & Retail
💶 Banque, Finance & Assurance
✈️Défense
Aujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.
Dans le cadre de notre développement, nous recherchons un Data Engineer en Île-de-France.
Quelles missions au quotidien ?
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le Datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le Datalake
Les exploiter pour atteindre la finalité business (exposition de Business View, réintégration des résultats dans le SI, service de scoring, …)
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Accompagner les Data Engineers sur son périmètre pour garantir la qualité des livrables
Expertise souhaitée
Expertise en SPARK et PySpark
Expertise sur Databricks
Une expérience sur un cloud provider public comme Azure (idéalement), AWS, ou GCP
Connaissances avancées d'outils de BI comme PowerBI (idéalement) ou Spotfire
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Etre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT
Maitrise des Traitements Big Data en mode Streaming
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
A propos de vous ?
Diplômé d'une école d'ingénieurs ou équivalent
Au moins 3 ans d'expérience en tant que Data Engineer
Expérience en mode de Delivery Agile (Scrum, Kanban, etc.…)
Vous avez un bon niveau d’anglais tant à l’écrit qu’à l’oral
Pourquoi nous rejoindre ?
Un suivi et un accompagnement au quotidien
Un organisme de formation certifié Qualiopi, un abonnement linkedin learning pour chaque salarié et des partenariats avec des spécialistes pour d’autres expertises
De nombreux événements : Afterworks, Communautés métiers, Happy talks…
une Expérience personnalisée basée sur vos besoins grâce au Prédictive Index
Notre package « unBeelievable » : 100% du titre de transport, Tickets restaurants, CSE, Prime de participation ...
Nombreux évènements (afterworks, sport, etc) et des communautés métiers dynamiques
Le processus de recrutement ?
Échange téléphonique (15 min)
Entretien 1 RH pour apprendre à vous connaître
Entretien 2 avec votre futur N+1 pour appréhender la relation managériale
Entretien 3 avec un Responsable commercial pour avoir la vision stratégique
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer,CGI,"Niort, Nouvelle-Aquitaine, France",https://fr.linkedin.com/jobs/view/data-engineer-at-cgi-3902057928?position=3&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=IMZKLUIu9KltYvuQgJMBZg%3D%3D&trk=public_jobs_jserp-result_search-card,"Description de poste
Big Data, Data Science, Data analyse, Data architecture ... Ça n’a pas de secret pour vous ?
Que vous commenciez votre carrière professionnelle ou que vous soyez spécialiste de l’une de ces disciplines, intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.
Si vous souhaitez intégrer nos équipes à Niort et accompagner les plus grands acteurs du secteur des Assurances, cette annonce est susceptible de vous intéresser.
En tant que Data Engineer, vous serez responsable de la conception, du développement, de la gestion et de l'intégration des systèmes basés sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop. Ce rôle implique la mise en place d'architectures évolutives et hautement disponibles pour répondre aux besoins de traitement et de stockage de données de l'entreprise.
Fonctions et responsabilités
Vos responsabilités seront les suivantes:
-Maintenir et développer des solutions basées sur les services AWS pour le stockage, le traitement et l'analyse de données
-Utiliser les services AWS appropriés tels que Amazon EC2, S3, RDS, Lambda, etc., pour répondre aux exigences du projet.
-Créer et maintenir les configurations Terraform pour la gestion de l'infrastructure en tant que code (IaC) sur AWS
-Participer à la maintenance et à la mise en place d'environnements OpenShift pour l'hébergement d'applications et de services
-Gérer et administrer les clusters Kafka pour garantir la disponibilité, la performance et la sécurité du système de messagerie
Participer à l’assistance utilisateurs sur les briques de la plateforme Hadoop Cloudera Data
-Travailler avec les projets et les devOps pour assurer un traitement efficace des données
En rejoignant CGI, vous bénéficiez notamment d’une offre complète de formations (techniques, métiers, développement personnel,…), de flexibilité grâce à notre accord télétravail (jusqu’à 3 jours de télétravail par semaine), d’une politique de congés avantageuse (27 jours de congés payés, RTT, congés ancienneté et enfant malade,…) et d’un package d’avantages intéressant (régime d’achats d’actions, participation, CSE,...).
Qualités requises pour réussir dans ce rôle
Ayant une première expérience en tant que Data Engineer, vous avez une première expérience relative aux points suivants:
-Développement et intégration sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop
-Connaissance avancée de l'administration Kafka, y compris la configuration, la gestion et la résolution des problèmes
-Mise en œuvre de l'infrastructure en tant que code à l'aide de Terraform
-Bonne compréhension des bonnes pratiques de sécurité pour les systèmes cloud, les clusters Kafka et les plateformes Hadoop
CGI est un employeur inclusif et attentif aux candidatures des personnes en situation de handicap, à l’évolution de carrières des hommes et des femmes et au bien-être de nos salariés LGBT+. Dans un souci d’accessibilité et de clarté, le point médian n’est pas utilisé dans cette annonce. Tous les termes employés se réfèrent aussi bien au genre féminin que masculin.
Ensemble, en tant que propriétaires, mettons notre savoir-faire à l’œuvre.
La vie chez CGI est ancrée dans l’actionnariat, le travail d’équipe, le respect et un sentiment d’appartenance. Chez nous, vous pourrez exploiter votre plein potentiel parce que…
Nous vous invitons à devenir propriétaire dès le jour 1 alors que nous travaillons ensemble à faire de notre rêve une réalité. C’est pourquoi nous nous désignons comme associés de CGI, plutôt que comme employés. Nous tirons profit des retombées de notre succès collectif et contribuons activement à l’orientation et à la stratégie de notre entreprise.
Votre travail crée de la valeur. Vous élaborerez des solutions novatrices et développerez des relations durables avec vos collègues et clients, tout en ayant accès à des capacités mondiales pour concrétiser vos idées, saisir de nouvelles opportunités, et bénéficier d’une expertise sectorielle et technologique de pointe.
Vous ferez évoluer votre carrière en vous joignant à une entreprise bâtie pour croître et durer. Vous serez soutenus par des leaders qui ont votre santé et bien-être à cœur et qui vous permettront de saisir des occasions afin de parfaire vos compétences et élargir les horizons.
Joignez-vous à nous, l’une des plus importantes entreprises de conseil en technologie de l’information (TI) et en management au monde.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['OpenShift'], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['1'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,PROXIAD,Greater Nice Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-at-proxiad-3901014428?position=4&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=akF4kKw%2FOHmdQHXCvd6JKA%3D%3D&trk=public_jobs_jserp-result_search-card,"Contexte
En tant que Data Engineer, votre rôle consistera à réaliser la conception, le développement, les tests unitaires, la qualification, l'intégration continue et la mise en production d'évolutions sur les projets du pôle produits scoring.
Ces projets Big Data GCP ont pour objet de développer des traitements de croisement de données, exploration data en mode agile (scrum et Safe), industriel (respect de normes), sur l'environnement Google Cloud Platform.
1 : Conception
Spécification et conception d'une solution se basant sur les développements existants.
Mettre en question les choix techniques dans le but de concevoir un logiciel répondant au mieux à la demande au moindre coût et avec la qualité demandée.
Conception de l'expression de besoins, de la réponse à l'expression de besoins à l'aide des besoins métiers remontés par le Product Owner.
2 : Réalisation
Développement de nouvelles fonctionnalités sur les composants des applications du pôle produits scoring en environnement GCP (DataProc, GCS, BigQuery, Airflow...)
Tests des développements réalisés
Revue de code des développements des autres développeurs
Mise en production via CICD des développements
3 : Suivi du RUN applicatif
Prendre en charge avec les autres membres de l'équipe le RUN des applications du pôle produits scoring. Cela inclus les tâches de rapport quotidien, la gestion des problèmes applicatifs, le soutien aux utilisateurs.
Compétences attendues
Maîtrise opérationnelle :
Confluence
Implémentation de l’intégration continue (Utilisation de la chaine CI/CD existante )
Connaissance des principes DevOps
Jira
Anglais (lu, écrit)
Maîtrise avancée :
Elaborer un cahier de recette
Big Query
Spécifications technique et documentation
Développement :Python, SQL, Scala, Javascript, GitLab
Expertise
GCP : Expérience significative en tant que Data Engineer Cloud. Mise en pratique des produits GCP et en particulier Dataproc, Big Query, composer, workflow, PubSub
Développement : Java
Compréhension générale des travaux BigData et du profiling
Informations complémentaires :
Télétravail 2 jours par semaines
Rémunération aux alentours des 45K€
Expérience requise : 6 ans
Localisation : Mougins
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go', 'JavaScript'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery', 'Big Query'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Confluence'], 'Other': ['DevOps', 'Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
CDI - DATA ENGINEER SPARK SCALA JUNIOR - H/F,ITNOVEM.,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/cdi-data-engineer-spark-scala-junior-h-f-at-itnovem-3899544280?position=5&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=Z77%2B2%2BZ%2F6%2B2HP48vuwZ0vg%3D%3D&trk=public_jobs_jserp-result_search-card,"ITNOVEM, qui sommes-nous ?
Filiale technologique du groupe SNCF, intégrée à la Direction du Digital et des Systèmes d’information, Itnovem
.
se positionne comme expert de l’Internet Industriel. Porteuse de grands projets de la révolution digitale, notre société est en constante recherche de profils pour rejoindre la grande aventure de l’Internet des objets, de la data science et de l’accompagnement des projets digitaux.
Qu’il s’agisse de maintenance prédictive, d’aide à la décision sur la maintenance des infrastructures, de gare 4.0, d’usine du futur, ou de sécurisation des assets, nos équipes font valoir à la fois une expérience métier et une expertise technique sans cesse renouvelée, dans le respect des valeurs du groupe :
Excellence
,
Innovation
,
Collectif
,
Agile
,
Engagement.
CONTEXTE
Au sein du pôle Factory Data & IA et dans le cadre de la montée en charge des projets, nous sommes à la recherche d'un·e data engineer Scala/Spark junior.
Rattaché·e aux équipes Data Engineering et en collaboration avec les membres de l’équipe, son rôle sera de contribuer aux projets data sur stack Scala/Spark et à l’amélioration de l’outillage et des process internes.
Le recrutement intervient dans le cadre de la création d’un plateau projet dédié à l’activité TGV sur Nantes.
MISSIONS
Participer au développement des projets data sur stack Scala/Spark
Etre acteur de la mise en place de bonnes pratiques de communication entre les plateaux nantais et parisiens
Avec l’appui de l’équipe, être impliqué·e dans la roadmap technologique (pratiques, outils) et de l’amélioration continue du périmètre Scala/Spark
Contribuer proactivement à la qualité et aux compétences des équipes Data Science et Engineering : veille techno, capitalisation…
LE PROFIL RECHERCHE
Compétences métiers & outils :
Expérience professionnelle (alternance, stage) ou académique sur le langage Scala et le développement d’applications Spark
Connaissances autour du SQL (principes, langage, modélisation)
Appétence sur les aspects fonctionnels et métiers d’un projet
Notions de CI/CD (notre stack : Maven, Gitlab, Jenkins, Artifactory, Ansible)
Compétences transverses :
Intérêt prononcé pour le software engineering
Aisance relationnelle
Proactivité et clarté dans la communication
Rigueur et organisation
Force de proposition
Bonne communication écrite et orale
Expériences et formations
Titulaire d’un bac+5 spécialisé génie logiciel / développement ou expérience équivalente.
Vous venez d’obtenir votre diplôme ou occupez déjà votre premier poste dans le domaine du développement de pipelines Data.
Localisation
Poste basé à Saint Denis, possiblement à Lyon
Télétravail jusqu’à 3 jours par semaine.
D’autres raisons de rejoindre ITNOVEM !
🚀 En tant que filiale SNCF, des opportunités de carrières internes vous sont offertes.
📚 ITNOVEM croit en la formation continue de ses collaborateurs et leur donne l’opportunité de s’inscrire à une formation par an minimum.
🚊 Vos titres de transport sont pris en charge à hauteur de 75%.
🍽️ Via la carte titres-restaurant Swile, vous bénéficiez de 9,25 € par jour dont 60% pris en charge par ITNOVEM.
💻 Chez ITNOVEM, vous bénéficiez jusqu’à 3 jours de télétravail par semaine.
🏖️ ITNOVEM vous permet de profiter de 28 congés et de 16 RTT pour les cadres et 10 pour les non-cadres. Par ailleurs, 2 des 3 jours de congés pour enfant malade sont rémunérés.
👫 La mise en œuvre de l’égalité professionnelle femmes/hommes est primordiale chez ITNOVEM. A chaque nouvelle embauche, l'entreprise s'engage à proposer une rémunération équivalente tant aux femmes qu'aux hommes.
♻️ ITNOVEM incite tous les collaborateurs à trier leurs déchets et les gobelets ont été bannis. Par ailleurs, chaque année, ITNOVEM participe à « La grande collecte », une initiative SNCF qui permet de collecter les PC devenus obsolètes en leur offrant une seconde vie
Show more
Show less","{'ProgLanguage': ['Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['CI/CD'], 'FrSoftSkills': ['Communication', 'Collaboration', 'Organisation'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Initiative']}","{'JobDetail': ['Junior'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
Junior Data Engineer (H/F/N),Ekimetrics,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/junior-data-engineer-h-f-n-at-ekimetrics-3903416527?position=6&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=rKkK5yorfgP2vq5BIs0%2BJQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Ekimetrics
est leader en data science et fournisseur de solutions AI. Depuis 2006, nous utilisons la data science au service de l’optimisation de performance marketing, business, et de la transition vers une performance plus durable.
Si vous êtes passionné.e de data, ou de technologie en général, et que vous avez envie d’être acteur.rice de votre avenir professionnel, votre place est sûrement chez Ekimetrics !
📊Ekimetrics, c’est:
• 400 expert.e.s en data science
• 1000 projets divers et variés pour plus de 350 clients
• 4 bureaux : Paris, Hong Kong, Londres et New York
• 1 milliard de $ de profits générés pour nos clients depuis 2006
• 7000 tonnes de CO2 évitées pour nos clients en 2022
🌱Chez Ekimetrics, nous avons l’ambition d’accompagner nos clients à repenser leur business model, en réconciliant performance économique, environnementale et sociale, grâce à la data science.
C’est pourquoi nous avons en interne toutes les compétences nous permettant de répondre aux besoins de nos clients: Product Managers, Product Designers, Data Architects, Data Engineers, DevOps Engineers, Data Scientists.
Pourquoi recrutons-nous ?
En tant que Data Engineer, vous serez impliqué dans des projets stimulants avec des clients internationaux de premier plan dans des industries diverses, en construisant des solutions analytiques sur mesure pour répondre aux enjeux de nos clients. Vous travaillerez en équipe, avec d'autres consultants Ekimetrics (data engineers, data scientists, software engineers) sur 1 ou 2 projets simultanément. Vous bénéficierez de nos partenariats technologiques et d’une offre de formation pour vous accompagner dans votre montée en compétences.
Plus particulièrement vos responsabilités seront de
:
• Concevoir et développer des solutions permettant de collecter et préparer la donnée ;
• Implémenter et industrialiser des pipelines de données dans des environnements Cloud (Azure, GCP, AWS, Databricks, Snowflake) ;
• Développer des outils destinés à faciliter l’exécution et le déploiementdes pipelines de données (CICD, DevOps, MLOps) ;
• Approfondir vos connaissances en GenAI, Machine Learning, MMO ;
• Participer aux activités de R&D. (Veille, formations, animation de Meetups, Hackathons, etc.)
Le profil et les compétences recherchées
:
• Bac+ 5 Ecole d'ingénieur ou Équivalent ;
• Première expérience sur des sujets Big Data (Projet ou expérience professionnelle) ;
• Connaissances avancées en base de données et en développement (Python, SQL, Spark) ;
• Expérience dans un environnement Cloud ;
• Connaissances avancées en acquisition de données ;
• Appétence pour la Data Science.
🤝 Pourquoi nous rejoindre ?
Rejoindre Ekimetrics, c’est intégrer une entreprise dont les valeurs s’appliquent au quotidien :
• Evoluer dans un environnement entrepreneurial et non traditionnel (
#curiosité)
• Être capable de donner et recevoir du feedback pour s’améliorer en continu (
#excellence
)
• Se former dès son arrivée et en continu grâce à une expérience apprenante unique, riche de nombreuses ressources (internes, externes, live et digital) alliant savoirs techniques, savoir-être et savoir-faire (
#transmission
)
• Faire partie d’une communauté accueillante et soudée(
#plaisir
)
• Imaginer des solutions inattendues & sortir de sa zone de confort (
#créativité
)
En 2023, Ekimetrics a obtenu le statut d’entreprise à mission qui témoigne de notre ambition forte en matière de RSE. Notre raison d’être: Faire de la data science et de l’intelligence artificielle l’accélérateur de la transformation durable des organisations.
Nous sommes également certifiés Great Place to Work© en France, au Royaume-Uni et aux Etats-Unis, et notre bureau de Hong Kong a reçu le prix Best Companies to Work for in Asia 2023©.
🤩 Vous aurez accès à…
• Au catalogue de formation Eki.Academy qui contient des programmes qui vous feront monter en compétences sur nos solutions et nos métiers, des parcours apprenants sur notre plateforme digitale ainsi que des programmes dédiés à nos enjeux prioritaires, dont la sensibilisation aux sujets environnementaux avec la Climate School ;
• Une vie sportive, artistique, musicale, ludique, caritative et engagée : de notre salle de sport privatisée à nos expositions d’art, en passant par des jeux vidéo et des concerts, ou encore les défis RSE sur la plateforme Vendredi. Toutes ces initiatives sont portées par nos Eki.People ;
• De nombreux évènements et séminaires pour rester proche de votre communauté ;
• Des locaux modernes dans un quartier dynamique au cœur de Paris (Grands boulevards) ;
• Une politique de télétravail flexible.
🔄Notre processus recrutement :
🔸Un entretien RH avec un.e recruteur.se
🔸Un test technique ou
peer-to-peer
interview selon profil
🔸Une étude de cas avec un.e consultant.e
🔸Un entretien final avec un.e Manager ou Partner
Nous serions ravi.e.s de vous donner de plus amples informations lors d’un entretien et attendons votre candidature avec impatience!
En tant qu’employeur, Ekimetrics offre à tous les mêmes opportunités d’accès à l’emploi sans distinction de genre, ethnicité, religion, orientation sexuelle, statut social, handicap et d’âge. Ekimetrics veille à développer un environnement de travail inclusif qui reflète la diversité dans ses équipes.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'ML', 'Machine Learning', 'Cloud'], 'FrSoftSkills': ['Créativité', 'Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer confirmé (H/F),BforBank,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-confirm%C3%A9-h-f-at-bforbank-3918327555?position=7&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=EDXCCYjnmeSErR6yzcCRGQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Sur le modèle d'une
""Tech company"",
BforBank place
l'humain et le digital
au cœur de sa transformation. Notre mission,
offrir à nos clients une expérience bancaire incomparable
pour répondre à leurs besoins et usages mobile. 🌟 📱
Rejoindre BforBank c’est
rejoindre une équipe engagée
dans un
grand projet de développement stratégique en France et en Europe.
Nous sommes aujourd’hui 350 passionné(e)s et
recherchons nos talents pour construire la banque de demain
. 🚀
Nous croyons en la force du collectif, chaque jour rassemblés autour de nos valeurs, de simplicité, d'optimisme et d’engagement, encourageant chacun à oser, essayer et accepter d’échouer.
🎯 Au sein de la Direction Technologie, la Data Factory a pour objectifs de piloter, définir, déployer et opérer les meilleures solutions technologiques répondant aux cas d’usage data et d’automatisations de processus de la banque au travers de plateformes. Également, la Data Factory contribue au développement des produits, à la cristallisation et à la diffusion des pratiques au sein des Squads BforBank sur les usages data dans la banque.
Tu rejoindras une squad en charge de résoudre des problématiques métiers en créant des solutions applicatives utilisant les données, des data products, avec pour finalités la prise de décision via des moteurs de calcul ou des dashboards, la création de flux réglementaires, la création de data layer ou de reportings.
🚀 Tes missions principales sont les suivantes :
· Participer aux analyses, études d’impacts et cadrage techniques
· Concevoir des solutions en respectant les bonnes pratiques d’architecture data et développement
· Réaliser le développement de nouveaux data products et assurer la maintenance évolutive et corrective des data products existants
· Rédiger la documentation technique des data products
· Assurer un support aux testeurs
· Reporter de ton activité à ta Squad et travailler dans une démarche d’efficacité collective
Concrètement tu seras amené(e) à produire les livrables suivants :
· Réaliser du code applicatif à l’état de l’art sur notre nouvelle Data Platform
· Créer des data layer et des rapports sur notre outil de Data Visualisation
· Rédiger les documentations techniques liées à ta solution, incluant le modèle de données, les procédures, l’ordonnancement
Ce que tu maîtrises :
· Maitrise des services managés de GCP (BigQuery, dataproc, dataflow, CloudSQL …)
· Maitrise du langage Python, Pandas, Spark
· Maitrise de la modélisation de base de données et du langage SQL
· Maitrise d’une chaine CI/CD (GitLab…)
· Bonne connaissance de Kafka
· Bonne connaissance d’un outil d’intégration de données type ETL (Informatica…)
· Connaissance de l’infra as code (Terraform)
· Connaissance d’un outil de reporting (Looker, BO…)
🤝 Ce poste est fait pour toi si :
· Tu es passionné(e) par la Data et leurs usages
· Tu es orienté résolution de problème, est curieux(se) et force de proposition
· Tu apprécies le travail en équipe
· Tu as un bon relationnel et est rigoureux(se)
· Tu as une bonne capacité d’analyse et rédactionnelle
· Tu t’adaptes rapidement aux changements
🎓
Formation :
Tu es diplômé(e) d’un master en école de commerce, école d’ingénieur ou équivalent.
Chez BforBank nous recherchons avant tout des compétences. Tu ne disposes pas du diplôme requis mais as des expériences équivalentes ? N'hésite pas à postuler !
💼
Expérience :
Expérience confirmée de 3 ans en tant que Data Engineer.
En rejoignant BforBank tu trouveras…
· Un projet ambitieux de transformation digitale et culturelle à l’échelon européen, terrain d’innovation et d’ouverture d’esprit
· Une organisation apprenante, proposant un large choix de formations toute l’année, et qui favorise l’échange avec les autres marques du Groupe
· Une promo RSE multi-métiers qui fait évoluer en continu les actions de BforBank vers une banque plus responsable
· Une organisation du travail en mode Agile, impliquant un degré élevé de collaboration et d'autonomie tout en travaillant avec un groupe de pairs diversifiés.
· Une Direction Technologie en pleine expansion, porteuse de nombreux défis stratégiques
Mais aussi…
De 2 jours à 5 jours de télétravail modulables par semaine, dans la limite de 84 jours par an (frais de fonctionnement pris en charge)
25 jours de congés + 16 jours de RTT
80% du coût de la mutuelle d’entreprise pris en charge / couvert
Avantages collaborateurs Crédit Agricole : taux et tarifs préférentiels
Des frais de transports remboursés à 75%
Un restaurant d’entreprise
Des douches pour les sportifs et un tarif avantageux auprès d’une salle de sport toute proche
📍 Le poste est basé à La Défense, dans des locaux flambant neufs !
BforBank s'engage à garantir l'égalité des chances aux candidats car nous sommes convaincus de la richesse apportée par la diversité et l'inclusion dans nos équipes.
Rencontrons-nous !
Le processus de recrutement se déroule en 4 étapes :
🧑🏼‍💻
Call de 30 minutes avec notre équipe Talent Acquisition
Echange avec le Data Factory Manager et notre équipe Talent Acquisition (présentiel)
Echange avec une personne de l’équipe avec qui tu seras amené à travailler (visio)
Echange avec le CTO (visio ou présentiel)
Notre processus de recrutement dure en moyenne 3 semaines et l’équipe Talent Acquisition se tiendra à ta disposition pour te donner un maximum de visibilité sur l’avancée du process.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['Pandas', 'R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
(Senior) Data Engineer,Mirakl,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-mirakl-3904071960?position=8&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=yt%2BOLeejTmunadfgqcN3%2FQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Mirakl, leader et pionnier de l’économie de plateforme, propose aux entreprises une suite unique de solutions leur permettant de transformer significativement leur e-commerce afin d'accélérer de façon durable et rentable leur croissance. Depuis 2012, Mirakl accompagne les entreprises B2C et B2B avec la technologie la plus avancée, sécurisée et évolutive leur permettant de digitaliser leur activité et d'élargir leur offre via la marketplace ou le dropship, faciliter la gestion des catalogues et des paiements de leurs fournisseurs pour plus d'efficacité, offrir une expérience d'achat personnalisée à leurs clients, et augmenter leurs profits grâce au retail media. Basée à Paris et Boston, Mirakl est certifiée Great Place to Work.
A propos de Mirakl Labs
Nos équipes techniques et produits, nommées Mirakl Labs, sont principalement réparties entre nos 2 hubs situés à Paris et à Bordeaux. Elles collaborent au quotidien afin d'adresser les problématiques de nos clients et utilisateurs en répondant à différents challenges liés aux nouvelles fonctionnalités, à la scalabilité, la sécurité et l’ergonomie…
Elles opèrent en mode agile et s'organisent en Squads composées d'un Squad Lead, de 5 développeurs, d'un Product Manager et d'un QA. Chaque Squad est spécialisée sur un scope fonctionnel afin de concevoir et réaliser de nouvelles features, leurs évolutions et des APIs (avec un découpage en micro-services). Nos équipes Infrastructure, Architecture, Sécurité, Documentation, Product Design, Data et Support opèrent en transverse en apportant leur expertise et de la cohérence sur l’ensemble des produits.
Toutes les équipes sont responsables de leur périmètre et chacun des collaborateurs apporte son expérience et ses idées. Innovation, feedback et implication dans les prises de décision sont au cœur de notre philosophie.
Et pour favoriser ce partage avec d’autres passionnés, nous sommes sponsors, speakers, et hôtes de différents événements, meetups, et associations de la scène Tech en France. Au cours des dernières années, nous avons participé à des événements tels que Devoxx, ReactEurope, ProductConf et Flupa UX Days.
A propos du job
La solution SaaS Mirakl est le moteur des marketplaces des plus importants e-commerçants à travers le monde. Cette solution gère et produit de gros volumes de données qui présentent des challenges extrêmement intéressants pour les spécialistes de la donnée (produits, commandes, clients, niveaux de stock, prix, messages, appels API, données de navigation, séries temporelles, données géolocalisées etc.).
En tant que (Senior) Data Engineer au sein de l’équipe Data Mirakl, vos principales missions seront de :
contribuer à l'enrichissement de la Data Platform (ETL)
améliorer la robustesse de nos pipelines de production pour nos applications Machine Learning (inférence real time etc.)
Intégré(e) dans une équipe de spécialistes de la donnée (data engineers, machine learning engineers, data scientists, data analysts), vous êtes un des acteurs clés pour garantir la place de Mirakl comme solution dominante sur son marché.
Notre stack et nos outils
Apache Spark, Kafka, AWS, Databricks, Python, Airflow, Mlflow, Tensorflow, Delta lake, Superset, Kubernetes, Redshift, SQL, Terraform, Ansible
Au quotidien, vous allez :
Participer à la définition et à l’implémentation d’une architecture performante, robuste, scalable et aux coûts maîtrisés pour nos applications Spark ainsi que pour nos pipelines de production de Machine Learning (évaluation des feature stores, refactoring de DAG Airflow)
Accompagner les Data Scientists lors de leur mise en production (relecture de code, pair programming) et mettre en place les best practices
Optimiser et améliorer la CI/CD de l’équipe en collaboration avec l’équipe SRE
Assurer la montée en compétence des membres de l’équipe sur les sujets de MLOps et Data Engineering
Réfléchir à la meilleure façon d'intégrer les données Google Analytics dans la data platform
Partager ses connaissances et présenter les travaux devant toutes les équipes Labs
Ce qu’on peut vous apporter :
Des projets data driven, divers et variés (traitements massifs d’images, de textes, time series etc.) pour des produits différents de Mirakl
Une culture orientée sur la veille technologique
Des projets qui ont un vrai impact business devant être déployés sur des centaines de clients dans un contexte multilingue
Quelques exemples de sujets en cours :
Enrichissement des données produit à partir des images et des descriptions
Modération automatique des produits
Mapping automatique des données produit
Identification des produits à fort potentiels
Détection de comportements frauduleux
Sentiment analysis sur les messages échangés entre clients et vendeurs et dans les évaluations
Détermination de prix optimaux
Monitoring de la qualité de service des vendeurs
Des applications d’inférence en synchrone de nos modèles de ML
Vous aimerez ce job si :
Vous êtes passionné(e) par la data et les technologies modernes permettant d'en tirer partie
Vous vous intéressez à la data science et avez des connaissances générales sur les algorithmes de Machine Learning
Vous avez un background en développement et avez évolué dans un environnement Data
Vous avez a minima 4 ans d’expérience en environnement Machine Learning et/ou Data
Vous avez mis en production avec succès des applications Big Data faisant appel à du Machine Learning, du NLP, du traitement d’images dans des projets d'envergure, à fort volume de données
Votre maîtrisez Python, êtes un pro des frameworks data de la fondation Apache et êtes à l'aise dans un environnement AWS
Vous maîtrisez au moins un outil d’orchestration (Airflow, Data Pipeline ou tout autre outil similaire)
Vous présentez vos travaux de manière simple et accessible
Vous faîtes preuve d'un bon relationnel et vous aimez mentorer des collaborateurs
Vous parlez couramment anglais et français
Les plus pour le poste :
Vous avez une expérience significative dans le domaine du e-commerce
Vous avez déjà mis en place un Data Lake, Data Warehouse ou une Data Platform
Vous avez déployé des applicatifs en environnement Kubernetes
Vous avez mis en place des pipelines d'ingestion de données avec une approche CDC à l'aide de Debezium ou autre
Vous maîtrisez Java/Scala
Mirakl est engagée en faveur de la diversité, de l’égalité des chances et de l’inclusion. Nous célébrons nos différences car nous sommes convaincus que les qualités visibles et invisibles de chaque Mirakl Worker sont une source de force et d’innovation. Dans le cadre de cet engagement, nous étudions toutes les candidatures sans distinction de : genre, ethnicité, religion, orientation sexuelle, handicap, âge ou toute autre caractéristique protégée par la loi.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': ['TensorFlow'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible', 'Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Machine Learning', 'CI/CD'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Engineer | Python - Azure | IA & Machine Learning  | Paris ou Remote Partiel,Octopus IT - Expert du recrutement tech,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-python-azure-ia-machine-learning-paris-ou-remote-partiel-at-octopus-it-expert-du-recrutement-tech-3664568765?position=9&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=VOh7imz%2B%2FpNP0UBJ9b97qQ%3D%3D&trk=public_jobs_jserp-result_search-card,"La société
Créée il y a plus de 2 ans, cette startup est la première base de connaissance intelligente dédiée aux services clients. Leur mission ? En finir avec la frustration lorsque l'on contact un Help Desk.
Pour cela, elle propose aux entreprises la possibilité de délivrer une expérience client d'exception : rapide et de qualité. Grâce à leur moteur de recherche intelligent, cette entreprise est capable de centraliser toute la connaissance interne de l'entreprise (procédures, produits, modes opératoires, etc.) et la diffuse intelligemment dans les outils de production des conseillers de service client.
Résultat :
Plus besoin de chercher l'information
Des réponses instantanées et de meilleures qualitées
Une autonomie totale des collaborateurs
Après une croissance fulgurante, elle a su séduire à la fois de nombreuses scale up (Luko, OpenClassrooms, Japhy...) et grands groupes (BNP Paribas, La Poste, Fnac Darty...).
Après le recrutement de leur Lead Data (réaliser ensemble) et suite à l'annonce de leur levée de 2,5M€ pour tripler la taille de ses équipes, le but est maintenant de s'imposer très vite comme la base de connaissance de référence en France et en Europe. Pour ce faire, nous recherchons un Data Engineer.
Le poste
En travaillant main dans la main avec le Lead Data, ta mission sera de développer et de maintenir des flux de données complexes et robustes. La donnée étant au coeur de l' entreprise, dans le produit comme dans la stratégie, tu seras amené à travailler avec un panel d’interlocuteurs très variés :
Data Scientists sur des sujets comme le monitoring des modèles de production et l’enrichissement des données d’entrainement.
Product Team sur des sujets de performance et d’acheminement de données au service de fonctionnalités produit telles que le dashboard d’analytics à destination de nos clients.
Customer Success / Strategy sur des sujets de pilotage comme le suivi de l’utilisation de notre plateforme ou la mise en place de KPIs de performance.
Tu travailleras sur les problématiques suivantes :
Tu seras responsable de notre architecture de données et de son outillage, mais aussi de la mise en place de pipelines de données complexes et robustes.
Tu seras amené à mettre en place des outils de monitoring et d’alerting pour suivre de près nos nombreuses pipelines de donnée.
Tu seras garant de la qualité de nos données en assurant l’application des guidelines de code et des tests automatisés pour chacune de nos pipelines.
Tu seras amené à mettre en place des outils de reporting / insights à destination d’interlocuteurs variés (Data Science, Product, Customer Success, Clients, etc.).
Tu créeras et développeras des pipelines de données avec des outils de scheduling et d’orchestration.
La stack sur laquelle vous travaillerez :
Langage : Python, Javascript
Framework data : PyTorch, Transformers (Hugging Face), FastAPI
Database : PostgreSQL, MongoDB, ElasticSearch, Redis
Infrastructure : Azure, Docker, Kubernetes, Spark, RabbitMQ, Serverless, Terraform
Environnement / Test : PyTest, Gitlab (git + ci/cd)
BI : Metabase, Superset
Votre profil
Entre 1 et 3 ans d'expérience en CDI
Tu as une expérience significative sur des problématiques de Data engineering
Tu es quelqu'un de pragmatique
Un très bon niveau en Python et une très bonne rigueur dans le code
Bonne pratique de dev : clean code, TDD, BDD
Une bonne culture Ops
Une logique cloud (Aws, GCP ou Azure)
Le salaire & avantages
50-7O K€ selon expérience
RTT
Carte Swile & Mutuelle
2/3 jours de télétravail par semaine
Et plus encore…
Ce qu’on préfère
Être impliqué à fond dans une aventure avec de nombreux challenges techniques
Belles opportunités d'évolutions sur des postes d'Architecte, de Lead ou de Ml Ops
Beaucoup de workshops en interne et catalogue de formations à votre guise
Une opportunité de travailler sur un produit unique qui a déjà séduit de très beaux clients (BNP Paribas, Fnac Darty, Luko, OpenClassrooms)
La possibilité de travailler sur une stack très moderne, des problématiques complexes aussi bien en traitement de données, qu'en DevOps
Un plan de BSPCE (actions de l'entreprise) très intéressant et motivant !
Une culture d'entreprise fondée sur l'apprentissage, l'autonomie, la bienveillance et l'exigence
Le fait de travailler au quotidien avec des fondateurs passionnés par leur domaine d'expertise
Ce poste a été soigneusement choisi par votre coach. Powered by Octopus IT, cabinet d’Experts en Recrutement Tech (CDI et clients finaux uniquement) – Visitez nous pour plus d’opportunités :
www.octopusit.fr
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go', 'JavaScript'], 'DataBase': ['SQL', ' MongoDB', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': ['PyTorch'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['PostgreSQL'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['50'], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer (H/F) - Lille,Logic@l Conseils,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-lille-at-logic%40l-conseils-3811575649?position=10&pageNum=5&refId=T%2B%2BV0JbclKMpVoA2J23Itg%3D%3D&trackingId=11TBdmq9Jr5Wm4NJ8JV1bg%3D%3D&trk=public_jobs_jserp-result_search-card,"Dans le cadre du développement de nos activités sur la métropole Lilloise, nous recherchons un
consultant data engineer
(H/F) pour intervenir chez l'un de nos grands comptes clients.
Vos missions :
Recueillir
les besoins métiers et des équipes data
Concevoir et mettre en place les
traitements de données
Réaliser les
tests de validation
Assurer
l’alimentation du dataware
Réaliser les
ordonnancements des traitements
Etre garant de la
mise en place
, du
suivi
et de l’
exploitation
des outils déployés
Assurer
une veille technologique
régulière
Environnement technique :
Développement :
Python, Scala, R, Java,
Framework :
Spark,
Hadoop,
Outils Big data :
Yarn, Pig, Hive, Kafka, Splunk
Bases de données :
MongoDB, HBase, Cassandra
ETL :
Talend, Stambia
Plateforme :
Hortonworks, Cloudera, Map Reduce
,
AWS, GCP, Azure
Votre profil :
Vous disposez d’une expérience
d’au moins 2 ans en tant que data engineer
ou dans le domaine de l’analyse et du traitement de données.
Véritable
passionné de la data
, vous êtes
force de proposition
sur les solutions techniques à mettre en œuvre. Vous maitrisez l’anglais dans un contexte professionnel.
Compétences requises :
Analyses qualitatives et quantitatives (Intermédiaire)
Anglais (Intermédiaire)
Architecture fonctionnelle SI (Débutant)
Développement d'ouvrages, produits ou événements (Débutant)
Gestion des contrôles, tests et diagnostics (Débutant)
Gestion des risques (Intermédiaire)
Maîtrise des logiciels (Intermédiaire)
Mise en exploitation / Production et maintenance (Débutant)
Nos valeurs
Nous avons décidé de renverser la pyramide du management pour placer nos collaborateurs en tête des priorités de l’entreprise.
En effet, attaché à des valeurs fortes, telles que la proximité, la sincérité, la fidélité, la confiance et le respect, nous sommes persuadés que la réussite réside dans le bien-être de nos collaborateurs.
Cela se traduit par un accompagnement de proximité, de la transparence sans langue de bois, des échanges réguliers avec les managers référents, un accompagnement dans le développement de carrière qui est construit et jalonné avec les formations et certifications nécessaires et les missions en adéquation, pour mener à bien l’évolution de carrière.
Pour vous convaincre de nous rejoindre, nos avantages salariés complémentaires :
Environnement bienveillant et stimulant au sein de 3 pôles d’expertises
Formations et Certifications à la demande
Tickets restaurants : 13€ par ticket
Remboursement à 100 % des abonnements de transports en commun
Mutuelle frais de santé avec de hautes garanties
Prise en charge à 100% de l’assurance Prévoyance
Chèque Cadeau Culture 120 €
Compte CSE avec une cagnotte de 390 €
Compte CE : billetterie, voyages, culture, sorties, à des tarifs préférentiels
Des évènements chaque mois : activités associatives, sportives, afterwork, séminaire,
Partenariat Losc (participation aux match dans la loge VIP logical conseils – (Une Vingtaine de match par an)
Possibilité de télétravail
En intégrant Logic@l Conseils, vous participez à une réelle aventure humaine, alors pour postuler, il suffit de cliquer ci-dessous !
Tous nos postes sont ouverts, à compétences égales, aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['Cassandra', 'HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer,Mobiskill | WEFY Group,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-mobiskill-wefy-group-3907393935?position=1&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=gPwfRZiToccoAQLQfsQAHQ%3D%3D&trk=public_jobs_jserp-result_search-card,"La société :
Le produit repose sur la data, leur solution basée sur de l’intelligence artificielle permet de personnaliser le monde de la promotion et de la fidélisation. Ils viennent d'être rachetés et ont une forte ambition pour leur expansion internationale.
Les missions :
- Travailler avec les data scientists pour apporter des solutions
- Industrialiser les modèles
- Optimiser la performance du produit
- Développer des outils big data pour scaler
- Mentorer des profils plus juniors
Stack :
- Scala
- Spark / Spark Streaming
- Kafka
- GCP
- Cassandra
- Docker
Profil recherché :
- Entre 3 et 5 ans d'expérience dans le Data Engineering
- Expérience en Scala/Spark
- Expérience sur cloud (très idéalement GCP)
- Prêt à faire des missions polyvalentes
- Ouvert à d'autres technos (ils ont pour objectif d'implémenter prochainement des outils en Python)
Pourquoi les rejoindre :
- Expansion internationale : USA, Brésil, Russie, Espagne…
- Une stack à la pointe et un champs d’action pour POCer de nouvelles technos si il y a un intérêt business
- Un encadrement bienveillant : les 2 leads techniques sont deux excellents techs ET d’excellents mentors avec qui échanger sur comment faire avancer la société (tu serais le troisième maillons de la chaine).
- Politique remote hybride
- Des bureaux dans Paris intra-Muros
- Une rémunération pouvant dépasser 70k (avec package)
- Une entreprise très tech, particulièrement orientée Data
Hâte de vous en dire plus rapidement !
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Hybride', 'Remote', 'Junior'], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer,ALFI : Financial Markets Consultancy Services,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-alfi-financial-markets-consultancy-services-3916552424?position=2&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=xN%2Fs023Ps5wbxsXxbzaCfw%3D%3D&trk=public_jobs_jserp-result_search-card,"Le Data Engineer intervient au sein de l’équipe Engineering Open Big Data du Département Guilde Data, qui regroupe l’ensemble des expertises technologiques liées à l’ingénierie de la donnée, de l’automatisation et à l’exploitation des modèles de Machine Learning.
Votre rôle et vos missions :
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le Datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le Datalake
Les exploiter pour atteindre la finalité business (exposition de Business View, réintégration des résultats dans le SI, service de scoring, …)
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Accompagner les Data Engineers sur son périmètre pour garantier la qualité des livrables
Expertise souhaitée
Compétences techniques :
Expertise en SPARK et PySpark
Expertise sur Databricks
Une expérience sur un cloud provider public comme Azure (idéalement), AWS, ou GCP
Connaissances avancées d'outils de BI comme PowerBI (idéalement) ou Spotfire
Compétences transverses :
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Expérience en mode de Delivery Agile (Scrum, Kanban, etc.…)
Etre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)
Et :
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT
Maitrise des Traitements Big Data en mode Streaming
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
Conformément à la règlementation, et à notre politique d’égalité professionnelle, tous nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer H/F,Lincoln France,"Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-lincoln-france-3829857168?position=3&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=JgE3YnfQ5DjhRTX3PNYJ6Q%3D%3D&trk=public_jobs_jserp-result_search-card,"DATA ENGINEER H/F
CDI
3 ans minimum
Chez Lincoln
, nous formons une communauté d'innovateurs passionnés qui redéfinissent l'analyse de données depuis
plus de 30 ans
. En tant que
Pure Player Data
, notre expertise est reconnue dans les domaines
de la Modern BI, du Big Data et de la Science des données
.
Notre mission ?
Transformer les données en solutions concrètes pour nos clients grands comptes dans divers secteurs tels que la banque, le retail, les télécoms, l'industrie, la santé, etc.
Description de poste
Nous recherchons un
Data Engineer H/F
pour accompagner nos clients dans leurs projets stratégiques.
Vos missions :
Concevoir et développer des pipelines de données robustes et évolutifs.
Intégrer et transformer des données provenant de différentes sources.
Développer et mettre en œuvre des algorithmes de traitement de données avancés.
Collaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.
Assurer la qualité et la fiabilité des solutions développées.
Prérequis :
Maîtrise des langages de programmation (
Python, Scala, etc
.).
Connaissance approfondie des bases de données et des technologies
Cloud (GCP, AWS, Azure, Snowflake, etc.)
Expérience avec
MySQL, PostgreSQL, MongoDB.
Maitrise ETL/ELT (Talend, Stambia, etc.)
Solides compétences en conception et en optimisation de pipelines de données.
Expérience de travail en
méthode Agile
pour la gestion de projet et le développement de solutions.
Capacité à travailler de manière autonome et en équipe.
Excellentes compétences en communication et en résolution de problèmes.
Les plus du poste :
Environnement Collaboratif
: projets innovants favorisant le partage des connaissances.
Accompagnement individualisé et de proximité
: formations certifiantes, attribution d’un Career Manager pour vous orienter dans votre trajectoire professionnelle, opportunités d’évolution de carrière.
Flexibilité du Travail
: Télétravail et horaires flexibles pour votre équilibre vie professionnelle-personnelle.
Rémunération Compétitive
: Salaire compétitif avec des avantages sociaux attrayants.
Mobilité
: Possibilité de mobilité à Paris, Lyon ou Aix-en-Provence offrant des expériences diversifiées au sein de Lincoln.
Notre processus de recrutement :
un entretien RH (1h) et entretien technique (1h)
Cette annonce n’est pas faite pour vous si :
Vous êtes freelance et vous comptez le rester !
Toujours là ? Postulez et rejoignez nos
400 experts en Data
😉.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', ' MongoDB'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['MySQL', 'PostgreSQL', 'Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Résolution de problèmes', 'Flexibilité'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['400'], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer & Analyst - Paris - F/H/X - CDI,Partoo,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-analyst-paris-f-h-x-cdi-at-partoo-3892387658?position=4&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=2e4pIaI4dkTrxrF9KildUg%3D%3D&trk=public_jobs_jserp-result_search-card,"Partoo, who are we? 👀
Partoo est une scale-up saas B2B qui a à cœur d’aider les commerces locaux, grandes entreprises ou PME à se rapprocher de leurs clients. Pour cela, ils ont développé une plateforme tout-en-un et différentes solutions qui s’articulent autour de 3 propositions de valeur : Get found, Get chosen & Get clients.
À travers ces 3 propositions, ils ont développé plusieurs produits qui s’adaptent aux évolutions du parcours d’achat des clients :
🔎 Get found
Presence: Synchroniser les informations des magasins sur les principales plateformes (Google, Facebook, Waze, etc.), annuaires et GPS
Store Locator: Aider les clients à trouver le magasin qui leur convient grâce à des données locales actualisées et des filtres dédiés sur les sites web des enseignes
Réseaux sociaux: Gérer les publications sur Facebook, Google, Instagram, etc
🎯 Get chosen
Review: Centraliser, répondre et analyser les avis clients reçus sur Google et Facebook
Booster: Obtenir des avis positifs supplémentaires sur Google par le biais de SMS et de QR codes
🤗 Get clients
Messages: Centraliser et répondre à tous les messages de chat reçus via Google Business Messages, Messenger et bientôt aussi via Instagram, whatsapp, etc. (templates messages, conversations starter, appels manqués...)
Quelques chiffres 🗝️
> Un label Happy at Work et l'une des meilleures notes Glassdoor de l'écosystème avec 4.6/5 pour plus de 260 avis⭐️⭐️⭐️⭐️⭐️ ️️️️️️
> 450+ employés heureux, 37 nationalités différentes, des bureaux à Paris et Barcelone 🚀
> Ils gèrent 300 000 points de vente et travaillent de manière transversale avec +1000 chaînes (Carrefour, Generali, Toyota, Décathlon, Leroy Merlin etc.) et +6000 pme dans environ 150 pays
Notre mémo 2024: le mot du CEO (https://www.partoo.co/fr/blog/memo-2024/)
IMPACT 💥
Partoo compte aujourd’hui pas moins de 400 collaborateurs, qui œuvrent au quotidien à maintenir une croissance saine, en phase avec les enjeux et challenges économiques du moment.
Une des composantes clefs pour y parvenir réside en notre capacité à développer et maintenir un haut niveau d’efficacité opérationnelle. Dans cette logique, améliorer notre capacité à exploiter et utiliser la donnée présente dans nos systèmes est indispensable. Si nous avons déjà une équipe Data en place, celle-ci est aujourd’hui mobilisée presque exclusivement sur les thématiques data relatives au fonctionnement de notre application ainsi qu’à la construction d’éléments de visibilité pour nos clients.
Nous souhaitons donc recruter un Data Engineer & Analyst dont l’objectif principal sera de permettre aux équipes Opérations et client-facing de visibiliser et tirer le meilleur parti d’une donnée aujourd’hui difficile d’accès.
Manager : Adel Adman (cc. Clément Bouillaud, en charge de la team Operations)
TEAM 💙
Meetings récurrent avec les membres de Partoo :
Membre à part entière de l’équipe Data (elle-même intégrée dans l’équipe Produit), tu seras néanmoins en contact régulier avec les équipes Opérations, qui seront tes principales interlocutrices.
En d’autres termes, tu seras le pilier central entre les équipes Ops et Data.
Dans un premier temps, tu auras un meeting hebdomadaire avec Adel (Lead Data) et avec Clément (COO), le temps de cadrer tes premières priorités et de trouver la bonne récurrence de rencontre avec les équipes Opérations.
MISSIONS 🔥
Ton principal objectif consiste à faire en sorte que chaque personne, des équipes Opérations comme des équipes client-facing, ait accès à la donnée dont elle a besoin, au moment où elle en a besoin, sur le support le plus adéquat. Pour y parvenir, plusieurs missions seront tiennes :
Architecture
:
Créer des architectures de données robustes et évolutives pour collecter, stocker et analyser de grandes quantités de données provenant de diverses sources (Salesforce, Intercom, Chargebee, back office de Partoo, etc.)
Analyser et améliorer continuellement le modèle de données Salesforce (SF), en accompagnant l'équipe Ops dans le monitoring des anomalies et l'optimisation des performances
Intégrations et flux
:
Développer et optimiser des pipelines de données, assurant l'intégration fluide des données dans notre Data Warehouse depuis différentes sources, et inversement
Transformation & analyse
:
Concevoir et exécuter des requêtes SQL complexes pour l'analyse de données, permettant de soutenir les décisions business
Identifier et construire des KPI cruciaux, fournissant des insights précieux aux équipes business
Visualisation
:
Fournir aux équipes Ops et client-facing des outils de visualisation de données (Looker Studio, embedding, etc.), clés dans l'optimisation de notre gestion de clientèle.
Formation
:
Former les équipes Opérations sur l’exploitation des tables de notre Datawarehouse ainsi que sur l’usage de Looker Studio et propager les principales best practices associées. Tout ça, en collaboration au quotidien avec les équipes Ops !
DESIRED PROFILE 🎯
Compétences recherchées :
Une très bonne connaissance du langage SQL, notamment PostgreSQL et BigQuery.
Maîtrise du scripting Python et des notebooks pour l'analyse de données
D’excellentes capacités d'analyse pour comprendre les besoins business, identifier les anomalies dans les données et proposer des améliorations pertinentes
Une bonne aptitude à manipuler et analyser de grands ensembles de données et en extraire des insights actionnables
Une très bonne maîtrise d'au moins un outil de business intelligence tel que Looker Studio, PowerBI ou Tableau
Profils recherché :
Tu as plus de 3 ans d'expérience en Data Engineering /Advanced Data Analysis
Tu maîtrises les stacks de data les plus récentes (dbt, Airflow, Airbyte, etc.) et les meilleures pratiques en matière de données (ETL, reverse-ETL, etc.)
Tu es orienté(e) utilisateur et sais convertir les besoins commerciaux en solutions techniques
Tu sais communiquer avec les équipes et t'assurer que les meilleures pratiques sont adoptées
Tu es un team player !
Tu souhaites apprendre et grandir avec nous
RECRUITMENT PROCESS 🛠️
A first video call with Marine, Talent Acquisition Specialist, 45 min
Interview with Adel, Lead Data Engineer, 1h
Case Study
Interview with Clément, Chief Operations Officer, 1h
À compétences égales, ce poste est ouvert aux travailleurs et travailleuses en situation de handicap ou assimilés au sens de l’article L5212-13 du Code du travail. Partoo s’engage en faveur de la diversité, l’égalité professionnelle, l’emploi des travailleurs handicapés.
With equal skills, this position is open to disabled workers or those considered to be disabled within the meaning of Article L5212-13 of the French Labour Code. Partoo is committed to diversity, professional equality and the employment of disabled workers.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['PostgreSQL', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer,RSight®,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-rsight%C2%AE-3856216625?position=5&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=8Zpi5qSzXdHWTRv3cLq8Zg%3D%3D&trk=public_jobs_jserp-result_search-card,"Nous recherchons pour notre client, un
leader mondial des services et conseils en technologies
, un
ingénieur Databricks et Data Factory
qui rejoindra une équipe qui combine des compétences métiers avec une forte expertise data, analytique et d’intelligence artificielle pour mettre en œuvre des solutions qui visent à améliorer la gestion et la valorisation des données.
Descriptif des missions:
Vous êtes intéressé à travailler sur une solution ayant un impact direct sur les ambitions de notre client en matière de data (datadriven, data démocratisation) ? Alors devenez membre de l’équipe Corporate Data Lake de notre client ! Comme tout autre membre de l'équipe, vous :
Participer à la définition des composants informatiques supportant la fourniture de services
Développer, tester, industrialiser et déployer des composants en minimisant les impacts sur les utilisateurs (automatisation, 0 temps d'arrêt,...)
Documenter la bonne utilisation des services
Déployer et supporter nos fonctionnalités sur la plateforme
Apporter assistance et conseils aux utilisateurs métiers
Opérer la solution en opération courante (incluant le suivi de la qualité des services) et intervenir dans la résolution des incidents
Participer activement à l'amélioration continue des activités de l'équipe
Expliquer aux collaborateurs ce que le Corporate Data Lake peut faire pour eux
Configurer des espaces de travail pour eux
Fournir du coaching et de l'expertise lors de réunions en face à face ou sur les canaux communautaires
Participer à l'effort de support de la plateforme dans une approche ""vous la construisez, vous l'exécutez""
Contribuer aux premières phases de conception définissant l'avenir du Corporate Data Lake
Compétences:
1er expérience Azure (PaaS et IaaS)
Connaissance de Databricks et Data Factory
Maîtrise d'un ou plusieurs langages parmi : Python, Scala, Spark, PowerShell
Intégration et livraison continues (Jenkins, Azure Devops, GIT Lab CI, …)
Pratique des fondamentaux du génie logiciel (Gestion de Configuration, Tests,...)
Anglais : à l'aise pour assister à une réunion et rédiger de la documentation technique
Bonne capacité d'écoute, orientation client/utilisateur
Expression orale et écrite adaptée à l'interlocuteur
Curiosité et adaptation aux changements technologiques
Bénéfices:
Un processus de recrutement court, un accompagnement personnalisé, une évolution qui s'adapte à votre trajectoire de carrière.
En plus de votre quotidien lié à votre mission, vous pourrez entreprendre, être formé, passer des certifications.
Plan d'épargne pour la retraite collectif, mutuelle, tickets restaurant, des congés d'ancienneté, un catalogue CE, des accords d’entreprise relatifs au télétravail et à la parentalité et autres avantages.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Stage - Data Engineer - ML (H/F),Withings,"Issy-les-Moulineaux, Île-de-France, France",https://fr.linkedin.com/jobs/view/stage-data-engineer-ml-h-f-at-withings-3613476264?position=6&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=DoyNgT9yj6tnq%2BmyCF3KHQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Chez Withings, nous développons des appareils de santé connectée : nos balances connectées, montres hybrides, tensiomètres, moniteurs de sommeil et tous les dispositifs de notre gamme sont aujourd'hui utilisés par des millions d'utilisateurs. Notre objectif est de permettre la prévention, le dépistage et l'accompagnement d'un certain nombre de maladies chroniques via des produits et des services innovants afin de révolutionner la manière dont on prend soin de notre santé.
Au sein de l'équipe Machine Learning, nous développons des algorithmes pour extraire des informations physiologiques et médicales pour nos utilisateurs tels que le SPO2, la fréquence cardiaque, la détection de diverses pathologies comme la fibrillation atriale, l'apnée du sommeil...
Intégré.e au sein de l'équipe Machine Learning, tu auras une ou plusieurs des responsabilités suivantes :
Développer un outil de monitoring de la dette technique, des mauvaises pratiques de code, des failles de sécurité ;
Construire des dashboards de visualisation ;
Construire un système d'alerte pour notifier les contributeurs d'éventuels problèmes ;
Développer des outils permettant de corriger les éventuels problèmes de façon automatisée ;
Requirements
À la recherche d'un stage d'une durée de 3 à 6 mois ;
Préparation d'un Master en école d'ingénieur ou équivalent / année de césure possible ;
Maîtrise de Python ;
Maîtrise de Debian ou de Ubuntu, de Shell et de l'environnement Linux ;
Première expérience sur du développement logiciel ;
Culture DevOps (omniprésence du monitoring, automatisation des tâches, ...)
Compréhension de la culture et des besoins des différents membres de l'équipe ;
Rigueur, autonomie, prise d'initiative, curiosité
Benefits
Rejoindre l'aventure Withings, c'est :
Intégrer un des pionniers et leaders mondiaux de la santé connectée, plusieurs fois primé au Consumer Electronic Show
Contribuer à des projets innovants et ambitieux pour la santé de demain dans un environnement agile et en constante évolution
Intégrer une entreprise internationale, membre de la FrenchTech 120, dont les équipes sont basées à Issy-les-Moulineaux, Boston, Hong-Kong et Shenzhen
Participer à l'amélioration continue de nos produits et services en les bêta-testant avant leur sortie, notamment lors de nos nombreuses sessions sportives entre collègues
Participer à la Withings Med Academy en assistant à des conférences de professionnels de santé afin de renforcer ses connaissances dans le domaine médical
Collaborer avec des collègues passionnés et célébrer ensemble chacune de nos réussites !
Toutes les candidatures reçues sont étudiées indépendamment de l'origine ethnique, des croyances, de la religion, du genre, de l'orientation sexuelle ou de la santé des candidats. Withings aspire à offrir et garantir l'égalité des chances aux candidats et seules les personnes habilitées (RH et Management) auront accès aux informations concernant votre candidature.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Machine Learning'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Hybride'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Stage - Data Engineer,Exotec,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/stage-data-engineer-at-exotec-3918170659?position=7&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=WBU9RcALIUZp84dp5t1Vfw%3D%3D&trk=public_jobs_jserp-result_search-card,"Chez Exotec, nous mettons l'excellence technologique au service de la redéfinition des relations entre humains et robots. A travers le monde, nos solutions révolutionnent la façon dont nos clients délivrent leurs produits aux consommateurs finaux. Nous contribuons au succès des plus grandes marques du commerce et de l'industrie, tout en améliorant les conditions de travail de leurs salariés.
Par l'alliance de l'intelligence artificielle et d'un hardware performant, nos robots sont désormais déployés dans le monde entier et leur succès a fait de nous la première licorne industrielle française.
Rejoindre Exotec, c'est l'opportunité de donner du sens à vos compétences. Grandissez avec plus de 800 ExoPeople dans le monde entier pour faire de vos idées des réalités.
La révolution robotique portée par Exotec ne fait que commencer, vous en êtes ?
Au sein du pôle Data, de la DSI d'Exotec, votre rôle sera de participer au développement de l'environnement et de l'infrastructure Data d'Exotec.
Pour cela :
Vous participez à la mise en œuvre des composants techniques de la plateforme de données d'Exotec
Vous travaillez sur la collecte dans la plateforme de données provenant de sources multiples : Salesforce, ERP, logiciels développés en interne
Vous nettoyez, mettez en qualité et préparez les données afin de les rendre disponibles pour les différents cas d'usage qui en ont besoin
Vous migrez des reportings existants vers la plateforme de données et mettez en œuvre de nouveaux cas d'usage pour répondre aux besoins de l'entreprise
Vous travaillerez au sein de l'équipe data et en étroite collaboration avec la software factory, ainsi qu'avec les utilisateurs des métiers qui ont besoin de rendre intelligibles les données disponibles
Requirements
Vous êtes étudiant(e) d'une école d'Ingénieur généraliste avec une spécialisation programmation ou informatique
Vous recherchez un stage de fin d'études d'une durée de 4 à 6 mois
Vous avez idéalement une première expérience en Data Engineering et le développement de pipeline de données
Vous maitrisez Python, l'ETL et SQL,
Curieux(se) et rigoureux(se), vous souhaitez rejoindre une équipe jeune et dynamique ainsi que vous investir dans des projets complexes et excitants
Vous avez un niveau d'anglais courant
Chez Exotec, nous garantissons l'égalité des chances dans notre processus de recrutement. L'ensemble des candidatures reçues sont étudiées indépendamment de l'âge, du genre, de l'origine, de la religion, de la couleur de peau, de la nationalité, du sexe, du handicap, de l'orientation sexuelle ou de toute autre distinction protégée par la loi. Nous mettons en place un environnement de travail inclusif et respectueux de toutes les différences. En rejoignant le Pacte Parité, Exotec s'engage pour un écosystème French Tech plus paritaire.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,StackEase,"Marseille, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-at-stackease-3906618983?position=8&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=dus3QZepxedeM%2FgZcg7KtA%3D%3D&trk=public_jobs_jserp-result_search-card,"Context :
It is challenging in many ways to develop a net zero electricity grid. Adding renewable energies means more need for storage and for grid balancing. Grid-scale batteries handle these issues but their operation is not quite simple.
A battery has access to many different revenue streams, each being specific. No revenue alone is enough to make it economically viable. All revenues stacked and optimized together ensure profitability in the long run and a fast deployment.
StackEase’s ambition is to develop optimization and trading algorithms that merge the battery revenue streams and ensure their sustainable development. The final tool will be an autopilot that makes the best decisions in real time, controls the battery accordingly and sends the corresponding orders to  the market.
About StackEase:
StackEase is a deeptech spinoff from the INRIA (French Institute for Research in Computer Science). It was created in August 2023 and secured its first fundings. Members are located in Marseille and Paris.
Our values are innovation, customer satisfaction, merit and sustainability. The company's purpose is to leverage Machine Learning and Mathematical Optimization to accelerate the energy transition.
Missions :
Define and develop the backend architecture of StackEase
Set up databases and data pipelines collecting battery and market data
Deploy and maintain optimisation algorithms and forecasts
Develop a robust and scalable SaaS platform for 24/7 battery management with high cyber standards
Participate in the UI/UX product definition
Skills Wishlist :
Scientific BS/MS/PhD with 2+ years of experience in software engineering
Experience with the common backend tools: Python, Git, Kubernetes, SQL/NoSQL … Knowledge of the AWS environment is a plus
Enthusiastic, rigorous, autonomous and willing to be involved in major technical decisions
Knowledge/Interest in the energy sector and ancillary services
Compensation :
45k€ - 60k€ salary range (incl. healthcare, unemployment rate, vacations, …)
Flexible remote work policies
You do not need to meet 100% of the requirements to apply, we will study all applications: please send your resume to jobs@stackease.fr. References and a cover letter are also welcome but not mandatory.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['Machine Learning'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer / Big Data,ALTEN,"Antibes, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-big-data-at-alten-3896177673?position=9&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=9KPNQINsEC%2BkKrRJPbAmCQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Alten is one of the 3 main consulting societies in France. It is present in more than 30 countries in the word. Sophia-Antipolis is the first and largest technopole in Europe, located in the South of France between Nice and Cannes, near Antibes. It’s also called the European Silicon Valley.
Reporting to our consulting team, you will work in an IT environment as an IT Business Analyst, taking part in projects in an Agile environment.
Job Description
The mission consists in taking part toprojectsthat are collecting, processing, and converting raw data into information (Flat Data + KPIs) that can be interpreted by data/business analysts.
TheData accessibilityis the ultimate goal of this mission, by enabling the Digital data users to utilize data for their business decisions.
This role is more Pipeline-centric, for which you need in-depth knowledge of distributed systems and computer science.
The mission scope will encompass the design and delivery of some key components of the Digital Data platform(Spark environment, Scala language) including following activities:
Participate to specifications reviews, propose technical solutions and perform feasibility studies.
Acquire datasets that align with business needs.
Develop algorithms to transform data into useful, actionable information.
Develop, construct, test, and maintain optimal data pipeline architectures.
Create new data validation methods and data analysis tools.
Ensure compliance with data governance and security policies.
Identify ways to improve data reliability, efficiency, and quality.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Prepare data for predictive and prescriptive modeling.
Work with data and analytics experts to strive for greater functionality in our data systems; which requires a closed collaboration with the other trains.
Develop software according to Amadeus Standards, including documentation
Perform code reviews in line with Amadeus quality standards.
Conduct unit, package and performance tests of the software and ensure a level of quality in line with the Amadeus guidelines.
Participate in the validation / acceptance phase of the product cycle ensuring the fine-tuning necessary to finalize the product.
Produce software documentation necessary for the application and issue it to the requesting departments.
Support the end user in the Production phase by debugging existing software solutions in response to Problem Tracking Records (PTR) and Change Requests (CR) issued from Product Management or Product Definition.
As part of the team, the consultant will, as well, work as DevOps, releasing the software load to production, monitoring jobs and being involved in maintenance activities. Quality analyst activities are also handled by the Devs.
Our current data platform is a MapR architecture, and we are in the process to migrate the whole platform to the Azure cloud., which is one of the key focuses for the team in the upcoming months. This will trigger easier adoption of data and foster collaboration within Amadeus around data.
Qualifications
Technical skills:
Previous experience as a data engineer or in a similar role
Experience building or optimizing “big data” data pipelines, architectures and data sets.
Hands-on experience with Scala (>2 years or a strong experience with Java/C++ with a good knowledge level of Scala)
Experience with big data tool: Spark, Kafka, MapR , Hadoop
Understanding extract, transform, and load ETL systems
Knowledge of cloud services: MS Azure
Soft skills:
Agile Mindset: must be comfortable working with Agile values and artifacts
Fast learning: must be able to adapt quickly to the existing environment and new changes
Analytical thinking and problem-solving mindset: must be able to quickly identify, implement work-around to solve incidents and work on long term data solutions
Team spirit, knowledge sharing, Empathy: must be able to work in a team and to communicate clearly with the other team members and users
Pro-activity, Professionalism, Opennessand Innovative mindset
Various:
English: professional level
Knowledge of Scrum framework and Agile methodologies.
Knowledge of airline business is a plus
Additional Information
ALTEN places the career development of the Engineers at the heart of its model and allows you to quickly take on responsibilities and evolve in line with your professional objective. It's the promise of growing your skills on concrete subjects in a project team, with a permanent contract as an ALTEN consultant!
Do you recognize yourself in this description? Then send us your CV.
Our teams will be delighted to study your application and meet you!
Show more
Show less","{'ProgLanguage': ['Java', 'C++', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Empathy', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['2'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,ASTRELYA,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-at-astrelya-3910760230?position=10&pageNum=7&refId=2YID7uch9sWjo1I7wCmyNQ%3D%3D&trackingId=v8lA3BhOGBaAvAsiUoQ%2FXA%3D%3D&trk=public_jobs_jserp-result_search-card,"ASTRELYA est un groupe de conseil et d’expertise IT fondé en 2017, présent en France (Paris et régions) et en Suisse (Genève). Aujourd'hui plus de 280 collaborateurs accompagnent nos clients dans l’accélération et la transformation de leurs organisations.
Dans le cadre de notre développement, nous recherchons un
Data Engineeer F/H
.
Vos rôles et responsabilités :
Développements Java Spark
Optimisation et gestion des évolutions de l&#39;architecture pour intégrer des calculs sur des volumétries de plus en plus importantes
Support technique auprès des équipes de développement et du responsable applicatif
Conception des solutions applicatives cohérentes avec l&#39;ensemble du SI et avec les normes et standards
Développer et garantir les pratiques de développement et de documentation associés (DevOps
L’environnement technique dans lequel vous évoluerez :
Java, Scala, Spark, écosystème Hadoop, environnement DevOps
Les compétences recherchées :
Formation : École d’ingénieur ou équivalent Bac+5
Expériences : Minimum 5 ans d’expérience
Langues : Anglais technique
Excellent relationnel, force de proposition, autonome
Pourquoi rejoindre ASTRELYA ?
Une gestion de carrière personnalisée et un management de proximité
Une politique active de formations / certifications (technique, métier, leadership)
Une offre variée de missions d’expertise
Un engagement RSE fort : Ecovadis Gold, Signataire de la charte pour la diversité, du Pacte des Nations Unies et mise en place du Mécénat de compétences
Un programme de cooptation attractif
Afterworks, conférences techniques et activités sportives réguliers
Cette annonce vous correspond ? Postulez !
🚀
Tous nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Java', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': ['Leadership', 'Organisation'], 'EnSoftSkils': ['Leadership']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Intégrateur/Data Engineer,Apollo Plus,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/int%C3%A9grateur-data-engineer-at-apollo-plus-3915774077?position=1&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=auvz94%2F%2BKdvLumd1UTOF%2FA%3D%3D&trk=public_jobs_jserp-result_search-card,"Nous, c'est Apollo Plus.
Notre ambition ? Devenir La solution SaaS basée sur l’IA, d'aide à la décision et de prédictions de la demande dans les secteurs du tourisme, de l’hôtellerie et du retail.
Depuis notre lancement sur ce marché en 2017, nous n'avons pas arrêté de grandir et d'enrichir notre application pour accompagner nos clients dans leurs différents challenges et questionnements :
Est-il possible de prédire l’intention et le comportement des visiteurs ?
Quelle segmentation des visiteurs construire à partir de nombreuses sources ?
Où et pour quelle offre générer de l’affluence supplémentaire ?
Quelle est l’élasticité prix et la disponibilité à payer de nos clients ?
Comment enrichir nos données avec de l'open data pertinente ?
Comment pouvons-nous améliorer la connaissance de nos visiteurs ?
Comment mesurer les campagnes marketing et les plans de communication ?
Apollo Plus est présent sur les marchés français, allemand, espagnol, belge et américain.
Le périmètre du poste concerne essentiellement le parcours et la transformation des données depuis leur stockage chez un client jusque dans nos bases de données.
Missions
Intégrer de nouveaux clients : traduire les besoins métier en pipelines de traitement de données, codés en Python
Implémenter de nouveaux connecteurs pour récupérer les données (solutions de billetterie, de paiement, de réservations, de gestion hôtelière, etc.)
Implémenter de nouvelles features d'analytiques (KPIs, granularité des chiffres, etc.)
Participer à l'évolution et au maintien de nos pipelines de traitement de données
Améliorer l'implémentation, le test et le backtesting de nos algorithmes de ML
Améliorer la configurabilité (par ex. permettre aux équipes métier de configurer directement les règles de calcul)
Améliorer l'outillage (CI/CD, tests, monitoring)
Stack technique
React, GraphQL
Django, Graphene-Django, DRF
pandas, scikit-learn, SQLAlchemy
PostgreSQL, ClickHouse
prefect
Azure, Azure Devops (équivalent GitHub), Docker, Linux
Compétences requises
Bonne connaissance du langage Python et du SQL
Expérience avec la manipulation de dataframes (par ex. avec pandas, spark)
Maîtrise pratique de git
Compétences appréciées
Familiarité avec l'algorithmique
Familiarité avec les bases des OS : process, thread, mémoire, réseau...
Expérience avec un cloud provider (Azure, AWS, ...)
Expérience en infra (VM, SSH, containerisation, ...)
Expérience avec un outil d'orchestration de workflow (Prefect, Airflow, ...)
Anglais professionnel
Processus de recrutement
Rencontre RH (Google Meet - 30 minutes maximum) pour échanger sur votre parcours et vos aspirations ainsi que notre trajectoire et nos besoins
Tech interview (Google Meet - 30 minutes maximum) pour préciser l'adéquation compétences/besoins
Case Study (à distance ou en présentiel) suite à l'échange Tech
Debrief (en présentiel - peut-être fait à la suite du Case Study) avec le CTO et RH et remise d'offre
Informations complémentaires
Type de contrat :
CDI
Date de début :
20 mai 2024
Lieu :
Paris
Expérience :
> 6 mois
Télétravail ponctuel autorisé
Salaire :
entre 36000€ et 45000€ / an
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['Pandas', 'R'], 'BigData': ['Spark'], 'MachingLearning': ['Scikit-Learn'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': ['Linux'], 'DBMS': ['PostgreSQL'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['36000'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (F/H/X),Goaheadspace,"Pantin, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-x-at-goaheadspace-3915359112?position=2&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=xUgTcqgGkUmSsyChPhHiIA%3D%3D&trk=public_jobs_jserp-result_search-card,"MFG Labs est une société de conseil et réalisation experte en data, qui aide les entreprises à améliorer leurs prises de décision, à automatiser leurs processus et à créer de nouveaux services grâce à la data science, au design et à l'utilisation des dernières technologies.
MFG Labs intervient à toutes les étapes de votre transformation data : de la création d'une feuille de route de projets data, à la découverte d'insights, à la modélisation de problématiques complexes, de la création d'un modèle prédictif à l'implémentation technique d'une solution data sur-mesure
MFG Labs accompagne ses clients de différentes manières :
Stratégie
Solutions
Fondations
MFG Labs déploie une approche holistique pluridisciplinaire, en mêlant des data scientists, des designers, des data engineers et des consultants, afin d'apporter des solutions complètes de bout en bout à des problématiques complexes.
Dans le cadre du développement de l’équipe, nous recherchons un.e Data Engineer à
Pantin (magasins généraux).
Au sein de l’équipe Data Technology, vous aurez pour mission de travailler sur des problématiques de collecte de la donnée sur tout type de support digital : web, mobile, application, voire IoT.
Votre rôle au sein de l’équipe :
Faire partie d’une équipe pluridisciplinaire avec des talents en Design de Service, Consulting et Data science.
Développer des applications de production intégrant différents outils : des Mathématiques Appliqués, Machine (Deep) Learning, Recherche Opérationnelle, Statistiques.
Développer des pipelines de traitement de données avec l’équipe de Data Science pour : ingérer, transformer et délivrer des données et modèles à nos applications.
Déployer des applications utilisant les derniers outils mis à disposition par les différents Clouds publics.
À propos de vous :
Vous êtes titulaire d'un niveau Bac +4/Bac +5 d'une école d'ingénieur
Vous avez au minimum deux ans d'expérience hors stage ou alternance
Vous êtes rigoureux·se vis-à-vis de vous-même et des autres quant à la qualité du code.
Vous avez quelques connaissances et compétences solides en développement et en en Data Ingénierie au sens large.
En
développement
Python 3 et SQL
Framework de traitement de données (Spark ou équivalent)
Docker
GIT
En +
Framework permettant de déployer des APIs (Flask ou équivalent)
CI/CD
La pratique d'au moins un cloud (AWS, GCP ou Azure) est appréciée
En Data Ingénierie
Datawarehouse ou Datalake
Data Pipelines Batch et/ou Straming
En +
Outils de BI (Tableau, Power BI…)
Outils MLOps (Sagemaker, VertexAI, etc.)
Si vous vous reconnaissez dans cette annonce, n'hésitez pas à postuler !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': ['Statistiques'], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': [], 'Other': ['ML', 'Statistiques', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer H/F,Dalkia,"Angers, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-dalkia-3907349741?position=3&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=5wEEPPCD7CRNwrkdwQtzoQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Descriptif du poste
Et si vous faisiez équipe avec nous ? Rejoindre Dalkia, c'est plus de sens et d'implication contre le réchauffement climatique ; plus de relations humaines, avec un métier de service animé par l'esprit d'équipe ; plus de technicité, avec des projets ambitieux et innovants fondés sur nos expertises ; plus d'employabilité, avec des parcours diversifiés et individualisés. Rejoindre Dalkia, c'est rejoindre plus qu'une entreprise : un collectif de 20 000 collaborateurs engagés en faveur de la transition énergétique.
Dalkia Froid Solutions, acteur majeur de la réfrigération, spécialisé dans les services énergétiques pour les process industriels et tertiaires, recherche un(e) Data Engineer
(H/F)
. Rattaché (e) au Responsable Data au sein de la Direction des Systèmes D'Informations, vous êtes le garant du bon déroulement des développements de flux de données et de leur préparation pour leur analyse. Vous aurez l'opportunité de rejoindre une équipe en construction.
Candidater chez Dalkia Froid Solutions, c’est avoir l’envie d’intégrer un grand groupe à l’esprit familial. L’humain est au cœur de nos métiers, nous donnons la chance à tous, afin de découvrir nos talents de demain. Venez renforcer notre Direction des Systèmes d'Informations et contribuez à l'optimisation énergétique à travers la data !
Vos Principales Missions
Définir l'architecture ETL et développer les jobs d'intégration de données pour notre environnement Big Data.
Assurer le monitoring quotidien des jobs et optimiser les performances de traitement.
Garantir la qualité et l'intégrité des données en industrialisant leur nettoyage.
Adapter les DataMarts pour le reporting en collaboration avec les équipes métiers : comprendre et analyser les besoins utilisateurs, et rédiger les spécifications fonctionnelles et techniques.
Vous serez également ammené à collaborer avec l'équipe Infrastructure pour définir les besoins techniques et planifier les investissements. En lien avec votre équipe vous conduirez des projets variés et participerez à la mise en oeuvre de rapports BI et de modèles de machine learning.
Lieu :
Siège Social - Angers / Télétravail possible à raison de 2 jours par semaine après période d'essaie
Votre profil
Diplômé (e) d'un bac + 5 minimum spécialisé en Data Engineer ,vous avez de bonnes qualités relationnelles afin d'accompagner le déploiement des projets. Votre rigueur et votre logique sont incontestables. Vous aimez travailler en équipe pour accompagner l'entreprise vers l'excellence opérationnelle.
Côté Outils ? Vous maitrisez les langages SQL et Pyhton et vous avez déjà pratiqué les outils DBT et GitLab. Une première expérience avec un outil de BI/Datavisualisation est souhaitée.
La connaissance des outils Qlik Sense ou Talend serait un plus!
Prêt(e) à faire une différence avec nous ? Postulez dès maintenant !
Ensemble, nous contribuons collectivement à la transition énergétique. C'est pourquoi chez Dalkia Froid Solutions, nous sommes convaincus que chacun peut participer à relever ce défi. De ce fait, chaque candidature recevra la même attention.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Analytics Engineer,Vestiaire Collective,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/analytics-engineer-at-vestiaire-collective-3875996826?position=4&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=8vuJPLbUeY03j4erAEmiMw%3D%3D&trk=public_jobs_jserp-result_search-card,"Vestiaire Collective is the leading global online marketplace for desirable pre-loved fashion. Our mission is to transform the fashion industry for a more sustainable future by empowering our community to promote the circular fashion movement. Vestiaire was founded in 2009 and is headquartered in Paris with offices in London, Berlin, New York, Singapore, Ho Chi Minh and Hong Kong and warehouses in Tourcoing (France), Crawley (UK), Hong Kong and New York.
We currently have a diverse global team of 700 employees representing more than 50 nationalities. Our values are Activism, Transparency, Dedication and Greatness and Collective. We are proud to be a BCorp.
About The Role
This role is central to our data strategy and requires a balance of technical expertise and business acumen. As a Junior Analytics Engineer, you will be at the heart of our data-driven initiatives, working closely with cross-functional teams to transform raw data into a single source of truth data mart. Your work will directly influence key decisions in finance, payment systems and business performance.
What You'll Do
Design, implement, and maintain efficient and reliable data pipelines using a modern data stack: Airflow, Snowflake, DBT
Develop advanced data models to support complex analytics, including financial reconciliations, cost effectiveness and profitability models. Collaborate with finance, payments and tech teams to understand their data requirements and translate these into sophisticated technical solutions
Ensure scalability and performance of our data infrastructure to handle large-scale, multi-faceted data sets from diverse sources
Implement and maintain data quality checks and monitoring systems for accuracy and consistency
Innovate and integrate new technologies and methodologies to enhance data capabilities across finance domains
Assist the finance team in building key dashboards in Tableau to enable data driven decision making
Who You Are
Required Qualifications:
Bachelor’s/Master’s in Computer Science, Engineering, Statistics, or related field
At least one previous experience in analytics engineering, with strong skills in ETL and data modeling, an awareness of data warehousing and dataOps practices
Proficient in SQL and programming languages like Python or R
Experience with cloud data technologies and big data tools
Desirable Skills:
Apache Airflow: an understanding of workflow management
Git: Solid knowledge in version control and CI/CD integration
Cloud Service: AWS, Snowflake or similar cloud experience
Data Visualization Tools: Proficiency in tools like Tableau, Looker, Snowsight
Previous experience in DBT for data modeling
What we offer
🎁
A meaningful job with an impact on the way people consume fashion and promote sustainability
Flexible work possibilities
The opportunity to do career-defining work in a fast-growing French-born scale up
The possibility to work as part of a globally diverse team with more than 50 nationalities
Two days to help Project - reinforcing your activist journey and volunteer for an association
Significant investment in your learning and growth
Competitive Compensation And Benefits Package
As full member of our entrepreneurial project, you will be eligible to free shares
Vestiaire Collective is an equal opportunities employer
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': ['Statistics'], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Full', 'Junior'], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (F/H) - Alternance,Vertbaudet,"Tourcoing, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-alternance-at-vertbaudet-3853542017?position=5&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=BlTZ4D5w5ZRpcFiIPssV8A%3D%3D&trk=public_jobs_jserp-result_search-card,"A PROPOS DE NOUS !
Vertbaudet est le pure-player leader européen du monde de l'enfant, au travers de 10 sites web adaptés aux besoins de chaque marché (France, Allemagne, Suisse, Royaume-Uni, Belgique, Pays-Bas, Espagne, Portugal, Autriche et .com pour le reste du monde). En France, Vertbaudet est également présent dans 72 magasins.
Vertbaudet s’appuie sur une communauté de plus de 3 millions de parents, clients actifs de nos sites qui bénéficient d’une expérience d’achat personnalisée.
Vertbaudet développe à la fois une offre exclusive au travers de sa marque propre et propose également des marques leaders ou innovantes.
Par son expertise et sa créativité, Vertbaudet répond à tous les besoins des parents pour leurs enfants de 0 à 12 ans en Mode, Chaussures, Maison, Puériculture et Jouets.
Créé en 1963, Vertbaudet emploie plus de 1000 personnes en France et a généré un chiffre d’affaires supérieur à 330 millions € HT en 2023.
Poste:
Rattaché(e) à notre Direction Informatique, tu rejoins le département Intelligence Clients et plus particulièrement « la team » de Florent, notre Responsable Data DSI. Notre mission est d’optimiser la valeur de nos clients par la bonne connaissance de leurs profils et de leur activité
Tu intégreras une équipe de Data Engineers pour apprendre à leur côté et participer à nos projets : construction de flux de données, maintenance d’une plateforme Data moderne, développements d’algorithmes complexes, mise en place de règles DataQuality …
Tu manipuleras des outils et des méthodes variés : Stambia, SQL, CI/CD, Snowflake, Airflow, Cloud Azure …
Tu seras accompagné(e) par un Data Engineer Senior qui saura te faire grandir et te permettre de délivrer de la valeur pour notre équipe et l’entreprise.
Pour cela, tu dois avoir l’envie d’apprendre de nouveaux langages et de nouveaux outils, de perfectionner ta connaissance du SQL, de la rigueur. Tu auras l’occasion de découvrir des domaines métiers différents : Marketing, Produit, Finance, Logistique.
Profil:
Vraiment sympathique toutes ces missions, non ? Alors, si tu es en formation type Bac+5 ingénieur ou équivalent universitaire avec une spécialisation en data/statistique (ex : ENSAI, Polytech, Master Siad, ISN, Econométrie...), tu es peut-être la personne qu'il nous faut !
Le SQL n’a plus de secret pour toi ?
Tu es capable d’extraire de la donnée et de la manipuler aisément ?
Ton anglais
is good enough
pour collaborer avec nos filiales internationales ?
Tu n’as pas peur de paramétrer un cluster Azure ?
Envoie-nous vite ta candidature !
Plus que tes expériences professionnelles, nous nous attacherons à ta passion, ta compréhension de la marque, ton envie de t'immerger dans notre univers. Concrètement, nous recherchons un(e) véritable passionné(e), quelqu'un qui n'a pas peur de s'investir dans ses missions, quelqu'un de fiable, de réactif, qui comprend les enjeux du business digital et qui possède une sensibilité client.
Tu es disponible pour une alternance de 24 mois à partir de Septembre 2024
Pour faciliter le traitement de ta candidature,
merci d'indiquer sur ton CV le rythme d'alternance que propose ton école
Lieu : TOURCOING (59)
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': ['Créativité'], 'EnSoftSkils': []}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '12', '12', '12']}"
Data Engineer,Harnham,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-harnham-3901400227?position=6&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=ziWhFh8xzvynvlEVy14t8w%3D%3D&trk=public_jobs_jserp-result_search-card,"DATA ENGINEER
LA DEFENSE (92)
56K EUR
Rejoignez une équipe en tant que Data Engineer au sein d’une start-up orientée Intelligence Artificiel, et aidez à façonner l’avenir technologique. C’est votre chance de mettre en pratique vos compétences techniques et de jouer un rôle dans le futur du domaine des données.
VOTRE MISSION :
Concevoir et développer des pipelines de données pour assurer la collecte, le traitement, et le stockage efficaces des données pour comprendre les besoins.
Collaboration avec les équipes métiers
Elaborer des processus de validation des données et mettre en place des tests automatisés
Concevoir et implémenter des modèles de données
VOTRE PROFIL :
Au moins 6 mois de stage en tant que Data Engineer
Diplômé d’un Master (2 ans minimum)
Maitrise d'un ou plusieurs Clouds Publics (AWS; GCP; Azure)
Maitrise d'outils classiques (Python, Spark, SQL)
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Big Data Engineer Databricks Senior - H/F - CDI,Talan,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/big-data-engineer-databricks-senior-h-f-cdi-at-talan-3909664424?position=7&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=9Jk4wfhD6YOUymiIHt4IyA%3D%3D&trk=public_jobs_jserp-result_search-card,"Talan est un groupe international de conseil en transformation et en innovation par la technologie, créé en 2002.
Nos 5000 consultantes et consultants partagent à travers le monde l’audace d’innover, le goût de l’excellence, et l’envie de relever les défis les plus complexes.
Nous accompagnons les entreprises dans des secteurs variés : énergie, industrie, transport, finance, luxe… à travers 3 grandes expertises :
Le Conseil en Management et Innovation (320 Consultants en France)
La valorisation des données, leurs structurations, et leurs usages (Data et Technologies)
L’intégration de solutions logicielles (Cloud et Applications Services)
Nos valeurs : engagement, respect, partage, esprit d’équipe et optimisme.
Talan est une entreprise responsable, reconnue par ses collaborateurs et attachée à la diversité. Des aménagements peuvent être proposés si vous êtes en situation de handicap.
Retrouvez nos engagements RSEiciet nos actions en faveur de la diversitéici
Job Description
Nous sommes à la recherche d’un Big Data Engineer Databricks Sénior qui sera en charge de l’intégration des données: acquisition, préparation, modélisation et stockage, exposition, . Vous devrez faire preuve d’un état d’esprit à la fois innovant, méthodique, orienté solution (et non problème!), et communiquant.
Responsabilités
Manager des Big Data Engineer et Cloud Engineer
Coacher techniquement les membres de l’équipe: solution et code review sur site, recommandation sur les formations à suivre, certifications à réaliser, …
Analyse des besoins techniques métiers, définition de l’architecture solution et logiciel, référent technique, développement et optimisation, code review, maintenir les pratiques Devops “You build IT, You run IT”, support à recette et mise en production, documentation, et parfois assumer le rôle de Scrum Master,…
Benchmark de solutions et conseil auprès de notre client sur les solutions technologiques à adopter, en lien avec leurs besoins
Partage de connaissances et formations interne
Qualifications
Issu(e) d’une formation supérieure (école d’ingénieur, master,…)
Vous disposez d’au moins 4 années d’expérience dans le domaine du Big Data (et particulièrement sur le framework Spark), et au moins 6 années d’expérience dans le développement logiciel
Vous maîtrisez ledéveloppement logiciel (Scala, Python …), et vous disposez de solides expériences dans la mise en place de pipelines de données
Vous maîtrisez leFramework Spark (idéalement sur Databricks) etson optimisation
Expérience sur une plateforme Cloud serait un plus et idéalement AWS
Expérience sur des flux temps réelserait un plus : Kafka + Spark Streaming
Vous maîtrisez les bases de données SQL et le langage SQL
Vous avez de l'expérience sur les méthodes de stockage: HDFS, S3,,…
Vous avez de bonnes connaissances en devOps : Jenkins, Gitlab, Maven, …
La connaissance des concepts suivants serait un +: DataOps, DataVault, DataMesh..
Connaissance de l’Agilité
Autonome
Organisé(e)
Sens du partage
Bonne communication
Orientation produit et solution
Additional Information
AVANTAGES
:
Plan de formation pour accompagner votre carrière (formations éditeurs, certifications) grâce à nos partenariats nous accordant une position de partenaire privilégié, et management de proximité par des experts
Locaux modernes en centre-ville
Top 5 du Palmarès Great Place to Work
Télétravail jusqu’à 5 jours selon les missions, prime d’équipement de 100€
Mobilité en France et à l’étranger
Top 1% des entreprises évaluées par Ecovadis dans le domaine social, environnemental et éthique
Tickets restaurant, prime vacances, 50% transport (abonnement transport public), mutuelle
Permanence handicap (consultant dédié aux collaborateurs en situation de handicap et aux proches aidants)
Actionnariat salarié
Prime de cooptations
RTT
PROCESS RECRUTEMENT
:
L’équipe recrutement s’engage à vous proposer un processus de recrutement rapide et fluide
1 entretien RHpar Teams (45min)
1 test technique
1 entretien technique par Teams (1heure)
1 entretien opérationnel avec le responsable de domaine, au siège (1heure)
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Alternance - Data Engineer H/F,METEOJOB by CleverConnect,"Villeneuve-d’Ascq, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/alternance-data-engineer-h-f-at-meteojob-by-cleverconnect-3891237691?position=8&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=7%2FvZRxNxZ9KZ0pZ0CaSZ5A%3D%3D&trk=public_jobs_jserp-result_search-card,"Description Du Poste
Au sein du groupe Auchan, les équipes d'Auchan Retail International sont au service des différents pays dans lesquels Auchan est implanté.
Experts des métiers du Digital, de la Finance, du Juridique, des Ressources Humaines et de la gestion d'entreprise, leur mission est d'accompagner les entités Auchan pays à se construire, se structurer et se développer.
Nous recherchons aujourd'hui
un(e) alternant(e) Data Engineer
(H/F).
La mission sera de concevoir et développer les nouveaux flux de données (batch et temps réel) en étroite collaboration avec l'équipe actuelle de Data Engineer, Data Scientist et les équipes métiers.
Les Principales Missions Seront Les Suivantes
Analyser les besoins des métiers et les traduire en spécifications techniques (en contribuant également à l'élaboration des contrats d'interface et à la modélisation physique)
Développer des pipelines de données au sein de son périmètre
Veiller à la qualité et l'efficience des développements de l'équipe
Contribuer à la construction de la plateforme data et des services en remontant les besoins non couverts par le framework
Travailler en étroite collaboration avec l'équipe actuelle de Data Engineers, Data Scientists et les équipes métiers.
Tu Évolueras Dans Un Environnement Technique Composé De
Google cloud platform (BigQuery, VertexAI)
SQL
Python
Description Du Profil
Votre profil :
Étudiant(e) en école d'Ingénieurs ou Master Informatique ou équivalent, vous recherchez une alternance et êtes passionné(e) de la Data pour participer à des projets d'envergures au sein d'une équipe Data dynamique.
Compétences Techniques
A l'aise (ou ayant envie de le devenir) dans l'environnement cloud GCP et ses services associés (Bigquery, GCS, Pubsub, Cloud function, Composer etc…)
Vos connaissances sur les langages SQL et Python et des grands principes de modèle seront vos alliés pour répondre aux besoins de cette offre.
Savoir Être
Réactif, avec le sens du service, vous justifiez de bonnes capacités d'écoute et d'un bon relationnel.
Curieux, autonome et proactif.
Parce qu'Auchan Retail est convaincu que la diversité fait la richesse d'une entreprise, nous étudions, à compétences égales, toutes les candidatures et adaptons le processus de recrutement et le poste à tous les profils.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Senior Data Engineer,Foxintelligence,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-foxintelligence-3822491504?position=9&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=qoKsI34biGUtx2FrnXb9Bg%3D%3D&trk=public_jobs_jserp-result_search-card,"Chez Foxintelligence, nous sommes des amoureux de la data. 🤓
Notre mission est de rendre les consommateurs et les marques plus intelligents, grâce à une donnée de marché révolutionnaire issue de l'intelligence collective. Pour les consommateurs, Foxintelligence rend accessibles et utiles leurs données transactionnelles issues de leurs boîtes mail ou comptes bancaires. En les anonymisant, nous créons des statistiques uniques qui permettent aux marques de rester en phase avec les produits qui se vendent le mieux et les attentes des consommateurs.
Nous publions une application grand public aimée par des millions de personnes (Cleanfox, 5m+ de téléchargements, 4.8/5 💖 sur l'app store et le play store) et préparons le lancement d'une app de bilan carbone à l'expérience radicalement nouvelle. Grâce à notre plateforme data, elle va rendre possible une prise de conscience massive des enjeux du changement climatique 🌍 et des actions individuelles possibles pour le combattre. Nous pensons que l'information est une force de changement puissante, lorsqu'elle s'appuie sur la data.
Utilisant ces données personnelles totalement anonymisées, notre plateforme SaaS foxintelligence.io est devenue la référence de la market intelligence digitale en Europe avec des dizaines de grands groupes clients. Le point commun entre Deliveroo, Just Eat, Mano Mano, BackMarket ou The Boston Consulting Group ? Ils vont très vite (on ne s'ennuie pas avec eux !) et ils nous font tous confiance !
Soutenus par des investisseurs de premier plan (17m levés auprès de Daphni, Partech, GFC et eFounders) et récemment intégré au groupe NielsenIQ nous sommes maintenant à la conquête de l'Europe et en phase d'hyper-croissance.
Si notre FoxHQ reste à Paris, notre équipe de plus de 120 talents pluridisciplinaires (tech, data et business) travaille depuis partout en France et même en dehors (ex. Turquie). Nous pensons que notre politique remote-first, notre culture forte (bienveillance, exigence, résilience et data-first) et notre innovation permanente en termes de modes de travail (ex. grille de salaire transparente, formation au changement climatique, transparence sur la stratégie) sont les clés de notre réussite collective.... et de ta réussite avec nous !
Job Description
Nous recherchons un
Senior Data Engineer
pour rejoindre l'équipe chargée des applications et des services internes. Tu seras responsables du cycle de vie complet de tes projets, du design à la mise en production. Les projets récents accomplis par l'équipe comprennent une plateforme d'enrichissement des données de géolocalisation capable de gérer des centaines de millions de data points par jour ainsi qu'un API de machine learning basée sur ChatGPT capable de catégoriser automatiquement nos données.
Tes responsabilités
Design, développement et maintenance de nos services internes d'enrichissement des données
Travailler en étroite collaboration avec nos product managers, data analysts, et autre partie prenantes pour définir les besoin techniques et les spécifications
Coacher et guider nos profils junior
Diagnostiquer et résoudre les différents problèmes techniques qui peuvent survenir (pas d'astreinte)
Requirements
Compétences nécessaires (ce qu'il te faut pour réussir sur ce poste):
Hard Skills
Excellent niveau en SQL et très bonne connaissance du service BigQuery de GCP
Une bonne connaissance de Python est requise
Des connaissances sur Airflow sont fortement recommandées
De bonnes connaissances sur K8s plus généralement en gestion d'infrastructure
Soft Skills
Excellente communication
Capacité à gérer plusieurs priorités et à s'adapter à un environnement en constante évolution
Expérience
Avoir travaillé avec BigQuery
Une solide expérience en gestion de bases de données relationnelles
Ladies
Les études montrent que les femmes ont moins tendance à postuler à une offre d'emploi quand elles n'ont pas toutes les qualifications. Ladies, ne vous mettez pas de barrière et donnez-nous la chance de nous faire notre propre avis, nous serons toujours ravis d'échanger avec vous ! Si notre raison d'être vous parle, postulez !
Self-made data lovers
Les diplômes c'est bien, les skills c'est mieux et l'expérience t'en donne. Aucun diplôme n'est requis chez nous, ce sont les compétences et l'énergie qui comptent !
Recruitment process
Round 0 : entretien (fit) avec notre Head of People
Round 1 : entretien avec un ou 2 manager(s) de l'équipe
Test technique / Etude de cas
Round 2: entretien avec notre CTO
Benefits
Avantages/ce que nous offrons :
Salaire et variable compétitifs
Remote friendly (+ budget aménagement de l'espace de travail @Home)
Bonus Cooptation
Bonne assurance santé (Alan - prise en charge à 50%)
Titres restaurants (swile) : pris en charge à 60% par Fox
Système de crèche subventionné par Fox
Culture forte et pratiques de management à la pointe (stratégie et résultats financiers transparents, feedback 360, grille de salaire innovante et transparente etc.)
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Statistiques'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': ['Remote', 'Junior', 'Senior'], 'TypeContract': [], 'Salary': ['0'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (Snowflake),MindPal,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896992742?position=10&pageNum=10&refId=YRlmHfDXD2q8y%2FLEi2ngtQ%3D%3D&trackingId=Cnl6cwGQ8OVBr%2BVZq52Q3w%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for experienced
Data Engineers
with knowledge of
Snowflake
platform.
Responsibilities
Creating and managing data in the Snowflake environment
Designing and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms
Optimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately
Automating data processing workflows using tools such as Airflow or other workflow management tools
Deploying and configuring tools to monitor and report on the performance of the Snowflake system
Requirements
Minimum 1 year of experience as a Data Engineer
Ability to use Snowflake
Very good knowledge of SQL and programming in Python
Ability to work with databases, including the Snowflake platform
Knowledge of ETL tools and data integration
Ability to work in a team and good communication skills
Fluent English in speaking and writing
We Offer
B2B contract type
Full-time job
Remote and flexible working hours
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer H/F,Meilleurtaux,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-meilleurtaux-3912494325?position=1&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=r4C2Hgw4OP5G7JgzOclkoA%3D%3D&trk=public_jobs_jserp-result_search-card,"Description De L'entreprise
Scale-up emblématique du paysage Tech français et marque connue auprès d’un large public, Meilleurtaux est la marketplace incontournable pour le crédit, l’assurance et le placement. Chaque année, plus de 3 millions de Français utilisent nos services.
Notre mission : offrir à nos clients les meilleurs produits, au meilleur prix avec le meilleur conseil. Notre but : redonner le pouvoir à nos clients et leur faire gagner du temps et de l’argent.
Nos sites web et applis mobiles recueillent plus de 100M de visites par an. Nous travaillons avec de nombreux partenaires banques, assurance et gestionnaires d’actifs.
Notre ADN d’innovation et de remise en question permanente pour servir nos clients nous conduit à lancer chaque semaine de nouveaux projets, pilotés par l’une de nos équipes produit.
Quelques Chiffres
👉 près de 2000 collaborateurs au sein du groupe dont 1000 collaborateurs au sein de notre réseau de franchises.
👉 + de 350 agences réparties sur l’ensemble du territoire.
👉 x2 CA en l’espace de 4 ans.
👉 une satisfaction client à 4,8/5 en 2022.
👉 100 M de visites sur les sites et applis du groupe.
Description Du Poste
Vous souhaitez rejoindre la fintech leader en crédit et en assurance, en forte croissance, innovante, dynamique et débordante de projets ? Ce qui suit va vous intéresser !
Contexte de ce recrutement
🚀
Nous sommes engagés dans le développement d’une
Customer Data Platform.
Cette Plateforme De Données Est Au Cœur De La Stratégie De Croissance De L’entreprise Va Nous Permettre De
Augmenter la Customer Lifetime Value (CLTV) de nos clients,
D'intégrer dans tous nos produits des composants IA innovants,
Réduire nos coûts d’acquisition,
Faciliter le pilotage du business à travers une optimisation de nos outils de BI.
Vous vous épanouirez dans notre environnement en évolution rapide, où l'adaptabilité est essentielle. Au-delà de la résolution de défis techniques, nous souhaitons que vous contribuiez activement à la construction de la culture d'ingénierie de Meilleurtaux, à l'amélioration des pratiques et à la promotion d'un environnement collaboratif et innovant.
Vos missions 📝
Créer et maintenir une infrastructure de données de pointe en permettant aux utilisateurs finaux d'accéder à de la donnée précise et de qualité ;
Développer de nouveaux modèles de données et des pipelines.
Ils ont auront pour objectif de prendre en charge une grande variété de cas d'utilisation (de l'analyse et du reporting à l'apprentissage automatique et à l'innovation de produits) ;
Explorer en permanence de nouvelles technologies de données ;
Tester les solutions les plus innovantes et prometteuses du marché en vue de pouvoir améliorer nos capacités en matière de données ;
Recruter, encadrer et accompagner votre équipe de Data Engineers au quotidien ;
Partager et défendre vos meilleures pratiques d'ingénierie de données au sein des principaux organes de décision de l'entreprise.
Notre stack technique
🛠
Développement : Python, React, java, Salesforce
CI-CD : Git, Docker
Infrastructure cloud : GCP et Azure
Bases de données : Google BigQuery et Databricks
BI : Qliqsense
Ce poste nécessite d'interagir avec de nombreuses équipes au sein de Meilleurtaux que ce soit sur le plan technique (équipes IT, Data Scientist et BI) et/ou fonctionnel (Product Managers).
Ceci n’est qu’un avant-goût de la superbe aventure que vous vous apprêtez à rejoindre, le poste étant évidemment amené à évoluer en fonction de vous et vos propositions.
Qualifications
Pourquoi êtes-vous notre TOP candidat ? 🧐
Avec une première expérience réussie de 2 à 4 ans en tant que Data Engineer.
Vous savez créer des architectures de données efficaces, évolutives et robustes.
Vous concevez des systèmes adaptés au présent mais également à l'avenir et qui résistent à l'épreuve du temps.
Bien entendu, il est important que vous ayez de très bonnes compétences techniques que ce soit en Python ; SQL et les autres outils pouvant exister en ingénierie de données.
La Big Data n'a plus de secret pour vous. Spark & Hadoop sont vos plateformes de référence sur ce domaine.
Vous avez déjà participé au déploiement d'infrastructure en Big Data.
Idéalement, vous faîtes partie d'une communauté de professionnels de la Data vous permettant d'être toujours au fait des dernières actualités.
Le must : cette expertise a été acquise au sein de l'industrie Fintech / Assurtech ou secteur équivalent.
Les soft-skills attendus pour réussir chez Meilleurtaux ?
Du leadership : vous savez embarquer vos interlocuteurs dans un projet structurant et à forts enjeux pour l'entreprise.
De la curiosité et de l'apprentissage continu : dans un domaine en constante évolution, nous recherchons une personne connectée aux nouvelles technologies et aux dernières innovations.
De l'adaptation : vous savez prendre en considération les contraintes de l'entreprise et évaluer les risques techniques.
Comment se déroule le processus de recrutement chez nous ?
1 -
Premier échange avec la Team RH pour apprendre à mieux vous connaître.
2 -
Rencontre sur place avec votre futur manager, l'équipe éventuellement et un membre de l'équipe RH.
3 -
Réalisation d'une étude de cas et restitution avec le manager (selon le poste).
4 -
Votre candidature a été retenue ? Félicitations et bienvenue à bord 🎉
Qu'attendez-vous pour participer à la construction de cette équipe de Data Engineers et partager votre expertise au sein d'une équipe passionnée par les sujets de Data ?
Informations supplémentaires
En Nous Rejoignant , Meilleurtaux Vous Offre 😍
Un package attractif comprenant primes individuelles et collectives (participation et intéressement).
La mutuelle ALAN avec des tarifs avantageux, une prise en charge rapide des remboursements et la prévoyance prise en charge à 100% par Meilleurtaux.
Une carte SWILE avec 10€ de tickets restaurant par jour travaillé.
Une prise en charge de votre transport de ""mobilité douce"" (trottinette, scooter, vélo...) dans le cadre du Forfait Mobilités Durables.
L'accès au CSE, sans condition d'ancienneté.
2 jours de télétravail / semaine après votre période d'intégration et une prise en main de votre poste.
Pour Vous Proposer Un Environnement De Travail Stimulant Et Propice à L'épanouissement Et L'atteinte De Ses Objectifs, Meilleurtaux a Mis En Place 💥
Des moments de convivialité (afterwork, Lunch & Learn, point d'actualité Groupe, soirées d'entreprise, secret coffee...)
La possibilité de choisir votre matériel de travail (ordinateur & téléphone)
Un dispositif d'accès à la formation à travers l'attribution d'une licence Openclassrooms (formation au choix selon vos envies et besoins).
Nous n'attendons plus que vous !
Le manque de confiance peut parfois nous empêcher de postuler à un emploi. Mais nous allons vous révéler un secret : il n’existe pas de candidat « parfait ». Donc, n’hésitez pas à postuler si ce poste vous donne envie de vous dépasser tous les jours.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Adaptabilité', 'Leadership'], 'EnSoftSkils': ['Leadership']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Consultant Data Engineer,WHIZE,"Neuilly-sur-Seine, Île-de-France, France",https://fr.linkedin.com/jobs/view/consultant-data-engineer-at-whize-3907486262?position=2&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=w6CYjt5HexGT1E7LfIFO3Q%3D%3D&trk=public_jobs_jserp-result_search-card,"Offre d’emploi pour un CDI : Consultant Data Engineer
WHIZE est spécialisée dans le développement de solutions sur mesure en architecture Serverless (Azure, Amazon WS, Google CP) et de solutions basées sur l'écosystème Microsoft 365 (SharePoint, Teams, Power Platform).
Vos missions :
Concevoir des solutions de traitement de volume très important de données.
Développement de flux de données et préparation de leur analyse.
Préparation des données pour l'analyse des données collectées.
Profil recherché :
2 ans minimum d’expérience.
Maîtrise du langage Python et Scala
Connaissance d'un ou plusieurs ETL du marché (Talent , SSIS, Azure Data Factory, ...)
Forte expertise en SQL
Être à l’aise avec un ou plusieurs outils Devops (Jenkins, git, GitHub, gitlab, docker, kubernetes, etc…)
Connaissances appréciées :
Hadoop, Spark, Kafka
Connaissance des systèmes NoSQL : Elasticsearch, HBase, Cassandra, Redshift
Connaissance de l'offre data d'un des providers Cloud (GCP, Azure, AWS)
Qu’attendez vous pour nous rejoindre ?
Vous ferez partie d’une société à taille humaine et qui bénéficie des avantages d'un grand groupe. Nous adressons une centaine de clients en direct dont la moitié sont des grands comptes.
Vous serez accompagné(e) et managé(e) par le CEO de WHIZE (THE WHIZE MAN).
Vous allez compléter notre équipe dynamique et travailler avec nous dans une ambiance Start-up et conviviale.
Vous occuperez des postes intéressants et évolutifs.
Vous bénéficierez des évènements internes organisés pour parler tech, business et projets.
Vous réaliserez des projets à forte valeur ajoutée.
📍 : Neuilly-Sur-Seine+ Télétravail
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'Cassandra', 'HBase', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
"Big Data Engineer – Paris, France (H/F)",Astek,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/big-data-engineer-%E2%80%93-paris-france-h-f-at-astek-3470775874?position=3&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=6%2FLH5FlHsKFTsDTg3Z1MyA%3D%3D&trk=public_jobs_jserp-result_search-card,"CDI
Paris - France
Publiée il y a 2 mois
Le Groupe Astek
Ce Que Nous Allons Accomplir Ensemble :
Nous rejoindre en tant que
Big Data Engineer (H/F),
afin d’accompagner un opérateur télécoms, Leader en Europe dans la création d’une infrastructure cloud (IAAS) performante, robuste et sécurisée.
Un challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !
Votre Mission, Si Vous L’acceptez :
Qualifier les données et les résultats
Conception technique des solutions
Décliner les impacts de la stratégie et des innovations technologiques au sein des processus et outils de l’exploitant SI
Assurer l’accompagnement et le déploiement des évolutions des processus et outils
Contribuer aux programmes de transformation DevOps, Cloud et catalogues des offres SI
Développer des fonctions transverses et les « uses cases »
Accompagner la phase de mise en production
Votre Future Équipe :
Vous intégrerez une équipe à la fois technique et fonctionnel, qui œuvre chaque jour pour développer et maintenir en conditions opérationnelles l’ensemble des solutions IT !
L’équipe est en interaction avec des clients à la fois internes et externes.
Votre stack de jeu
Système d’exploitation : Linux
Environnement Big data : Hadoop, Spark, Scala
Cloud computing : GCP ou AWS
Base de données : No SQL (Cassandra, Mongo DB)
Dataviz : Power BI ou Kibana
Des notions en développement feront la différence !
Les Petits Plus Du Projet :
Vous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.
Vous ?
Diplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.
Vous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?
Vous faite preuve de proactivité et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?
Alors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !
Astek
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de
ses 7800 collaborateurs
qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.
Tous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.
Rencontrons-nous
Notre projet commun vous plait ?
Postulez à cette annonce, et soyez transparent !
Maud, notre Talent Acquisition Referent, vous contactera pour un premier échange.
Puis vous rencontrerez Yoram, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .
Enfin, vous rencontrerez Anthime, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.
Nos Plus
Astek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo
Une politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)
Notre charte de la Diversité
Mots-clés :
ingénieur – ingénieure – consultant – consultante – developpement – Scala – Data
Caractéristiques de l'emploi
Catégorie Ingénieur
Job Industry Télécom / Média
Postuler en ligne
Nom *
Prénom *
Email *
Un email valide est requis.
Téléphone *
Un numéro de téléphone valide est requis.
Joindre un CV *
Mots-clés :
ingénieur – ingénieure – consultant – consultante – developpement – Scala – Data
Show more
Show less","{'ProgLanguage': ['Scala', 'R', 'Go'], 'DataBase': ['SQL', 'Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
"Big Data Engineer – Lille, France (H/F)",Astek,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/big-data-engineer-%E2%80%93-lille-france-h-f-at-astek-3839097187?position=4&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=NviWlQb3wBpX4sQPbEeH4w%3D%3D&trk=public_jobs_jserp-result_search-card,"CDI
Lille - France
Publiée il y a 2 mois
Le Groupe Astek
Ce Que Nous Allons Accomplir Ensemble :
Nous rejoindre en tant que
Big Data Engineer (H/F),
afin d’accompagner un opérateur télécoms, Leader en Europe dans la création d’une infrastructure cloud (IAAS) performante, robuste et sécurisée.
Un challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !
Votre Mission, Si Vous L’acceptez :
Qualifier les données et les résultats
Conception technique des solutions
Décliner les impacts de la stratégie et des innovations technologiques au sein des processus et outils de l’exploitant SI
Assurer l’accompagnement et le déploiement des évolutions des processus et outils
Contribuer aux programmes de transformation DevOps, Cloud et catalogues des offres SI
Développer des fonctions transverses et les « uses cases »
Accompagner la phase de mise en production
Votre Future Équipe :
Vous intégrerez une équipe à la fois technique et fonctionnel, qui œuvre chaque jour pour développer et maintenir en conditions opérationnelles l’ensemble des solutions IT !
L’équipe est en interaction avec des clients à la fois internes et externes.
Votre stack de jeu
Système d’exploitation : Linux
Environnement Big data : Hadoop, Spark, Scala
Cloud computing : GCP ou AWS
Base de données : No SQL (Cassandra, Mongo DB)
Dataviz : Power BI ou Kibana
Des notions en développement feront la différence !
Les Petits Plus Du Projet :
Vous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.
Vous ?
Diplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.
Vous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?
Vous faite preuve de proactivité et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?
Alors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !
Astek
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de
ses 7800 collaborateurs
qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.
Tous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.
Rencontrons-nous
Notre projet commun vous plait ?
Postulez à cette annonce, et soyez transparent !
Maud, notre Talent Acquisition Referent, vous contactera pour un premier échange.
Puis vous rencontrerez Yoram, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .
Enfin, vous rencontrerez Anthime, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.
Nos Plus
Astek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo
Une politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)
Notre charte de la Diversité
Mots-clés :
ingénieur – ingénieure – consultant – consultante – developpement – Scala – Data
Caractéristiques de l'emploi
Catégorie Ingénieur
Job Industry Télécom / Média
Postuler en ligne
Nom *
Prénom *
Email *
Un email valide est requis.
Téléphone *
Un numéro de téléphone valide est requis.
Joindre un CV *
Mots-clés :
ingénieur – ingénieure – consultant – consultante – developpement – Scala – Data
Show more
Show less","{'ProgLanguage': ['Scala', 'R', 'Go'], 'DataBase': ['SQL', 'Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data engineer Palantir - H/F - Paris,Lojelis,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-palantir-h-f-paris-at-lojelis-3901067276?position=5&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=EUheVK8VM82DLRjEgxHx8g%3D%3D&trk=public_jobs_jserp-result_search-card,"Présentation de la société
Lojelis
est né en 2005, d’un projet entrepreneurial de Sylvain Jourdy, toujours à la tête du groupe. Historiquement intégrateur de solutions ERP, lojelis a évolué au fil des années dans un objectif d’accompagnement des entreprises dans leurs problématiques technologiques et métiers.
Experts en
ERP, Développement, Business Intelligence, AMOA
et
PMO,
nous sommes 170 collaborateurs répartis sur 6 agences en France. Nous conseillons nos clients sur tous les secteurs d’activités et sans distinction de taille. Nos équipes les accompagnent tout au long de leurs projets, de la conception à la mise en production et au support.
Nous vous proposons donc de rejoindre une entreprise qui mise sur la proximité avec ses collaborateurs et où la créativité et la prise d’initiatives y sont fortement encouragées.
Description des missions
Dans le cadre de divers projets en région parisienne, nous recherchons un(e) Data Engineer maistrisant Palantir Foundry.
Le contexte ?
Nous aidons nos clients dans leur transformation digitale.
Votre rôle sera d'accompagner les utilisateurs métiers dans l'expression, la formalisation, l'analyse et la modélisation de leurs besoins SI, ainsi que dans le suivi des projets.
Vos missions ;
Construire, livrer et maintenir des produits de données.
Travailler avec les équipes produits afin de développer de nouvelles fonctionnalités.
Etre responsable de la construction, de la livraison, de la maintenance et de la documentation d'artefacts de données.
Conseiller sur l'architecture des flux de données de bout en bout.
Profil recherché
Votre profil ?
Après 1 à 2 ans en tant que Data Engineer ou Data Scientist, vous désirez évoluer au sein d'un grand groupe afin de continuer à développer vos compétences.
You speak and write English in a professional environment
Vous maitrisez les technologies Python et SQL.
Vous avez déjà travaillé sur l'outil Palantir Foundry.
Nous recherchons surtout une personne dotée d'un bon relationnel qui apprécie le travail en équipe et sait se montrer force de proposition.
Les petits +
Nous nous engageons à éviter toute discrimination et à traiter tous nos candidats de manière équitable. C’est pourquoi l’ensemble de nos intervenants au processus de recrutement sont formés au recrutement et à la non-discrimination peu importe sa nature selon la loi applicable.
Notre processus de recrutement ?
Etude du CV
Echange téléphonique pour faire connaissance
Un entretien RH pour parler plus en détail de vos aspirations et de Lojelis
Un entretien métier avec la manager du pôle AMOA
Prise de décision et retour
Notre processus d’intégration ?
Informations transmises en amont de votre arrivée : documentations, planning de votre premier jour, …
Parcours d’intégration : présentation de Lojelis, des locaux, des équipes, des projets, des outils et process, formations au besoin, …
Exemples de ce qu'il vous attend chez Lojelis
Qualité de vie au travail et RSE
Des managers de pôle qui vous accompagnent au quotidien dans votre carrière
Carte titres restaurant Edenred : 8,80€ par jour pris à 60% en charge par Lojelis
Mutuelle couvrant gratuitement les enfants
Remboursement de 75% de l’abonnement transports en commun
10 jours de compensation/an dès 6 mois d’ancienneté
Prime vacances (convention Syntec)
Primes de cooptation
3 jours de télétravail possibles par semaine
Un CSE offrant différents avantages
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Créativité'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer,HarfangLab,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-harfanglab-3849760187?position=6&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=ee2YG0IB53MykNwqdA7klQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Who we are?
HarfangLab
is a
cybersecurity scale-up
, and we have developed an
Endpoint Detection and Response
(EDR) software to
detect and mitigate modern cyberattacks
on a company's workstations and servers. Our algorithms detect abnormal behaviors and generate security alerts or block program execution.
From 50 to 100 employees in 2023, HarfangLab is experiencing hypergrowth and has already achieved several significant milestones: winners of the Ministry of Defense's cyber challenge in 2019, recipients of the BPI's I-Nov competition in 2020, and software certified by ANSSI in 2021.
Our initial clients include CAC40 industrial companies and government entities. We completed our
first funding round of €5 million in 2021 and our second funding round of €25 millions in 2023
, which will enable us to strengthen our teams, and to expand internationally in Europe.
Our mission is to
protect businesses and government agencies from modern cybersecurity threats
(cybercrime, data theft, influence)
that endanger the economic health of companies and the security of the nation
.
What you will do with us?
You will work within the
Artificial Intelligence team
, consisting of 5 individuals, under the direct and daily supervision of the team lead.
This team designs, implements, and deploys supervised algorithms for detecting malicious behavior.
As a
Data Engineer
you will:
Gather requirements from stakeholders,
Manage data for the AI and CTI departments,
Design, develop, and maintain the existing data warehouse,
Implement a data lake if deemed appropriate,
Create data pipelines using ELT processes,
Design tools for data visualization.
About You
Hard Skills
Master’s degree in Computer Science, Engineering, or a related field,
Proven experience as a Data Engineer, 2 years minimum,
Proficient in Python,
SQL: Strong in SQL syntax and query optimization, NoSQL will be a big plus,
Competence in data warehousing and data lake architecture,
Proficiency in at least one ELT tool and strong understanding of related processes.
Soft Skills
Strong communication and teamwork skills,
Excellent problem-solving and attention to detail,
You enjoy learning and sharing your knowledge with others,
You demonstrate initiative - when an opportunity arises to improve existing processes, you seize it.
About Us
Our office and Team Life:
Offices located in the heart of Paris, near Bourse (75002),
High-quality equipment based on preferences and needs (PC, Mac, additional screens, etc.),
Thanks to our Office Manager, we regularly organize events such as seminars, happy hours, themed evenings, and more,
An onboarding process to welcome each new colleague with an explanation of the roles and a mentor to support you during your early days!""
A great team that always seeks to improve their skills
And more:
An attractive package: Base salary + profit sharing,
Flexible remote work options,
A mentor to guide you throughout your probationary period,
Health insurance: The best health insurance with Alan and Moka Care, a mental health at work app,
Meal vouchers: We use the Swile card and also have access to a discount platform through our works council,
7 to 11 additional days off (RTT) per year, in addition to the 25 days of paid vacation. GymLib subscription, covered 80% by HarfangLab,
Access to training and events of your choice and according to your professional needs.
The recruitment process
A 30-minutes call with our Talent Acquisition Manager,
A 30-minutes visio interview with the Hiring Manager,
A 1 hour on-site interview + 30 minutes with the team for a team fit assessment,
A psychometric test to assess your motivations and soft skills,
A final HR video appointment to review your soft skills and motivations.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Teamwork', 'Initiative']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['7', '7'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Senior Data Engineer (H/F),relevanC,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-h-f-at-relevanc-3845776902?position=7&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=%2BIBvHbhwTiBQPpKpHBIS8g%3D%3D&trk=public_jobs_jserp-result_search-card,"relevanC est une filiale du Groupe Casino et a été fondée en 2017.
Nous avons des bureaux en France, au Brésil et en Colombie et opérons à l'échelle mondiale.
Nos solutions de Retail Media permettent à nos clients de générer de nouvelles sources de revenus publicitaires grâce à des annonces pertinentes et personnalisées.
En tant que Data Engineer tu auras accès aux données de nos clients internes (enseignes du groupe Casino) et externes à traiter au sein de notre data warehouse. Tes missions seront les suivantes :
travailler en étroite collaboration avec tous les autres membres de la squad
écrire / relire du code en respectant les bonnes pratiques de développement ainsi que les tests unitaires et participer
assurer la co-responsabilité du déroulement des déploiements, des mises en production et du bon fonctionnement des applications avec les autres membres de la squad
rédiger la documentation technique quand cela est nécessaire
mettre en œuvre les bonnes pratiques relatives au RGPD telles que définies par le tech lead
Ce CDI basé à Paris centre (1er arrondissement) débutera dès que possible.
Faire partie de relevanC, qu’est-ce que ça signifie ?
Travailler sur une stack technologique de pointe (Python, PySpark, Google BigQuery, Apache, Airflow…)
Être membre à part entière d’une équipe dynamique et passionnée aux profils très variés (chefs de projets, développeurs, designers, animations commerciales)
Travailler dans un environnement stimulant et relever des nouveaux défis chaque jour
Rejoindre une entreprise en pleine expansion avec des opportunités fortes de développements et d’innovation
Profil recherché
Diplômé(e) d’une grande école d’ingénieur ou profil universitaire spécialisé en Data / Informatique / Math / Stats.
5 ans (et plus) d’expérience en Data Engineering
Appétence forte pour le marketing digital et le retail, force de proposition, business oriented et moteur d’innovation
Une maitrise parfaites des bonnes pratiques de développement
Solides compétences en Python, Spark et SQL
Une expérience sur Google Cloud Platform est un plus
Lien vers notre politique de traitement des données : https://relevanc.com/fr/politique-de-protection-des-donn%C3%A9es-recrutemen
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Chef', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data engineer senior - PAU,Capgemini,Greater Pau Area,https://fr.linkedin.com/jobs/view/data-engineer-senior-pau-at-capgemini-3905823820?position=8&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=l8ZlSgutdgTLQMyhKsDHJQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Capgemini :
Choisir Capgemini, c’est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l’inspiration d’une communauté d’experts dans le monde entier, vous pourrez réécrire votre future.
Rejoignez nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.
Vos missions :
Au sein d'une équipe pluridisciplinaire, vos missions seront de :
Concevoir, développer et modéliser des outils de traitement et visualisation de la données
: mettre en place des pipelines de collecte, stockage et transformation de la données dans un contexte DevOps ; mettre en place des outils de nettoyage de la données afin de la rendre accessible et exploitable aux Data analysts et Data scientists.
Industrialiser des solutions dans un contexte Cloud DevOps
.
Collaborer à la stratégie projet et au développement de l'offre portée par Capgemini
: se positionner comme expert dans votre domaine, apporter et concevoir des solutions innovantes garantissant à nos clients compétitivité et respect des normes liées à leur secteur d'activité.
Accompagner de jeunes développeuses et développeurs dans leur montée en compétence
: apporter conseils techniques et retours d'expériences ; les faire bénéficier de vos savoirs et expertises ; les challenger en leurs proposant au quotidien un accompagnement qui leur permettra de sortir de leur zone de confort.
Participer à la vie de l'équipe data de Pau
: proposer, organiser et assister aux événements et ateliers.
Votre profil :
Diplôme en informatique, Data, Big Data ou équivalent
Minimum 4 années d'expérience sur un poste similaire en ESN, client final ou cabinet
Anglais courant
Maitrise du langage de programmation Python (notamment Pandas)
Maitrise des technologies Cloud (AWS, Azure ou GCP) et outils DevOps (Ansible, Kubernetes, Terraform…)
Maitrise des processus de mise en place de pipeline, gestion et traitement de la donnée.
Aptitude démontrée à travailler de manière autonome tout en collaborant efficacement au sein d’une équipe multidisciplinaire
Forte appétence pour le challenge et l'innovation.
3 raisons de nous rejoindre
:
Qualité de vie au travail
: accord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu
: certifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE
: plan actionnariat, activités à tarifs préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités :
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
A propos de Capgemini :
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profil de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': [], 'DataAnalytics': ['Pandas', 'R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible', 'Kubernetes'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '55', '55', '55']}"
Data Engineer (F/H) - en alternance,Carrefour,"Massy, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-en-alternance-at-carrefour-3884390443?position=9&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=j2jnbwpHUusdjaD5Xbcjaw%3D%3D&trk=public_jobs_jserp-result_search-card,"Le saviez-vous ?
Nous rejoindre, c’est rejoindre l’un des leaders mondiaux de la distribution qui met l'accent au quotidien sur la diversité, la RSE et le digital, pour satisfaire nos clients et nos collaborateurs. En tant que partenaire premium des Jeux Olympiques et Paralympiques de Paris 2024, nous partageons les valeurs du sport en permettant à nos équipes de se dépasser et encourageons une alimentation saine au juste prix pour tous.
Vous cherchez à travailler dans une entreprise dynamique où votre travail rime avec impact social et environnemental ? Bienvenue chez nous !
Data Engineer (F/H) - en alternance
En tant qu' alternant, vous intégrerez la plateforme supply chain où vous serez amené à appuyer le pôle prévision et optimisation particulièrement sur des sujets data et d’analyse de données.
Au sein d'une équipe composée de data scientists et de data engineers organisée en mode Scrum Agile, vous travaillerez pour améliorer au quotidien un outil de calcul de prévision (prévision de la demande des entrepôts Carrefour). Vous participez à l'évolution fonctionnelle et technique de l'application.
🎯 Les missions
Dans ce cadre, vous serez amené à
Explorer et analyser les données du datalake carrefour
Participer au cadrage des nouvelles fonctionnalités
Développer les évolutions des traitements, des modèles statistiques et de machine learning de prévision et des reporting
Tester les fonctionnalités développées
Répondre aux demandes utilisateurs
👥 Profil
Vous êtes en école d’ingénieur, en master 2 ou équivalent avec une spécialisation data science, data engineering, statistique, informatique.
Vous avez une expérience en traitement et analyse de données.
Vous avez un esprit d’analyse et la capacité de travailler en équipe et à distance.
Vous êtes autonome et rigoureux, fluide dans votre communication orale et écrite et à l'écoute des besoins de vos interlocuteurs.
Vous êtes reconnu pour vos capacités d'anticipation, votre sens de l'initiative et votre réactivité.
Vous avez une bonne connaissance des langages suivants
SQL
Python
Une connaissance de GCP et de Big query serait un plus.
Une connaissance même théorique de la méthodologie agile serait un plus
Une connaissance de GIT serait un plus.
Encore plus de bonnes raisons de nous rejoindre
Intégrer une équipe conviviale à taille humaine au sein d‘un grand groupe
12 % de remise sur achat
📝 Informations complémentaires
Date de début  09 septembre 2024
Durée  1 an
Lieu  Lyon
Déplacements en magasin et en concurrence dans la région parisienne
Avantages 50 % du titre de transport pris en charge par Carrefour
Envie de rejoindre l’aventure ?
Chez Carrefour, nous avons à cœur de ne passer à côté d’aucun talent et sommes fiers de compter des équipes représentatives de la société dans son ensemble. Nous encourageons ainsi tous types de profils à postuler à cette offre et garantissons un processus de recrutement dénué de toutes formes de discriminations.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Big Query'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Statistiques'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer +3 years xp,OntraaK,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-%2B3-years-xp-at-ontraak-3909457589?position=10&pageNum=12&refId=RDfIAW6D%2Bq5WzkINrCwGag%3D%3D&trackingId=pm1pYKxQsqU4o%2FAja6BOYg%3D%3D&trk=public_jobs_jserp-result_search-card,"Company Description
OntraaK is a start-up focused on operational excellence powered by advanced technologies such as process mining, intelligent process automation, and execution management systems. Our goal is to enable deep knowledge of real processes, identify inefficiencies, and automate processes at scale. We work with a pragmatic and flexible approach, maximizing value for our customers during proof of value or organization-wide implementation projects. Our team consists of data engineers, business and process experts, and managers who are certified on the most advanced solutions in the market.
We are looking for
2 Experienced Data Engineers
. You have 3 to 5 years in data engineering positions. You are passionate about playing with big set of datas from multiple sources. You love to improve complex situations with data driven decisions, to work in an innovative environment, and at the end of the day to transform all this in high valuable insights ? You love challenges and are ready to take off by developing your skills with a dynamic team ?
Let’s have a chat !
As a Data Explorer by OntraaK, you will deliver highly valuable data-driven insights through your data expertise. Our passion for solving complex and challenging problems four our customers is what is moving us every day. Your role as a Data Explorer is key to deliver this value !
Role Description
This is a full-time data engineer role at OntraaK. As a data engineer, your day-to-day tasks will include data exploration, connecting source systems, data preparation, data modeling, and implementing data solutions. This is a hybrid role based in Paris, with the flexibility for some remote work.
Qualifications
Data Engineering, Data Modeling, and Extract Transform Load (ETL) skills
Data Warehousing and Data Analytics skills
Experience in working with large and complex datasets
Proficiency in SQL and programming languages such as Python or Java
Strong problem-solving and analytical skills
Excellent communication and collaboration abilities
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
Experience with process mining or intelligent process automation is a plus
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Organization', 'Flexibility']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Senior Data engineer (H/F),Believe,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-h-f-at-believe-3904676141?position=1&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=yWYK7VADP87Qgb%2BjxbIrEg%3D%3D&trk=public_jobs_jserp-result_search-card,"Description De L'entreprise
Believe est avant tout une passion pour la musique, la technologie et le marketing numérique, partagée par plus de 1720 talents dans plus de 50 pays. C'est un esprit visionnaire et entrepreneurial qui nous anime et fait de nous un leader mondial de la distribution numérique de musique.
Believe est une tribu d'experts qui relève avec succès les défis de la transformation de notre industrie musicale au quotidien. C’est une aventure, une aventure humaine, propice et stimulante pour nous tous.
Enfin, Believe est une histoire qui a débuté en 2005 et que nous devons continuer à raconter, maintenant et avec vous. Believe a pour mission de développer les labels et les artistes de manière adaptée à chaque stade de leur carrière ; dans tous les marchés locaux dans le monde ; avec respect, expertise, équité et transparence.
www.believe.com
Ready to #setthetone with Believe?
Description Du Poste
Contexte
Vous intégrez la Tribu Data Office de Believe et la squad Dataplateforme en tant que Data engineer.
La Dataplateforme s'appuie sur une stack AWS / Snowflake avec des pratiques à l'état de l'art en data engineering et data modélisation, et des contraintes de fort volume et haute performance.
Au Sein De La Squad Organisée Selon La Pratique Scrum, En Charge Du Build Et Du Run De Son Périmètre, Vous Aurez Pour Responsabilité D'implémenter Les Flux De Données Depuis L'ingestion Des Nouvelles Sources Jusqu'à Leur Préparation Pour Exposition Au Travers Des Niveaux Bronze, Silver Et Gold (warehouse) Vous Maitrisez Ainsi
La conception et le développement d'un pipeline d'ingestion sur AWS (Step function / python / S3) La transformation de données (Python, Spark Scala / Databricks, SQL) Les pratiques de test (TU Python, TU Spark Scala) La modélisation Data en flocons Les pratiques de run (surveillance, monitoring, fix) La rédaction de la documentation (Data galaxy / Confluence) Les enjeux de performances, de suivi des couts et de sécurité
Qualifications
Pourquoi nous rejoindre ? Chez believe, notre leitmotiv est simple : ouverture d’esprit, passion et implication ! On appréciera aussi votre agilité, votre sens de l’innovation, votre excellent relationnel et votre enthousiasme !
Vous êtes de niveau BAC+3 à BAC+5 (Écoles d'ingénieurs, BTS, DUT, DESS, Mastère).
Vous justifiez d’une expérience significative (au moins 3 ans) dans les technologies de notre stack: AWS, Snowflake, python, spark scala, SQL.
L'esprit Craft est une évidence.
Vous êtes convaincu de l’importance de la donnée, et pensez qu’elle est à mettre en regard des besoins métiers. Bref, vous aimez la donnée et êtes pragmatique.
Vous aimez vous tenir au courant des nouvelles évolutions technologiques, et pratiquez une veille régulière.
Votre niveau d’anglais est courant, à l’oral comme à l’écrit.
Informations supplémentaires
Set the tone with us
Chez Believe, nous avons deux cœurs : nos collaborateurs et nos artistes.
Nous croyons en la force de nos collaborateurs, qui s'épanouissent chaque jour en développant leur potentiel... Notre objectif est d'offrir à nos collaborateurs le meilleur environnement possible pour qu'ils puissent s'épanouir.
Rock the job
Programme de formation et de coaching sur mesure
Une politique de télétravail
Un programme de bien-être ""Pauses"" avec de nombreuses activités et animations en interne
Accès à Eutelmed, la plateforme numérique de santé mentale et de bien-être qui permet de parler à un psychologue expérimenté
Un restaurant d'entreprise sain et éco-responsable
Une assurance santé individuelle ou familiale
Avantages CE
Un rooftop
Une salle de sport avec des cours gratuits
Sing in harmony
Des groupes d'ambassadeurs pour s'engager sur la réduction de l'empreinte carbone et environnementale de Believe et l’équité professionnelle Femme/Homme.
Mise en place du Forfait mobilité durable: remboursement jusqu’à 600€ des frais de transport en commun/avec une faible empreinte carbone.
Congé 2nd parent de 5 jours calendaires rémunérés à 100% (en plus du congé légal paternité ou du congé d’adoption, nous ne l’attribuons pas au congé maternité)
Believe s’engage à garantir l’égalité des chances en matière d’emploi, sans tenir compte de l’origine, du sexe, des mœurs, de l’orientation sexuelle, du genre, de l’âge, de la situation de famille, de l’état de grossesse, d’une prétendue race, des opinions politiques, des activités syndicales, des convictions religieuses, de l’apparence physique, du nom de famille, du lieu de résidence, de l’état de santé, ou en situation de handicap.
Découvrez nos nouveaux locaux : bit.ly/believeoffice
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Confluence'], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5', 'Bac+3'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
"Data Engineer F/H - Système, réseaux, données (H/F)",UpMan Consulting,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-syst%C3%A8me-r%C3%A9seaux-donn%C3%A9es-h-f-at-upman-consulting-3901572843?position=2&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=ZKfIreukuM7MAeHx18f0CA%3D%3D&trk=public_jobs_jserp-result_search-card,"Cette offre d’emploi est fournie par Pôle emploi
Description
Descriptif du poste: C est maintenant l occasion pour toi de rejoindre UpMan Consulting Notre ambition est de trouver les meilleurs profils Data Engineer pour intervenir chez nos clients grands comptes de la métropole lilloise. On te propose une expérience professionnelle en adéquation avec ce que tu souhaites réellement. Tu découvriras une ambiance de travail saine & bienveillante, tu participeras activement au développement d une Happy StartUp, actuellement en forte croissance. Où convivialité rime avec efficacité & où ta performance individuelle contribue à notre réussite globale. Tes missions / compétences techniques Si tu l acceptes, ton rôle & tes missions seront les suivantes : * Réaliser le processus d intégration de nouvelles données (réflexion sur la solution, mise en place d ETL, règles de nettoyage, anonymisation ) * Être garant de l'accès aux sources de données. * Maîtrise de la donnée et être le garant de sa qualité (référencement, normalisation et qualification) afin d'en faciliter l'exploitation par les équipes (Data Analysts et Data Scientists). * Maîtrise de technologies Big Data et Cloud : Hadoop, Spark * Assurer la supervision et l'intégration de données structurées et non structurées venant de sources multiples, tout en veillant à garder des données de qualité. * Assurer le suivi, la cartographie et la documentation des données intégrées * Afin de garantir une bonne exécution de ta mission, nous recherchons les compétences techniques suivantes : Langages de programmation : * SQL * Python / Pyspark * Java/Scala (plus rare, mais important) Différents types d ETL & orchestrateurs : * Airflow * Dagster * Prefect * SSIS/informatica * Talend Plateformes cloud : * GCP * Microsoft Azure * AWS Base de données relationnelles & NoSQL : * postgreSQL, MySQL,... * Redis, graphDB * Data warehouse/data transform : * Snowflake, Bigquery (très important) * DBT Qualité & compétences nécessaires * Communiquant.e dans l âme * Avoir une bonne capacité de synthèse & l esprit critique * Travail d équipe * Curiosité aiguë * Comprendre les objectifs & les besoins Nice to have * Anglais courant * Connaissance de la méthodologie DevOps * Notions en Data-science The office Pas de full remote (pour l instant) mais de l hybride dans la plupart des missions. En moyenne, 2 jours de télétravail par semaine. Cependant, les portes de nos bureaux à Wambrechies sont toujours ouvertes pour accueillir nos collaborateurs pendant leurs journées de télétravail & passer une bonne journée tous ensemble ! Le salaire Junior : 30K à 36K Maîtrisant : 37K à 43K Expert : 44K à 50K & plus + notre package avantage Profil recherché: Ton Profil Tu es une personne passionnée & passionnante. Tu as envie d'évoluer, de partager, de participer à une mission collective & découvrir LA nouvelle façon de collaborer avec une ESN made in Lille. Tu peux justifier d'une expérience forte & significative en tant que Tech Lead Java, dans le développement Java ! Pas besoin d'avoir trop ou pas assez de diplômes, chez nous, ce sont les compétences qui priment  ! On se rencontre, on discute, on échange sur tes envies professionnelles & on laisse la magie opérer. L'envie de grandir & de monter en compétences est ton moteur au quotidien. Tu aimes les problématiques complexes et les défis technologiques. On dit de toi que tu es un.e agiliste dans l'âme, qui effectue une veille constante, à l'affût de tout ce qui évolue autour de toi... Ne réfléchis plus, saute le pas & découvre UpMan Consulting, tu ne seras pas déçu. Tu balances ta démission ?
PROFIL SOUHAITÉ
Expérience
Expérience exigée de 1 An(s)
Source: Pole emploi (https://www.pole-emploi.fr)
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['MySQL', 'PostgreSQL', 'Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Hybride', 'Remote', 'Full', 'Junior'], 'TypeContract': [], 'Salary': ['30K', '1'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Consultant Data Engineer F/H,SOFTEAM,"Île-de-France, France",https://fr.linkedin.com/jobs/view/consultant-data-engineer-f-h-at-softeam-3839971789?position=3&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=vE0RySd%2Bbfx6mGbdJZuJRg%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous évoluez dans le domaine de la Data et souhaitez intégrer
un leader de la transformation numérique spécialisé dans les secteurs de la Banque-Finance-Assurance ?
Rejoignez notre communauté et développez vos compétences chez
SOFTEAM DATA !
VOTRE MISSION
Pour le compte d'un client grand compte, vos missions seront :
A
comprendre le besoin
de nos clients au travers de missions de type : aide aux choix d’outils, cadrage des besoins, POC ;
Recueillir et analyser les besoins et proposer une architecture technique
adaptée aux cas d’usage des clients ;
Conduire des projets de
déploiement des Modern Data Platform
(Applications Cloud Native, Migration d’applications existantes) et participer à la mise en œuvre
Fournir une expertise technique approfondie
aux équipes projets
Rédiger la documentation permettant à l'IT d'assurer la maintenance ;
VOTRE PROFIL
Vos
compétentes techniques
:
Vous avez un minimum de
3
années d’expérience sur des
projets Data sur les outils ETL
(Talend, SQL Server) et
Reporting
(Power BI, Tableau Software).
Idéalement au moins une première expérience sur des projets
Cloud AWS ou Azure ou GCP
.
Vous avez une grande aisance dans la
communication orale et écrite
alliée à un esprit de synthèse, de la rigueur et un très bon sens de la formalisation
Fournir une expertise technique
approfondie aux équipes projets
Réaliser une veille technologique
permanente sur les tendances du marché et les perspectives concurrentielles
Vos
atouts
:
Vous êtes diplômé d’une formation Bac+5 en informatique ;
Vous avez des compétences approfondies dans un ou plusieurs de ces domaines : Opérations / Gestion des systèmes, Conception ou développement de logiciels, Processus DevOps et outillage, Stratégie d'entreprise, Infrastructure Cloud (virtualisation, mise en réseau, stockage, base de données), Sécurité et conformité ;
Vous souhaitez vous impliquer dans le développement d’équipes et de communautés techniques autour du Cloud et des solutions Data
Vos expériences passées vous ont amené à mettre en avant votre capacité à intégrer des problématiques fonctionnelles complexes, comprendre les enjeux métiers, analyser les risques et être force de propositions,
Enfin, on vous apprécie pour vos qualités de communication, de rédaction et votre bon relationnel…and a good level of english would be a definite asset !
NOUS VOUS OFFRONS
Des missions engageantes auprès des grands acteurs du marché.
Un management de proximité, à l’écoute et bienveillant.
La possibilité d’évoluer dans chacune des facettes des métiers de l’IT !
QUI SOMMES NOUS ?
Softeam Data est le département de Softeam, marque de Docaposte, spécialisé dans les différents domaines de la Data (Hadoop et Cloud) et de l’informatique décisionnelle. Nous apportons notre expertise à nos clients, principalement des grands comptes de la place financière française, dans des projets de transformation digitale et cognitive.
Plus de 1650 Softeamien.nes sont dédié.es à la transformation métier et digitale de nos clients et ont généré 191 M€ de chiffre d’affaires en 2019.
SOFTEAM est labellisé Happy At Work !
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
Data Engineer,MindPal,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-at-mindpal-3914438789?position=4&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=tzbXTPpYKqPNK%2FnKPcV6Lw%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for
Data Engineer!
Responsibilities
Designing, creating, and maintaining data processing systems
Analyzing and optimizing data processing workflows
Collaborating with the team to ensure data quality and efficiency
Testing and implementing new solutions
Requirements
At least 2 years of experience in designing and creating data processing systems
Proficiency in tools and programming languages related to data engineering (e.g. Hadoop, Spark, Scala, Python)
Excellent knowledge of databases and SQL language
Ability to work in a team and communicate effectively with other departments
Communicative English skills
Experience with AWS/AWS Glue is a plus
We Offer
B2B contract
Full-time job
Remote work and flexible hours
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,MindPal,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-at-mindpal-3896997018?position=5&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=%2Fub5oFSbqtnXh5AwV%2Fm5NQ%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for
Data Engineer!
Responsibilities
Designing, creating, and maintaining data processing systems
Analyzing and optimizing data processing workflows
Collaborating with the team to ensure data quality and efficiency
Testing and implementing new solutions
Requirements
At least 2 years of experience in designing and creating data processing systems
Proficiency in tools and programming languages related to data engineering (e.g. Hadoop, Spark, Scala, Python)
Excellent knowledge of databases and SQL language
Ability to work in a team and communicate effectively with other departments
Communicative English skills
Experience with AWS/AWS Glue is a plus
We Offer
B2B contract
Full-time job
Remote work and flexible hours
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer PySpark Senior H/F,DELANE SI,"Boulogne-Billancourt, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-pyspark-senior-h-f-at-delane-si-3907364439?position=6&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=uezP%2FZfGmiHzY1ZvRTUyGA%3D%3D&trk=public_jobs_jserp-result_search-card,"DELANE SI
est une société de conseil et de services, une ESN, intervenant sur des projets à forte valeur ajoutée auprès de clients grands comptes des secteurs Banque, Assurance et Finance de Marché.
Nos engagements ?
Être à votre
écoute
, afin de rapidement comprendre vos attentes et objectifs.
Echanger en toute
transparence
afin d’engager un partenariat durable avec vous.
Et enfin, la
réactivité
qui nous permet de répondre à vos attentes dans les meilleurs délais.
Vous cherchez à
booster votre carrière
? Vous cherchez un
nouveau défi
dans une structure en pleine évolution ?
De notre côté, nous cherchons
un tout
: un parcours, une expérience, mais avant tout ... une personnalité !
Rejoignez-nous !
Dans le cadre d’un projet d’envergure chez l’un de nos clients grands comptes, nous cherchons notre futur
Data Engineer PySpark Senior
qui sera en charge des missions suivantes :
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables ;
Consolider ces données au fur et à mesure de leur alimentation récurrente ;
Accompagner les Data Engineer sur son périmètre pour garantir la qualité des livrables.
Issu d'une formation de Bac+5, vous justifiez d'une première expérience d'au moins 6 ans sur un poste de Data.
Vous justifiez d'une expérience dans le domaine de l’assurance.
Vous avez des connaissances avancées en développement en Spark et idéalement PySpark.
Connaissances requises sur les outils de BI comme Power BI.
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory.
Rémunération envisagée :
50-55K
Un anglais courant est exigé.
Conformément à la règlementation en vigueur et à notre politique d’égalité professionnelle et de diversité ; nous encourageons tous les talents à postuler, indépendamment de leur origine, identité, âge, handicap ou de leurs caractéristiques personnelles.
Un processus de recrutement
simple et rapide
:
Un premier entretien avec un(e) de nos chargé(e)s de Recrutement, nos experts métiers, qui seront amené à en savoir plus sur
vous et votre parcours
afin de pouvoir vous présenter à
l’ensemble
de nos ingénieurs d’affaires ;
Un second entretien avec un ou plusieurs ingénieur(s) d’affaires afin de pouvoir vous
présenter une ou plusieurs missions
;
Un dernier entretien avec notre client dans le cadre d’une dernière validation.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
"Data engineer - Aéronautique, Spatial et Défense",MP DATA,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-a%C3%A9ronautique-spatial-et-d%C3%A9fense-at-mp-data-3731603307?position=7&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=uXhP2MeXYg5mj9%2BVe8QIRg%3D%3D&trk=public_jobs_jserp-result_search-card,"MP DATA recrute
un(e) Data Engineer (H/F).
Dans le cadre de la transformation digitale industrielle, l’équipe de data engineering en charge de l’exploitation du Cluster Big Data cherche à se développer : afin de répondre aux attentes des départements opérationnels en termes de :
Valorisation et d'exploitation des données de l'entreprise
Déploiement des outils de traitement de données pour une utilisation industrielle.
Le Data Engineer doit faire l'interface entre plusieurs services :
L'infrastructure hardware du Cluster,
Les Data Architect,
Les Data Scientists.
Il doit être capable de comprendre les contraintes industrielles liées à chaque :
Use-case d'exploitation des données,
Les contraintes et les algorithmes proposés par les Data Scientists,
Proposer des solutions robustes d’implémentation des traitements d’ingestion ou de transformation de données dans les écosystèmes Big Data.
Ta mission (si tu l'acceptes) :
En tant que membre du service DATA au sein du département Data Intelligence, le Data Ingénieur est un acteur clé dans le traitement et la mise à disposition des données au sein de l’entreprise. Il sera en particulier impliqué dans les missions suivantes :
Conception et développement de solutions permettant la collecte, l’organisation, le stockage et la modélisation des données. Ceux-ci doivent être suffisamment sécurisés et lisibles pour les Data Analysts et Data Scientists,
Assurer l’accès aux différentes sources de données, et veiller à la qualité des données,
Donner un accès facilité aux Data Analysts et Data Scientists afin exploiter les données dans des conditions optimales,
Mise à jour permanente sur les technologies et les langages utilisés dans le but de partager ses connaissances et aider à l’avancement des projets,
Contribution, sous la responsabilité opérationnelle de notre Architecte, aux meilleures pratiques, normes, politiques, méthodes, outils et procédures sur le Cluster Big Data,
Il est contributeur de la mise en place d’une politique de données respectueuse des réglementations en vigueur.
Profil recherché :
Ingénieur issu d’une grande école, vous avez des connaissances en modélisation et machine learning (deep learning, random forest, svm…). Acquises lors de votre scolarité ou de vos expériences passées (stage ou césure).
Vous avez de bonnes connaissances en Python pour coder ces algorithmes.
Suite à votre cursus ingénieur ou vos expériences professionnelles, vous avez des appétences métiers dans les domaines de l’aéronautique, spatial, défense etc.
Vous êtes reconnu(e) pour votre autonomie, votre excellent relationnel et votre capacité à être force de proposition.
Vous êtes intéressé pour vous dépasser en data science & data engineering et vous avez une première expérience dans ce domaine, comme :
C/C++ / Java / Rust
Spark / PySpark
Kafka
Cloud : AWS / GCP / Azure
Technologies de stockage : Snowflake / S3 / GCS / Azure Blob
Django / Flask
Git Lab
SQL : Postgres / MongoDB
CI/CD
Déroulement des entretiens :
C’est très simple :
1. Préqualification téléphonique de 5 min avec un membre de l'équipe,
2. Second échange de 30 min (Visio / Physique) avec un Manager pour faire connaissance,
3. Réalisation d'un Test Technique,
4. Échange avec la Direction Technique
5. Bienvenue dans la Team
Avantages
Ⓜ️ Remboursement 50 % de ton titre de transport
🏥 Mutuelle Hélium
💳 Carte Edenred
☀️ Des évènements et Team Building
🍬 Candy bar
😎 Terrasse plein sud
Pour te faire une idée sur MP DATA, je t'invite à regarder notre site et nos vidéos sur Welcome to the jungle.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'C++', 'R', 'Go'], 'DataBase': ['SQL', ' MongoDB'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer DevOps H/F,Inetum,"Courbevoie, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-devops-h-f-at-inetum-3843956952?position=8&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=7MW%2B24OTCIqkGbxkfdoiXw%3D%3D&trk=public_jobs_jserp-result_search-card,"Détail de l'offre
Informations générales
Entité de rattachement
Nous sommes une ESN agile, un groupe international certifié Top Employer Europe 2024.
A l'ère de la post-transformation digitale, nous mettons tout en œuvre pour que chacun de nos 28 000 athlètes du digital puisse se renouveler perpétuellement, en vivant positivement son propre flow digital.
Chacun de nos talents peut ainsi façonner son parcours de carrière selon ses appétences, entreprendre de manière pragmatique avec ses clients pour un monde à impact positif, innover localement dans 27 pays et harmoniser son investissement professionnel et son bien-être personnel.
Rejoignez Inetum. Live your positive digital flow.
Tous nos postes sont ouverts aux personnes en situation de handicap.
Description du poste
Métier
Conseil et Intégration - Business Consulting
Intitulé du poste
Data Engineer DevOps H/F
Contrat
CDI
Description De La Mission
Qui sommes-nous ?
Nous sommes une ESN agile et un groupe international. A l'ère de la post-transformation digitale, nous mettons tout en œuvre pour que chacun de nos 27 000 collaborateurs puisse se renouveler perpétuellement, en vivant positivement son propre flow digital. Chacun d'entre eux peut ainsi façonner son parcours de carrière selon ses appétences, entreprendre de manière pragmatique avec ses clients pour un monde plus positif, innover localement dans 26 pays et harmoniser son investissement professionnel et son bien-être personnel.
Rejoignez
Capital Market, entité Inetum en Finance de Marché
. Nous accompagnons les acteurs majeurs du secteur de la finance en France et à l’International.
Cultivant la double compétence technique et fonctionnelle, nous intervenons sur des projets innovants à haute valeur ajoutée.
Quelles sont nos valeurs ?
🏆 Excellence Notre culture de l’excellence naît de notre audace.
🤝 Engagement S’associer et grandir ensemble !
🛰 Innovation Nos FabLab au service de la transformation digitale de nos clients.
Missions proposées
Pour accompagner notre forte croissance, nous recherchons des
Data Engineer DevOps
pour le compte d’un acteur majeur de la finance de marché en Europe et dans le monde. Dans ce contexte international et exigeant, vous travaillez sur la conception de solutions Big Data afin de répondre aux besoins des opérationnels métiers.
Pour mener à bien ce projet, vous aurez pour responsabilités de
Comprendre les enjeux des équipes Data et les accompagner. Faire le lien entre les environnements (datalake, datawarehouse et environnement de déploiement du modèle) grâce à des pipelines sophistiqués
Être référent et garant des bonnes pratiques pour le développement des langages utilisés par l'équipe. Accompagner les Data Scientists dans l'optimisation de leurs algorithmes
Assurer la viabilité des solutions de datamining et de machine learning de l'équipe Data et les mettre en production.Construire et optimiser des pipelines de données complexes (ETL et ELT)
Coordonner le développement et les opérations grâce à l’automatisation des flux de travail, la création de services Web prédictifs.
Déployez ces modèles en utilisant les dernières techniques et pratiques (API REST, Docker, Tensorflow Serving, etc.)
Analyser et résoudre les anomalies liées aux performances et à l’évolutivité des solutions Cloud BI et Big Data
Profil
Profil souhaité
De formation Ingénieur Grande Ecole ou équivalent, vous possédez une première expérience réussite de trois ans minimum sur un poste équivalent idéalement en banque d’investissement ou asset management.
Vous êtes familier avec l’environnement Big Data (data grids, compute grids, REST based architectures, SGBDR, No-SQL Databases, GPUs)
Vous avez déjà travaillé avec la méthodologie Agile
Une certaine aisance technique est également requise (Jenkins, Docker, Ansible, Git, Scala, Kubernetes, Python/Java, Maven)
Une double compétence Cloud (AWS, Google Cloud, Azure) serait un véritable plus
Evoluant dans un contexte international, la maîtrise de l'Anglais est nécessaire.
L’aisance relationnelle, de l’autonomie, la gestion des priorités, des capacités d’analyse et de synthèse, … le savoir-être est une composante importante dans notre processus de recrutement.
Tous nos postes sont ouverts aux personnes en situation de handicap.
Et pourquoi Inetum Capital Market ?
😄 Des missions intéressantes
🤩 Des perspectives d'évolutions professionnelles et financières
😎 Les avantages d'un grand groupe international
😉 Un suivi régulier
✈️ Une aide à la mobilité géographique que vous soyez localisé en France ou à l'étranger
👨‍🎓 Des formations certifiantes
🥳 Des moments de FUN !
Localisation du poste
Localisation du poste
France, Ile-de-France, 75 Paris
Ville
Courbevoie
Critères candidat
Niveau d'études min. requis
Bac+5
Niveau d'expérience min. requis
Plus de 2 ans
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': ['TensorFlow'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible', 'Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Big Data engineer – Ingénieur des données massives (H/F),DGSE - Direction Générale de la Sécurité Extérieure,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/big-data-engineer-%E2%80%93-ing%C3%A9nieur-des-donn%C3%A9es-massives-h-f-at-dgse-direction-g%C3%A9n%C3%A9rale-de-la-s%C3%A9curit%C3%A9-ext%C3%A9rieure-3778473628?position=9&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=iBOhk%2F4d%2BBZvLIrtTbFbPQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Introduction
La Direction Générale de la Sécurité Extérieure, DGSE, recrute Big Data engineer – Ingénieur des données massives (H/F).
Le poste est situé à Paris.
La nationalité française est obligatoire.
Domaine métier
Sciences et Technologies
Votre environnement de travail
Le flux de données traitées par la DGSE est équivalent à celui des GAFAM. Ces données sont au centre du travail des analystes de renseignement, qui doivent pouvoir compter sur des systèmes leur permettant de rechercher, croiser, traiter ces données, en temps réel ou en batch. Dans ce contexte, la DGSE cherche à renforcer ses équipes de traitement de la donnée massive.
Au sein d'un service centré sur le stockage, l'exploitation et la valorisation des données, nous vous proposons d'intégrer les équipes en charge des plateformes de stockage ou des traitements temps réel des données. Ces équipes pluridisciplinaires développent et maintiennent de bout en bout diverses plateformes reposant sur les technologies Kafka, Yarn, Hadoop, HBase ou encore Elasticsearch. Plus spécifiquement, l’équipe Stockage administre des entrepôts Big Data ainsi que des couches d’accès à leurs données. L’équipe Temps réel conçoit des algorithmes répondant à des besoins de temps de réaction très courts (levée d’alertes, enrichissement à la volée, réponse à des besoins opérationnels).
En nous rejoignant, vous découvrirez :
un environnement unique, qu'aucune autre structure ne peut vous proposer,
un métier proche du renseignement et de l'opérationnel,
une action sur l'intégralité de la chaîne, du développement au déploiement en production,
un minimum de 48 jours de congés par an,
une ambiance propice à l’épanouissement professionnel.
Vos missions
Les missions des équipes auxquelles vous serez amenés à contribuer seront déterminées en fonction de votre expérience et de vos appétences.
Vous serez en charge de plusieurs activités parmi les suivantes :
concevoir, implémenter et optimiser des algorithmes de traitement de données distribués (Scala, Spark, Java),
garantir le bon fonctionnement, la disponibilité et la performance des plateformes de traitement,
participer à l’évolution de l’architecture, en intégrant de nouveaux composants (frameworks, bibliothèques, …) permettant de mieux répondre aux besoins,
assurer une veille technologique constante pour rester au plus haut niveau et garantir une adéquation des clusters existants avec l’état de l’art du domaine,
contribuer à l'amélioration continue de l'équipe,
interagir avec l’équipe SRE/Devops pour améliorer la fiabilité des architectures, l’automatisation des déploiements et l'observabilité des systèmes mis en œuvre.
Votre profil
Vous êtes titulaire d’un diplôme en informatique, niveau master ou école d’ingénieur, ou pouvez démontrer une expérience équivalente.
Vous devez posséder les compétences et qualités suivantes :
bonnes connaissances fondamentales logicielles (structures de données, algorithmique, architecture),
maîtrise des langages Scala, Java ou python, vous n'avez pas peur de monter en compétences sur ceux que vous ne maîtrisez pas,
adepte de l'intégration continue, vous êtes familier de Gitlab CI, Github Actions ou Jenkins,
familier avec les bonnes pratiques de développement collaboratif (usage de git, pratique de relecture de code).
En bonus :
première expérience avec un framework de traitement en streaming (SparkStreaming, KStream, Storm, Flink, ...),
convaincu de l'importance de l'observabilité des systèmes qui regroupe métrologie, logging et tracing, vous avez déjà mis en place une stack de ce type (Prometheus, Telegraph, OpenTelemetry, Jaeger, ELK, …),
familier avec un outil de gestion de configuration (Ansible, Puppet, ...),
expérience sur les clusters Kafka, Hadoop, HBase ou Elasticsearch de plusieurs nœuds.
Les plus de l’offre
Contexte d’activités unique
Diversité des projets
Technologies à la pointe
Contact
Envoyez-nous votre candidature à l’adresse :
dgse-macandidature.cer.fct@intradef.gouv.fr
Plus d’information sur www.dgse.gouv.fr > Nous rejoindre.
RESTEZ DISCRET SUR VOTRE CANDIDATURE A LA DGSE
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['HBase', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': ['Ansible', 'Puppet'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,Harnham,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-harnham-3909639055?position=10&pageNum=15&refId=4Y4I0MAYAq8eeUJ%2FxaKDAA%3D%3D&trackingId=1CSnqICdRwScGUYo4BHNzQ%3D%3D&trk=public_jobs_jserp-result_search-card,"DATA ENGINEER
PARIS (75) / LILLE (59)
50K-65K EUR
Cloud AWS – Python – PySpark - Glue
Une nouvelle opportunité s’ouvre pour un Data Engineer dans le domaine de l’assurance. La personne rejoindra une grande équipe ayant pour projet la création d’une nouvelle plateforme Data ! Si vous recherchez un défi stimulant dans un environnement qui encourage les nouvelles idées, cette opportunité est faite pour vous.
VOTRE MISSION :
Collaboration avec les Data Architectes pour garantir un bon alignement sur l’architecture
Developpement des processus d’ingestion pour la diffusion des données dans le Datalake
Mise en place de mécanisme pour générer les couches de données organisées
Implementation d’outils MLOps pour la mise en œuvre d’algorithmes ML
Conception de Data Warehouse pour accélérer la génération de modèles en Etoile
Travail dans une équipe agile avec le train Agile Release, le Product Owner et le SCRUM Master
VOTRE PROFIL :
3 à 5 ans d’expérience en tant que Data Engineer
Compétences solides en Cloud AWS, Python, PySpark et SQL
Compétences avec les services AWS Cloud Computing : Glue ; Athéna ; Redshift
Capacité à travailler en autonomie
Francais et Anglais : Fluent obligatoire
Les outils BI tels que Power BI ou QuickSight est un plus
N’hésitez plus pour postuler !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer H/F,AXEAL,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-axeal-3913998011?position=1&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=cyRIjxyXCNx9BJ%2Fl7OpG9g%3D%3D&trk=public_jobs_jserp-result_search-card,"AXEAL, filiale d'HEVERETT Group porte une réelle ambition digitale et propose des solutions de conseil dans les domaines des technologies industrielles.
Nous sommes spécialistes de l'Ingénierie du Numérique appliquée à l'Industrie 4.0 dans les secteurs de l'Energie, de la Défense, de l'Aéronautique et du Naval.
Parce que nous ciblons des clients qui développent des projets innovants, nous sommes en mesure de proposer les meilleures missions à nos consultants.
Actuellement en forte croissance, nous sommes à la recherche d'un(e) Data Engineer pour rejoindre cette aventure passionnante et participer aux transformations technologiques de demain.
Ce Que Nous Proposons
En tant que Data Engineer, vous aurez les responsabilités suivantes :
Définir, architecturer, mettre en place et maintenir les outils et infrastructures permettant la valorisation des données dans un contexte « Data centric ».
Vous aurez à garantir le périmètre technique applicatif allant la captation des données (source hétérogène, temps réel, structuré ou non, multi protocole) à son stockage (SQL, nosql) en passant par son traitement (batch, micro batch ou stream) dans un environnement on-Premise, cloud ou hybride.
Le dimensionnement, la sécurité, la pérennité et l'accessibilité sont des éléments clés à prendre en compte.
La capacité à travailler en équipe avec des méthodologies/outils collaboratifs est un pré requis.
Vous êtes diplômé(e) d'un doctorat, d'une grande École d'Ingénieur et/ou d'un 3ème cycle universitaire spécialisé en que Data Engineer ou Mathématiques appliquées.
Rigoureux(se) et curieux(se), vous faites preuve d'autonomie et aimez relever de nouveaux challenges scientifiques et techniques.
La connaissance des protocoles industriels serait appréciée. Vous évoluerez dans un environnement mêlant projet industriel et projet de recherche/innovation.
Vous Avez Des Compétences Sur
Captation de la donnée : Protocole MQTT, OPC-UA, SQL, API RestFul, WS
Transformation de la donnée : Kafka, Spark, storm, Hadoop, Elastic Search, NIFI, ESBMule, Talend, MLFLOW
Stockage de la donnée : SQL, NoSQL (timeseries, graph, document)
Monitoring/Visualisation : Grafana, PROMETHEUS, superset, BI
Infrastructure : Docker, Kubernetes
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Hybride'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Junior Data Engineer,Constellium,"Voreppe, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/junior-data-engineer-at-constellium-3791294675?position=2&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=VuMdW2oTIzx9CYlGnswtXw%3D%3D&trk=public_jobs_jserp-result_search-card,"C
-TEC RECRUTE
Junior Data Engineer (all genders)
Constellium est un leader mondial du développement et de la fabrication de produits et de solutions en aluminium à haute valeur ajoutée pour un large éventail de marchés et d'applications, se concentrant en particulier sur l'aérospatiale, l'automobile et l'emballage.
L'équipe Digital centrale déploie les technologies de l'industrie 4.0 partout dans le groupe pour créer nos futurs processus de fabrication de classe mondiale. Elle est hébergée dans le centre R&D de Constellium à Voreppe, en France. Télétravail possible jusqu’à 2 jours par semaine.
Responsibilités:
Dans le cadre de notre stratégie digitale l’objectif du poste est de concevoir, développer et maintenir des applications autour de la collecte, du stockage et de l’utilisation de la donnée, avec pour finalité sa valorisation pour notre activité.
Le titulaire du poste devra :
Interagir avec les sites de production et les experts métiers pour comprendre leurs besoins.
Participer à la définition de l’architecture des solutions et des applications.
Concevoir, développer et maintenir les applications et les solutions axées sur la donnée.
Promouvoir les applications digitales et participer activement à leur déploiement.
Travailler en étroite collaboration avec une équipe de développeurs.
Le poste implique des déplacements professionnels occasionnels dans les usines du groupe en Europe et aux États-Unis.
Profile :
Un diplôme de niveau master en développement informatique lié à la gestion des données
Une première expérience en tant qu’ingénieur de données, avec des réalisations dans les domaines suivants :
Développements d’application en Python ;
Architecture applicative ;
SQL / Modélisation de données ;
Connaissance de Git / GitHub ;
La connaissance du cloud Azure est un plus.
Le candidat idéal aura :
Un intérêt prononcé pour les données et les sujets liés à l’industrie ;
Une volonté avérée à apprendre et à délivrer ;
Une capacité à interagir avec des profils variés (notamment non informaticiens) ;
Une curiosité pour évaluer les technologies innovantes du monde digital.
Un niveau de Français et d'anglais courant écrit et parlé est essentiel.
Nous offrons:
Une politique salariale attractive
Un système d'intéressement
Un système de bonus
Une possibilité de télétraviller
Un restaurant d'entreprise
Des avantages importants par les oeuvres sociales
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Junior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,Harnham,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-harnham-3909599777?position=3&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=G5qtrNaKlOVlK%2BC16Tnbdw%3D%3D&trk=public_jobs_jserp-result_search-card,"DATA ENGINEER
PARIS (75)
UP TO 65K€ FIXE
Python - AWS - Glue - Pyspark
Une entreprise spécialisée dans l'assurance automobile des particuliers, recherche un Data Engineer. Dans le cadre de la création d'une nouvelle plateforme data en Europe, l'entreprise recherche un data engineer pour renforcer son équipe.
LE POSTE
En tant que Data Engineer, vous allez construire les pipelines pour la préparation et la transformation des données, y compris les données brutes, ingestion et données organisées. Vous serez responsable de la mise en œuvre des modèles de données qui contiennent les KPI et qui sont la source des analyses clés définies par le métier. Vous accompagnerez aussi les data engineer juniors dans leur montée en compétences.
Voici vos responsabilités au quotidien :
Travailler avec les data architectes pour garantir que le développement est aligné sur l'architecture cible
Développer les processus d'ingestion de données pour diffuser les données dans le Data Lake, à la fois en temps réel et temps différé
Implémenter le mécanisme pour générer la couche de données organisée avec les différents ensembles de données disponibles pour l'entreprise
Implémenter l'outil MLOps pour accélérer la mise en œuvre des algorithmes ML au niveau niveau de l'entreprise
Construire la data warehouse pour accélérer la génération de modèles
Travailler dans une équipe agile en étroite collaboration avec le train Agile Release, le Product Owner et le Scrum Master afin d'exécuter la tâche assignée pour atteindre les objectifs d'AgileTeam, en particulier communiquer avec le Scrum Master pour faire remonter les obstacles ou les améliorations pour l'équipe Agile
VOTRE PROFIL
3 à 5 ans d'expérience en gestion de données et technologies Big Data, en particulier : SQL,PySpark et Python
Plus de 3 ans d'expérience avec les services AWS Cloud Computing : Glue, Athena, Redshift et DynamoDB
Plus de 3 ans d'expérience dans l'implémentation de data warehouse
Français et anglais courant
Une expérience avec les outils BI (de préférence : QuickSight ou PowerBI) est un vrai plus
POUR POSTULER
Merci de me faire part de votre CV et je vous recontacterai au plus vite.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Junior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Senior Data Engineer,QUANT AI Lab,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-quant-ai-lab-3870444702?position=4&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=QQK86xMjdcq%2FXDnOI0H7%2BA%3D%3D&trk=public_jobs_jserp-result_search-card,"Chez QUANT AI LAB, nous travaillons avec nos clients pour les aider à résoudre leurs problèmes, de la définition de la stratégie à la mise en œuvre de solutions robustes et durables. Nous nous appuyons sur notre approche opérationnelle qui lie conseil et outils.
Nous avons le projet d'accompagner nos clients sur le long terme avec nos convictions, nos approches et notre plateforme QUANT. Rejoindre QUANT AI LAB, c'est rejoindre une aventure entrepreneuriale où chaque collaborateur est un élément important de ce projet.
C'est pourquoi, notre recherche vise à recruter plusieurs profils spécialisés, dont un
Senior
Data Engineer
pour rejoindre notre équipe interne de QUANT AI LAB.
Lieu de poste:
Paris.
Las compétences du candidat sont:
Titulé d'un diplôme universitaire
(Économie, Ingénierie...)
Natif(ve) en français.
Expérience au moins 5 ans
autant que Data Engineer.
Expérience avec
Scala.
Programmation en
Python.
Maîtrise de l'environnement Cloud
Azure (Databricks).
Maîtrise de
process ETLs.
QUANT AI LAB, ce que nous offrons:
Contribuer au développement d'une entreprise internationale en croissance continue et développant des projets et produits de pointe.
Intégrer une équipe multidisciplinaire avec une formation professionnelle et académique de haut niveau, composée de certains des meilleurs experts dans le domaine de l'intelligence artificielle, de l'analytique avancée et du traitement des données.
L'opportunité de travailler directement avec des clients d'envergure internationale et certaines des plus grandes entreprises du pays et du monde.
Un environnement de travail collaboratif, entrepreneurial et solidaire, et une équipe jeune et dynamique où chacun s'entraide pour grandir ensemble.
Une ambiance ludique et conviviale et des événements festifs (afterworks, séminaires...).
Un contrat à durée indéterminée et un salaire très compétitif, à déterminer en fonction du profil et de l'expérience.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': ['Salaire'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer Lille H/F,Jems Group,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-lille-h-f-at-jems-group-3887943159?position=5&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=YKBGlO4%2BmKgm4JyE4pKCsg%3D%3D&trk=public_jobs_jserp-result_search-card,"A propos de JEMS
Nous sommes le seul industriel de la donnée en Europe. Notre métier est de créer, manager et exploiter le patrimoine data de nos clients.
Nous avons la conviction que chaque entreprise peut adopter une démarche innovante de gestion de la donnée et créer des cas d’usage disruptifs en réduisant l'impact écologique et en diminuant la dette technique.
Nous comptons plus de 840 collaborateurs en mission chez nos clients grands comptes dans tout secteur d’activité : banque, assurance, santé, énergie, e-commerce, automobile, luxe, retail, transport, agritech…
Vos missions
Nous recherchons un(e) data engineer capable de trouver des solutions innovantes et robustes à l'ensemble des problématiques Data.
Vous aurez la charge de :
Participer à la conception et réalisation d'une solution Data depuis l'acquisition jusqu'à l'exploitation de la donnée en accompagnant la réflexion des directions métiers
Identifier, collecter et intégrer les données nécessaires à la résolution de problématiques métier et opérationnelles
Garantir la qualité des données en mettant en place les outils de mesure et de suivi adéquats en appliquant les règles de Data Gouvernance et de Data Management
Transcrire des besoins métier en règles de gestion data
Industrialiser le déploiement de vos réalisations à travers l'implémentation de tests unitaires, d'intégration et de non-régression
Vos compétences
En tant que Data Engineer vous maîtrisez :
Le langage SQL
Un langage objet (Python, JAVA, Scala)
Un framework de calcul distribué
L'intégration continue (Git, JUnit, SonarQube, Jenkins)
Un cloud provider (AWS, GCP, Azure) ou une distribution Big Data (Hortonworks, Cloudera, Datastax)
Les concepts de la modélisation relationnelle et non-relationnelle
Votre profil
Diplômé(e) d'une école d'ingénieur ou d'un parcours académique bac+5 , vous justifiez d'une expérience professionnelle d'au moins 3 ans dans un contexte projet Data. Rigoureux, proactif et autonome vous restez en veille technologique et êtes force de proposition. Vous êtes capable de prendre de la hauteur et vous adapter aux enjeux du projet.
Avantages à travailler chez JEMS
Une JEMS Académie au service de votre montée en compétences (formations et certifications sur les technologies de pointe)
Un accompagnement personnalisé et un management de proximité pour vous proposer des évolutions de carrière
Une intégration dans des communautés techniques et de pratiques JEMS (encadrement par des experts, échanges sur les bonnes pratiques, favoriser l'innovation...)
Une entreprise reconnue ""Great Place To Work""
Des évènements et séminaires inoubliables, des soirées d'agence conviviales
Mobilité
Une mobilité nationale et internationale pour vous accompagner dans vos projets de vie.
Diversité
Le Groupe JEMS porte fièrement sa valeur ""Diversité"" en se mobilisant pour l'inclusion et l'égalité des chances et en luttant contre toutes formes de discrimination.
Tous nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer Cloud Azure F/H,SOFTEAM,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-cloud-azure-f-h-at-softeam-3609925189?position=6&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=hEH4xMsVArxvZE38RT24qg%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous évoluez dans le domaine de la Data et souhaitez intégrer un leader de la transformation numérique spécialisé dans les secteurs de la Banque, du Luxe, de l'Assurance, de la Finance, de l'Energie et la possibilité d'évoluer au sein du Groupe Docaposte !
Softeam est labellisé ""HappyIndex® AtWork "" 2022 pour la 5ème année consécutive !
Nos collaborateurs travaillent en « mode projet » autour des Modern DATA Platform ☁ et technologies Big Data Azure Cloud / On premise et autre ""playeur"" du marché. Nous accompagnons de bout en bout nos clients sur des problématiques de Gouvernance, d’Intégration, de Visualisation et d’IA.
CE QUE NOUS RECHERCHONS
SOFTEAM Data recherche un(e) Data Engineer Cloud, disposant de solides connaissances techniques.
CE QUE NOUS ATTENDONS DE VOUS
En tant que Data Engineer Cloud, vous concevez, mettez en place et administrez des clusters et des solutions big data.
Vos missions détaillées :
Analyse et compréhension des besoins métiers,
Participation à la définition et à la conception de l’architecture,
Gestion des données : préparation, ingestion, traitement, contrôle qualité,
Développements des jobs Spark et automatisation des flux d’alimentation du Data Lake,
Tests de charge, tests unitaires…
Maintenabilité de la solution Big Data (Optimisation et performance des traitements Spark)
VOUS ETES
Ingénieur(e) de formation, vous disposez d'une expérience de 3 ans minimum en tant que Data Engineer.
Vous maîtrisez les langages Java, Scala ou Python et êtes expert sur les framework Spark et Hadoop.
Vous avez une expertise sur les services Data suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer…
NOUS VOUS OFFRONS
Des missions engageantes auprès des grands acteurs du marché.
Un management de proximité avec Gilles SALVADOR, Directeur du Centre d'Expertise Data, toujours bienveillant et à à l'écoute et avec qui vous pourrez échanger au quotidien sur les enjeux de votre mission et évoquer vos futurs projets afin que nous puissions vous aider à les réaliser.
La possibilité d’évoluer et de monter en compétences grâce à des formations et à des certifications auprès de nos clients et de nos consultants, des 12@13, notre Entité Softeam Institute, Organisme de formation interne de renommé qui délivre des formations auprès de nos clients...
QUI SOMMES-NOUS ?
SOFTEAM DATA est une marque de DOCAPOSTE spécialisée dans l'informatique décisionnelle et les nouvelles technologies. Nous apportons notre expertise à nos clients, principalement des Grands Comptes de la place financière française, dans des projets de transformation digitale et cognitive.
2000 Softeamien.nes sont dédié.es à la transformation métier et digitale de nos clients et ont généré 200 M€ de chiffre d’affaires en 2020.
SOFTEAM SPIRIT
Des communautés d'expertises sur les sujets de la Data
De super nouveaux locaux qui sont en plus accessibles facilement
Une école de formation intégrée
Des évènements : des soirées avec les consultants, des 12@13...
Une entreprise labellisée ""Happy at Work"" pour la 5ème année consécutive.
N’attendez plus, rejoignez SOFTEAM et venez nous rencontrer dans nos nouveaux locaux situés à la Défense #DevenezSofteamien !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
AI Engineer Intern,SITA,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/ai-engineer-intern-at-sita-3913856183?position=7&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=6d%2FsgnHB%2Futl0dC34z%2FiQw%3D%3D&trk=public_jobs_jserp-result_search-card,"ABOUT THE ROLE & TEAM:
The Data Intelligence team develops and deploys data-driven and AI-powered solutions to optimize operations in the aviation industry, with a specific focus on reducing the sector's CO2 emissions. For example, SITA Opticlimb helps to save fuel for each climb by computing optimal climb speeds considering number of factors like air density, weight, etc.
We are seeking a highly motivated
AI Engineer Intern
to join our Data Intelligence team. As an AI Expert Intern, you will have the opportunity to work closely with our AI Experts on cutting-edge projects, leveraging your skills and knowledge in AI development and optimization. This internship provides a valuable opportunity to gain hands-on experience in the field of AI and contribute to the success of our data intelligence solutions.
WHAT YOU WILL DO:
Collaborate with AI Experts to develop and optimize AI models and algorithms for aviation operations
Assist in implementing and maintaining the AI pipeline and infrastructure for data processing and model training
Support AI Experts in conducting experiments and tests to evaluate the performance and accuracy of AI models.
Pre-process and analyze data using Python and relevant libraries (e.g., Pandas, NumPy) to prepare it for model training and evaluation
Contribute to the development of data pre-processing techniques, feature engineering approaches, and model evaluation methodologies
Document development processes, procedures, and findings
Stay up-to-date with the latest AI research and industry trends to contribute innovative ideas to the team
EXPERIENCE:
Currently pursuing a Bachelor's or Master's degree in Computer Science, Data Science, or a related field.
Solid understanding of AI and machine learning concepts.
Proficiency in Python and experience with libraries such as Pandas, NumPy, and scikit-learn.
Knowledge of data pre-processing, feature engineering, and model evaluation techniques.
Strong analytical and problem-solving skills.
Excellent communication and teamwork abilities.
Passion for learning and staying up-to-date with AI advancements.
NICE-TO-HAVE:
Aeronautical background is a plus
WHAT WE OFFER:
SITA’s workplace is all about diversity, many different countries and cultures are represented in our workforce. We collaborate in our impressive offices, embracing a hybrid work format. As part of our global benefits, we offer:
🏡
Flex-week:
Work from home up to 2 days/week (depending on your Team's needs).
⌚
Flex-day:
You may wish to flex your arrival time at the office, to beat the rush hours or you may want to leave the office earlier to pick up your kids from school or to go to your favorite game: We support you in being open about your needs and routine with you manager.
🌎
Flex-location:
Benefit for 30 working days from anywhere around the world each year!
🙌🏽
Competitive benefits
according to the local market
SITA is an Equal Opportunity Employer and values a diverse workforce. In support of our Employment Equity Program, women, aboriginal people, members of visible minorities, and/or persons with disabilities are encouraged to apply and self-identify in the application process
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['Pandas', 'NumPy', 'R'], 'BigData': [], 'MachingLearning': ['Scikit-Learn'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Teamwork']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer - H/F - CDI,CubeRH,"Tourcoing, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-cdi-at-cuberh-3862951855?position=8&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=UcyKVwyyFAHcs5s4Wologg%3D%3D&trk=public_jobs_jserp-result_search-card,"La société :
Notre client est une entreprise de services spécialisée dans le domaine de la Data, à échelle humaine, qui se distingue par son expertise développée autour de trois axes principaux : la Data généraliste, l'outil de reporting Power BI et la solution cloud Microsoft Azure.
Au cœur de sa mission, l'éco-responsabilité constitue un pilier essentiel. En adoptant cette approche, elle offre une vision complète et intégrée de la gestion des données. Grâce à une équipe de consultants experts, elle simplifie les opérations, optimise les données et les valorise, offrant ainsi à ses clients une opportunité de progression.
En pleine croissance, l'entreprise recherche à agrandir son équipe avec un Data Engineer H/F.
Si vous recherchez une équipe qui met en avant votre développement professionnel et votre épanouissement, vous êtes au bon endroit !
L'objectif de la mission :
Initier et développer des outils d’infrastructure dans l’objectif de façonner et transformer les données.
Agir tel un allié des Data Scientists, pour industrialiser leurs algorithmes et flux de données.
Être un acteur de choix dans l’architecture big data afin de répondre aux différents cas métiers.
Protéger et garantir l’intégrité des données en assurant leur sécurité et leur accessibilités
Mettre en œuvre des pratiques de sécurité redoutables pour protéger les données les plus confidentielles et sensibles
Travailler en collaboration avec les équipes techniques dans l’ingestion des données.
Les conditions de travail :
Une mutuelle avantageuse et des tickets restaurants
Remboursement intégral de vos frais de transports en commun
Des trottinettes électrique et des vélos à disposition
Primes (notamment de cooptation)
Télétravail
Votre profil :
Vous maitrisez des technologies big data ainsi que les environnements Google cloud platform, Amazon web service et/ou Microsoft Azure
Vous connaissez le langage SQL et Python
Vous possédez des bases solides en architecture data (notamment big data et informatique décisionnelle
Etre force de proposition sur la mise en oeuvre et l'utilisation des solutions big data
Connaitre les principes de sécurité
Process entretien :
Deux premiers échanges avec le cabinet Cube RH;
Entretiens avec le client (RH et avec le manager).
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Azure', 'Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer Cloud (F/H),Apside,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-cloud-f-h-at-apside-3904088503?position=9&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=%2B%2BPQMVY2DJykjwzejMYJVg%3D%3D&trk=public_jobs_jserp-result_search-card,"💥
Découvrez la Vie Apsidienne
📹
et vous aussi, devenez Apsidien
On aurait pu demander à Chat GPT de vous démontrer en quoi
Apside est l’ESN qu’il vous faut,
mais on préfère que vous le découvriez vous-mêmes 👇😏
🔥
Découvrez votre future mission
👉
Contexte
Rejoignez notre Practise Cloud/Data, afin d’intervenir sur des sujets à haute valeur ajoutée !
Notre
client migre actuellement toutes ses applications vers le cloud AWS.
De plus, dans le cadre du développement d'un produit de restitution automatisée de données, ils recherchent actuellement développeur data ayant déjà travaillé sur un projet similaire. La solution produit est techniquement conçue en lien avec le Tech Lead validant l'architecture logicielle à mettre en place sur le cloud AWS.
Secteur
: culture/média
Méthode de travail
: Agile Safe
😎 Mission
Capter les données (structurées et non structurées) produites dans les différentes applications
Intégrer les éléments
Structurer la donnée (sémantique, etc…)
Cartographier les éléments à disposition
Nettoyer la donnée (élimination des doublons, etc…)
Valider la donnée
Créer les référentiels de données
Environnement technique
:
Python
Lambda
Step Function
AWS / AWS RDS
PostegreSQL
Snowflake
Spark
📍
Localisation
La Défense
💰
Le package salarial que nous vous proposons
Contrat :
CDI
Avantages groupe :
carte ticket restaurant Swile, prime de mobilité, RTT, accord télétravail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle à 100% etc…
Avantages agence :
Communauté Cloud/Data, afterworks, communauté techlead
Formation :
certifications techniques, cours particuliers d’anglais en interne, accès à un catalogue de formations grâce à notre plateforme e-learning (
Academy by Apside
) ou via nos organismes partenaires.
🔮
Ô vous futur Apsidien, qui êtes-vous ?
Au moins 4 ans d'expérience en tant que Data Engineer
Maitrise de l’environnement cloud AWS
Force de proposition, bon relationnel et autonome
😏
Apside a suscité votre curiosité ?
Dans un environnement marqué par une accélération des évolutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients à créer de la valeur et à adresser leurs enjeux stratégiques en leur mettant à disposition des expertises technologiques (
Data / IA, Cloud, Cyber
) et une expérience sectorielle (
Industrie, Banque, Assurance, Service, Secteur Public
). Pour un accompagnement global, le groupe propose des offres transverses autour du
Handicap
(Apsid’EA), du
Digital Learning
, et du
Conseil
.
🤔
Et votre place dans tout ça ?
👉 Notre volonté
est de vous accompagner dans la construction et l’épanouissement de votre carrière
en nous appuyant notamment
sur 3 piliers :
Une
rémunération
à hauteur de vos investissements et de vos compétences
Une
trajectoire professionnelle
stimulante sur mesure
Un
engagement
autour des valeurs Apsidiennes : la qualité de vie et des conditions de travail au cœur de nos enjeux
Engagée pour
un monde plus inclusif et plus responsable
, Apside réinvente l’ESN et propose l’Engagement Sociétal et Numérique. Découvrez notre démarche RSE ainsi que notre vision de l’Entreprise Engagée.
Convaincu ? A vous de jouer, envoyez-nous votre CV !
Rejoignez l’aventure Apsidienne et découvrez notre vision d’une ESN singulière et résiliente
🚀
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Senior Data Engineer H/F – CDI – Paris,IODA Group,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/senior-data-engineer-h-f-%E2%80%93-cdi-%E2%80%93-paris-at-ioda-group-3908890202?position=10&pageNum=17&refId=l8SH%2B3yTgmzGUPyUpkaAhw%3D%3D&trackingId=En91OVaDWAVBsJRBkXEEVg%3D%3D&trk=public_jobs_jserp-result_search-card,"IODA Group, cabinet de conseil en plein essor en France et à l'international, à taille humaine et orienté business, mixant la Data, le Marketing, le Digital & la Technologie, est à la recherche de talents passionnés pour rejoindre son équipe.
Si tu es un(e)
Senior Data Engineer
chevronné(e) avec
au moins 4 ans d'expérience
, ce poste est fait pour toi !
Ce poste est basé à Paris mais nous avons également des bureaux à Bordeaux, l’Ile de la Réunion et Barcelone.
Le Job 💻
En tant
que Senior Data Engineer
chez IODA Group, tu seras aux commandes des missions suivantes :
Développer de nouveaux modèles de données et des pipelines
Tester les solutions les plus innovantes et prometteuses du marché en vue de pouvoir améliorer nos capacités en matière de données
Comprendre les enjeux business et savoir les traduire dans un environnement technique
Assister nos clients dans le cadrage des projets et contribuer au design fonctionnel et technique avec une vision avant-gardiste
Assumer le rôle de référent, coacher les consultants juniors et faire évoluer son équipe
Compétences techniques requises 🔧
Pour ce poste, nous recherchons une personne aux compétences multiples avec :
Une expérience approfondie des technologies liées aux données, y compris les modèles d'architecture Big Data (Spark, Hive, Impala...)
Une expérience approfondie des services Cloud (AWS / Azure / GCP)
Une expertise en langages de programmation : Python, Java, et si possible Scala
Une mise en production de cas d'usage Data, notamment en Machine Learning
Une capacité à mettre en place des modèles de données flexibles et évolutifs (optimisation de stockage et traitement, regroupement, agrégation, partitionnement…)
Une maitrise des bases de données SQL (conception, exploitation …)
Une connaissance en DevOps et en développement de flux de données (data pipelines) avec une maîtrise de Docker/Kubernetes et des chaînes CI/CD seraient un plus
Profil recherché 🌟
Si tu es diplômé(e) d'une école d’ingénieur / génie informatique Bac+5, que tu as à minima 4 ans d’expérience et que tu as une expérience solide dans le monde de la Data, alors nous voulons te rencontrer !
Si tu es une personne entreprenante, capable de travailler en équipe en s’adaptant à divers profils, de superviser, de prioriser et de gérer plusieurs actions, d’avoir d'excellentes compétences en communication, présentation et coordination, nous souhaitons toujours te rencontrer !
De plus, si tu as des compétences avérées en analyse et résolution de problèmes, associées à une aptitude à assimiler rapidement de nouvelles technologies, tu es bien la personne qu’il nous faut !
Ce qui t'attend chez IODA Group 🌈
En nous rejoignant, tu auras droit à :
Une équipe dynamique et motivée qui reconnaîtra et encouragera tes talents et tes idées
Une diversité de projets stimulants dans différents secteurs d'activités
Des plateformes internes de R&D pour toujours être à la pointe de la technologie
Des perspectives d'évolution concrètes pour faire décoller ta carrière
Un CDI avec une rémunération fixe attractive et une part variable selon ton profil (voire des bonus complémentaires si tu surperformes !)
Deux jours de télétravail par semaine après la période d'essai
Des avantages tels que des tickets restaurants, une participation au titre de transport, une mutuelle...
Une participation active à la vie de l'entreprise avec des afterworks, des événements, des séminaires et bien plus encore !
REJOINS-NOUS
dès maintenant pour une aventure où les données deviennent une source infinie d'opportunités ! 🚀✨
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication', 'Résolution de problèmes'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Junior', 'Senior'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Engineer,Harnham,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-harnham-3909658681?position=1&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=qUGt7EsqBEzusK1%2FE031NA%3D%3D&trk=public_jobs_jserp-result_search-card,"Data Engineer
Paris / Lille
Up to 65k€
CDI - Pas Freelance
Cette entreprise internationale dans le monde de l'assurance developpe son equipe data et recherche un nouveau Data Engineer.
Responsabilités :
Collaborer avec les architectes de données pour garantir que le développement est aligné sur l'architecture cible.
Développer des processus d'ingestion de données, facilitant le streaming de données en temps réel et par lots dans le lac de données.
Mettre en œuvre des mécanismes de génération de couches de données sélectionnées comprenant divers ensembles de données pour la consommation commerciale.
Diriger la mise en œuvre de MLOps pour rationaliser le déploiement d'algorithmes ML à l'échelle de l'entreprise.
Établir des cadres pour l'entreposage de données afin d'accélérer la génération de modèles en étoile.
Travailler en étroite collaboration au sein d'une équipe agile aux côtés des trains de publication agile, des propriétaires de produits et des maîtres scrum pour atteindre les objectifs de l'équipe.
Participer activement aux cérémonies de la communauté de pratique de l'ingénierie des données, en particulier aux chapitres, pour partager des connaissances techniques et favoriser la collaboration.
Candidat idéal :
Must have :
3 à 5 ans d'expérience en gestion de données et en technologies Big Data, avec une maîtrise de SQL, PySpark et Python.
Une expérience pratique approfondie (3+ ans) avec les services de Cloud Computing AWS : Glue, Athena, Redshift et DynamoDB.
Une expérience avérée (3+ ans) dans la mise en œuvre des entrepôts de données.
Maîtrise de l'anglais.
Un Plus :
Connaissance supplémentaire des services de Cloud Computing AWS : CDK, SageMaker, fonctions Lambda, Lake Formation, Kinesis.
Familiarité avec les outils BI (de préférence QuickSight ou PowerBI).
Maîtrise de Git, Jupyter Notebook, pip, Java, Apache Airflow.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer,Pictarine,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-at-pictarine-3911913926?position=2&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=XgvgsQOypETxNXQ%2Bsrs8Vw%3D%3D&trk=public_jobs_jserp-result_search-card,"Mission and challenges 🎯
Si tu es enthousiaste à embarquer dans la nouvelle équipe data de Pictarine pour la faire rayonner avec tout ton savoir-faire, alors c’est l’aventure qu’il te faut! 🏔️
Avec plus de 1K tables, 2M de clients et 4M de commandes en 2022, les équipes de Pictarine ne sont jamais à court d’idées pour explorer de nouveaux horizons. 🚀
En tant que Data Engineer chez Pictarine tu vas pouvoir utiliser toutes tes compétences SQL pour garantir la qualité de la data sur GCP, accompagner et challenger les besoins data.
Tu évolueras au sein de l’équipe Engineering, composée des pôles dev & data.
Ton rôle comprendra les aspects suivants 👇🏻
Tu es garant de la qualité de la data !
En simplifiant la structure de la data et réduisant le nombre de tables
En transformant les données pour les rendre facilement utilisables
En orchestrant le flux des données de manière continue et automatique
Tu accompagnes et challenges les équipes de Pictarine !
En co-construisant des solutions data appropriées
En élevant le niveau de jeu des méthodes data existantes
En faisant rayonner la data autour de bonnes pratiques et d’outillages adéquates
Profil Recherché
About you 💎
Tu as au moins 5 ans d’expérience sur un poste similaire
Tu maîtrises le data warehouse BigQuery et son langage SQL
Tu es à l'aise avec les services GCP
Tu as de bonnes connaissances dans la conception de modèles de données et les stratégies d'optimisation des requêtes SQL
Tu as des compétences en DevOps pour le déploiement et la gestion efficace des pipelines de données
Tu as une bonne maîtrise de Python & Github
Tu es organisé, rigoureux et portes une grande attention aux détails
Tu es doté d’excellentes qualités relationnelles, de communication et de vulgarisation
Tu as une passion pour résoudre des problèmes business avec la programmation
Tu es curieux de tester des nouvelles technologies
Tu es un team player et toujours à l'affût de nouvelles idées
Work @ Pictarine✨
Un environnement de travail agile, collaboratif, international et multiculturel
Des perspectives d’évolution rapides
Des locaux tout beaux à Labège avec du matériel dernier cri (mais aussi des snacks à profusion et un frigo à boissons toujours bien rempli)
Un apprentissage permanent : conférence, meet-up, Pictarine Academy, cours d’anglais.
Des events tous les mois : massage, pilates, TGIF, team building .
Un environnement de travail flexible : horaires, politique de remote hybride.
Un package de rémunération attractif : salaire compétitif, RTT, mutuelle & prévoyance 100% prise en charge, intéressement.
Des petits + : Développement de photos gratuit, subvention sport, 3 jours “entraide familiale”, jours de congés en plus avec l'ancienneté... 🤫 on ne te dévoile pas tout !
Recruitment process ⚙️
Tu souhaites nous rejoindre ? Viens rencontrer les gens avec qui tu vas bosser :
1er échange pour apprendre à se connaître avec Marie - Engineering Manager Data (15’)
Entretien Manager avec Marie (60-90’)
Test pratique afin de nous montrer tes talents 🙂 (3 heures)
Entretien final avec 2 membres du Codir (90’)
Welcome aboard !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Hybride', 'Remote'], 'TypeContract': [], 'Salary': ['100', '100'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer,NW,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-nw-3904072453?position=3&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=vDQfEmvWaoDACO%2Fg0SceBQ%3D%3D&trk=public_jobs_jserp-result_search-card,,"{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Hybride', 'Remote'], 'TypeContract': [], 'Salary': ['100', '100'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer Paris H/F (H/F),Inventiv IT,"Levallois-Perret, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-paris-h-f-h-f-at-inventiv-it-3910222334?position=4&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=thOWBDZ31CHfsFXdwy4SpA%3D%3D&trk=public_jobs_jserp-result_search-card,"Cette offre d’emploi est fournie par Pôle emploi
Description
Inventifs Wanted vous propose un CDI précédé d'une formation professionnalisante accélérée de 400h, dans le cadre d'une POEI (Préparation Opérationnelle à l'Emploi Individuel), pour que vous soyez pleinement opérationnel et 100% à l'aise dans la mission qui vous sera confiée. Situé au cœur de Levallois, un écrin où il fait bon vivre et travailler, Inventiv IT bénéficie d'un cadre vibrant et dynamique, idéal pour s'épanouir. Nous sommes en quête d'une étoile filante avec une expérience d'au moins 5 ans minimum dans le domaine de la data science et de la data engineering. Vous êtes un leader galactique doté d'une expertise en data, optimisation de processus, et méthodologies Agile. Ce que vous apporterez à notre galaxie : - Cartographier les constellations de données : Étudier les besoins, cadrer les projets et définir les périmètres d'interventions afin de réaliser le release plan des différents livrables. - Ingénierie des étoiles : Vous participerez au développement des projets ou des services data en développant une chaîne de traitement de données robuste et automatisée. - Architecture céleste: Participer activement à l'ingestion et la mise en qualité des données selon les bonnes pratiques de la Factory et gérer le traitement, l'agrégation et la sauvegarde des données. En étroite collaboration avec le chef de projet, les OPS et les architectes, vous participerez aux activités d'architecture, conception et développement. - Mise en production de la galaxie : Effectuer l'intégration continue (avec le versioning, le packaging, les tests et le déploiement) pour assurer une bonne mise en production de notre appareil cosmique. - Surveillance des nébuleuses: Contribuer professionnellement et activement à la veille scientifique et technique, aux projets R&D, et à la construction d'assets et de services techniques orientés data. Participer également aux autres activités du pôle Data Science & Engineering (reporting d'activité, communication interne et externe, collaboration avec les universités et laboratoires associés). Votre profil stellaire comprendra : - Maîtrise de l'univers data : Votre expertise en data et modélisation des données forme la colonne vertébrale de notre exploration. Vous savez comment transformer des nébuleuses de données brutes en systèmes solaires d'informations structurées et exploitables. - Architecte des flux de données: Vous êtes expert dans la gestion de la chaîne de transformation des données, de l'ingestion à la visualisation. Votre capacité à optimiser les processus de traitement et de calcul assure la fluidité et l'efficacité de notre exploration. - Ingénieur de l'industrialisation: Grâce à votre habileté à industrialiser les flux de données, vous garantissez la scalabilité et la robustesse de nos systèmes, préparant notre architecture à l'inconnu. - Créateur d'Outils Visuels: Votre talent dans la conception d'outils graphiques et la data visualisation illumine le chemin, permettant à tous de naviguer avec clarté dans le cosmos des données. - Guide et Mentor: Vous accompagnez chaque projet data, assurant la compréhension et l'atteinte des objectifs. Votre capacité à analyser et comprendre les besoins métier, et à rédiger avec précision technique et scientifique enrichit la connaissance collective et solidifie notre quête. - Arsenal Technologique: Votre maîtrise de Microsoft Azure Data Lake Storage, Spark / Scala, Git / Azure DevOps, Airflow / DataDog / Nifi, et plus encore, est l'arsenal qui nous propulse dans cette aventure. Chaque outil, chaque langage, est une étoile dans la galaxie de vos compétences, illuminant notre chemin vers l'avant-garde technologique. Votre Mission, si vous l'acceptez: Faire partie de la Data Factory d' Inventiv IT, c'est accepter de naviguer vers l'inconnu, d'explorer de nouveaux horizons et prêts à s'embarquer dans cette aventure cosmique.
PROFIL SOUHAITÉ
Expérience
2 An(s)
Source: Pole emploi (https://www.pole-emploi.fr)
Show more
Show less","{'ProgLanguage': ['Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Chef', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
DATA Engineer Azure F/H,SOFTEAM,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-azure-f-h-at-softeam-3839971802?position=5&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=8Q3yQextnpgvyUs%2Brm5KYg%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous évoluez dans le domaine de la
Data
et souhaitez intégrer un leader de la transformation numérique spécialisé dans les secteurs de la
Banque, du
LGuxe
, de l'Assurance, de la Finance, de l'Energie
et la possibilité d'évoluer au sein du Groupe
Docaposte
!
Softeam
est labellisé ""
HappyIndex® AtWork
"" 2022 pour la
5ème
année consécutive !
Nos collaborateurs travaillent en «
mode projet
» autour des
Modern DATA Platform
☁ et
technologies Big Data Azure Cloud / On premise
et autre ""playeur"" du marché. Nous accompagnons de bout en bout nos clients sur des problématiques de
Gouvernance, d’Intégration, de Visualisation et d’IA
.
CE QUE NOUS RECHERCHONS
SOFTEAM Data
recherche un(e)
Data Engineer Cloud Azure
, disposant de solides connaissances techniques.
CE QUE NOUS ATTENDONS DE VOUS
En tant que
Data Engineer Cloud Azure
, vous concevez, mettez en place et administrez
des clusters et des solutions big data
.
Vos missions détaillées :
A
comprendre le besoin de nos clients
au travers de missions de type : aide aux choix d’outils, cadrage des besoins, POC ;
Recueillir et analyser les besoins et proposer une architecture technique
adaptée aux cas d’usage des clients ;
Conduire des projets de
déploiement des Modern Data Platform
(Applications Cloud Native, Migration d’applications existantes) et participer à la mise en œuvre ;
Développer et maintenir des cas d’usages clients avec
les outils et les infrastructures Big Data / Cloud Azure. Modéliser et analyser des données dans le Cloud
. Garantir la sécurité / compliance des données ;
Fournir une expertise technique approfondie
aux équipes projets ;
Rédiger
la documentation permettant à l'IT d'assurer la maintenance.
VOUS ETES
Ingénieur(e) de formation
, vous disposez d'une expérience de
3 ans
minimum en tant que
Data Engineer
.
Vous avez un minimum de
4
années d’expérience sur des
projets Data et idéalement au moins une première expérience sur des projets Cloud Azure
ou à défaut une certification
Azure
avec l’ambition de vous préparer à d’autres.
Vous maîtrisez au minimum
un langage de programmation
(Spark, Scala, Python, Java, R) ;
Vous avez une grande aisance dans la
communication orale et écrite
alliée à un esprit de synthèse, de la rigueur et un très bon sens de la formalisation ;
Fournir une expertise technique
approfondie aux équipes projets ;
Réaliser une veille technologique
permanente sur les tendances du marché et les perspectives concurrentielles.
NOUS VOUS OFFRONS
Des
missions engageantes
auprès des
grands acteurs du marché
.
Un
management de proximité
avec Gilles SALVADOR, Directeur du Centre d'Expertise Data, toujours bienveillant et à l'écoute et avec qui vous pourrez
échanger au quotidien
sur les
enjeux de votre mission
et évoquer vos
futurs projets
afin que nous puissions vous aider à les réaliser.
La
possibilité d’évoluer
et de
monter en compétences
grâce à des
formations et à des certifications
auprès de nos clients et de nos consultants, des 12@13, notre Entité Softeam Institute, Organisme de formation interne de renommé qui délivre des formations auprès de nos clients...
QUI SOMMES-NOUS ?
SOFTEAM DATA
est une marque de
DOCAPOSTE
spécialisée dans
l'informatique décisionnelle
et les
nouvelles technologies
. Nous apportons notre expertise à nos clients, principalement
des Grands Comptes
de la place
financière française
, dans des projets de transformation digitale et cognitive.
2000 Softeamien.nes sont dédié.es à la transformation métier et digitale de nos clients et ont généré
200 M€ de chiffre d’affaires
en 2020.
SOFTEAM SPIRIT
Des
communautés d'expertises
sur les sujets de la
Data
;
De super
nouveaux locaux
qui sont en plus accessibles facilement ;
Une
école de formation
intégrée ;
Des
évènements
: des soirées avec les consultants, des 12@13... ;
Une entreprise labellisée
""Happy at Work""
pour la 5ème année consécutive.
N’attendez plus, rejoignez SOFTEAM et venez nous rencontrer dans nos nouveaux locaux situés à la Défense #DevenezSofteamien !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Senior Data Engineer,RSight®,"Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-rsight%C2%AE-3913324068?position=6&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=tY%2FcpQNsIOvXDKVnH0msxA%3D%3D&trk=public_jobs_jserp-result_search-card,"Nous recherchons activement un(e)
Data Engineer confirmé
, pour le compte de l’un de nos clients,
une ESN, leader de l’Ingénierie et du Conseil en Technologies
. Vous interviendrez sur des
projets à forte valeur ajoutée
d’une
Leader mondial du secteur de la réassurance
.
Responsabilités
Construire, livrer et maintenir les produits de données (pipelines de données, services, APIs...).
Collaborer étroitement avec les équipes produits pour développer de nouvelles fonctionnalités liées aux produits, notamment les fonctionnalités liées :
Au pipelinage des données au sein ou entre plusieurs produits.
Aux capacités d
’
analyse et de data warehousing pour l
’
exploration des données, la science des données et la BI.
Au calcul parallèle sur de grands volumes de données.
Développer des artefacts ou fonctionnalités de données (pipelines de données, services de données, APIs...) en suivant des modèles de pointe (architecture Medaillon, gitflow).
Conseiller sur l
’
architecture des flux de données de bout en bout.
Collaborer avec divers intervenants (propriétaires de produits, propriétaires de solutions, analystes de solutions de données, développeurs, chefs techniques, architectes) pour livrer des artefacts de données dans un esprit d
’
équipe.
Compétences requises
Python, SQL, Databricks, Palantir Foundry, DataViz : Expert
MSDevOps, Jenkins, Artifactory, Container Registry : Confirmé
Technique de parallélisation, programmation distribuée : Confirmé
Delta Lake, architecture Medaillon, blobStorage, fileshare : Confirmé
Compétences humaines
Orientation vers la résolution de problèmes avec une forte pensée analytique.
Autonomie et rigueur dans la manière d
’
aborder les défis techniques.
Aptitude à conseiller sur l
’
architecture des flux de données.
Capacité à collaborer avec divers intervenants pour atteindre les objectifs dans un esprit d
’
équipe.
Bénéfices
Opportunité de travailler sur des projects d
’
envergure.
Un processus de recrutement court, un accompagnement personnalisé, une évolution qui s
’
adapte à votre trajectoire de carrière.
En plus de votre quotidien lié à votre mission, vous pourrez entreprendre, être formé, passer des certifications.
Un environnement de collaboration positif où chacun est valorisé et intégré.
Une culture forte et bienveillante, et une grande place laissée à la liberté.
La diversité et l
’
envergure des projets.
Une approche pragmatique, qui répond aux vrais enjeux des entreprises.
Une équipe d’ingénieurs animée par l’innovation dans un environnement collaboratif hétérogène et inclusif.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Chef'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': ['Résolution de problèmes', 'Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,MindPal,"Marseille, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-at-mindpal-3914442063?position=7&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=spIKVJhzAUv59XOxHP8m1w%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for
Data Engineer!
Responsibilities
Designing, creating, and maintaining data processing systems
Analyzing and optimizing data processing workflows
Collaborating with the team to ensure data quality and efficiency
Testing and implementing new solutions
Requirements
At least 2 years of experience in designing and creating data processing systems
Proficiency in tools and programming languages related to data engineering (e.g. Hadoop, Spark, Scala, Python)
Excellent knowledge of databases and SQL language
Ability to work in a team and communicate effectively with other departments
Communicative English skills
Experience with AWS/AWS Glue is a plus
We Offer
B2B contract
Full-time job
Remote work and flexible hours
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
DATA Engineer GCP F/H,SOFTEAM,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-gcp-f-h-at-softeam-3852605158?position=8&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=WcFAXjIUmdRk9lGYaUhJog%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous évoluez dans le domaine de la
Data
et souhaitez intégrer un leader de la transformation numérique spécialisé dans les secteurs de la
Banque, du Luxe, de l'Assurance, de la Finance, de l'Energie
et la possibilité d'évoluer au sein du Groupe
Docaposte
!
Softeam
est labellisé ""
HappyIndex® AtWork
"" 2022 pour la
5ème
année consécutive !
Nos collaborateurs travaillent en «
mode projet
» autour des
Modern DATA Platform
☁ et
technologies Big Data Azure Cloud / On premise
et autre ""playeur"" du marché. Nous accompagnons de bout en bout nos clients sur des problématiques de
Gouvernance, d’Intégration, de Visualisation et d’IA
.
CE QUE NOUS RECHERCHONS
SOFTEAM Data
recherche un(e)
Data Engineer Cloud GCP
, disposant de solides connaissances techniques.
CE QUE NOUS ATTENDONS DE VOUS
En tant que
Data Engineer Cloud GCP
, vous concevez, mettez en place et administrez
des clusters et des solutions big data
.
Vos missions détaillées :
A
comprendre le besoin de nos clients
au travers de missions de type : aide aux choix d’outils, cadrage des besoins, POC ;
Recueillir et analyser les besoins et proposer une architecture technique
adaptée aux cas d’usage des clients ;
Conduire des projets de
déploiement des Modern Data Platform
(Applications Cloud Native, Migration d’applications existantes) et participer à la mise en œuvre ;
Développer et maintenir des cas d’usages clients avec
les outils et les infrastructures Big Data / Cloud GCP. Modéliser et analyser des données dans le Cloud
. Garantir la sécurité / compliance des données ;
Fournir une expertise technique approfondie
aux équipes projets ;
Rédiger
la documentation permettant à l'IT d'assurer la maintenance.
VOUS ETES
Ingénieur(e) de formation
, vous disposez d'une expérience de
3 ans
minimum en tant que
Data Engineer
.
Vous avez un minimum de
4
années d’expérience sur des
projets Data et idéalement au moins une première expérience sur des projets Cloud GCP
(Compute, Stockage), ou à défaut une certification GCP avec l’ambition de vous préparer à d’autres.
Vous maîtrisez au minimum
un langage de programmation
(Spark, Scala, Python, Java, R) ;
Vous avez une grande aisance dans la
communication orale et écrite
alliée à un esprit de synthèse, de la rigueur et un très bon sens de la formalisation ;
Fournir une expertise technique
approfondie aux équipes projets ;
Réaliser une veille technologique
permanente sur les tendances du marché et les perspectives concurrentielles.
NOUS VOUS OFFRONS
Des
missions engageantes
auprès des
grands acteurs du marché
.
Un
management de proximité
avec Gilles SALVADOR, Directeur du Centre d'Expertise Data, toujours bienveillant et à l'écoute et avec qui vous pourrez
échanger au quotidien
sur les
enjeux de votre mission
et évoquer vos
futurs projets
afin que nous puissions vous aider à les réaliser.
La
possibilité d’évoluer
et de
monter en compétences
grâce à des
formations et à des certifications
auprès de nos clients et de nos consultants, des 12@13, notre Entité Softeam Institute, Organisme de formation interne de renommé qui délivre des formations auprès de nos clients...
QUI SOMMES-NOUS ?
SOFTEAM DATA
est une marque de
DOCAPOSTE
spécialisée dans
l'informatique décisionnelle
et les
nouvelles technologies
. Nous apportons notre expertise à nos clients, principalement
des Grands Comptes
de la place
financière française
, dans des projets de transformation digitale et cognitive.
2000 Softeamien.nes sont dédié.es à la transformation métier et digitale de nos clients et ont généré
200 M€ de chiffre d’affaires
en 2020.
SOFTEAM SPIRIT
Des
communautés d'expertises
sur les sujets de la
Data
;
De super
nouveaux locaux
qui sont en plus accessibles facilement ;
Une
école de formation
intégrée ;
Des
évènements
: des soirées avec les consultants, des 12@13... ;
Une entreprise labellisée
""Happy at Work""
pour la 5ème année consécutive.
N’attendez plus, rejoignez SOFTEAM et venez nous rencontrer dans nos nouveaux locaux situés à la Défense #DevenezSofteamien !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Ingénieur Data Talend (F/H),Thales,"Vélizy-Villacoublay, Île-de-France, France",https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-talend-f-h-at-thales-3890948785?position=9&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=pENNDDIYneLegG1YTnqB5g%3D%3D&trk=public_jobs_jserp-result_search-card,"QUI SOMMES-NOUS ?
Thales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.
Thales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.
QUI ETES-VOUS ?
Diplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.
CE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :
En tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.
VOS MISSIONS :
• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Talend.
• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.
• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.
• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.
• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.
• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.
• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.
Innovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Tech Lead Data Engineer,AXA en France,"Hauts-de-Seine, Île-de-France, France",https://fr.linkedin.com/jobs/view/tech-lead-data-engineer-at-axa-en-france-3905641945?position=10&pageNum=20&refId=%2FS%2FUEqH%2F4ToPDYrc6skyJQ%3D%3D&trackingId=8BK0UmBOqb%2Be1Wnkmtu6Tw%3D%3D&trk=public_jobs_jserp-result_search-card,"Environnement
En tant que
Tech Lead Data Engineer F/H
, vous allez contribuer directement aux projets des directions métier (ex : fraude santé, multiéquipements, pricing IARD, optimisation du lead management, fragilité auto, …) d’AXA France et à la construction du socle technique Big Data.
Vous allez intégrer une équipe d'une dizaine de personne composée de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus métier de la Direction Transformation Digital Tech et DATA (DT2).
La Direction Transfo. & Tech. d'AXA France en quelques mots :
Une organisation agile en feature teams : tribus, guildes, squads
Des projets sur des applications innovantes à fort trafic (web, mobile…)
Des méthodologies craft (TDD, BDD, clean code, code review…) et DevOps
Une communauté de partage de bonnes pratiques (BBL, dojo, meetup, conf…)
Votre rôle et vos missions
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
D’accompagner techniquement les Data Engineer de l’équipe (coaching, code review, pair programming…)
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le data lake
Les exploiter pour atteindre la finalité business (exposition de business view, réintégration des résultats dans le SI, service de scoring, …)
De travailler à la création du socle technique Big Data et industrialiser le cycle de développement de l'équipe
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Votre profil
D'une formation supérieure en informatique ou scientifique (Master ou Diplôme d'ingénieur), vous justifiez de plusieurs expériences significatives (+ de 7 ans)
sur du développement big data, en particulier sur du PySpark.
Compétences techniques :
Connaissances avancées en développement en
PySpark (Spark avec le langage Python)
Maitrise de l'environnement
Microsoft Azure
Connaissances avancées d'outils de BI comme
PowerBI
Compétences transverses :
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Expérience en mode de delivery Agile (Scrum, Kanban, etc...)
Driver et accompagner des Data Engineer sur le plan opérationnel
Et Idéalement :
Avoir une expérience en tant que lead
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA
Maitrise des Traitements Big Data en mode Streaming avec Kafka
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
Mais pourquoi AXA France ?
Nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs ! Les avantages que nous proposons à nos salariés sont nombreux.
Nous choisir, c’est bénéficier par exemple :
D’un package de rémunération complet comprenant un salaire fixe, un complément de rémunération variable, des primes, de la participation et de l’intéressement, la possibilité d’acquérir des actions AXA, ou encore des solutions d’épargne avantageuses ;
Equilibre vie Pro / Perso. : D’un cadre de travail flexible jusqu’à 3 jours de télétravail possible par semaine, des tickets restaurant pour les jours télétravaillés ou encore une participation à l’achat d’un écran ou fauteuil ergonomique ;
D’une politique visant à concilier vie personnelle et vie professionnelle avec 28 jours de congés payés, entre 14 et 16 RTT selon les années, des formules de travail à temps partiel ou encore des jours d’absence rémunérées pour la rentrée scolaire ou un déménagement par exemple ;
De la possibilité de s’engager pour une cause qui vous tient à cœur grâce à nos associations telles que AXA Atout Cœur, AXA Compétences Solidaires ou encore AXA Prévention ;
Et bien plus encore ! Perspectives de développement des compétences et de carrières immenses, CE, conciergerie, offres privilèges, soutien en cas d’épreuve personnelle…On s’arrête là, la liste est longue
Qui sommes nous ?
AXA est un des leaders de l’assurance et de la gestion d’actifs dans le monde.
Nous aidons nos 108 millions de clients à traverser les petites et grandes difficultés de la vie.
Chaque jour, nous agissons ensemble pour inventer la meilleure manière de les protéger et voulons donner à chacun les moyens de vivre une vie meilleure.
Un challenge qui donne le sourire et envie de se lever le matin !
Chez AXA, nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C’est pour cette raison que nous menons une politique RH engagée qui favorise la diversité, qui préserve l’équilibre vie privée-vie professionnelle et accélère le développement des compétences et des carrières.
Ainsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une véritable culture d’expertise, accélérant le développement des compétences de chacun et proposant une rémunération attractive.
Pourquoi nous rejoindre ?
Vous êtes porteur d’idées et d’initiatives innovantes ? Vous proposez des solutions et êtes au service du client ? Faites partie de notre grande famille en rejoignant
Un leader mondial offrant des opportunités de carrières intéressantes
Une entreprise qui donne une place de choix à l’innovation, à l’initiative et aux actions solidaires (notamment via l’association AXA Atout Cœur)
Un environnement inclusif à tous les niveaux (mixité, handicap, initiatives pour favoriser l’insertion des jeunes, orientation sexuelle, etc.)
Un accès à de multiples avantages (congés, temps partiel, télétravail, etc.)
Un cadre stimulant, qui permet de rencontrer des collaborateurs performants et d’enrichir ses compétences
Victime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Teams'], 'Other': ['DevOps', 'Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['3', '3'], 'Level': [], 'Experience': ['a', 'n', 's', '7', '7', '7']}"
Data Engineer (Cloud Azure) - Confirmé F/H,VISEO,"Boulogne-Billancourt, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-cloud-azure-confirm%C3%A9-f-h-at-viseo-3904219460?position=1&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=zf%2FExEFZwQw2YJkORJaEPA%3D%3D&trk=public_jobs_jserp-result_search-card,"Rejoignez
VISEO
et explorez un univers de possibilités où
l'Humain
, le
Collectif
et le
Challenge
sont au cœur de notre ADN.
Qui êtes-vous ?
Si pour vous être
Data Engineer Cloud Azure Confirmé F/H
, c'est :
Faire partie d'une communauté d'experts en ingénierie des données et en solutions cloud ;
Concevoir, construire et maintenir des pipelines de données robustes et évolutifs sur Microsoft Azure ;
Maîtriser les services Azure tels que Azure Data Factory, Azure Data Lake, Azure SQL Database, et Azure Synapse Analytics ;
Découvrir les nouveautés tel que Microsoft Fabric
Appliquer les meilleures pratiques en matière de sécurité, de qualité des données et de gouvernance de l'information ;
Être familiarisé avec les technologies de transformation de données à grande échelle (Spark, Azure Databricks) ;
Collaborer étroitement avec les équipes BI et analytics pour transformer les données en insights précieux...
Alors, vous êtes le talent que nous recherchons !
Si vous avez une solide expérience en tant que
Data Engineer
, une passion pour le cloud
Azure
et que vous êtes prêt à élargir vos compétences et à avancer dans votre carrière, c'est le moment parfait pour nous rejoindre.
Ce que nous allons faire ensemble ?
Découvrez comment votre quotidien sera transformé en rejoignant l’équipe projet de Nabil. Rattaché à notre agence de Boulogne-Billancourt, vous pourrez :
Accompagner et conseiller le client dans son adoption des Modern Data Platform dans Azure (Etude d’architecture, préparation des DAT, FinOps, DevOps) ;
Concevoir des architectures Cloud Data exploitant efficacement les services de données managés d'Azure ;
Conduire des projets de déploiement des Modern Data Platform (Applications Cloud Native, Migration d’applications existantes) et participer à la mise en œuvre ;
Etablir des relations de confiance avec les décisionnaires techniques et métiers afin de favoriser l’adoption du Cloud Microsoft Azure à long terme au sein de l’entreprise ;
Réaliser une veille technologique permanente sur les tendances du marché et les perspectives concurrentielles ;
Partager votre expertise et apprendre continuellement au sein d'une équipe animée par l'innovation.
Ce que nous avons à vous offrir ?
Des formations, des certifications Azure et un système de mentoring ;
Un environnement stimulant, souple et agile pour des parcours sans limite ;
Un engagement fort pour l’environnement, la société, l’égalité et l’inclusion : la plateforme Vendredi, atelier fresque du climat, Café Joyeux, l'institut Imagine, Yumaincap...
Une organisation flexible pour un bon équilibre vie pro / vie perso.
Ce qui nous différencie ?
Une communauté engagée d'experts en data et cloud Azure : des talks toutes les semaines, la participation à des évènements techniques (ateliers, rencontres d’experts, Tech An Hour, BBL, Rex, Sponsoring…) ;
Des dispositifs collectifs d’épargne salariale (PEE et PERECO) et la possibilité de devenir actionnaire VISEO ;
Un partenariat privilégié avec Microsoft (Gold Partner) vous donnant accès aux dernières innovations et technologies Azure.
Envie de nous rejoindre ? Contactez-nous et faites partie du #VISEOspirit
En tant qu'employeur, VISEO promeut activement la diversité et l'inclusion.
À propos de VISEO
Avec plus de 3 000 collaborateurs répartis sur 5 continents, VISEO allie agilité et expertise technique pour faire du digital un moteur essentiel de compétitivité et de performance.
#PositiveDigitalMakers
GDPR MESSAGE:
Our privacy policy has been updated to comply with the new regulations. We invite you to consult it by clicking here: https://www.viseo.com/fr/politique-de-confidentialite. The VISEO Group uses the data collected as part of your application to assess your suitability for the job in question. We use the Jazz HR tool to help us in our recruitment process. This tool complies with current regulations on the protection of personal data. The tool is hosted in the United States and is PrivacyShield certified for HR data. In all cases, we may keep your file for 5 years so that we can contact you again if another position matches your profile. You may, of course, object to this. To find out more about how your data is used and how you can exercise your rights, please consult our privacy policy https://www.viseo.com/fr/politique-de-confidentialite.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Lead Data & Cloud Engineer (H/F),fifty-five,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/lead-data-cloud-engineer-h-f-at-fifty-five-3910829083?position=2&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=8bM2m%2FNdFrusnjgAx55njQ%3D%3D&trk=public_jobs_jserp-result_search-card,"fifty-five est une data-company d'un genre nouveau qui aide les marques à exploiter les données pour améliorer le marketing, les médias et l'expérience client grâce à une combinaison de services de conseil et de technologie spécialisés.
En tant que pilier data et marketing du Brandtech Group, nous offrons des services qui combinent le conseil en stratégie, les services de cloud, le conseil en média et l'expérience client.
fifty-five, c'est plus de 400 experts du numérique. Des digital consultants, des spécialistes du tracking et du média, des ingénieurs et des data scientists, travaillent tous en étroite collaboration pour fournir des conseils marketing de haut niveau et une assistance technique aux marques, dans tout type d'industrie, partout dans le monde.
Partenaire des annonceurs de la collecte à l'activation et l'exploitation des données, nous aidons les organisations à devenir de véritables entités omnicanales maîtrisant l'efficacité de leur écosystème digital et ses synergies avec le monde physique.
Basé à Paris, nous opérons sur 3 fuseaux horaires depuis nos 10 bureaux, situés à Paris, Londres, Genève, Milan, Shanghai, Hong Kong, Shenzhen, Taipei, Singapour et New York. fifty-five attache une importance particulière au bien-être de ses collaborateurs, ce qui lui a permis de figurer dans le classement Best Workplaces France en 2018.
Contexte :
L'équipe d'ingénierie développe et met en oeuvre les solutions techniques permettant la réalisation de pipelines de données et de datalake pour nos clients : récupération de datas sur de multiples sources de données (APIs, files, etc.), data cleaning, data processing, automation et monitoring de l'ensemble. L'équipe s'appuie sur des technologies récentes (docker, kubernetes, terraform, notebooks, etc.) et met en place ses projets dans les différents clouds du marché (GCP, Azure, AWS...).
Mission :
Nous sommes à la recherche d'une personne capable de réaliser des projets techniques pour répondre aux besoins de nos clients (par exemple: système de recommandations de produits, détection d'anomalies, ranking). Les activités vont du chiffrage et du sizing technique à la mise en œuvre des architectures, en passant par la revue des spécifications fonctionnelles et la production de code.
Le Data & Cloud Lead gère une équipe d'environ 4 Data & Cloud Engineer qu'il accompagne au quotidien (suivi des tâches, validation des livrables, code review, suivi de la formation, etc.). Le Data & Cloud Lead intervient également directement sur les missions, principalement dans les discussions avec nos clients (recueil des contraintes techniques, validation d'architecture, etc.) mais aussi dans le delivery (déploiement d'infrastructure participation au code). Il sera également amené à participer à la R&D et à accompagner les équipes transverses dans la mise en place d'outils de travail internes (librairies pour les data scientists, environnement Notebooks pour les data analysts et data scientists, développement de frameworks sur différents cloud providers, etc.).
Passionné.e par la data et le cloud ? Envie de partager tes connaissances et faire grandir des profils plus juniors ? Envie de collaborer avec des profils experts, aussi bien côté technique (Data, Cloud, DevOps, Data Science, etc.) que business (digital marketing, analytics, media, CRM, CDP, etc.) ? N'hésite plus et postule !
Compétences et expériences :
5 ans d'expérience avec une première expérience professionnelle en tant que Lead et/ou Architecte
Maîtrise des architectures data
Maîtrise de Python et SQL
Maîtrise des environnements Cloud. Certifié sur GCP, Azure ou AWS
Maîtrise d'au moins un data warehouse (BigQuery, Snowflake, etc)
Maîtrise des concepts liés aux APIs (OAuth, REST, etc.)
Bonne connaissance de Docker
A l'aise avec les notions d'Infrastructure as Code (Terraform)
A l'aise avec les pratiques GitOps et les concepts autour du CI/CD
Connaissance autour des Notebooks (Jupyter)
La maîtrise d'un orchestrateur,comme Apache Airflow, est un plus
Esprit d'équipe (collaborer aux tests unitaires, revue de code, partage de code, sprints)
Bon niveau en français et en anglais
A déjà travaillé en mode projet avec des interlocuteurs variés (consultant, data analyst, data scientist), idéalement client facing
Une expérience en marketing digital est un plus
Nous proposons :
un bureau au centre de Paris avec terrasse et jardin
un environnement multiculturel avec des collaborateurs aux nationalités multiples (France, Royaume-Uni, Etats-Unis, Chine, Tunisie, Italie et plus)
des projets avec nos bureaux à Londres, Hong Kong, New York, Shanghai, Genève, Shenzhen et Taipei
des TGIF et supers soirées
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Junior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Ingénieur H/F,Alteca,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-ing%C3%A9nieur-h-f-at-alteca-3846531570?position=3&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=k2G88SleWl5Pfdf0F8CVuA%3D%3D&trk=public_jobs_jserp-result_search-card,"TRAVAILLER CHEZ ALTECA
Chez Alteca le management de proximité, le bien-être et l'évolution de nos collaborateurs sont des priorités qui nous permettent, chaque jour, de proposer la meilleure expertise possible à nos clients.
Et c'est grâce à ces convictions, que nous sommes aujourd'hui référencés auprès de partenaires grands comptes dans les secteurs de la Banque, de l'Assurance ou encore de la Distribution
(Si vous souhaitez en savoir plus sur Alteca, RDV sur l'onglet ""Qui sommes-nous ?"").
________________________________
>
En tant que
Data Ingénieur (H/F),
tu seras rattaché(e) au
Pôle Data
de notre Agence toulousaine
.
Ton Manager sera Christian COT (Responsable du Pôle) ou l'un de ses Référent technique.
Interlocuteur privilégié
, il assurera ton
intégration
durant tes trois premiers mois dans l'entreprise, ta montée en compétences, tes
points de suivi
tout au long de tes missions et tes
Entretiens Annuels
.
>
Tu auras également accès à une
communauté technique d'experts
grâce aux nombreux
Webinar
organisés tout au long de l'année, tu pourras aussi participer aux
projets de notre Centre de R&D
ou encore assister aux
MID'INNO
(présentations de nos collaborateurs sur des thématiques données comme la Green Tech, l'IA...).
>
Rejoindre ALTECA c'est aussi rejoindre une entreprise engagée sur la RSE (labellisée
Ecovadis Silver 2022
), pour le bien-être de ses salariés (labellisée
HappyAtWork
pour la 5ème année consécutive), et dans la formation de ses stagiaires et alternants (labellisée
HappyTrainees
pour la 4ème année consécutive).
TES MISSIONS
En coordination avec les équipes d'un de nos clients grands comptes, tes missions seront les suivantes
:
Concevoir les Data Models et Data Pipelines
: cartographier et documenter les types de données et leur usage, concevoir les solutions, les processus, élaborer la stratégie de validation des solutions
Concevoir et spécifier l’infrastructure
: spécifier les solutions d’acquisition en fonction des flux, les solutions de traitement adaptées aux modèles, estimer les besoins et coûts, spécifier les solutions de gestion des données
Accompagner et guider les équipes : présentation des études et solutions, accompagnement dans les expertises, les parcours de formation…
Mettre en œuvre l’infrastructure, développer et maintenir
les services de traitement de données, supervision / monitoring des infrastructures
La majeure partie des projets de nos clients sont sur des
environnements techniques récents
: Python, FastAPI (ou Django / Flask) et des ORM type SQLAlchemy, Pandas, Numpy, Xarray, ETL, Airflow, BDD (timescaleDB, IngluxDB, MongoDB, Elastic, NoSQL Redis, PostgreSQL), Kafka, Prefect, Nifi, Pandas, Numpy, Xarray, Dask) Docker, Kubernetes, Gitlab et en méthodologie Agile Scrum.
Ton profil :
diplômé(e) d'un Bac+5 dans le domaine de la Data (Université ou Ecole d'Ingénieur), tu as au moins 7 ans d'expérience sur un poste similaire et tu sais évoluer en environnement Agile. Tu possèdes également une expérience dans le développement de logiciel (Python) dans une architecture orientée microservices et API
Ta personnalité :
tu es une personne organisée et rigoureuse. Tu disposes d'un bon relationnel et tu aimes le travail en équipe. Tu as la capacité de vulgarisation et de démonstration. Tu apprécies et contribues à développer un contexte de travail bienveillant et qui favorise le partage de connaissances, l’accompagnement au changement. Enfin, tu et motivé pour les grands projets d’infrastructure (moyen / long terme)
Type de contrat proposé :
temps plein |
Niveau de poste :
confirmé
________________________________
Process de recrutement :
ν
Malivanh te contactera pour un premier échange téléphonique, puis elle te recevra dans le cadre d'un
entretien RH
.
ν
Si cet entretien est validé, tu rencontreras alors Christophe, le Responsable du Pôle Digital, pour un
entretien technique.
________________________________
NOS AVANTAGES
Transport pris en charge à 75% | Tickets resto pris en charge à 60% | Mutuelle prise en charge à 60%
10 jours de RTT en plus des 25 jours de CP | Mode de travail : hybride | Accès au CSE (billetterie, voyages...)
Des parcours de formations personnalisés (75% de nos collaborateurs ont suivis au moins 1 formation en 2022)
En tant que signataire de la charte de diversité en entreprise, Alteca favorise un environnement de travail inclusif et respectueux de tous. A compétences égales, tous nos postes sont ouverts aux personnes en situation de handicap.
Votre chance c'est votre talent, la nôtre c'est de le développer : rejoignez-nous !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', ' MongoDB'], 'DataAnalytics': ['Pandas', 'NumPy', 'R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['PostgreSQL'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Hybride', 'Temps plein', 'Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '7', '7', '7']}"
Senior Data Engineer,DCube,"Boulogne-Billancourt, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-dcube-3892422399?position=4&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=UesBbmydvpha%2B8AyqbhxKQ%3D%3D&trk=public_jobs_jserp-result_search-card,"La Data, c’est votre dada, vous êtes câblé(e) comme ça : vous aimez collecter et traiter des données pour les mettre à disposition des autres.
Votre mission, si vous l'acceptez :
Comprendre et analyser des besoins métiers.
Concevoir et réaliser des processus optimisés d’ingestion et de traitement de données dans des architectures lakehouse et data mesh.
Organiser et urbaniser le stockage des données.
Industrialiser des modèles de Machine Learning.
Exposer des données via le développement d’APIs.
Réaliser des tests de validation des traitements.
Être consultant(e) chez dcube :
Nos consultant.e.s sont toutes et tous en CDI, ils interviennent en mission chez nos clients, mais ils ont aussi le choix de travailler depuis les locaux de dcube ou depuis chez eux en télétravail ponctuel.
Ils travaillent parfois à plusieurs sur un projet, parfois en solo, mais ils peuvent toujours compter sur le soutien de leur manager référent et de leur lead practice pour échanger sur des sujets techniques et/ou humains !
Nos clients sont majoritairement des PME, mais 30% d'entre eux sont des grands groupes. Nous proposons à nos consultant.e.s des missions dans toutes sortes de secteurs : finance, assurances, BTP, médias, services...
Nos consultant.e.s sont fier.e.s de leur apporter leur expertise pour répondre à leurs problématiques techniques et métiers en leur proposant des solutions sur mesure.
Vous seriez parfait(e) pour rejoindre nos équipes si :
Vous êtes ceinture noire en Data Engineering avec une expérience d’au moins 5 ans.
Vous êtes rodé(e) à la mise en place de pipelines de données en mode batch et/ou streaming autour des technologies Snowflake, Databricks, Microsoft Fabrics ou Azure Synapse Analytics, DBT, Azure.
Vous avez un peu travaillé sur des architectures lakehouse et/ou data mesh.
Vous maîtrisez les principes de modélisation Data Vault et schéma en étoile.
Quand votre grand-mère vous demande quel est votre métier, pour faire simple, vous répondez que vous êtes facilitateur(trice) de la vie des gens. Vous aimez rendre service en accompagnant les clients dans leur transformation, en leur apportant de la valeur et en les aidant à résoudre leurs problèmes grâce à votre savoir-faire technologique.
Bonus :
Curieux(se) et gourmand(e), vous vous enfilez de grosses tartines d’Azure au petit déj.
Vous êtes une star du NoSQL, Machine Learning ou développement d’APIs.
Les cubes de Power BI n'ont pas de secrets pour vous.
Pourquoi choisir dcube ?
Notre vision :
dcube rassemble des femmes et des hommes de valeur, engagés avec leurs partenaires, pour partager une expérience humaine et technique enrichissante.
Quelques avantages :
Rémunération à partir de 50k.
Télétravail hybride et flexible.
Un accompagnement tout au long de votre carrière par un référent technique et un manager dédié hyper sympas.
Une montée en compétence continue avec le financement de nombreuses formations et certifications, ainsi que la possibilité d’échanger entre experts sur vos sujets techniques de prédilection lors de nos dcube learning.
L’organisation d’événements internes qui rassemblent (Afterworks, Meetups, Webinars, etc.)
Un ordinateur portable et un package télétravail pour équiper votre bureau.
Votre pass Navigo pris en charge à 100%.
Une mutuelle réactive et généreuse (Alan) et une carte ticket restaurant (Swile).
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': ['Hybride'], 'TypeContract': ['CDI'], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer Cloud - Azure Confirmé F/H,VISEO,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-cloud-azure-confirm%C3%A9-f-h-at-viseo-3904216858?position=5&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=xRPBF%2Bq%2F%2BS2quxjA%2BssoLQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Rejoignez VISEO et explorez un univers de possibilités où l'Humain, le Collectif et le Challenge sont au cœur de notre ADN.
Qui êtes-vous ?
Si pour vous être
Data Engineer Cloud Azure Confirmé F/H
, c'est :
Faire partie d'une communauté d'experts en ingénierie des données et en solutions cloud ;
Concevoir, construire et maintenir des pipelines de données robustes et évolutifs sur Microsoft Azure;
Maîtriser les services Azure tels que Azure Data Factory, Azure Data Lake, Azure SQL Database, et Azure Synapse Analytics ;
Découvrir les nouveautés tel que Microsoft Fabric
Appliquer les meilleures pratiques en matière de sécurité, de qualité des données et de gouvernance de l'information ;
Être familiarisé avec les technologies de transformation de données à grande échelle (Spark, Azure Databricks) ;
Collaborer étroitement avec les équipes BI et analytics pour transformer les données en insights précieux...
Alors, vous êtes le talent que nous recherchons !
Si vous avez une solide expérience en tant que Data Engineer, une passion pour le cloud Azure et que vous êtes prêt à élargir vos compétences et à avancer dans votre carrière, c'est le moment parfait pour nous rejoindre.
Ce que nous allons faire ensemble ?
Découvrez comment votre quotidien sera transformé en rejoignant l’équipe projet de Grégory. Rattaché à notre agence de Toulouse, vous pourrez :
Accompagner et conseiller le client dans son adoption des Modern Data Platform dans Azure (Etude d’architecture, préparation des DAT, FinOps, DevOps) ;
Concevoir des architectures Cloud Data exploitant efficacement les services de données managés d'Azure ;
Conduire des projets de déploiement des Modern Data Platform (Applications Cloud Native, Migration d’applications existantes) et participer à la mise en œuvre ;
Etablir des relations de confiance avec les décisionnaires techniques et métiers afin de favoriser l’adoption du Cloud Microsoft Azure à long terme au sein de l’entreprise ;
Réaliser une veille technologique permanente sur les tendances du marché et les perspectives concurrentielles ;
Partager votre expertise et apprendre continuellement au sein d'une équipe animée par l'innovation.
Ce que nous avons à vous offrir ?
Des formations, des certifications Azure et un système de mentoring ;
Un environnement stimulant, souple et agile pour des parcours sans limite ;
Un engagement fort pour l’environnement, la société, l’égalité et l’inclusion : la plateforme Vendredi, atelier fresque du climat, Café Joyeux, l'institut Imagine, Yumaincap ... https://www.viseo.com/fr/notre-demarche-rse
Une organisation flexible pour un bon équilibre vie pro / vie perso
Ce qui nous différencie ?
Une communauté engagée d'experts en data et cloud Azure : des talks toutes les semaines, la participation à des évènements techniques (ateliers, rencontres d’experts, Tech An Hour, BBL, Rex, Sponsoring…) ;
Des dispositifs collectifs d’épargne salariale (PEE et PERECO) et la possibilité de devenir actionnaire VISEO ;
Un partenariat privilégié avec Microsoft (Gold Partner) vous donnant accès aux dernières innovations et technologies Azure.
Envie de nous rejoindre ? Contactez-nous et faites partie du #VISEOspirit
En tant qu'employeur, VISEO promeut activement la diversité et l'inclusion.
À propos de VISEO
Avec plus de 3 000 collaborateurs répartis sur 5 continents, VISEO allie agilité et expertise technique pour faire du digital un moteur essentiel de compétitivité et de performance.
#PositiveDigitalMakers
GDPR MESSAGE:
Our privacy policy has been updated to comply with the new regulations. We invite you to consult it by clicking here: https://www.viseo.com/fr/politique-de-confidentialite. The VISEO Group uses the data collected as part of your application to assess your suitability for the job in question. We use the Jazz HR tool to help us in our recruitment process. This tool complies with current regulations on the protection of personal data. The tool is hosted in the United States and is PrivacyShield certified for HR data. In all cases, we may keep your file for 5 years so that we can contact you again if another position matches your profile. You may, of course, object to this. To find out more about how your data is used and how you can exercise your rights, please consult our privacy policy https://www.viseo.com/fr/politique-de-confidentialite.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Consultant·e Data Engineer,Ntico,"Villeneuve-d’Ascq, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/consultant%C2%B7e-data-engineer-at-ntico-3902424755?position=6&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=sfs99cvTMUAxpvjSb9EVrQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Sois acteur de ta réussite et rejoins notre équipe de 140 collaborateurs·trices qui ne font pas que des projets, mais qui vivent une vraie expérience humaine unique !
💡 Partage, Progrès, Plaisir : nos valeurs, ton avenir !
🌐 Présents à Lille, Orléans, Montpellier : des expert·e·s partout en France !
💼 + de 40 clients qui nous font confiance
🧑‍💻 Recrutement sur profil
🎯
TA MISSION :
* Tu intègres une communauté Data, en tant que Data Engineer.
* Tu conçois et modélises les données et identifies les sources et flux à réaliser.
* Tu es en lien permanent avec les équipes métiers et IT.
* Tu formes et transmets ton savoir.
* Tu es garant·e de la qualité des livraisons.
🧑‍💻
TES COMPÉTENCES :
Talend, ODI, Stambia, Kafka, API, Bases SQL, et NoSQL, GCP, AWS
🥇
TON PROFIL :
Tu es expert·e des flux de données.
La manipulation et le traitement des données est une seconde nature.
Tu as le sens du service et tu apportes des solutions innovantes.
Tu aimes transmettre et partager ton savoir.
Tu justifies impérativement d’au moins 3 ans d’expérience et tu as développé·e une autonomie sur ton domaine de compétence.
Tu souhaites diversifier tes compétences pour être toujours à la pointe des cas d’usages métiers et des nouvelles technologies Data.
🙌
NOS AVANTAGES :
✨ Pourquoi nous rejoindre ?
💪
Développement Continu
: Chez Ntico, tu montes en compétences grâce à nos communautés d’experts et nos formations !
🤝
Management de proximité
: On t'écoute, on te valorise et on t'accompagne dans ton projet pro, en toute transparence !
🎉
Moments conviviaux
: Sport, culture, DIY, insolite… Tu peux participer à nos événements tous les mois, et en proposer ! On n’est jamais à court d’idées pour des animations uniques !
Ntico, c'est un cadre de travail bienveillant, un environnement dynamique où l'épanouissement personnel est aussi important que le succès collectif !
Postule dès maintenant et prépare-toi à vivre une expérience humaine unique ! ✨
De notre côté, on te contacte dans les 72h suivant ta candidature et on te propose un processus de recrutement rapide. 🚀
Ntico s'engage activement en faveur de l'inclusion professionnelle des personnes en situation de handicap, tout en promouvant la mixité, la diversité et l'égalité au sein de son effectif.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
"Data Analyst – Lille, France (H/F)",Astek,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-%E2%80%93-lille-france-h-f-at-astek-3839093481?position=7&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=2nnLsE%2BysTgP0uI%2B0%2FQPuA%3D%3D&trk=public_jobs_jserp-result_search-card,"CDI
Lille - France
Publiée il y a 2 mois
Le Groupe Astek
Ce Que Nous Allons Accomplir Ensemble :
Nous rejoindre en tant que
Data Analyst (H/F),
afin d’accompagner un opérateur télécoms, Leader en Europe dans la gestion de ses solutions robustes, efficientes et sécurisées à destination d’environnements variés.
Un challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !
Votre Mission, Si Vous L’acceptez :
Etude sur les données pour aide au cadrage du besoin Métier.
Force de proposition sur des solutions analytiques adaptées aux besoins utilisateurs (avec nos équipes UX).
Analyse des besoins métiers, accompagnement à la formulation et à la définition de KPI métiers.
Modélisation des données.
Conception de Datasets Big Query.
Conception et Réalisation de Datamarts et de Dashboards (PowerBI / Looker)
Analyse de la qualité de la donnée source, pour challenger les équipes Digitales / Data Engineers.
Votre Future Équipe :
Au sein d’un environnement riche et complexe, vous évoluerez avec des experts passionnés à la fois techniques et fonctionnels (Ingénieurs spécialisés, chef de projet, scrum master, product owner …).
L’équipe est en interaction avec des clients à la fois internes et externes.
Votre stack de jeu
SQL – Confirmé
Modélisation de Données – Confirmé
Outils de Data Viz – Confirmé
GCP – Junior
Anglais – Professionnel
Les Petits Plus Du Projet :
Vous interviendrez de A à Z sur des projets riches fonctionnellement et ambitieux techniquement. Un challenge portant sur des millions d’utilisateurs dans un environnement technique innovant et stratégique.
Vous ?
Diplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.
Vous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?
Vous faite preuve d’un bon relationnel et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?
Alors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !
Astek
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de
ses 7800 collaborateurs
qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.
Tous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.
Rencontrons-nous
Notre projet commun vous plait ?
Postulez à cette annonce, et soyez transparent !
Maud, notre Talent Acquisition Referent, vous contactera pour un premier échange.
Puis vous rencontrerez Martin, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .
Enfin, vous rencontrerez Jérémy, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.
Nos Plus
Astek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo
Une politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)
Notre charte de la Diversité
Mots-clés :
ingénieur – ingénieure – consultant – consultante – data – analyst – modélisation – données
Caractéristiques de l'emploi
Catégorie Ingénieur
Job Industry Télécom / Média
Postuler en ligne
Nom *
Prénom *
Email *
Un email valide est requis.
Téléphone *
Un numéro de téléphone valide est requis.
Joindre un CV *
Mots-clés :
ingénieur – ingénieure – consultant – consultante – data – analyst – modélisation – données
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Big Query'], 'SoftBigDataProcessing': [], 'Automation': ['Chef'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': ['Confirmé', 'Junior'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Consultante/Consultant Data Engineer alternance - GRENOBLE,Capgemini,"Grenoble, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/consultante-consultant-data-engineer-alternance-grenoble-at-capgemini-3862396000?position=8&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=iEnQtBt6OzplzL6ySEaPBA%3D%3D&trk=public_jobs_jserp-result_search-card,"Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d’une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.
Vos missions
:
Faisant partie intégrante de l’équipe DataValue, composée d’une quarantaine de collaborateurs, vous participerez activement à :
• Intervenir sur les différentes phases d'un projet dans un environnement Cloud et Agile.
• Contribuer à la gestion de la qualité des données et extraction et analyse de celle-ci, ainsi qu’à la présentation des données dans leur forme raffinée.
• Proposer des nouvelles lectures de données via un travail de fouille sur les gisements d’information, notamment client.
• Adopter une posture de consultant : proposer de nouvelles solutions et accompagner le client dans ses choix.
Votre profil
:
• En école d’ingénieur ou en université, vous êtes à la recherche d'une alternance d'une durée de 1 an.
• Connaissances approfondies des ETL (Talend, Informatica ou SSIS), du traitement de données (Spark, Python, Scala) ainsi que des bases de données (Oracle, SQL Server, Postgres).
• Faculté pour se montrer curieux, autonome et proactif dans la réalisation de ses tâches.
• Capacité à faire preuve de rigueur et à travailler en équipe.
• Bon niveau d’anglais (B2 minimum).
3 raisons de nous rejoindre
:
Qualité de vie au travail
: accord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu
: certifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.
Avantages groupes & CSE
: plan actionnariat, tarifs préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités
:
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handiaccueillant.
En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
À propos de Capgemini
:
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle', 'SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '55', '55', '55']}"
Senior Data Engineer (H/F),Believe,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-h-f-at-believe-3726297642?position=9&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=PWmdK6CxbV%2FSaJRt2OQbCA%3D%3D&trk=public_jobs_jserp-result_search-card,"Description De L'entreprise
Believe est l'un des leaders mondiaux du marché de la musique numérique. Believe a pour mission d’accompagner les artistes et les labels locaux dans l’écosystème digital en leur offrant des solutions à chaque étape de leur carrière et développement
Ce sont plus de 1900 salariés dans 50 pays qui accompagnent artistes avec expertise, respect, équité et transparence.
Afin de soutenir notre forte croissance sur tous les continents, nous sommes constamment à l’affût de nouveaux Believers. Rejoignez-nous afin qu’ensemble, nous ayons un impact fort et plus positif sur l’industrie musicale !
Believe est cotée sur le compartiment A du marché réglementé d’Euronext Paris (Ticker : BLV, ISIN : FR0014003FE9).
www.believe.com
Ready to #setthetone with Believe?
Description Du Poste
Contexte
Le Tribe « Customer Finance » est composé de plusieurs Squad, parmi elles la squad Finance Ingestion qui a pour mission de développer des outils et des applications pour la collecte de royalties auprès des plateformes de streaming de musique ainsi que préparer les données afin de faire la distribution des royalties auprès des producteurs de musiques.
En tant que Senior Data Engineer, tu intégras une équipe de Data Engineering travaillant avec les principes du framework agile Scrum. Cette équipe est composée essentiellement de 5 Data Engineer et 1 Software Engineer.
Nous Avons Un Écosystème Composé De
Un socle de gestion des données (Delta Lake) plus d’1.5 milliard de lignes /mois
Data processing avec Scala et Spark utilisant le runtime de Databricks
Orchestration de nos data pipelines avec Airflow managé
Des APIs déployées avec AWS Lambda et API Gateway pour faire interagir les utilisateurs avec notre interface front (PHP)
AWS RDS pour hoster la base de données back-end sous PostgreSQL
Versionning du code sous GitLab avec un environnement de dev, staging et production
Infrastructure sous AWS
Les missions du Senior Data Engineer au sein de l’équipe :
Accompagner les développeurs à écrire du code propre, qualitatif et conforme aux standards de l’équipe
Interagir avec l’architecte, les équipes infrastructures Cloud pour concevoir les solutions de data engineering
Proposer des améliorations continues et être garant de réduire les dettes techniques
Développer des flux de données (data pipelines) avec de l’Apache Spark et du Scala
Faire de l’orchestration via Airflow avec du Python
Maintenir le workflow GitLab afin de garantir une bonne productivité de l’équipe de développement
Effectuer des revues de codes des autres membres de l’équipe
Collaborer les membres de l’équipe dev pour atteindre l’objectif du sprint
Faire du support applicatif et fonctionnel de l’application auprès des opérationnel
Qualifications
Qualifications du Data Engineer
5-8 ans d’expérience en Scala
Une maitrise horizontale de tous les composants d’une plateforme de data
Expérience en programmation fonctionnel
Connaissance d’un effect system en Scala (ZIO ou cats)
Excellente maîtrise de l’API Spark en Scala avec pour but de guider l’équipe sur les bonnes pratiques
Expérience en développement backend
Une bonne maitrise des services AWS (API Gateway, Lambda, SNS, SQS, S3, Cloudwatch, VPC)
Développer avec un état d’esprit Keep it Simple, Stupid (KISS)
Excellente compétence dans la gestion de relation avec une équipe en remote
Bonne communication pour gérer les différents points de vue et expliquer les contraintes aux utilisateurs
Optionnel
Expérience en PHP (framewok Symfony ou Laravel)
Informations supplémentaires
Set the tone with us
Chez Believe, nous avons deux cœurs : nos collaborateurs et nos artistes.
Nous croyons en la force de nos collaborateurs, qui s'épanouissent chaque jour en développant leur potentiel... Notre objectif est d'offrir à nos collaborateurs le meilleur environnement possible pour qu'ils puissent s'épanouir.
Rock the job
Programme de formation et de coaching sur mesure
Une politique de télétravail
Un programme de bien-être ""Pauses"" avec de nombreuses activités et animations en interne
Accès à Eutelmed, la plateforme numérique de santé mentale et de bien-être qui permet de parler à un psychologue expérimenté
Un restaurant d'entreprise sain et éco-responsable
Une assurance santé individuelle ou familiale
Avantages CE
Un rooftop
Une salle de sport avec des cours gratuits
Sing in harmony
Des groupes d'ambassadeurs pour s'engager sur la réduction de l'empreinte carbone et environnementale de Believe et l’équité professionnelle Femme/Homme.
Mise en place du Forfait mobilité durable: remboursement jusqu’à 600€ des frais de transport en commun/avec une faible empreinte carbone.
Congé 2nd parent de 5 jours calendaires rémunérés à 100% (en plus du congé légal paternité ou du congé d’adoption, nous ne l’attribuons pas au congé maternité)
Believe s’engage à garantir l’égalité des chances en matière d’emploi, sans tenir compte de l’origine, du sexe, des mœurs, de l’orientation sexuelle, du genre, de l’âge, de la situation de famille, de l’état de grossesse, d’une prétendue race, des opinions politiques, des activités syndicales, des convictions religieuses, de l’apparence physique, du nom de famille, du lieu de résidence, de l’état de santé, ou en situation de handicap.
Découvrez nos nouveaux locaux : bit.ly/believeoffice
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['PostgreSQL'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Remote', 'Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '8', '8', '8']}"
Alternance - Data Engineer Junior (H/F),Transdev,"Issy-les-Moulineaux, Île-de-France, France",https://fr.linkedin.com/jobs/view/alternance-data-engineer-junior-h-f-at-transdev-3879679114?position=10&pageNum=22&refId=xJ%2FHB6j7MCr9%2BdBcMIDMwQ%3D%3D&trackingId=UCZZDRclG746BsTGQUMbdw%3D%3D&trk=public_jobs_jserp-result_search-card,"Transdev recrute un Data Engineer Junior (H/F) en alternance
Rejoignez un Groupe international fortement ancré dans les territoires
Votre destination
La Direction ""IT, Data, Digital, Cybersecurity"" définit la Stratégie de Transformation du Groupe et suit sa mise en oeuvre au travers de 9 programmes stratégiques.
Parmi ces programmes, au sein du département Digital, notre équipe ""Data & Technology Office"" anime le programme Data Powered qui vise à augmenter la maturité Data du Groupe Transdev et de ses filiales à l'international, ainsi qu'à valoriser ses assets Data encore peu exploités autour d’une méthodologie innovante axée sur la valeur des cas d’usage à destination de profils bien définis.
Dans ce contexte, nous mettons en place un certain nombre d’environnements techniques qui ont besoin d’être gouvernés et nous réalisons des développements de manipulation de données pour les besoins propres du Groupe. Nous aidons aussi les pays à lancer leurs propres programmes et nous les accompagnons sur tous les volets du changement.
Votre feuille de route
Le/la Data Engineer Junior a la responsabilité :
de la structuration des développements des traitements d’intégration de données hétérogènes (en lien avec l’Architecte Data et le/la Data Engineer Senior)
de la mise en place de processus fiables et automatisés de test et de déploiement.
En parallèle, il/elle travaille avec l’équipe de Data Science afin de comprendre leurs besoins et préparer les données nécessaires aux cas d’usage.
Votre parcours
Actuellement en école d'ingénieur ou université, vous préparez un Master en Sciences de l'information, Sciences de la donnée et vous recherchez une entreprise où réaliser vos deux années d’alternance (M1/M2).
Vous avez un vrai intérêt pour l’ingénierie de la donnée.
Enfin, vous avez idéalement une première expérience significative (stage, projet étudiant solide) en tant que Data Engineer.
Vos atouts
Vous êtes très à l'aise (voire bilingue)
en français et anglais
à l'oral comme à l'écrit. C'est un prérequis pour ce poste car vous serez amené(e) à travailler avec les filiales de Transdev à l'étranger.
Vous avez une bonne compréhension du Data Engineering, vous connaissez les environnements Cloud (notamment Amazon Web Services) ainsi que les technologies Git.
La connaissance accrue des langages SQL et Python est indispensable. La c onnaissance de Snowflake est un vrai plus.
Enfin, rigoureux(se), curieux(se), et proactif(ve), vous avez une réelle capacité à vous adapter, à gérer vos projets de manière autonome, tout en travaillant de manière collaborative dans des équipes pluridisciplinaires.
A savoir
Alternance basée à Issy-les-Moulineaux (92)
Démarrage en septembre 2024 pour un an ou deux ans
Rythme idéal : 4 jours/1 jour (ou 3 semaines /1 semaine)
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Junior', 'Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Senior Data Engineer,netcarbon,"Bordeaux, Nouvelle-Aquitaine, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-netcarbon-3909777157?position=1&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=hrlicBfcntsfyCNt8%2F4V%2BA%3D%3D&trk=public_jobs_jserp-result_search-card,"A propos de netcarbon
Le rapport du GIEC est clair pour lutter contre le changement climatique, nous devons :
🏭
Réduire nos émissions de CO2
🌱
Capter le CO2 de l'atmosphère
Quand il s’agit de capter du CO2,
la végétation a un super pouvoir.
Grâce à la photosynthèse, les plantes absorbent le CO2 contenu dans l’atmosphère pour le stocker dans le sol.
Le problème c’est que ce qui ne peut pas être mesuré, ne peut pas être amélioré.
Netcarbon
entre alors en scène ! Netcarbon est une startup
spécialisée dans la mesure du carbone.
Mais pas n’importe quelle mesure, nous utilisons des données satellites pour avoir une vision globale, homogène et quotidienne du carbone. Heureusement nous ne travaillons pas seuls, Netcarbon est soutenu par le CNES et l’Agence Spatiale Européenne.
Comment cela fonctionne ?
🛰️
La donnée satellite pour mesurer le carbone
Le coeur de netcarbon, c’est les données issues des satellites en orbite au dessus de nos têtes. Grâce à cela, nous pouvons mesurer le CO2 absorbé par la végétation en temps réel et partout sur Terre.
🖥️
Un SaaS pour améliorer et valoriser sa captation carbone
Pour rendre accessible la mesure du carbone à tous, netcarbon a conçu une application qui permet à nos clients de connaître leur captation carbone et d’identifier la végétation à mettre en place pour capter plus de CO2.
Qui sont nos clients ?
👨‍🌾
L’agriculture
Grâce à leurs champs les agriculteurs peuvent stocker énormément de CO2. En utilisant l’application netcarbon, les coopératives et agro industriels peuvent encourager les agriculteurs à stocker plus de CO2 et donc contribuer à lutter contre le changement climatique.
🌳
Les villes
Pour lutter contre le changement climatique et rendre nos villes plus agréables à vivre, il est indispensable de mettre plus de végétation. L’application netcarbon aide les décideurs politiques à concevoir des villes plus durables en mettant la végétation et ses bienfaits au centre de tous projets.
Le poste
Ton poste consistera à consolider les infrastructures de traitement de données de Netcarbon incluant le traitement de données satellites pour que notre produit puisse être déployé massivement. Pour cela, tu travailleras sur deux axes clés :
Datafactory (ETL) :
Pour suivre la captation de carbone, nous avons construit une datafactory qui permet d’analyser des données satellites partout sur Terre. L’objectif sera de consolider la datafactory.
Cloud / Architecture :
Netcarbon s’appuie sur plusieurs Cloud Providers pour le traitement de données, pour le stockage ainsi que le déploiement de son produit. Tu aideras l’équipe à construire la meilleure stratégie cloud pour déployer notre produit.
En tant que membre central de la team data, tu porteras une mission indispensable pour le développement de netcarbon :
consolider nos ETL et insuffler les meilleurs pratiques de data engineering à la team.
Tu interagiras avec la Science Team, la Dev Team et surtout, tu auras l’occasion de faire grandir une solution répondant au défi de notre siècle : lutter contre le changement climatique.
Tes missions
Réaliser un audit de nos ETL et de notre Architecture cloud.
Consolider nos ETL & notre Architecture afin de soutenir le développement et l’amélioration de notre solution de mesure du carbone.
Déployer les meilleures pratiques de data engineering au sein de la Team.
Garder un œil sur les nouvelles technologies et les avancées dans le domaine de la data engineering et des données satellites (stac, cog, xarray, zarr, dask, ...)
Profil idéal
Tu es le candidat idéal si:
Tu as une expérience solide avec des bases de données SQL et noSQL dans le cloud.
Tu as déjà travaillé avec des données géospatiales / imagerie / satellite.
Tu sais ce que signifient GCP, GeoPandas, STAC, GEE, COG, xarray et Dask.
Tu es proactif et tu n’as pas peur de mettre les mains dans le cambouis pour faire un peu de dev ops ou proposer une nouvelle architecture data.
Tu sais qu’une data pipeline bien huilée vaut mieux qu’un modèle chiadé. (ie: tu as déjà collaboré avec une équipe machine learning).
Tu as un bon esprit de synthèse et capable de communiquer efficacement avec ton équipe.
Tu n’as pas pour projet de devenir data scientist.
Formation et Expérience
3 ans d’expérience minimum dans une équipe data.
master ou équivalent en informatique, ingénierie ou science des données.
Process de recrutement
Le processus de recrutement se divise en 3 étapes :
Un entretien en visio de 30/45 minutes avec le head of data
Réalisation d’un cas d’usage à effectuer en une semaine
Un entretien final avec les co-fondateurs de Netcarbon (idéalement en présentiel pour que tu puisses rencontrer ta future team 🤩)
Processus de recrutement
Merci de postuler sur notre job indeed pour nous permettre de centraliser les demandes.
https://fr.indeed.com/job/senior-data-engineer-97612e2a2f4cf205
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['Pandas', 'R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Senior Data Engineer Databricks,Visian,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-databricks-at-visian-3893237152?position=2&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=V8EstfE8XicXmnSHXYl6dQ%3D%3D&trk=public_jobs_jserp-result_search-card,"La Direction des Systèmes d’Information de notre client grand compte dans l’énergie recherche un profil
Data Engineer Databricks Senior
pour concevoir, développer et maintenir les architectures Data nécessaires à l’exploitation de ses données par les analystes métiers et data scientists.
Le D
ata Engineer Senior
intègre une équipe en charge du lakehouse (AWS + Databricks) pour la B2C.
Missions
:
Contribution à la conception de outils de traitement BigData (Ingestion / Traitement / Analyse)
Cadrage technique des besoins émis par les consommateurs de la plateforme Data
Garantir la mise en production des traitements au sein de la plateforme
Optimisation du code et de la capacité des VMs mise en œuvre pour chaque traitement
Garantir la disponibilité et l’outillage pour les équipes métier, ainsi qu’aux utilisateurs de la plateforme (data scientists / data analystes / data engineer)
Etre en relation avec les équipes infrastructure afin d’assurer le cadrage et le déploiement des solutions valides
Support aux équipes consommatrices
Analyse d’anomalies et proposition solution court / moyen terme
Développement sous Databrick (Python / SQL / Spark / Airflow)
Etre force de propositions techniques
Profil recherché :
Formation ingénieure ou universitaire de niveau Bac+5 à dominante informatique et mathématiques
Expérience de 3 à 5 ans minimum sur un poste similaire
Code source (composants applicatifs et tests unitaires)
Maîtrise de Databricks + Python + SQL + Spark + Airflow
Avoir une première expérience sur de la MCO et Support
Compétences en langage SQL et en programmation (Python et PowerShell)
Aisance dans l’utilisation des API (REST, SOA)
Maîtrise de l’anglais
Rigueur, capacité d’analyse et d’adaptation
Autonome, vous savez travailler en équipe et avez l’esprit d’initiative
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer,eXalt,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-3901508929?position=3&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=M%2BMvcApOtpH2QjDIjUux3A%3D%3D&trk=public_jobs_jserp-result_search-card,"Descriptif du poste
Nous recherchons un
Data Engineer Confirmé H/F (minimum 4 ans d’expérience dans la fonction)
pour rejoindre notre communauté sur le
pilier Data Engineering & Big Data.
Vos missions:
Concevoir et développer des pipelines et des flux de données.
Intégrer et transformer des données provenant de différentes sources.
Développer et mettre en œuvre des algorithmes de traitement de données avancés.
Collaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.
Assurer la qualité et la fiabilité des solutions développées.
Conseiller les équipes clients sur les solutions à mettre en place.
Profil recherché
Titulaire d’un Bac+5, Ecole d’Ingénieur
Maîtrise d’un ou plusieurs langages de programmation (Python, Scala, Spark, etc.).
Expérience approfondie des technologies Big Data (Hadoop, Spark, Kafka, Talend, etc.)
Expérience avérée en environnement Cloud (AWS, GCP, ou Azure).
Solides compétences en conception et en optimisation de pipelines de données.
Expérience de travail en méthode Agile
Capacité à travailler de manière autonome et en équipe.
Excellentes compétences en communication et en résolution de problèmes.
Maîtrise de l’anglais (oral & écrit dans un contexte international professionnel).
Déroulement des entretiens
Un entretien RH avec Estelle, à la suite duquel vous saurez tout (ou presque) d’eXalt Value,
Un entretien technique avec un Manager assorti d’un test technique, lors duquel vous aurez l’occasion de démontrer vos talents mais aussi d’apprendre avant même de dire oui,
Un entretien final avec la Directrice Associée ou le Directeur Opérationnel, pour finir de vous convaincre de nous rejoindre 😊
Votre environnement eXalté:
Un environnement de travail Collaboratif favorisant les initiatives et projets transverses à la Practice Data & IA (Lab IA, Data Hub, etc.).
Un collectif de consultants passionnés, s’intéressant aux tendances innovantes du secteur.
Une Practice de proximité, privilégiant la montée en compétence de ses collaborateurs (formations, coachings, mentorats, etc.)
Un suivi individualisé et de proximité par un.e Data Sales Manager référent du compte client, un.e Chargé.e RH et un.e Practice Manager
Une équipe ouverte et dynamique, qui privilégie les moments de partage et de convivialité (séminaires, eXaltemps, meet-up, déjeuners d’équipe, etc.)
Qui sont-ils ?
eXalt
est un cabinet de conseil IT
Pure player Data
& IA basé à Paris (1er arrondissement).
Notre offre s’articule autour de 4 piliers réunis au sein d’une même communauté pour un accompagnement à 360° alliant une expertise technique et méthodologique à une approche conseil métier:
Data Gouvernance & Project
Data Engineering & Big Data
Data Performance & Analytics
Data Science & IA
Filiale du groupe eXalt créé en 2018,
regroupant plus de
950 collaborateurs en France
(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)
et à l’international
(Colombie, Etats-Unis, Espagne, Belgique),
eXalt Value
démontre une
expertise approfondie
dans le domaine de la Data & IA et conseille les entreprises dans le déploiement de leurs stratégies data-driven.
Bénéficiant de la renommée et des relations client du groupe eXalt
(1er dans la catégorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value
est en pleine croissance et regroupe aujourd’hui une communauté d’expertise de plus de 60 collaborateurs en région parisienne.
Nos consultants interviennent sur d
es projets d’envergure stimulants
dans divers secteurs d’activité, Banque & Assurance, Médias, Transports, Retail, Tourisme, etc.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Résolution de problèmes'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Engineer H/F,Proxiel,"Montpellier, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-proxiel-3913995044?position=4&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=w8S9Uma4TwjKIiHoLu%2Bw%2Bw%3D%3D&trk=public_jobs_jserp-result_search-card,"Depuis 1999, PROXIEL accompagne des entreprises dans leur développement en assurant des prestations de conseil et d'ingénierie dans le domaine des technologies.
Proxiel : C'est plusieurs pôles d'activités.
Nous mettons un point d honneur à associer votre bien-être - adaptabilité en fonction de vos contraintes (possibilité de télétravail). Des solutions alternatives, peuvent être envisagées, dans la mesure où elles sont compatibles avec le business que nous entreprenons. Nous souhaitons que chacun de nos salariés s investissent dans nos projets et que notre entreprise soit animée par un projet commun : la réussite de chacun !
Notre approche est simple alors restons transparents dans nos échanges.
Notre siège est implanté à Montpellier PROXIEL. Nous disposons également d une agence sur Paris
Vous présentez des compétences dans les nouvelles technologies en qualité de techniciens développeurs ingénieurs, coté développement ou réseau, vous êtes basés ou mobile sur MONTPELLIER PARIS LYON TOULOUSE MARSEILLE AIX EN PROVENCE, NICE rejoignez-nous !
Bonjour,
Nous recherchons pour notre partenaire un grand compte sur Montpellier un Data engineer :
Compétences
Expérience en architecture de systèmes distribués Big Data
Scala/Java (expérience obligatoire dans l'un des deux langages)
Ecosystème Big Data (Hadoop, Spark, Apache Kafka, Avro, Nifi)
Maîtrise de la CI/CD et des outils de déploiement et orchestration (Jenkins, GitLab, Kubernetes, Docker, Ansible)
Concepts fondamentaux de Kafka
Bases de données NoSQL (Cassandra, BigTable)
Moteur de recherche (Elastic Search)
Bac +5 ou équivalent
Min 3 ans d'expérience
Show more
Show less","{'ProgLanguage': ['Java', 'Scala', 'R'], 'DataBase': ['SQL', 'NoSQL', 'Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': ['Avro'], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['Apache Kafka'], 'Automation': ['Ansible', 'Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['Big Data', 'CI/CD'], 'FrSoftSkills': ['Adaptabilité'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer | Python - Spark - Hadoop | Spécialisé en Big Data | Paris ou Remote Partiel,Octopus IT - Expert du recrutement tech,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-python-spark-hadoop-sp%C3%A9cialis%C3%A9-en-big-data-paris-ou-remote-partiel-at-octopus-it-expert-du-recrutement-tech-3837194913?position=5&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=HYiCQ7lqeOTOgLlDeC8znA%3D%3D&trk=public_jobs_jserp-result_search-card,"La société
Créée il y a 7 ans, cette entreprise de conseil en hyper croissance, se compose d'environ 90 personnes. Elle est devenue experte en Data et IA (NLP, Deep Learning, Machine Learning) et accompagne leurs clients sur l’ensemble de leurs projets data à travers la valorisation de leurs données.
Leur valeur ajoutée ? Leur spécialisation en Data ce qui leur permet d'offrir 3 expertises métiers distinctes : la Data Science, la Data Engineering et le Machine Learning Engineering. Autour de ces expertises gravitent bien sûr les métiers de Lead et d'Architecte.
Une autre de leur force est leur formation interne (avec des profils de seniors ou d'architectes) et externe (avec des partenaires pour passer les certifications).
Chez eux, le collaborateur est placé au centre des préoccupations, permettant ainsi de créer une cohésion et une véritable culture au sein de l'entreprise. Par exemple la majorité des projets se font en équipe et non seul.
Connu et reconnu pour leur expertise en Big Data, ils sont devenu le partenaire principal d'un grand groupe du CAC 40 et ont pris le lead sur tous les sujets touchant à la transformation Big Data de ce groupe.
Pour poursuivre leur croissance, répondre à leurs ambitions et développer de nouveaux marchés, nous recherchons plusieurs profils pour renforcer leurs effectifs.
Le poste
En les rejoignant vous travaillerez sur les problématiques suivantes :
Mise en place et/ou scale d'architectures
Construction de Datalake
Mise en production de model de ML
Pipelining de données
Streaming de données et temps réel
La stack sur laquelle vous travaillerez :
Python, Scala, Spark, Architectures distribuées : Hadoop, HDFS, Cloud : Aws, GCP, Azure
Votre profil
A partir de 3 ans d'expérience en CDI
Vous avez une expérience significative sur des problématiques Big Data
Très bonne compétences en Python et/ou Scala et en Spark
Vous êtes familier avec Hadoop, Hive, Hbase
Une logique cloud (Aws, GCP ou Azure)
Le salaire & avantages
50-60 K€ selon expérience
RTT
Carte Swile & Mutuelle
3/4 jours de télétravail par semaine
Et plus encore…
Ce qu’on préfère
Être impliqué à fond dans une aventure avec de nombreux challenges techniques
Belles opportunités d'évolutions sur des postes d'Architecte, de Lead ou de Ml Ops
Très bonne ambiance, équipe solidaire et orientée partage d’informations
Beaucoup de workshops en interne et catalogue de formations à votre guise
Ce poste a été soigneusement choisi par votre coach. Powered by Octopus IT, cabinet d’Experts en Recrutement Tech (CDI et clients finaux uniquement) – Visitez nous pour plus d’opportunités :
www.octopusit.fr
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Senior'], 'TypeContract': ['CDI'], 'Salary': ['50'], 'Level': [], 'Experience': ['a', 'n', 's', '7', '7', '7']}"
DATA ENGINEER - H/F - (STG-4572),Banque de France,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-stg-4572-at-banque-de-france-3814497304?position=6&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=gQMqXLid0WUm%2BDlp1dyxLg%3D%3D&trk=public_jobs_jserp-result_search-card,"Présentation de la direction générale et du service
La Direction Générale du Système d’Information (DGSI) a pour rôle de concevoir et mettre en œuvre la stratégie informatique de la Banque de France. Elle veille à la mise à niveau de l'architecture informatique, à l'intégration de l'innovation, à la sécurité numérique de la Banque de France mais aussi à la bonne gestion de son patrimoine de données. La DGSI regroupe plus de 2 000 personnes (internes et externes) réparties sur différents sites, à Paris, Vincennes, Marne-la-Vallée, Paris La Courneuve et Poitiers.
Au sein de la Direction Générale du Système d’information, le Lab incarne le centre d’innovation de la Banque de France. Outil d’anticipation, de facilitation et de construction des offres innovantes, le Lab Banque de France a pour mission de faire découvrir aux métiers de la banque centrale les nouveaux usages, les nouvelles technologies et les nouvelles méthodes pour conduire des projets de transformation de façon agile et rapide.
Descriptif de mission
Dans le cadre de son plan de travail sur l'Intelligence Artificielle, le Lab propose un stage afin de développer et de concrétiser les sujets suivants :
Maintenance de scraping de sites avec l’IA (évaluer si à partir de scraping existant, l’IA pourrait corriger les erreurs liées aux évolutions des sites scrapés.)
Évaluation du produit Giskard (framework pour tester les modèles LLM)
Gestion des CRA (Comptes Rendus d’Activités) des prestataires (traitement à base de OCR les CRA transmis par les prestataires, contrôle sur les consommations, voir la possibilité de faire des traitements IA)
Profil recherché
Formation recherchée :
Master (1 ou 2) Data Engineer, Data Scientist
Master (1 ou 2) Ingénierie des systèmes numériques
Compétences :
Technologie: IA, IA Générative
outils de gestion/analyse de la data: PowerBI ou autres
Langages de programmation: Python, ReactJS, Java
Environnements: Cloud, Docker, Git
Qualités :
Capable de travailler à la fois en autonomie et en équipe.
Force de proposition dans son domaine d'expertise
Une très bonne communication autant à l'écrit qu'à l'oral
Contactez nos ambassadeurs
La Banque de France est une institution socialement responsable, attachée au respect de la diversité sous toutes ses formes, à la lutte contre les discriminations, à favoriser la parité Femme/Homme et à garantir un environnement de travail de qualité.
Des aménagements de poste peuvent être organisés pour tenir compte des handicaps des personnes recrutées.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data ingénieur (H/F),Abeille Assurances,"Bois-Colombes, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-ing%C3%A9nieur-h-f-at-abeille-assurances-3819474490?position=7&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=wLLtaB7AES6VNQd3lV%2Bm1Q%3D%3D&trk=public_jobs_jserp-result_search-card,"Rejoignez les équipes de la Data Factory d’Abeille assurances ! Au sein de cette direction composée d’une 40aine de collaborateurs, l’enjeu pour vous sera de gérer les plateformes Data pour les rendre disponibles et accessibles au business ! Ainsi, vous contribuez à la modernisation du SI Data, accompagnez les directions métiers dans la mise en œuvre de leur besoin en termes de données, et assurer la disponibilité des services au quotidien.
A ce titre, vos missions principales sont les suivantes :
Mise en place et maintenance des architectures décisionnelles en intégrant des nouveaux outils ou services en conformité avec la stratégie d’entreprise. A ce titre vous serez amené à tester des solutions (benchmark/POC), à coordonner ou être lead technique Data pour des projets d’intégration et de déploiement de solution en fonction de votre expérience, documenter les divers aspects y compris des best practice et normes d’utilisation, ainsi que développer des outillages facilitant l’intégration ou l’industrialisation des nouvelles applications.
Mise en place de nouvelles interfaces techniques pour des sources de données encore non référencées dans le SI Data et participation à leur standardisation et déploiement.
Participer activement à tous les process transverse de gestion IT au quotidien pour le SI Data : Gestion des identités, Conformité, Cartographie, Audit, Test d'intrusion, Remédiations des vulnérabilités, Plan de recours, Plan de tests et Plan de maintenance.
Facilitation de l’usage des données dans l’entreprise dans un environnement de plus en plus hybride On-Premise, Cloud, SaaS
Profil et compétence :
BAC +5 Master ou Ingénieur IT spécialité Data et IT
Compétences techniques : Python, Big Data,
La connaissance d’outils de DataVIZ (Qlik serait un plus) ainsi que Dataiku ou des outils d’IA.
Gestion de projet indispensable .
Sens de l’analyse précise du contexte afin de trouver la solution la plus efficace et pertinente
Vous êtes autonome , êtes doté(e) d’un bon relationnel notamment face à des interlocuteurs métiers et experts techniques variés. Être bien organisé. Faire preuve d’une grande rigueur .Vous avez une aisance naturelle pour le travail en équipe et avez l’habitude de vulgariser les demandes, faire comprendre avec pédagogie les contraintes métiers et des équipes IT.
Vous évoluez dans un environnement évoluant très rapidement. Aussi, vous êtes curieux.se, apprendre en permanence, La veille technologie est essentielle
Contraintes liées au poste
: astreintes occasionnelles
Anglais :
indispensable (travail avec les éditeurs)
Ce que nous avons à vous proposer ?
Une rémunération globale, composée :
D’une part fixe,
D’une part variable : Individuelle, et collective, via l’Epargne salariale (Participation / intéressement),
Une surcomplémentaire retraite (PERE - Plan d’Epargne Retraite Entreprise)
Du télétravail, encadré par un accord qui prévoit jusqu’à 2,5 jours de télétravail par semaine, accessible après la période d’intégration (sauf exception), Le télétravail donne droit à des titres restaurant, une aide à l’équipement et une indemnité internet mensuelle.
Une mutuelle interentreprise avantageuse.
Un remboursement de transport flexible, encadré par un forfait mobilité durable, pour favoriser les mobilités douces.
Entre 26 et 29 jours de congés payés et 15 jours de RTT pour un temps plein.
Un CSE avec des offres et services attractifs.
Des offres de produits d’assurance et un accompagnement personnalisé,
Un environnement de travail chaleureux et convivial 100% en flex-office, facilement accessible en transport
Et après ?
Chez Abeille Assurances, la mobilité interne est un vrai levier pour développer la carrière et les compétences de nos collaborateurs et collaboratrices, avec un objectif ambitieux de 50% des postes pourvus en interne.
Nous favorisons les mobilités au sein du Groupe Aéma, de Macif, Aesio Mutuelle, Ofi Invest. Ensemble, nous formons le 5ème groupe d’assurance en France.
Quels sont nos engagements ?
L’assurance d’être soi-même :
Chez Abeille Assurances nous sommes convaincus que la diversité est une richesse. Nous nous engageons à traiter les candidatures sans considération de sexe, d’âge, d’origine, de handicap ou de conviction. La direction et les collaborateurs s’engagent au quotidien sur les sujets de diversité et d’inclusion, en témoigne nos différentes communautés : LGBT+, Egalité professionnelle et Handicap.
Et nous ?
Compagnie majeure de l’assurance en France forte de ses 3000 collaborateurs, 1000 agents généraux d’assurance et de ses 180 ans d’expérience, Abeille Assurances dispose d’une gamme étendue de produits et services d’assurance, de protection, d’épargne et de retraite. Abeille Assurances est par ailleurs le partenaire historique de l’AFER, la première association d’épargnants en France (avec près de 754 000 adhérents).
Plus d’informations sur abeille-assurances.fr
Abeille Assurances est une entité d’Aéma Groupe, né en janvier 2021 du rapprochement entre Aésio Mutuelle, Macif, et Ofi Invest. Ce groupe imagine chaque jour les contours d’un monde plus juste et plus humain en plaçant la prévenance au cœur de la relation avec ses adhérents, sociétaires et entreprises clientes. Il couvre les besoins de protection de 11 millions de personnes et répond aux besoins assurantiels et serviciels de 1 français sur 6.
Plus d’infos sur aemagroupe.fr
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Hybride', 'Temps plein'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '180', '180', '180']}"
Data Analyst H/F,Valeuriad,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-analyst-h-f-at-valeuriad-3741219622?position=8&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=lOT9PrrG2hiHK9%2BBwW%2FxsQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Rejoins la Team Data
créée par Nicolas Greffard,
Docteur en Intelligence Artificielle
, déjà composée de 20 Data Scientists et Data Engineer talentueux 😍
Nous recherchons de nouvelles pépites pour rejoindre notre équipe de choc et répondre aux multiples problématiques Data science de nos clients nantais mais également contribuer à nos projets de R&D et travailler sur des conférences incroyables (DevFest, Salon de la Data) 🤩
Ta future mission si tu l'acceptes 😉
Nous te proposons d'intervenir au sein de nos grandes DSI clientes, sur des sujets de collecte, d'alimentation et de transformation de données autour de l’intelligence artificielle.
Le job en détail 🤩
Toutes Les Missions Ne Sont Pas Identiques, Mais Voici Des Exemples De Choses Sur Lesquelles Nos Data Analyst Sont Intervenus
Mettre en oeuvre des outils informatiques, des techniques et des méthodes statistiques pour permettre d'organiser, synthétiser et traduire efficacement des données ;
Fournir un appui analytique à la conduite d'exploration et à l'analyse complexe de données ;
Créer des algorithmes de recherche de données qui permettent d'explorer les données utiles ;
Procéder à l'industrialisation du procédé pour les données les plus intéressantes. Et organiser, synthétiser et traduire les informations pour faciliter la prise de décision ;
Gérer les opérations et l'administration, la modélisation et l'architecture des sources de données. Et s'assurer que les bases de données existantes soient opérationnelles et intègres ;
Donner un sens aux données à l'aide de ses connaissances analytiques (SQL, analytics/BI, statistiques basiques) ;
D’intégrer de nouveaux jeux de données (Open Data, crowd sourcing, API, fichiers, etc.).
Nous intervenons sur
des données Big Data
(Hadoop, Hive, Spark, etc...), NoSQL (Neo4j, Redis Graph, Redis, mongo) avec toujours quelques bases de données Oracle indéboulonnables. Mais aussi régulièrement sur des environnements
Cloud
(principalement AWS et GCP). Côté outillage et ETL, les missions récentes étaient principalement sur Informatica, Dataiku et Dig Dash. A retenir : nous faisons de tout !
Pourquoi choisir Valeuriad ? 😊
En plus d’être aujourd’hui un acteur nantais reconnu de l’expertise IT, nous nous inscrivons depuis notre création dans une démarche d'entreprise
Opale
et
Holacratique
, où l'ensemble de nos prises de décisions et projets sont réalisés par et avec l'ensemble de nos 120 coéquipiers 💪
Rejoindre Valeuriad, c'est
pouvoir s'investir dans la co-construction
de l'entreprise :
Par un rôle, avec une fiche de poste et un temps dédié (gestionnaire des Ci’s, porteur des partenariats écoles, organisateur d’événements, PO des projets internes, gestion de l'Académie Valeuriad…).
Par les projets stratégiques (200 jours mis à disposition pour les coéquipiers chaque année) pour créer et faire grandir des projets structurants (création de nouveaux avantages à l'ancienneté, création d'indicateurs mensuels pour être toujours plus transparents, mécénat de compétences pour des associations caritatives...).
Par les projets cagnottes (150€ par coéquipiers et par an) pour réaliser des projets collaboratifs qui te tiennent à cœur avec d'autres Valeurieux (découverte du cécifoot, challenge écologique, challenges sportifs pour des dons à des associations humanitaires, borne photo...).
Par les ateliers collaboratifs, chaque mois des brainstorming et ateliers de travail sont proposés par les différents porteurs de projets et sont ouverts à tous les volontaires.
Mais avant-tout nous sommes une
équipe soudée
, des collègues qui apprécient passer du temps ensemble lors de nos soirées hebdomadaires et se créer des souvenirs inoubliables 🤩 C'est pour ça que chez Valeuriad, le plus important pour nous reste le savoir-être : des passionnés, du dynamisme, des sourires, de l'écoute et le sens de la fête 😉
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'Neo4j'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': ['Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Statistiques', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (M/F),SESAMm,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-m-f-at-sesamm-3771461770?position=9&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=oH%2FhN4EBRj4DmceeTT87Lw%3D%3D&trk=public_jobs_jserp-result_search-card,"SESAMm
SESAMm (www.sesamm.com) extracts behavioral insights from millions of unstructured text data sources across the public internet in over 100 languages, helping clients distill valuable intelligence on businesses and their customers to make investment decisions and manage risk.
We work with many well-known firms around the world in private equity, investment banking, insurance, and asset management to apply Natural Language Processing and Artificial Intelligence algorithms to track e-reputation, competitive positioning, sentiment analysis, risk and ESG factors.
SESAMm is growing quickly, with over 90 team members around the globe and offices in Paris, New York, London, Metz, Tunis, and Tokyo.
Key technologies: Alternative Data, Natural Language Processing, Machine Learning and Quantitative Analysis.
Are you looking for a new challenge? Do you dream of joining a growing company, proposing new ideas, and taking part in innovative processes?
At SESAMm, everyone brings their ideas, skills, and creativity. We value collaboration, support and teamwork. We learn every day from the experience of this great team and we constantly take up new challenges. We are proud of the work accomplished and to participate in the success of this great adventure!
The Data Engineer Role
As a Data Engineer, you will build and scale data components to key SESAMm products, such as raw data ingestion pipeline, job scheduling and ETL design/optimization,t optimize cloud solutions for Product Data Platform, and set up the best data development practices for other tech members. Communicate the work of your team with weekly updates.
You're joining a team that is already highly dynamic and adaptable, ready for expansion. This is a chance for you to play a key role in influencing and shaping the team's culture.
Key Activities
❖ Design and implement best data pipeline for our Text-based products (ingestion, processing, exposition) :
Test and design state-of-the-art data ingestion pipelines
Implement efficient streaming services
❖ Take part in the acquisition of new data sources
For each new data source, describe its feasibility and potential
Create and maintain data collection and centralization pipelines
Integration of data enrichment modules created by Data Scientists
❖ Develop data request tooling for Technical teams
Ease the use of data requesting engines
Optimize architecture and data pipelines
❖ Implement and maintain critical data systems
Process and integrate data in our systems
Ensure maintainability and efficiency
Used technologies : PySpark, AWS EMR, Databricks, SQL, MongoDB,,, An excellent level of English proficiency, very good interpersonal skills and strong motivation are required. The candidate will need to demonstrate autonomy and innovation in the face of a constantly changing environment. The list of tasks described is not necessarily exhaustive and may be modified according to the constraints of the company and the evolution of its needs.
Desired Background and Skills
Education
Engineering school/university with specialization in IT, software engineering or data science. Other types of profiles are welcome to apply as long as they have significant IT experience
Experience
At least 3 years of experience in data engineering with a successful implementation of a cloud-based data processing pipeline
Skills
Good understanding of different databases and data storage technologies
Very good knowledge of distributed computing systems, such as Spark
Good knowledge of cloud computing systems, such as AWS, GCP, Azure ML
Development: Be at ease with Python
Good communication and popularization skills: understand technical team needs and issues, collaborate with several internal teams. Team player.
Additional skills: strong interest in Data Science / Natural Language Processing.
You should be able to work in a product team and show high motivation. This job requires autonomy, curiosity toward a changing environment and real dedication to solving problems for clients
Benefits of Working at SESAMm
Flexibility:
Team members can work remotely and have the opportunity to work with colleagues around the world.
Work environment:
SESAMm is multicultural, with technology, sales, and management teams in the US, France, and Tunisia making important contributions to the company's growth.
Career development:
SESAMm is growing quickly, which means the opportunities for your own growth are continually expanding, and that you can shape the company's culture and evolution.
Professional evolution:
SESAMm values training and knowledge-sharing. We organize internally and externally led training sessions and pay for access to educational platforms.
Transparency:
You will be kept apprised of the company's continuing evolution and performance through monthly ""Ask Me Anything"" meetings, frequent business/finance updates, and strategy-sharing discussions.
Well-being:
Building a good culture is key to building a good business at SESAMm. We focus on being a learning organization that values and supports teamwork, thoughtfulness, and work/life balance.
Job Specifics
Location: Paris or Metz
Duration: Permanent contract
Type of contract: Full time
Start Date: As soon as possible
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', ' MongoDB'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML', 'Machine Learning', 'Cloud'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Teamwork', 'Creativity', 'Collaboration', 'Organization', 'Flexibility', 'Interpersonal Skills']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Analyst / Analytics Engineer (H/F),METEOJOB by CleverConnect,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-analytics-engineer-h-f-at-meteojob-by-cleverconnect-3902873375?position=10&pageNum=25&refId=bvabfaeIkTn6w1fyYCK6qA%3D%3D&trackingId=HEC1%2BOT9CwGgp%2Bz4EHoEpQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Entreprise
Description de l'entreprise
CleverConnect est une scale-up franco-allemande de la HR Tech en croissance fondée il y a 10 ans par des ingénieurs. Nous sommes présents en France et Allemagne. Depuis la fusion avec Talentry en 2022, nous partageons l'ambition de devenir le leader des software solutions du Talent Acquisition en Europe
Nous accompagnons actuellement plus de 10 millions de candidats par an à trouver le bon poste Pour cela, nous mettons en relation les recruteurs et les candidats avec des solutions digitales et des plateformes SaaS innovantes pour rendre le processus de recrutement plus efficace. Nos technologies permettent aux candidats de trouver des opportunités plus ciblées et de valoriser leur personnalité et motivation
Rejoignez notre équipe internationale de 200 collègues qui partagent la même culture et les mêmes valeurs, et qui sont pleinement engagés dans un projet à fort impact sociétal
Si vous voulez en savoir plus : www.cleverconnect.com
Description Du Poste
Description du poste
En tant que Data Analyst, quelles seront vos responsabilités ?
Collaborer avec les départements Product, Sales, Marketing et Communication pour comprendre les besoins métier et les traduire en solutions de données.
Implémenter et optimiser des modèles de données à l'aide de DBT et garantir la qualité et l'intégrité des données. Vous serez en charge de transformer et mettre en forme les données du datawarehouse en approche ELT.
Utiliser BigQuery et DBT pour analyser de grands ensembles de données et en tirer des insights exploitables.
Créer et maintenir des rapports et des tableaux de bord dans des outils de dataviz type Looker Studio, Superset, PowerBI ou Metabase. Ces tableaux de bords peuvent concerner les besoins internes (product, communication, marketing, sales, etc.) et externes (embedded dans nos solutions à destination des clients).
Participer à la conception des pipelines d'ingestion de données avec le Data Engineer.
Effectuer des analyses ad hoc et fournir des recommandations basées sur les données pour soutenir les décisions métier.
Qualifications
Description du profil :
Qui êtes-vous ?
Vous avez au moins 5 années d'expérience en tant que Data Analyst dans un environnement similaire.
Techniquement et idéalement ,
Maîtrise avancée de SQL et expérience de travail avec des ensembles de données à grande échelle.
Expérience pratique avec BigQuery, DBT ou équivalents requis. Expériences Snowplow et Elasticsearch appréciées.
Familiarité avec les outils de visualisation de données tels que Looker Studio, Superset, PowerBI ou Metabase.
Être à l'aise dans le scripting python pour automatiser certaines transformations de données.
Avoir déjà manipulé un outil de Web Analytics tel que Google Analytics.
Expérience dans un environnement Agile et capacité à travailler en collaboration dans des équipes interfonctionnelles.
Q
ue trouverez-vous chez CleverConnect ?
Une équipe dirigeante accessible, bienveillante et à l'écoute
Des bureaux au cœur des villes et la possibilité de faire du télétravail
Des opportunités de formation, d'évolution et de mobilité en Europe
RTT, mutuelle, carte déjeuner, remboursement 50% transport, forfait mobilité durable
Notre Processus De Recrutement Comprend
Entretien initial avec un Responsable de l'Acquisition de Talents
Entretien avec le Manager (découverte/évaluations techniques)
Dernier entretien avec le Directeur IT ou CPTO.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '10', '10', '10']}"
Data Engineer - H/F,Free Pro,"Marseille, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-free-pro-3861276685?position=1&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=8580IxIBEcGpb2%2B5NQW08Q%3D%3D&trk=public_jobs_jserp-result_search-card,"Au sein de la Direction Technique & innovation, dans le service recherche et innovation, vous travaillez sur des projets d’envergure, notamment autour de la 5G et du Edge. Ce pôle d’innovation défriche et prépare les technologies de demain, le tout dans un contexte de croissance avec une forte composante technique. Dans une équipe d’ingénieurs experts FULL STACK passionnés dans les développements Back / Front / DevOps / Système / Embarqué ainsi que PO / CP, vous intégrez une équipe dont le moteur est sa capacité à apporter des réponses techniques innovantes en rupture de l’existant, via des POC fiables et rapides, qui préfigureront les produits Free Pro de demain.
Vos Missions
Développer les solutions techniques de collecte, stockage et transformation de la donnée, dans un contexte MLOps :
Maîtrise avancée des langages de programmation pour le traitement des données, tels que Python, et SQL
Capacité à construire et à maintenir des architectures de données robustes et des pipelines de données évolutifs
Expérience approfondie avec les systèmes de gestion, le maintien et la documentation de base de données (SQL, NoSQL) et des outils d'extraction de données.
Expérience dans la manipulation et l'analyse de grands ensembles de données (Big Data) avec des outils tels que Hadoop, Spark, ou Kafka
Industrialiser et automatiser le nettoyage de la donnée
Mettre en place et maintenir les batchs, c’est-à-dire les automatisations d’une série de traitements en vue de l'intégration dans des modèles statistiques
Compréhension des algorithmes d'apprentissage automatique et de leur mise en œuvre pratique
Gérer le cycle de vie de la donnée conformément à la politique de gouvernance des données de l'entreprise (RGPD...)
Réaliser les tests unitaires et d’intégration
Assurer le suivi de production et la maintenance
De formation Bac+3 à Bac+5, vous justifiez d’une expérience réussie sur ce type de poste. Si vous souhaitez monter en compétence sur la technique et évoluer dans un environnement en perpétuel évolution, ce poste est fait pour vous.
Compétences Requises
Compétences solides en statistique et en modélisation mathématique en vue de l'intégration dans des modèles de Machine Learning
Aptitude à travailler avec des outils de visualisation de données comme Matplotlib, Seaborn, Tableau, Power BI.
Des connaissances spécifiques dans les domaines de la 5G du Edge computing ou de l’IA seraient un atout supplémentaire.
Autonomie et prise d’initiative
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI', 'Matplotlib', 'Seaborn'], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'ML', 'Machine Learning', 'Statistiques'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5', 'Bac+3'], 'Experience': ['a', 'n', 's']}"
DATA ENGINEER – F/H – ALTERNANCE,La Mutuelle Générale,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-f-h-%E2%80%93-alternance-at-la-mutuelle-g%C3%A9n%C3%A9rale-3909189678?position=2&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=t8Ie4MR4QMK2e8YkVMymPA%3D%3D&trk=public_jobs_jserp-result_search-card,"A PROPOS DE NOUS
Débutez ou faites évoluer votre carrière
au sein d’un acteur singulier de l’économie sociale et solidaire,
avec un métier et une culture qui vous ressemblent!
La possibilité de télétravailler de 1 à 4 jours par semaine, selon la fonction et l’organisation du site, permet à nos 1900 collaborateurs de
bénéficier d’un cadre de travail source de sens, de confiance mutuelle et de responsabilisation
.
89% d’entre eux recommandent La Mutuelle Générale !
Posez vos questions à nos ambassadeurs ici:
https://lamutuellegenerale.career-inspiration.com/
VOTRE ENVIRONNEMENT DE TRAVAIL
La Direction des Systèmes d’Information est un partenaire auprès des Métiers et a pour ambition de leur délivrer l’offre, la valeur ajoutée, les services et les compétences. Dans ce contexte, nous renforçons l’équipe Datalake de la Direction des Systèmes d’Information en recrutant un(e) apprenti(e) Data Engineer.
MISSIONS
Vous identifiez et vous répondez aux besoins des directions fonctionnelles ;
Vous participez à la conception et au développement des solutions performantes pour la construction, la maintenance et l'enrichissement du Datalake de la Mutuelle Générale ;
Vous participez au développement et à l’industrialisation des projets Data Sciences avec les méthodologies Agiles et DevOps ;
Vous contribuez ainsi à des uses case innovants ;
Vous assurez une veille technologique autour de l'écosystème Big Data.
CE QUE VOUS POUVEZ ATTENDRE DE NOUS
Une montée en compétences sur des technologies récentes et innovantes
Un réel accompagnement pour découvrir le métier de Data Engineer
Un environnement de travail bienveillant et responsabilisant
Jusqu’à quatre jours de télétravail par semaine
VOS ATOUTS POUR REUSSIR
Formation:
De formation supérieure BAC + 4/5, vous avez une appétence pour travailler dans un environnement Data/BIG Data (AWS, Snowflake, Python, Spark, SQL …)
Dans votre boîte à outils:
De nature curieuse, vous appréciez prendre des initiatives
Autonome, vous êtes capable de remonter les difficultés / alertes
Structuré(e), vous avez une bonne capacité d’écoute, d’analyse et de synthèse
Rigoureux(se), vous savez prioriser et trouver sans cesse des axes d’amélioration
Si vous vous reconnaissez, alors n’hésitez plus, postulez et rencontrons-nous au plus vite !
Contrat
{{:}}
Apprentissage 12 ou 24 mois, à compter de septembre 2024
Lieu de travail:
75013 - Paris
Vous trouverez également chez nous:
Open travail, Tickets restaurant, RIE, Mutuelle, Remboursement transport, …
Mots clés: #devops, #bigdata, #assurance, #alternance
Pour aller plus loin, visionnez cette vidéo
https://www.youtube.com/watch?v=GdYDg60Ryus
et/ou rendez-vous sur
https://www.lamutuellegenerale.fr/ et https://www.lamutuellegenerale.fr/les-salaries-de-lamutuelle-generale-plebiscitent-lopen-travail-et-pres-de-9-sur-10-la-recommandent
Conformément aux engagements pris par La Mutuelle Générale en faveur de l'intégration des personnes en situation de handicap, le poste proposé est ouvert à tous.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Ingénieur data Spark expérimenté (F/ H),Thales,"Vélizy-Villacoublay, Île-de-France, France",https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-spark-exp%C3%A9riment%C3%A9-f-h-at-thales-3886251447?position=3&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=oSyZoLohhtClt8DitaAKHw%3D%3D&trk=public_jobs_jserp-result_search-card,"QUI SOMMES-NOUS ?
Thales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.
Thales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.
QUI ETES-VOUS ?
Diplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 5 ans d'expérience dans les technologies Big Data.
CE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :
En tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.
VOS MISSIONS :
• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Spark.
• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.
• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.
• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.
• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.
• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.
• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.
Innovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer (F/H),Akuo,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-akuo-3905522805?position=4&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=IwoO0Zykt%2F%2BB1AG8NMgw8g%3D%3D&trk=public_jobs_jserp-result_search-card,"Fondé en 2007, Akuo est un producteur indépendant d’énergie renouvelable implanté dans plus d'une vingtaine de pays à travers le monde. Le Groupe est présent sur l’ensemble de la chaîne de valeur : développement, financement, construction et exploitation de centrale d’énergie solaire, éolienne et de stockage.
Les équipes d'Akuo allient pertinence technologique, valeur ajoutée environnementale et gain sociétal pour développer des projets exemplaires et porteurs de sens dans les territoires. Le Groupe est notamment le pionnier de l'agrivoltaïsme, des solutions de stockage et a été le premier à développer des centrales solaires flottantes de grande envergure en France.
Fidèle à ses valeurs et ses convictions, Akuo est une entreprise responsable, attachée à la diversité de ses équipes, engagée et déterminée dans l'inclusion de tous.
Description du poste
Le poste est à pourvoir à Paris, dans le département IT/OT d'Akuo, et plus particulièrement au sein du pôle Data, dont l’objectif est de construire et faire évoluer la stratégie ‘data’ du groupe.
En tant que Data Engineer, vous ferez évoluer la base de données d’Akuo. Vous aurez pour objectif de faciliter l’exploitation de ces données en optimisant tout le chemin de la donnée, de l’extraction jusqu’au stockage et au nettoyage de la donnée.
Vos
missions opérationnelles
seront les suivantes:
BUILD :
Mise en place de l'architecture Data : concevoir et mettre en place les flux d'intégration de données,
Répondre aux demandes du métier (équipes techniques et supports)
:
participer à toutes les phases de projets, de l'analyse des besoins à la réalisation des tests tout en respectant les critères de qualité, de délai et de coût.
RUN :
Traitement, structuration et transformation de données (Python),
Collecter en temps réel, stocker et modéliser les données issues des centrales électriques (Time Series) en relation avec nos équipes opérationnelles,
Assurer un support technique sur les problématiques Data : remontée des tickets.
Qualifications
Nous sommes à la recherche d’un profil junior qui aurait entre 2 et 3 ans d’expérience, alternance incluse, au sein d’équipe ‘data’.
Python & SQL: vous savez coder. Vous connaissez et savez lancer des procédures stockées,
Maitrise des concepts de modélisation de données et de conception d'architectures Data,
Compétences en gestion de bases de données relationnelles,
Langue
: La maîtrise de l’anglais est requise pour pouvoir échanger avec une partie de nos équipes qui n’est pas francophone.
Informations supplémentaires
Eléments essentiels:
La localisation: siège d’Akuo au 140 avenue des Champs-Elysées,
La rémunération: fourchette entre 45k€ et 49k€ fixe selon profil + 5% de rémunération variable,
Le type de contrat: CDI,
Le poste est ouvert au télétravail, jusqu’à 2 jour par semaine + 10 jours flottants par an (soit 100 jours par an).
Autres avantages:
Jusqu’à 12 jours de repos par an,
Prime vacances, chèques-cadeaux,
Intéressement / participation,
Ticket-restaurant (10€ x 19 par mois) pris en charge à 60%,
Prise en charge des transports en commun à 75% du tarif actuellement en vigueur ou forfait mobilité durable,
Locaux agréables avec terrasses aménagées et salle de sport.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Junior'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
"Data Analyst – Paris, France (H/F)",Astek,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-%E2%80%93-paris-france-h-f-at-astek-3839098102?position=5&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=F63flraIlpTMmewFRIj9ZA%3D%3D&trk=public_jobs_jserp-result_search-card,"CDI
Paris - France
Publiée il y a 2 mois
Le Groupe Astek
Ce Que Nous Allons Accomplir Ensemble :
Nous rejoindre en tant que
Data Analyst (H/F),
afin d’accompagner un opérateur télécoms, Leader en Europe dans la gestion de ses solutions robustes, efficientes et sécurisées à destination d’environnements variés.
Un challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !
Votre Mission, Si Vous L’acceptez :
Etude sur les données pour aide au cadrage du besoin Métier.
Force de proposition sur des solutions analytiques adaptées aux besoins utilisateurs (avec nos équipes UX).
Analyse des besoins métiers, accompagnement à la formulation et à la définition de KPI métiers.
Modélisation des données.
Conception de Datasets Big Query.
Conception et Réalisation de Datamarts et de Dashboards (PowerBI / Looker)
Analyse de la qualité de la donnée source, pour challenger les équipes Digitales / Data Engineers.
Votre Future Équipe :
Au sein d’un environnement riche et complexe, vous évoluerez avec des experts passionnés à la fois techniques et fonctionnels (Ingénieurs spécialisés, chef de projet, scrum master, product owner …).
L’équipe est en interaction avec des clients à la fois internes et externes.
Votre stack de jeu
SQL – Confirmé
Modélisation de Données – Confirmé
Outils de Data Viz – Confirmé
GCP – Junior
Anglais – Professionnel
Les Petits Plus Du Projet :
Vous interviendrez de A à Z sur des projets riches fonctionnellement et ambitieux techniquement. Un challenge portant sur des millions d’utilisateurs dans un environnement technique innovant et stratégique.
Vous ?
Diplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.
Vous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?
Vous faite preuve d’un bon relationnel et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?
Alors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !
Astek
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de
ses 7800 collaborateurs
qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.
Tous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.
Rencontrons-nous
Notre projet commun vous plait ?
Postulez à cette annonce, et soyez transparent !
Maud, notre Talent Acquisition Referent, vous contactera pour un premier échange.
Puis vous rencontrerez Martin, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .
Enfin, vous rencontrerez Jérémy, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.
Nos Plus
Astek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo
Une politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)
Notre charte de la Diversité
Mots-clés :
ingénieur – ingénieure – consultant – consultante – data – analyst – modélisation – données
Caractéristiques de l'emploi
Catégorie Ingénieur
Job Industry Télécom / Média
Postuler en ligne
Nom *
Prénom *
Email *
Un email valide est requis.
Téléphone *
Un numéro de téléphone valide est requis.
Joindre un CV *
Mots-clés :
ingénieur – ingénieure – consultant – consultante – data – analyst – modélisation – données
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Big Query'], 'SoftBigDataProcessing': [], 'Automation': ['Chef'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': ['Confirmé', 'Junior'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Lead data engineer,Ippon Technologies,Greater Tours Area,https://fr.linkedin.com/jobs/view/lead-data-engineer-at-ippon-technologies-3893228101?position=6&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=7a8Vs0HE7rPte%2FjFc20ZEw%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes-nous ?
Ippon, c'est l’énergie du collectif au service de la technologie !
Nous sommes un cabinet de conseil et d’expertise technique.
Nos équipes ont pour leitmotive de transformer des idées innovantes en solutions de haute qualité avec un focus particulier sur la valeur apportée aux utilisateurs.
Aujourd’hui, nous souhaitons recruter un Lead Data Engineer pour lancer et développer l'équipe data tourangelle et intégrer la Practice Data composée de 70 consultants au national.
Notre spécialité est de répondre aux enjeux de nos clients et avoir un impact fort dans leur volonté de devenir Data-driven.
Membre de la Practice Data, le futur Lead Data Engineer apportera son expérience et son mindset pour lead des équipes sur des missions à forte valeur ajoutée. Encadré par un mentor et le support de notre communauté Data dynamique, il pourra s'épanouir sur des sujets tendance et assurera sa montée en compétence et celle des autres.
La communauté Data propose un cadre et des évènements pour faire de la veille collective, partager leurs retours d'expériences, débattre de sujets Data, etc. Notre lead sera un acteur majeur au sein de ce cadre.
Les missions
Intervenir sur les data platforms de nos clients pour développer de nouveaux pipelines de données en appliquant les bonnes pratiques de développements
Travailler en collaboration avec les métiers, les data analystes et les data scientists pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship, etc.)
S'appuyer sur des sachants Cloud Builders pour construire des assets Data sur une infrastructure résiliente aux petits oignons
Apporter son expertise sur un domaine en constante évolution
Participer aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups)
Capitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne
Le profil recherché
Tu es de tempérament curieux, force de proposition, tu as le sens du partage, l’envie de t'améliorer en continue et de participer activement à une communauté data pleine de projets.
Une expérience sur le cloud est un gros (gros, gros) plus, une veille active sur des sujets cloud est indispensable.
Idéalement, notre curieux saura identifier le pokémon parmi cette liste et nous expliquer les autres mots qui n'en sont pas un :
Modern Data Platform
Data Warehouse
Data Lakehouse
Pikachu
Cloud Native
Serverless
Si pour toi aussi ""tout part du besoin métier"", que tu t'identifies dans cette liste au père noël et sauras nous en parler par expérience :
Public Cloud Provider (AWS, GCP, Azure)
Query engine (Spark, EMR, Databricks, Starburst, Athena...)
Langage de programmation data (Python, SQL, Scala...)
Stockage (Redshift, Snowflake, BigQuery, Stockage objet S3/CloudStorage...)
Framework d'ingestion/streaming de données (Kafka, Amazon Kinesis, Google DataFlow, ...)
Outil de visualisation (Tableau, Metabase, Superset...)
Le delivery et les projets en production faisant partie de notre ADN, tu es capable de livrer du code de qualité dans des environnements agiles.
Les + du job
Travailler avec une équipe où tu pourras apporter tes propres idées !
Devenir ceinture noire en Data grâce à notre programme d’accompagnement de carrière Blackbelt ! (formations, certifications AWS/Azure, mentoring, coaching)
Une évolution de carrière adaptée à tes expériences et tes souhaits
Le partage de connaissances (articles, meetup, blog), participation des à conférences nationales ou internationales (AWS Summit, Salon du big data)
Une charte de télétravail
Et Tours alors ?
Créée en 2023, l’agence est en plein développement et nous comptons sur toi pour y jouer un rôle majeur afin de construire l’agence qui te, et nous, ressemble. Tu intégreras une petite équipe de consultants experts et passionnés par la technique, et où le partage et la bonne humeur sont de mise.
Tu l’auras compris, en tant que lead data engineer, tu seras le référent Data au niveau de l’agence et tu accompagneras nos prochains data engineers tourangeaux dans l’accélération de leur carrière et expertises.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
"Data Engineer, Wheat",Louis Dreyfus Company,"Villeurbanne, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-wheat-at-louis-dreyfus-company-3892567948?position=7&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=1XqpxPIHqCkfK%2B%2B3ZoLmkA%3D%3D&trk=public_jobs_jserp-result_search-card,"Company Description
Louis Dreyfus Company is a leading merchant and processor of agricultural goods. Our activities span the entire value chain from farm to fork, across a broad range of business lines, we leverage our global reach and extensive asset network to serve our customers and consumers around the world. Structured as a matrix organization of six geographical regions and ten platforms, Louis Dreyfus Company is active in over 100 countries and employs approximately 17,000 people globally.
Job Description
LDC is looking for a Data Engineer to join the Global Digital Transformation and Analytics (DT & A) team of data engineering experts, supporting innovative solutions whose impact is transforming our business.
As a data engineer, you will provide the data to enable faster, better, data-informed decision-making within our business. You will be working with the
You will focus on creating high performant, easy to maintain, state of the art data pipelines, bringing data from source to insight. You will use the latest technology and you will see the impact of your work in transforming our business.
Duties & Responsibilities:
Ensure data availability, quality, and timeliness by designing and implementing performant pipelines and proactive monitoring. Ideate meaningful tests and monitoring to be proactively informed in case of data problems and to minimize regressions.
Create and maintain high quality, well documented code and data transformations to feed models and insights reliably.
Act as data steward with researchers, traders and operations to help them leverage the best data sets for their use case and the most relevant best practices to ensure data is always available
Retrieve, clean, model, transform and maintain data from many sources to better understand the factors influencing wheat global and local flows and prices
Troubleshoot and solve data issues from data ingestion to display in dashboards and reports
Provide technical support and guidance to data scientists and researchers
Experience
3-5 years of experience in technical, hands on roles
2+ years' experience in data manipulation in SQL: DB design, data modelling, data transformation pipelines, ETL, data management
2+ years’ experience with Python, including data manipulation and visualization packages (e.g., Pandas, Numpy, Plotly…)
Experience with data visualisation tools and technologies like PowerBI, Tableau, Looker.
Data mining principles: data collection, wrangling and quality checks using multiple data systems
Experience working with cloud technologies like Azure, AWS or GCP
Experience with any of the following will be considered a plus:
Hands on experience with data storage solutions and their application: SQL, NoSQL, data lakes
Principles of modern full stack development and exposure to Python frameworks like Dash or Flask, familiarity with APIs
Agile development and DevOps tools: CI / CD pipelines, testing and monitoring best practices.
Support for business-critical data transformation and applications via monitoring and alerting, and contributing to the solution of critical incidents under time pressure
What Soft Skills will make you successful
:
Quick learner with the ability to work independently and with little prompting
Excellent communication, collaboration and engagement skills
Critical thinking, inclination to make improvements, find new ideas and challenge the status quo
Flexibility and sense of adaptability in a quick paced, constantly evolving context
Passionate about improving data literacy and spreading best practices
Ability to perform under pressure, prioritize and escalate
Strong problem solving, quantitative and analytical abilities
Fluent in English
Experience in the commodities sector is considered as a plus
Education:
A Bachelor or Master in Computer Science, Business/Management Information Systems, Computer/Systems/Industrial Engineering, Business Analytics, Data Science, Operations Research or Statistics. Other backgrounds will be considered with relevant work experience.
Additional Information
Why you should apply for this role
:
You will be working in a very concrete line of business whose impact literally feeds the world. You will get a unique point of view on major data flows of commodities around the world, including the impact of climate change and major geo political events, the importance of sustainability and regenerative agriculture.
You will become an expert of the latest cutting-edge technologies in Data Engineering, leveraging Microsoft Azure Platform, working in collaboration with a team of technology and data science experts and accessing high quality training and development opportunities
You will see the impact of your role in transforming our business
What We Offer
We provide a dynamic and stimulating international environment, which will stretch and develop your abilities and channel your skills and expertise with outstanding career development opportunities in one of the largest and most solid private companies in the world.
We offer
Competitive salary and benefits
Flexible working
Access to Training and Development
Diversity & Inclusion
LDC is driven by a set of shared values and high ethical standards, with diversity and inclusion being part of our DNA. LDC is an equal opportunity employer committed to providing a working environment that embraces and values diversity, equity and inclusion.
LDC encourages diversity, supports local communities and environmental initiatives. We encourage people of all backgrounds to apply.
Sustainability
Sustainable value is at the heart of our purpose as a company.
We are passionate about creating fair and sustainable value, both for our business and for other value chain stakeholders: our people, our business partners, the communities we touch and the environment around us
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['Pandas', 'NumPy', 'R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI', 'Plotly'], 'Statistics': ['Statistics'], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Cloud', 'CI / CD'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Adaptability', 'Problem Solving', 'Collaboration', 'Organization', 'Flexibility', 'Initiative', 'Critical Thinking']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': ['Salary', 'Package'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer H/F,ELOSI,"Villeneuve-d’Ascq, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-elosi-3842107940?position=8&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=LEzdD9K6u9P8NJ4vMYEhtg%3D%3D&trk=public_jobs_jserp-result_search-card,"A propos d'Elosi
Créée en 2005, Elosi réunit maintenant 120 collaborateurs pour mener à bien les projets digitaux de ses clients depuis leurs locaux ou depuis notre centre de service et R&D à Villeneuve d’Ascq.
Le poste et vos missions
Pour l'un de nos clients grand compte de la métropole lilloise, nous recherchons un data engineer. Vous travaillerez en collaboration avec les équipes de développement.
Vos missions
:
Correction et création des nouvelles features d'intégration de données sur plusieurs produits digitaux ;
Création des pipeline d'intégration des données stambia ou outils internes vers google BigQuery ;
Mise en place des dashboard powerBI ;
Transformation des données en élaborant des modèles conceptuels.
L'environnement technique
Stambia, Google BiQuery, PowerBI
Votre profil
De formation supérieure en informatique, vous avec une expérience en développement de flux Stambia d'au moins 2 ans, vous connaissez Google BigQuery.
Vous aimez ""préparer"" les données brutes, faciliter leur exploitation et les rendre ""propres"" pour l'analyse qui suivra.
Votre anglais est opérationnel.
Nous rejoindre, c'est :
Rejoindre une communauté de passionnés et la participation à des conférences techniques (DevoXX, DevFest Lille, atelier LiveCoding, Pair programming, Apér’Ops…) ;
De la convivialité, du partage, de la proximité ;
Des perspectives d’évolutions tant technique que métier !
Des avantages : carte restaurant, formations, primes…
Si ce poste vous anime, n'hésitez plus et postulez !
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer Snowflake/ Azure Senior F/H,NODYA Group - Digital Services,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-snowflake-azure-senior-f-h-at-nodya-group-digital-services-3911222296?position=9&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=5UYfKxaHdBePWNbdCSMLqg%3D%3D&trk=public_jobs_jserp-result_search-card,"Les missions du poste
Nous cherchons un Data Engineer Snowflake/ Azure Senior pour rejoindre l'équipe Data Factory d'un grand leader français, situé en Île-de-France. Ce rôle implique la responsabilité de nouveaux développements de pipelines de données, en adhérant aux normes, meilleures pratiques, et principes CI/CD complets.
Responsabilités :
Création et modification d'objets de base de données Snowflake (bases de données, tables, vues, etc.).
Développement de procédures stockées Snowflake avec SQL & Javascript.
Mise en œuvre de l'environnement de développement vers l'environnement de production, en suivant le processus CI/CD.
Garantir un haut niveau de service, incluant la définition, la construction, l'exécution, et la gestion des incidents.
Garantir une qualité irréprochable des codes fournis par l'équipe de développement.
Le profil recherché
Diplôme d'ingénieur Bac+5 en informatique.
Plus de 5 ans d'expérience hors alternance et stage dans un poste similaire.
Une expérience dans le développement SQL et une maîtrise en technologies de plateformes de données (Snowflake, Azure SQL, MS SQL, etc.).
Expérience avec Azure DevOps et familiarité avec d'autres produits et technologies Azure (ADLS Gen2, Azure Data Factory, Azure Function, Azure Logicapp, Azure batch, ...).
Compétences en programmation Javascript appréciées mais non obligatoires.
Anglais courant indispensable pour collaboration avec des équipes internationales.
Connaissance des méthodologies Agile et outils tels que JIRA.
Qualités attendues :
Autonomie et confort dans le soutien aux besoins en données de plusieurs équipes.
Capacité à opérer techniquement dès le premier jour et à soutenir le développement de profils juniors.
Excellentes compétences en communication et en formalisation.
Bienvenue chez NODYA Group
Chez NODYA Group, votre parcours vers l'excellence et l'innovation commence dès le premier jour. Fondé sur la conviction que la Data et l'IA sont les catalyseurs de la transformation numérique, NODYA Group s'est positionné comme un pionnier, anticipant l'essor de l'IA dès 2016 et intégrant cette vision à l'ensemble de nos stratégies. En tant que membre de notre équipe, vous contribuerez à guider nos clients-partenaires à chaque étape de leur évolution dans l'univers de la Data et de l'IA pour créer un impact business positif et durable.
Votre avenir chez nous est non seulement une promesse de croissance mais aussi une opportunité de faire partie d'une vision qui valorise la créativité, le professionnalisme, l'équilibre et l'encouragement, des valeurs qui façonnent l'expérience que nous offrons à nos clients et collaborateurs.
Chez NODYA Group, vous serez au centre d'une culture d'entreprise dynamique et visionnaire, où chaque projet est une aventure, chaque défi une chance d'innover. Vous serez entouré par une équipe dirigeante qui valorise l'excellence, la proximité et le développement durable des compétences, dans un environnement où le mentoring et le développement personnel sont clés.
Si vous êtes prêt à exploiter le plein potentiel de la Data et de l'IA, à transformer les entreprises et à développer une carrière sans précédent, NODYA Group est l'endroit où vous devez être. Rejoignez-nous et ensemble, façonnons l'avenir de la technologie et du business. Parlons de votre projet, parlons de votre carrière – votre aventure commence maintenant chez NODYA Group.
Show more
Show less","{'ProgLanguage': ['Java', 'R', 'JavaScript'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA'], 'Other': ['DevOps', 'CI/CD'], 'FrSoftSkills': ['Communication', 'Créativité', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': ['Junior', 'Senior'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Analyst (H/F),Technology & Strategy,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-analyst-h-f-at-technology-strategy-3909691608?position=10&pageNum=27&refId=Uq1oQzOiE7s5uwPCeaspnQ%3D%3D&trackingId=fGc7VNvq1a5kgMCd9yukZQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Découvrez Novencia
:
Expert en Data et Intelligence Artificielle, nous aidons nos clients à exploiter et à valoriser leurs données sous toutes ses formes en les accompagnant sur des projets de Data Analyse, Data Gourvernance, Data Architecture, Data Science, et Data Engineering…
Vous avez une solide expérience de minimum 3 ans dans l'analyse des données et vous êtes à la recherche de nouveaux défis ? N'hésitez plus, la suite est pour vous !
Type de contrat : CDI
Démarrage : Dès que possible
Lieu : Lyon
Description du poste
:
Nous recherchons un Data Analyst expérimenté pour rejoindre notre équipe dynamique en région lyonnaise. Le candidat idéal devrait avoir une solide expérience dans l’analyse de données, ainsi que des compétences techniques en SQL, Python et Power BI (ou tout autre outil de visualisation de données).
En qualité de Data Anlayst (H/F)*, vous serez en charge de :
Collecter, nettoyer et analyser des données commerciales provenant de différentes sources.
Créer des rapports et des tableaux de bord pertinents pour les équipes opérationnelles et de gestion.
Identifier les tendances, les opportunités et les domaines d’amélioration à partir des données.
Collaborer avec les équipes métier pour comprendre leurs besoins et fournir des analyses approfondies.
Pré-requis :
Expérience professionnelle de minimum 3 ans en tant que Data Analyst.
Maîtrise des langages SQL et Python pour l’extraction, la transformation et l’analyse des données.
Connaissance approfondie de Power BI ou d’autres outils de visualisation de données.
Capacité à travailler de manière autonome et à communiquer efficacement avec les parties prenantes.
Vous disposez des savoir-être suivants :
Ambidextre : Agile à droite, Data à gauche
Coordinateur.trice : manœuvre un projet et une équipe
Fin.e connaisseur.euse : alimente, reporte et conçoit des entrepôts de données
Diplomate : résout les inévitables complexités de la réalisation d’un projet d’entreprise
Négociateur.trice : met du tact dans chaque discussion
Tenace : un seul objectif, la satisfaction client
Notre objectif commun est de co-construire votre carrière en fonction de vos aspirations et de vos compétences.
Contactez-moi en message privé ou par mail à s.ziki@technologyandstrategy.com !
Let's make it possible #together
*Nos postes sont ouverts aux personnes bénéficiant d’une Reconnaissance de la Qualité de Travailleur Handicapé (RQTH). T&S Groupe encourage la diversité et l’égalité sur le lieu de travail. Tous les candidats qualifiés H/F/* sont pris en considération pour un emploi sur un même pied d'égalité.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer / Tech Lead,Eulidia,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-tech-lead-at-eulidia-3915040845?position=1&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=3o0LKiJc0VGmZOodpXCl0A%3D%3D&trk=public_jobs_jserp-result_search-card,"A propos d’Eulidia :
Eulidia est un cabinet de conseil pure-player Data niché au sein du 1er arrondissement de Paris, France. Depuis 2008, nous utilisons les leviers de la Data (architecture, Data ingéniérie, Data management & gouvernance, Business Intelligence) et de la Data Science (GenAI & Machine learning) pour aider nos clients à se réinventer, innover et rendre leur business model plus performant.
Eulidia recherche un(e) Data Engineer/Tech Lead.
Si vous êtes intéressé à l'idée de rejoindre une société de spécialiste, partenaire des éditeurs Data & Cloud les plus innovants (Snowflake, Google, Dataiku, Azure, Talend-Qlik ...) et par le management de projets Data complexes, cette opportunité est très probablement votre prochain challenge professionnel et l'occasion d'intégrer un ecosystème Data en pleine croissance.
Vos Activités :
Accompagner sur le choix des technologies pour concevoir une plateforme de données répondant aux différents besoins
Concevoir et maintenir des pipelines de données résilients et scalable (Batch et/ou streaming)
Collecter, nettoyer et transformer la donnée en provenance de diverses sources
Développer et implémenter des processus de data quality et gouvernance
Travailler avec des data scientists et analysts afin d’implémenter des solutions data-driven
Collaborer avec les ingénieurs et autres collaborateurs pour assurer la livraison des projets
Accompagner la montée en compétence technique de vos collaborateurs
Découvrez un exemple de projet auquel vous pourriez contribuer sur notre site internet :
https://www.eulidia.com/data-stories-12.html
Vous correspondez à notre poste si :
Vous maitrisez les bases de données relationnelles et analytiques
Le data warehousing et les concepts de modélisation n’ont aucun secret pour vous
Vous êtes familier voire certifié sur des plateformes cloud telles que AWS, Azure ou GCP
Vous avez Une bonne compréhension des besoins de sécurité des données
Vous avez Une connaissance des technologies de big data tels que Spark et Hive.
Vos Soft skills :
Une excellente capacité à résoudre des problèmes
Une bonne communication et travail d’équipe
Une attention au détail particulière et de la rigueur
De la curiosité et du partage de connaissances
Ce que nous offrons :
De la formation en continue via notre organisme de formation interne (certifications, 5 à 7, conférence avec les éditeurs partenaires, squads interne)
Un accompagnement en mission et un suivi managérial de proximité basé sur la performance
Les avantages de nos partenariats avec Google Cloud et Snowflake
Un espace de travail hybride, convivial et confortable au cœur de Paris (baby foot, ping pong, café, fruits, jeux de société …)
Un package compétitif et une prime annuelle qui récompense la performance globale de l’entreprise
Une excellente assurance et couverture santé pour prendre soin de nos employés
Un séminaire d’entreprise annuel et de nombreux évenements afterworks
Show more
Show less","{'ProgLanguage': ['Scala', 'R', 'Go', 'HTML'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Machine Learning', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Hybride'], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer - F/H,Niji,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-niji-3848294394?position=2&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=kQVKXOCWIKbTTV931R6N9g%3D%3D&trk=public_jobs_jserp-result_search-card,"Nous sommes plus qu’un simple cabinet de conseil, qu'une agence de design et qu'une société de mise en œuvre technologique. Nous sommes le partenaire de la transformation digitale de nos clients grands-comptes et ETI, que nous accompagnons de l’idée à la réalité.
Nous associons, dans une même chaîne de valeur, conseil en stratégie, design de service et design émotionnel, management et valorisation de la donnée, ingénierie et conseil technologique, réalisation logicielle et expertise en cybersécurité.
Notre singularité repose sur les talents pluriels de nos équipes, au service de la satisfaction et de la performance de nos clients.
Le Pôle Data De Niji C'est Avant Tout Une Équipe à Taille Humaine Et Pluridisciplinaire, Composée De Consultants Et Experts Qui Conseillent Et Appuient Nos Clients Sur Toutes Les Étapes Du Cycle Des Données
de la collecte à la valorisation dans des services innovants,
en passant par les architectures de stockage et de services.
Nos consultants sont basés en Ile-de-France et en régions (Nantes, Rennes, Lille, Lyon et Bordeaux).
Nos 3 directeurs : experts confirmés de la gouvernance des données, de la data science de l'IA et des méthodologies d'industrialisation de l'IA (MLOps, DataOps), recherchent de nouveaux talents tous complémentaires avec plusieurs niveaux de qualification et séniorité, qui travailleront en synergie avec la large palette de compétences de Niji en développement, communication, cybersécurité et en conseil.
Intégrer le Pôle Data de Niji c'est avoir l'assurance d'être accompagné dans sa progression et le développement rapide de ses compétences ,vous suivez un
parcours de formation
riche et diversifié, visant à vous faire rapidement monter en expertise et à vous certifier.
En tant que
Data Engineer
, vos principales missions seront les suivantes :
Concevoir, développer et maintenir une architecture de données robuste, évolutive et sécurisée, en tenant compte des besoins spécifiques des clients.
Participer activement à la rédaction de propositions commerciales, en contribuant aux aspects techniques (outils, make or buy …) et en fournissant des estimations de projet.
Gérer et optimiser les pipelines de données, en assurant la collecte, le stockage, le traitement et la mise à disposition des données de manière fiable et performante.
Assurer la qualité des données en mettant en place des contrôles de qualité, des tests et des processus de validation, conformément aux exigences des clients.
Encadrer les projets de bout en bout, en veillant à ce qu'ils soient livrés à temps, dans les limites du budget afin d’assurer la satisfaction des clients.
Assurer la documentation technique, les bonnes pratiques et les standards de développement au sein de l'équipe.
Profil recherché
Si Vous
Avez obtenu un diplôme en université, école de commerce ou équivalent type bac +5
Maîtrisez l'anglais à l'écrit comme à l'oral
Avez de solides connaissances en architecture et en modélisation des données
Maîtrisez des technologies et des outils liés au Big Data (Hadoop, Spark, Hive, etc.)
Maîtrisez les outils d’industrialisation des pipelines data tel que docker, kubernetes, Dataiku, Jenkins…
Avez une expérience avec les langages de programmation utilisés dans le domaine des données, tels que : Python,R, Scala, SQL, etc.
Avez une expérience dans la conception et la mise en œuvre de pipelines de données
Avez des compétences en gestion d'équipe et en leadership technique
Avez des capacités à rédiger des propositions commerciales convaincantes
Alors… Venez participer au dynamisme de notre site en rejoignant notre Team Data Lyonnaise !
L'aventure Niji
Process de recrutement : premier contact RH puis rencontre avec nos opérationnels.
Rejoindre l'expérience Niji c'est avoir l'assurance de participer à une aventure humaine dans un environnement de travail motivant, challengeant et innovant.
NijiU: notre plateforme de formation digital learning contenant près de 3 000 modules en accès libre.
Nos valeurs : Audace - Bienveillance - Performance – Talent.
Si ces mots vous parlent, venez faire la différence chez Niji !
En rejoignant Niji, vous intégrez une entreprise dont la politique RSE contribue à la promotion de la diversité et de l’égalité des chances, notamment pour les personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['Big Data', 'ML'], 'FrSoftSkills': ['Communication', 'Leadership'], 'EnSoftSkils': ['Communication', 'Leadership']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Python Software Engineer,Redslim,"Île-de-France, France",https://fr.linkedin.com/jobs/view/python-software-engineer-at-redslim-3749529490?position=3&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=zZI5Gfd%2BYp%2B0ns3BE0%2Fkxw%3D%3D&trk=public_jobs_jserp-result_search-card,"Salary Range:
EUR 50k-80k
About Role:
The engineer we are looking for will work in the platform developer team on all aspects of the software development cycle; design, develop, test and deploy. The work will be composed of developing a low & no-code solution that enables users to configure and run batch data operations for the purpose of data reporting & visualisation. The solution is deployed and hosted in Azure and services used include Azure Web App, Storage Accounts, Azure Data Factory and Databricks. The main programming languages used by the team are either Python or C#.
About Team:
The central mission of the Redslim Product Team revolves around developing tools and services that support Redslim's data management services. The team's collaboration extends to various business domains, both internally and externally, helping develop products that support operational activities, activation, and client reporting solutions.
Key Responsibilities:
Collaborate with cross-functional teams to understand and define data harmonisation & transformation requirements.
Design, develop, and maintain data processing libraries, pipelines using Databricks and Python.
Optimize data processes for performance and scalability.
Ensure data quality and reliability by implementing data validation and testing procedures.
Requirements:
4+ years of software development experience.
Fluent English as a working language.
Extensible experience with Databricks and Python for data engineering.
Professional experience of deploying and testing software solutions in a cloud environment (ideally Azure).
Professional experience containerising software solutions to work at scale in the cloud.
Database experience including analytical SQL queries and OLAP databases.
Nice to Have:
Domain experience involving market research and/or retail data management.
Experience with modern software development methodologies, for example Test Driven Development, Agile Scrum and Lean Software Development.
Experience using Large Language Models to enhance user experience.
Front end development experience, preferably using React.
What We Offer:
Competitive salary and benefits package.
Opportunities for professional development and growth.
A supportive and collaborative work environment.
The chance to work on meaningful projects with real-world impact.
A remote or office based working environment with irregular face to face team meetings throughout the year.
Show more
Show less","{'ProgLanguage': ['Python', 'C#', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['50k'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Senior Data Engineer - Snowflake,FRG Technology Consulting,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-snowflake-at-frg-technology-consulting-3904054143?position=4&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=kkgSmI6CEzxxiNjUPdZBcA%3D%3D&trk=public_jobs_jserp-result_search-card,"Mon client, un End User sur Paris, est à la recherche d'un(e) Data Ops, pour intervenir sur l'ensemble de la chaîne décisionnelle au sein du pôle data.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer H/F,Groupe Havana,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-groupe-havana-3913990947?position=5&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=4NVBmhdECDN4o9JVXvp%2F7A%3D%3D&trk=public_jobs_jserp-result_search-card,"Groupe Havana : Créé en 2011, nous incarnons la transformation digitale en misant sur l'innovation, la performance et le bien-être au travail.
Notre Identité
200 experts déployés à travers la France.
Acteurs clés dans les domaines de la DATA, du Cloud et de l'intelligence artificielle.
Nous accompagnons nos clients dans la transformation digitale de leur SI
Nous recherchons un(e) data ingénieur(e) qualifié(e) pour nous aider à concevoir, construire et gérer nos systèmes de données.
Missions
Migration des données du SI entreprise vers la plateforme de données GCP
Participer à l'industrialisation des cas d'usage métiers du groupe
Assurer la qualité, la fiabilité et l'accessibilité des données.
Collaborer avec les équipes de data science pour comprendre les besoins en données et mettre en place les solutions appropriées.
Assurer la maintenance et l'optimisation des systèmes de données existants.
Compétences Attendues
Expérience en ingénierie de données, en particulier avec les technologies de big data
Connaissances GCP Big Query
Connaissances des langages de programmation tels que Python
Expérience avec les bases de données SQL et NoSQL
Connaissances des technologies de conteneurisation (Docker, Kube)
Compétences des technologies de CI/CD et IaC (Gitlab, Terraform)
Capacité à travailler en équipe et à communiquer efficacement
Compétences interpersonnelles
Avoir une capacité à travailler de manière autonome et à gérer efficacement plusieurs tâches simultanément
Intéressé(e) ?
Voici notre processus de recrutement après envoi de votre CV :
Un 1er échange téléphonique avec notre responsable recrutement Vincent, d'environ 10 minutes
Un 2ème échange téléphonique avec l'équipe commerciale, Christelle, Yves ou Lucas
Un entretien physique ou en visio avec le client final et le business manager en charge de vous accompagner
Ce poste n'est pas ouvert à l'alternance ou aux stages !
Poste ouvert aux personnes en situation d'handicap !
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['Big Query'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': [], 'Other': ['Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer senior Spark / Scala,Sibylone,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-senior-spark-scala-at-sibylone-3909127442?position=6&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=zWRvA8cUYHO5VrPJ4rS%2BAA%3D%3D&trk=public_jobs_jserp-result_search-card,"SIBYLONE
, société de conseil spécialisée dans les systèmes d’information de synthèse et de pilotage, aide ses clients à tirer toute la valeur de leur patrimoine de données, levier stratégique majeur de développement et de rentabilité.
Notre ambition
: rendre les différents acteurs de l’entreprise autonomes dans l’exploitation des données, libérer les usages Métier, pour qu’ils soient en mesure de relever les défis de performance, de couverture de risque, de financement, de conquête client, de RSE… qui s’imposent à eux.
Spécialistes reconnus,
nos consultants s’appuient pour cela sur une connaissance approfondie de l’activité business de nos clients, en lien avec nos trois piliers que sont le Métier, la Data et le Projet.
SIBYLONE
emploie environ 250 salariés et réalise un CA de 30m€ dans la prestation de services auprès de grandes entreprises (8 grands comptes représentant 80% du CA). SIBYLONE est une filiale du Groupe Smart 4 Engineering créé en 2020. Le groupe s’est constitué en procédant à l’acquisition de 12 sociétés en France (dont SIBYLONE), Italie, et en Espagne dans le domaine de l’ingénierie. Le groupe est détenu par Dzeta Conseil, acteur familial de l’investissement. Avec nos 3,000 ingénieurs / consultants hautement qualifiés, le Groupe offre ses services dans les domaines très porteurs du Digital, de la Data, de l’Intelligence Artificielle, de la Cybersécurité, du Cloud et des Logiciels.
Dans le cadre du développement de notre activité Data, nous recherchons
plusieurs Data Engineer
à l'aise avec spark et scala!
Le Data Engineer participe à la conception, la construction, le déploiement et le maintien en production d’architectures Big Data, ces dernières ayant pour objectif de permettre tant l’évolution que l’optimisation du système d’information décisionnel existant en permettant de nouveaux usages Analytics et IA.
Vous intégrerez une équipe projet Big Data dont l’objectif premier est de conduire des projets ayant traits à des problématiques d’architecture et de conception dans un contexte Big Data & Cloud.
Vos missions
Analyser, comprendre et cadrer une architecture permettant de répondre aux besoins métiers des clients
Concevoir et mettre en place des plateformes Data en tenant compte des contraintes tant techniques que fonctionnelles
Intervenir sur la conception et le déploiement d’environnements « clusterisés » (Hadoop sur des distributions telles que Cloudera ou Hortonworks) ou Cloud public
Développement de pipelines d’ingestion et de préparation
Gestion du stockage de données (systèmes de fichiers comme HDFS, bases SQL ou NoSQL)
Alimentation d’entrepôts de données (Hive, Impala, …)
Développer des applications d’exploration et de manipulation de données (SPARK / pySpark, Scala) afin d’alimenter les flux sortants, les reporting et d’exposer les données
Evoluer sur l’ordonnancement des traitements de données (Oozie, Bash / Shell)
Assurer le maintien en conditions opérationnelles des plateformes produites
Etablir, formaliser, et promouvoir les best practices
Pourquoi pas vous ?
Profil recherché :
De formation supérieure ingénieur en Informatique, vous justifiez d’une première expérience réussie en data engineering acquise dans un contexte projet au sein d’une start-up, d’un pure player, ou d’une ESN.
Vous disposez d’une bonne maitrise des langages propres aux environnements Big Data tels que :
Hadoop et ses distributions
Les solutions Cloud (Azure, AWS, GPC)
Spark, Scala, Python, Unix, SQL.
Une connaissance de : Docker, ELK, Kubernetes, Cassandra, Kafka, … serait un plus, de même que des fondamentaux DevOps (CI / CD).
Vous avez déjà évolué dans un contexte projet agile ou scrum et faites preuve de flexibilité, d’adaptabilité et savez être force de proposition.
Au-delà de vos compétences techniques, vous êtes curieux, autonome, organisé, doté d’un bon sens relationnel et d’un esprit de synthèse.
Les PLUS Sibylone !
Evoluer au sein d’une société qui exige le meilleur de ses collaborateurs tout en cultivant la cohésion et l’esprit d’équipe !
S’engager dans une politique RSE exigeante : labellisation Ecovadis
Avoir un Partenariat EcoTree France : 1 recrutement = 1 arbre planté !
Contribuer activement au bien-être de ses collaborateurs : participation aux frais d’abonnements activités ou achat 2 roues
Avoir de nombreux moments de convivialités : séminaires, afterworks, conférences et petits déjeuners, sports en groupe via l’application United Heroes
Donner une offre de formation innovante et à la pointe des nouvelles technologies
Accord de télétravail en vigueur
Vous vous reconnaissez dans la description du poste ?
Vous souhaitez travailler dans un environnement stimulant et dynamique ?
Vous souhaitez rejoindre une société ambitieuse ?
Vous souhaitez comprendre l’origine de Sibylone ?
Venez-nous rencontrer : L'équipe TA sera ravie d’échanger avec vous !
Ce poste est ouvert aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Bash'], 'DataBase': ['SQL', 'NoSQL', 'Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud', 'CI / CD'], 'FrSoftSkills': ['Adaptabilité', 'Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Ingénieur Data confirmé F/H,Mobile Tech People,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-confirm%C3%A9-f-h-at-mobile-tech-people-3887689223?position=7&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=hPIySwJlTshaog2MYOJd3Q%3D%3D&trk=public_jobs_jserp-result_search-card,"Missions :
Participer à la prise en charge du développement et de la maintenance de l infrastructure adaptée à l analyse de données (Développement de nouvelles fonctionnalités via des développements spécifiques ou ETL),
Intervenir en mode support (Suivre le développement et apporter du support aux autres développeurs sur les solutions logicielles),
Gestion de base de données SQL sur Azure
Documenter les réalisations effectuées via des manuels d exploitation,
Créations de script python pour récupérer les données depuis le Web Scraping
Faire les revus de codes,
Intervenir sur la mise à jour des langages et technologie,
Cultiver des relations opérationnelles avec les équipes métiers.
Formation d'ingénieur data juniors.
Environnement technique :
Java SE, Python, Spark, Hadoop, Hive, Scala, NoSQL, SQL, PSQL, Oracle Database, Dataiku, Informatica, Teradata, Azure, Datbricks, Git
Requirements
Description du profil
Nous cherchons une personne :
Qui a idéalement 4 ans min d expériences en tant que Data Engineer,
Ayant de solides compétences en programmation et bases de données sur les technologies ci dessus ainsi qu en Mathématiques appliquées pour la construction des algorithmes,
Aimant le travail d équipe,
Qui cultive sa curiosité et se tient informé des évolutions technologiques,
Autonome, autodidacte, proactive.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Junior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
DATA ENGINEER - Azure H/F,Squaar,"Écouflant, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-azure-h-f-at-squaar-3896400467?position=8&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=XwQWwVcKXaNeQhtCZ50OMg%3D%3D&trk=public_jobs_jserp-result_search-card,"Nous recherchons pour notre client, éditeur de logiciels spécialisés, un DATA ENGINEER - Azure H/F dans le cadre d'une mission Freelance de 6 mois renouvelable plusieurs fois.
Le Poste
Lieu : ANGERS (49) ,
TJM : 500-550€ (frais inclus) / Jour,
Rythme : 2j sur site / semaine pendant le 1er mois?
Nous recherchons pour notre client, éditeur de logiciels spécialisés, un DATA ENGINEER - Azure H/F dans le cadre d'une mission Freelance de 6 mois renouvelable plusieurs fois.
Vos Missions
Au sein d'une équipe informatique d'une dizaine de personnes et rattaché au CTO, vous intervenez dans une cadre d'un projet de refonte complète de la plateforme du client.
Vos Missions Sont Les Suivantes
Développement et maintenance de pipelines de données sur Azure.
Mise en place d'architectures de données efficaces.
Transformation de données en temps réel
Validation des données selon les règles métier et modélisation des structures.
Gestion de gros volumes de données clients.
Environnement technique : Écosystème Azure (Azure Data Factory, SQL, Azure Data Lake Store, Spark, Hive, Airflow, Python, Scala)
Vous
De formation au minimum Bac+4/5 en Informatique, vous justifiez d'au moins 3 années d'expérience en Data Engineering autour de l'écosystème Azure. (impératif).
Attention, vous serez LE référent sur ces sujets, votre capacité à travailler de façon autonome et à trouver l'équilibre entre qualité et efficacité tout en proposant des solutions fiables et durables est donc également impérative.
Vous êtes intéressé par le fait de répondre à des problématiques techniques notamment, de qualité, de performance et de sécurité.
Process
Après une visio avec un membre de notre équipe, vous rencontrez :
Le CTO et le Responsable Infrastructure en visio
Décision sous quelques jours
Envoyez-nous votre CV et si votre profil correspond vous serez contacté par un membre de l'équipe dans les 24h.
Au plaisir d'évoquer votre projet,
Squaar
Profil
De formation au minimum Bac+4/5 en Informatique, vous justifiez d'au moins 3 années d'expérience en Data Engineering autour de l'écosystème Azure. (impératif).
Attention, vous serez LE référent sur ces sujets, votre capacité à travailler de façon autonome et à trouver l'équilibre entre qualité et efficacité tout en proposant des solutions fiables et durables est donc également impérative.
Vous êtes intéressé par le fait de répondre à des problématiques techniques notamment, de qualité, de performance et de sécurité.
Environnement technique : Écosystème Azure (Azure Data Factory, SQL, Azure Data Lake Store, Spark, Hive, Airflow, Python, Scala)
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer H/F,Externatic,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-externatic-3911263499?position=9&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=x2FW23%2F4UjVmsyr5eUj%2F3g%3D%3D&trk=public_jobs_jserp-result_search-card,"Présentation de la société
Cabinet de recrutement Tech, la mission d’Externatic est de faciliter la rencontre entre candidats et entreprises finales (hors donc 0% d'ESN). Nous mettons à votre disposition notre réseau et notre connaissance du marché de la Tech (étude des salaires, tendances).
Notre moteur : vous accompagner sur du long terme pour trouver l’opportunité en CDI, qui correspond à votre projet professionnel, et surtout vous proposer un accès privilégié à des opportunités cachées au sein de pépites (startup / éditeur / DSI / PME).
Externatic en bref :
Plus de 13 ans de professionnalisme
+700 postes ouverts HORS ESN
33 consultants
Plus de 400 clients tous hors ESN : DSI, éditeurs, ETI/PME, Centre R&D, Startup/scaleup, organismes publiques et para-publiques, ...
Plus de 370 candidats accompagnés par an
Mission
Moi c'est Hadrien, Consultant chez Externatic et j’ai une offre très sympa à te partager, surtout si tu as envie de rejoindre une aventure ambitieuse à ses débuts !
Voici quelques infos ci-dessous, et je serais ravi d’échanger ensemble plus de détails !
L'entreprise
Le marché du recrutement est au début d'une grande révolution avec le développement de l'IA. Cependant, beaucoup de recruteurs sont encore mal outillés, ou ne sont pas familiers avec ces nouvelles technologies.
C'est dans ce contexte que cette jeune startup créée il y a un an est en train de créer une solution SaaS intégrant de l'IA dédiée aux acteurs du recrutement en France puis en Europe !
En quelques mois, ils ont déjà sorti un premier MVP qui est déjà utilisé chez une 30aine de clients !
La startup réunit une équipe solide de 6 personnes et un investisseur de renom qui soutient le projet.
Le poste
Tu intègres l'équipe Tech qui est composée de 3 développeurs actuellement et qui recherche son premier Data Engineer !
C'est un rôle crucial et clé qui va permettre d'aller encore plus loin au niveau de la roadmap et proposer une solution révolutionnaire sur le marché.
Ton rôle sera de mettre en place et maintenir une infrastructure Data performante et scalable, qui sera capable de gérer une montée en puissance au niveau des volumes de données à capter, structurer et traiter.
Tes enjeux seront par exemple de :
Concevoir, développer et optimiser le Data Model et l'architecture Data
Mettre en place et maintenir les pipelines de données
Optimiser les performances du système (vitesse de traitement des données, capacité de stockage, scalabilité...)
Assurer la sécurité et la confidentialité des données
Collaborer étroitement avec les développeurs, le fondateur et le CSM pour développer, tester et déployer de nouvelles features
Environnement technique : Python, PostgreSQL, Cloud et autres technos de la stack data à définir
Profil
Tes atouts
Tu as une formation Data et une expérience d'au moins 3 ans autour de sujets Data Engineering, où tu as travaillé sur des projets BUILD autour des infrastructures data (cloud, pipelines, data platforms, ...)
Tu as une solide expertise en Python, SQL, et une bonnes connaissances des technos que l'on retrouve sur les stacks data modernes (par exemple AWS, GCP, Snowflake, Databricks, DBT, Airflow, ...)
Et surtout, tu as envie de rejoindre une aventure startup à ses débuts et t'impliquer dans un projet passionnant !
Avantages
Conditions de travail
Localisation : Bureaux à Nantes et Bordeaux ou full remote
Rémunération fixe : entre 45 et 50K
BSPCE
Remote friendly
Le process (rapide et efficace !)
RDV avec Hadrien d’Externatic
Entretien avec CTO et Founder
Rencontre équipe
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': ['PostgreSQL', 'Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': ['CDI'], 'Salary': ['13'], 'Level': [], 'Experience': ['a', 'n', 's', '13', '13', '13']}"
Data Engineer | LLM/Python,Jus Mundi,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-llm-python-at-jus-mundi-3901457116?position=10&pageNum=30&refId=xJJSxqDLDzWu7D%2BmiW7m%2BA%3D%3D&trackingId=ZaL7tqq8Pl09fbrpRxRK5g%3D%3D&trk=public_jobs_jserp-result_search-card,"*Only applicants who send their CV on Welcome to the Jungle will be considered for this position.*
⭐Who We are⭐
Contributing to building the rule of law internationally is everyone’s responsibility. Jus Mundi facilitates and democratizes access to global legal information with unprecedented efficiency, thanks to a search engine that combines international legal expertise with artificial intelligence.
Based in Paris and NYC, 90% of Jus Mundi’s turnover is 40% from the US and 30% from the UK.
Clients include big law firms, such as DLA Piper, Freshfields, Dentons, and legal departments of multinationals such as governments including Japan and the UK, and universities such as the Sorbonne, and Cambridge.
Our team: Vision is nothing without a team to carry it!
Our team has friendly people from diverse backgrounds who are committed and strive to be the best!
70+ Team Members, with 30+ nationalities from different backgrounds, ambitious & innovative talents in a journey to disrupt and reinvent the entire legal industry.
Be Part of This Exciting Journey. Join Our International Team.
You will live your best life while working at Jus Mundi. We work hard, and we play hard! The impact of our work is meaningful. If you would like to have a career in which you are making a real impact, join us.
ℹ️ Job Description:
Today, we are looking for an engineer to join our new Team to work specifically around LLMs and ship more organic experiences to our users.
We’re leveraging LLM models to revolutionize legal research and build valuable products for our users. What we’re building in the short term? We’re developing interactive assistants and agents that leverage our database and perform tasks for our users and partners. But we want to do much more!
The legal industry is being revolutionized by LLMs. Lawyers are eagerly adopting cutting-edge technologies, and we are riding this wave! The moment is now.
We mainly follow a Kanban/Lean paradigm cherry-picking Events or Artifacts from other methods to create our own practice, matching our own organization.
We have an Engineering Culture. And have some practical principles
Do what you say, Say what you do.
Communication builds Trust. Trust improves Performance.
Try, fail and, learn. Iterate until success.
Leave your ego at the door.
Big Ideas, Pragmatical Implementation.
Our technical stack:
Main: Python, Go, Nuxt.js, Vue.js, and Node.js
Databases: PostgreSQL, Elasticsearch, Neo4j and Redis
Legacy (being replaced): Symfony with JQuery
Development and CI/CD: Docker, Git and Gitlab
⚡ Your Missions:
What you’ll do:
Create, test, maintain, and consume internal & external APIs (mainly Python, a little bit of Go)
Write conception Proposals, RFC and analysis
Help to create Proof Of Concept, Minimum Viable Product
Implement new features and fix bugs
Work on LLM application (llama-3, GPT-4, Mixtral, etc…)
Do prompt engineering, few short learning and fine-tuning
💼 Preferred Experience & Skills:
Who you are:
You are passionate about your craft and can communicate it
You are trustworthy
You have 3+ years in your field and have worked on part of our Tech Stack
You thrive in a fast-paced environment
You are climbing the slope of enlightenment (Dunning–Kruger effect)
Having worked on at least one LLM project
Knowing how to leverage LLM
🚀 Company Perks and Benefits:
😍 Working for a fast-growing global legal tech offering a disruptive product that is revolutionizing the way lawyers around the world interconnect and conduct legal research,
💻 Hybrid working organization, mix between remote and on-site,
💰Competitive salary & equity
🏖 5 weeks of vacation
🍼 Paid parental leave (under specific conditions)
🩺 A great complementary private health insurance (paid 100% for the employee and his children by the company)
🚊50% of public transportation reimbursed.
🍴Personal credit card to buy lunches during the week (
Swile
)
😍 Every quarter we organize a company-wide summit to
🌍 Travel (work abroad) policy: 8 weeks per year, you can live and work from where you want across the globe,
✈️ Relocation Package (to France)
Why us, why now?
We are structuring our organization to scale. There is a lot to do with high levels of ownership and freedom. Building and creating the practices and processes the Engineering will follow.
Confidence can sometimes hold us back from applying for a job. But we’ll let you in on a secret: there’s no such thing as a ‘perfect’ candidate. So however you identify and whatever background you bring with you, please apply if this is a role that would make you excited to come to work every day.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'Neo4j', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['PostgreSQL'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': [], 'Other': ['CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Organization']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['5'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Ingénieur/e,AXA en France,"Nanterre, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-ing%C3%A9nieur-e-at-axa-en-france-3919473325?position=1&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=9eIjBWMHrblYtuA%2Fvy7XHQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Votre rôle et vos missions
Créer des outils
Etablir des cahiers des charges avec les utilisateurs, en comprenant leurs besoins pour définir les solutions les plus adaptées
Lotir les développements en ayant le souci de la valeur ajoutée du 1er lot et de la cohérence des livraisons
Développer les interfaces des applications
Mettre en place des pipelines de traitement de données (nettoyage, transformation) et des pipelines de déploiement des applications sur le cloud
Assurer et garantir la production d'un code de qualité et de sa documentation technique
Etablir des plans de tests
Faire la maintenance des outils
Assurer la mise à jour régulière des outils et documents techniques
Développer les usages des modèles de données
Être référent pour les actuaires produits vis-à-vis de la DSI
Porter et suivre techniquement les demandes de développements de l’Actuariat IARD Entreprises auprès de la DSI
Prendre de la hauteur sur les problématiques techniques afin d'orienter l'équipe et anticiper les blocages et risques dans les phases de développement
Généraliser les bonnes pratiques d'industrialisation sur les différents traitements, en étant support pédagogue auprès des actuaires
Votre profil
Techniques
Savoir coder en Python, R, SAS
Savoir planifier un projet informatique
Connaître Git
Comprendre le fonctionnement de l’architecture d’un cloud
Connaître le domaine de l’Actuariat IARD, serait un +
Relationnelles
Savoir être à l’écoute des interlocuteurs nombreux et pluridisciplinaires (Actuariat, Directions Techniques, Informatique, Souscriptions, …)
Faire preuve de pragmatisme et de souplesse d’esprit pour imaginer des solutions permettant de concilier contraintes techniques et commerciales
Esprit de rigueur et de synthèse, sachant démontrer son assertivité et sa curiosité
Pourquoi nous rejoindre ?
Nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs ! Les avantages que nous proposons à nos salariés sont nombreux.
Nous choisir, c’est bénéficier par exemple
D’un package de rémunération complet comprenant un salaire fixe, un complément de rémunération variable, des primes, de la participation et de l’intéressement, la possibilité d’acquérir des actions AXA, ou encore des solutions d’épargne avantageuses ;
D’un cadre de travail flexible jusqu’à 3 jours de télétravail possible par semaine, des tickets restaurant pour les jours télétravaillés ou encore une participation à l’achat d’un écran ou fauteuil ergonomique ;
D’une politique visant à concilier vie personnelle et vie professionnelle avec 28 jours de congés payés, entre 14 et 16 RTT selon les années, des formules de travail à temps partiel ou encore des jours d’absence rémunérées pour la rentrée scolaire ou un déménagement par exemple ;
De la possibilité de s’engager pour une cause qui vous tient à cœur grâce à nos associations telles que AXA Atout Cœur, AXA Compétences Solidaires ou encore AXA Prévention ;
Et bien plus encore ! Perspectives de développement des compétences et de carrières immenses, CE, conciergerie, offres privilèges, soutien en cas d’épreuve personnelle…On s’arrête là, la liste est longue
Votre environnement de travail
Notre raison d’être chez AXA ? Chaque jour, nous agissons ensemble pour le progrès humain en protégeant ce qui compte. Une mission qui donne le sourire et envie de se lever le matin !
Un des leaders mondiaux de l’assurance dans la protection des biens, des personnes et des actifs, AXA c’est 145 000 collaborateurs et contributeurs qui s’engagent au quotidien pour nos clients, c’est 51 pays dans lesquels nous distribuons nos produits et services et plus de 90 millions de client qui nous font confiance dans le monde. Employeur citoyen et responsable, AXA s’engage au quotidien pour des causes sociétales & environnementales. Nous menons une politique inclusive engagée pour reconnaître et valoriser les différences individuelles. Ces ambitions vous parlent ? Alors venez changer le monde avec nous !
Victime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['3', '3'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Analyst - Lyon (F/H),Novencia Group,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-analyst-lyon-f-h-at-novencia-group-3842056375?position=2&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=pcb3zG3EL0bwAd9WUF3fkw%3D%3D&trk=public_jobs_jserp-result_search-card,"Carnet de route
Novencia accompagne ses clients dans leurs projets de transformation digitale, technologique et data.
Expert en Data et Intelligence Artificielle nous aidons nos clients à exploiter et valoriser leurs données sous toutes ses formes en les accompagnant sur des projets de Data Gouvernance, Data Architecture, Data Science, et Data Engineering.
Data Analyst
depuis au moins
3 ans,
tu es à la recherche de nouveaux défis. Boucle ta ceinture, la suite est pour toi !
Ton Rôle
Rattaché.e aux consultants de l’agence Lyonnaise, tu seras amené.e à travailler chez les clients finaux et/ou au sein de notre Datalab’.
En tant que Data Analyst, tu auras les missions suivantes :
T’immerger dans le contexte client ;
Mener des ateliers avec les équipes métier pour recueillir/ définir leurs besoins ;
Challenger leurs demandes ;
Participer au recueil et à la modélisation des données ;
Contrôler la fiabilité et la qualité des données ;
Structurer et exploiter les données grâce à des outils de Data visualisation ;
Analyser ces données et partager les résultats ;
Faire des recommandations au client.
Ton profil
De formation Bac+5 dans le domaine de la Data, tu as au moins 3 ans d’expérience sur un poste similaire.
Compétences techniques recherchées :
Expérience significative sur un outil de Data visualisation pour l’élaboration de rapports/tableaux de bord (Power BI/Tableau/QlikSense etc.)
Langage SQL (Python est un plus)
Modélisation de données
Connaissance des bases de données (e.g. Snowflake, Google BigQuery, SQL Server ou Oracle)
Utilisation d’un ETL (e.g. Azure Data Factory, SSIS, Informatica, Talend, Matillion)
Une expérience dans le fonctionnement d’une équipe agile.
Tu es passionné.e par la Data et souhaites évoluer au sein d’une communauté d’Experts de la Data, tu veux t’investir dans des projets data innovants au sein de notre Datalab’, tu aimes le travail en équipe mais sais être autonome sur tes sujets, tu es curieux.euse et force de proposition, tu es capable de chercher et trouver des solutions, tu as un bon niveau d’anglais.
Enfin, tu souhaites intégrer une agence à taille humaine où la bienveillance n’est pas qu’un mot marketing.
S’engager en faveur du handicap c’est garantir l’égalité des chances dès le recrutement. À compétences égales, nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle', 'SQL Server', 'Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer F/H,ARMOR GROUP,"La Chevrolière, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-armor-group-3828365820?position=3&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=l1gCM7cu9ZgGc%2FpuLpFSwA%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes nous ?
Centenaire (en 2022), ARMOR GROUP poursuit sa croissance grâce à une solide implantation, à la fois localement avec son site industriel situé à la Chevrolière en Loire Atlantique et mondialement avec ses différentes implantations industrielles, logistiques et commerciales.
Notre cœur de métier consiste à produire des films encrés, de la conception des encres à la fabrication de rubans, jusqu’à leur commercialisation.
Nous développons également de nouvelles activités industrielles innovantes : films collecteurs de courant, films photovoltaïques organiques et matériaux pour la fabrication additive (impression 3D).
Nous Sommes Une Entreprise Engagée Dans
La satisfaction de nos clients et la production de produits de qualité
L’investissement de notre outil de production pour améliorer la sécurité et les conditions de travail de nos salariés.
Les Plus D’ARMOR GROUP
Les conditions de travail, une politique sociale engagée (crèche d’entreprise, université interne pour les métiers de la production…)
Salaire fixe avec des primes liées au poste de travail, au transport, intéressement et participation, primes mensuelles etc.
Un comité d’entreprise et un restaurant d’entreprise sur site à tarif préférentiel.
Votre quotidien en nous rejoignant
Au sein de l'équipe Informatique Industrielle du Groupe, votre mission consiste à gérer et améliorer les systèmes de données industrielles du Pôle Industriel France et de ses filiales de production à l’international.
A Cet Effet, Vous Êtes En Charge De
Maintenir et faire évoluer l’architecture nécessaire à la valorisation de données,
Gérer et organiser les données industrielles et les flux de données entrants et sortants (SQL Server, SSIS, Historian),
Consolider les différentes sources de données On Premise dans notre Datalake Azure et Cubes de données,
Participer à la conception des reportings en lien avec les services métiers (Amélioration continu, process, qualité, production, …),
Optimiser et pérenniser les systèmes existants en temps réel,
Assurer l'accompagnement des utilisateurs clés.
Profil recherché
Doté d'un BAC+5 en Informatique dans le domaine de l'informatique décisionnelle (BI), vous avez une première expérience réussie en tant que Data Engineer.
Maîtrise des outils d’ETL tels que SSIS, Azure data factory.
Connaissance de la plateforme Azure Databricks est apprécié.
Connaissances générales en informatique (architecture, bases de données, méthodologies de développement…).
Connaissance du milieu industriel appréciée.
Qualités personnelles recherchées : capacité d’écoute et sens relationnel, force de proposition, pragmatisme, capacité de synthèse, goût du terrain, capacité de travail en équipe et à communiquer.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['5'], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
Data Analyst / Analytics Engineer (H/F),CleverConnect,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-analytics-engineer-h-f-at-cleverconnect-3904533153?position=4&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=JXSKMWdUYxr6hXO64nKJ6g%3D%3D&trk=public_jobs_jserp-result_search-card,"Description de l'entreprise
CleverConnect est une scale-up franco-allemande de la HR Tech en croissance fondée il y a 10 ans par des ingénieurs. Nous sommes présents en France et Allemagne. Depuis la fusion avec Talentry en 2022, nous partageons l’ambition de devenir le leader des software solutions du Talent Acquisition en Europe
Nous accompagnons actuellement plus de 10 millions de candidats par an à trouver le bon poste Pour cela, nous mettons en relation les recruteurs et les candidats avec des solutions digitales et des plateformes SaaS innovantes pour rendre le processus de recrutement plus efficace. Nos technologies permettent aux candidats de trouver des opportunités plus ciblées et de valoriser leur personnalité et motivation
Rejoignez notre équipe internationale de 200 collègues qui partagent la même culture et les mêmes valeurs, et qui sont pleinement engagés dans un projet à fort impact sociétal
Si vous voulez en savoir plus : www.cleverconnect.com
Description du poste
En tant que Data Analyst, quelles seront vos responsabilités ‍‍?
Collaborer avec les départements Product, Sales, Marketing et Communication pour comprendre les besoins métier et les traduire en solutions de données.
Implémenter et optimiser des modèles de données à l'aide de DBT et garantir la qualité et l'intégrité des données. Vous serez en charge de transformer et mettre en forme les données du datawarehouse en approche ELT.
Utiliser BigQuery et DBT pour analyser de grands ensembles de données et en tirer des insights exploitables.
Créer et maintenir des rapports et des tableaux de bord dans des outils de dataviz type Looker Studio, Superset, PowerBI ou Metabase. Ces tableaux de bords peuvent concerner les besoins internes (product, communication, marketing, sales, etc.) et externes (embedded dans nos solutions à destination des clients).
Participer à la conception des pipelines d’ingestion de données avec le Data Engineer.
Effectuer des analyses ad hoc et fournir des recommandations basées sur les données pour soutenir les décisions métier.
Qualifications
Qui êtes-vous ?
Vous avez au moins 5 années d'expérience en tant que Data Analyst dans un environnement similaire.
Techniquement et idéalement ,
Maîtrise avancée de SQL et expérience de travail avec des ensembles de données à grande échelle.
Expérience pratique avec BigQuery, DBT ou équivalents requis. Expériences Snowplow et Elasticsearch appréciées.
Familiarité avec les outils de visualisation de données tels que Looker Studio, Superset, PowerBI ou Metabase.
Être à l’aise dans le scripting python pour automatiser certaines transformations de données.
Avoir déjà manipulé un outil de Web Analytics tel que Google Analytics.
Expérience dans un environnement Agile et capacité à travailler en collaboration dans des équipes interfonctionnelles.
Q
ue trouverez-vous chez CleverConnect ?
Une équipe dirigeante accessible, bienveillante et à l’écoute
Des bureaux au cœur des villes et la possibilité de faire du télétravail
Des opportunités de formation, d’évolution et de mobilité en Europe
RTT, mutuelle, carte déjeuner, remboursement 50% transport, forfait mobilité durable
Notre processus de recrutement comprend:
Entretien initial avec un Responsable de l'Acquisition de Talents
Entretien avec le Manager (découverte/évaluations techniques)
Dernier entretien avec le Directeur IT ou CPTO.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '10', '10', '10']}"
Data Engineer F/H,CGI,Greater Clermont-Ferrand Area,https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-cgi-3883952151?position=5&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=hb9obuz0c%2BwRgicTmsXWsQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Description de poste
Big Data, Data Science, Data analyse, Data architecture, ... Ça n’a pas de secret pour vous?
Que vous commenciez votre carrière professionnelle ou que vous soyez spécialiste de l’une de ces disciplines, intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.
Fonctions et responsabilités
Alors venez rejoindre nos équipes de Data Engineer. Vous pouvez être amené à intervenir sur tout ou partie de ces missions :
Modélisation des Data Concepts d'un DataLake
Développement et maintenance des traitements d'intégration et de transformation de données
Intégration des développements dans la chaine CI/CD
Documentation technique et fonctionnelle
Rédaction de plan de tests
Réalisation de tests unitaires/qualifications
Au sein de la communauté Data, vous serez accompagné et vous pourrez échanger avec des collègues expérimentés et experts vous permettant de vous développer, de grandir et d’accomplir pleinement vos missions de conseil.
L’accompagnement managérial, la communauté Data et de nombreux évènements tout au long de l’année nous permettront de vous aider à atteindre vos objectifs dans un esprit de convivialité.
En rejoignant CGI, vous bénéficiez notamment d’une offre complète de formations (techniques, métiers, développement personnel,…), de flexibilité grâce à notre accord télétravail (jusqu’à 3 jours de télétravail par semaine), d’une politique de congés avantageuse (27 jours de congés payés, RTT, congés ancienneté et enfant malade,…) et d’un package d’avantages intéressant (régime d’achats d’actions, participation, CSE,...).
Qualités requises pour réussir dans ce rôle
Vous avez une formation Bac+3/5 en informatique, au minimum 1 an d’expérience et des aptitudes sur l’un ou plusieurs des domaines suivants :
Vous maitrisez :
Data Visualisation : Power BI, MicroStrategy, Tableau, Cognos…
Langages : SQL, Python, DAX, R…
Applications Cloud : Azure, Snowflake, Databricks, AWS...
Bases de données : Oracle, PostgreSQL, MySQL, Mongo db, Sybase…
ETL : Datastage, Talend, Informatica (PowerCenter) …
Vous avez/ Vous êtes :
Passionné du monde de la data
Curieux et appréciez le travail en équipe
CGI est un employeur inclusif et attentif aux candidatures des personnes en situation de handicap, à l’évolution de carrières des hommes et des femmes et au bien-être de nos salariés LGBT+. Dans un souci d’accessibilité et de clarté, le point médian n’est pas utilisé dans cette annonce. Tous les termes employés se réfèrent aussi bien au genre féminin que masculin.
Ensemble, en tant que propriétaires, mettons notre savoir-faire à l’œuvre.
La vie chez CGI est ancrée dans l’actionnariat, le travail d’équipe, le respect et un sentiment d’appartenance. Chez nous, vous pourrez exploiter votre plein potentiel parce que…
Nous vous invitons à devenir propriétaire dès le jour 1 alors que nous travaillons ensemble à faire de notre rêve une réalité. C’est pourquoi nous nous désignons comme associés de CGI, plutôt que comme employés. Nous tirons profit des retombées de notre succès collectif et contribuons activement à l’orientation et à la stratégie de notre entreprise.
Votre travail crée de la valeur. Vous élaborerez des solutions novatrices et développerez des relations durables avec vos collègues et clients, tout en ayant accès à des capacités mondiales pour concrétiser vos idées, saisir de nouvelles opportunités, et bénéficier d’une expertise sectorielle et technologique de pointe.
Vous ferez évoluer votre carrière en vous joignant à une entreprise bâtie pour croître et durer. Vous serez soutenus par des leaders qui ont votre santé et bien-être à cœur et qui vous permettront de saisir des occasions afin de parfaire vos compétences et élargir les horizons.
Joignez-vous à nous, l’une des plus importantes entreprises de conseil en technologie de l’information (TI) et en management au monde.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['MySQL', 'PostgreSQL', 'Oracle', 'Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['3'], 'Level': ['Bac+3'], 'Experience': ['a', 'n', 's']}"
Lead Data Engineer H/F,Neosoft,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/lead-data-engineer-h-f-at-neosoft-3879749134?position=6&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=KSuIL63EhPi3OfQhBM92XA%3D%3D&trk=public_jobs_jserp-result_search-card,"Tous nos postes sont ouverts au télétravail
Groupe indépendant de conseil en transformation digitale de près de 1800 collaborateurs, Néosoft s’est construit, depuis 2005, sur un modèle qui place l’excellence, le dépassement de soi et la RSE au cœur de sa stratégie.
En nous rejoignant, vous intégrez des communautés d’experts et de talents qui vous permettent de développer vos compétences et d’offrir à nos clients le meilleur accompagnement possible.
Notre savoir-faire s’articule autour de nos 6 domaines d’expertise :
Conseil & Agilité
Cybersécurité
Data
DevOps
Infrastructures & Cloud
Software Engineering
Nous recherchons pour intégrer notre
agence lilloise
un(e)
Lead Data Engineer H/F.
Nous aimerions vous voir rayonner au sein de notre communauté DATA (+100 collaborateurs) animée par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients à consolider un patrimoine Data responsable.
Vos missions :
Après une période d’intégration, en tant que
Lead Data Engineer
, voici à quoi ressembleront vos activités dans des contextes clients Retail ou Banque / Assurance / Finance :
Définir la stratégie d'ingénierie technique des données
Participer à la conception d'architectures de données robustes et évolutives
Optimiser les performances et la scalabilité
Exercer un véritable rôle de Leader technique
Votre profil :
Nous vous imaginons avec au moins 6 ans d’expériences sur des projets autour de la Data, une maîtrise des bases de données (SQL), des outils de transformation de la donnée (Talend, BigQuery, Airflow), un socle de compétences solides autours des langages Python, Spark, Scala, Hadoop, Java et dans un environnement Cloud.
👉
Votre carrière chez Néosoft
Depuis sa création, Néosoft place ses collaborateurs au cœur de sa stratégie. Notre culture pourrait se résumer en un mot : le collectif.
Nos communautés d’experts vous donnent la possibilité d’apprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.
Nous veillons à ce que chacun bénéficie d’un accompagnement de proximité et d’un suivi de carrière personnalisé auprès de votre manager dédié :
1 bilan d’activité trimestriel pour suivre le développement de vos compétences
1 entretien d’évaluation qui a lieu chaque année pour évaluer votre performance et déterminer vos nouveaux objectifs
1 entretien annuel auprès de votre RH dans le but de cartographier vos nouvelles compétences pour échanger sur vos projets professionnels et souhaits de formation
👉
Vos avantages
Formations et développement de l’expertise :
Vous disposez de temps alloué et rémunéré en contribuant au développement de votre expertise technique et de celle du groupe (Participations à des Tech days, animation d’une conférence à l’interne ou à l’externe, rédaction d’articles, rencontres avec nos candidats en processus de recrutement…)
Un abonnement illimité LinkedIn Learning offert
Bien-être au travail :
Un accord de télétravail flexible jusqu’à 100% de télétravail et personnalisable
Un partenariat avec Gymlib qui favorise le sport en entreprise
Des initiatives locales (afterworks, défis sportifs, team buildings, …)
Et bien plus encore :
Parce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive rémunérée dès l’arrivée du collaborateur
En plus de votre salaire : participation, compte épargne temps, actionnariat...
👉
Votre parcours candidat
Notre processus de recrutement se compose de deux étapes clés :
Un entretien de recrutement RH avec un Talent Acquisition Spécialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carrière possibles au sein de notre groupe
Un entretien d’évaluation technique pour réaliser un diagnostic de vos compétences techniques et identifier les compétences sur lesquels poursuivre votre évolution
Vous aurez également la possibilité de rencontrer pour compléter votre processus un acteur de notre pôle Business ou un pair de votre métier pour échanger sur son expérience collaborateur.
Nous avons hâte de vous rencontrer !
A bientôt,
L’équipe Néosoft 🖐
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['Salaire'], 'Level': [], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
Data Engineer & Architect - 100% Télétravail H/F,Proxiel,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-architect-100%25-t%C3%A9l%C3%A9travail-h-f-at-proxiel-3913994350?position=7&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=WogUOlUm8i9QjgIdZLo3FQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Créée en 1998, Proxiel est une société de services numériques. Proxiel est une société française basée à Montpellier et Paris. Proxiel a développé son expertise pour fournir une gamme de service complète qui va de l'étude à la réalisation, dans le domaine de la maintenance informatique, du développement de logiciel et de solutions informatiques.
Proxiel, fort de son expérience, délègue également, son savoir au coeur des organisations, au niveau national.
Depuis plusieurs années le Groupe Proxiel s'est doté d'un Pôle Formation, pour conseiller, mettre en oeuvre et accompagner ses collaborateurs et partenaires, dans le développement de leurs compétences.
Nous sommes experts dans le recrutement de profils tech. Nos consultants se donnent à fond pour entrer en contact avec vous !
Notre politique se poursuit lors de la contractualisation, le service ressources humaines est à la disposition de l'ensemble des collaborateurs.
Chez Proxiel, nous valorisons la collaboration et la proximité, tout en laissant place à l'autonomie.
Nous recherchons un Data Engineer & Architect (F/H) pour intégrer notre partenaire Grand Compte en 100% télétravail (déplacements ponctuels à effectuer sur Lyon).
Vous jouerez un rôle central dans la construction de pipelines de données robustes pour alimenter des systèmes d'IA générative de pointe. Ce rôle offre une opportunité passionnante de contribuer à des projets révolutionnaires à l'intersection de l'ingénierie des données et de l'intelligence artificielle, tout en assumant des responsabilités dans l'architecture des données.
Les Principales Responsabilités Sont Les Suivantes
Vous êtes en charge de la conception et de la construction de pipelines de données évolutifs permettant de récupérer, d'agréger et de prétraiter efficacement les données provenant de différentes sources, tout en garantissant une fiabilité et des performances élevées.
Vous êtes en charge de la conception des modèles de données, des solutions de stockage et des schémas d'accès.
Vous avez la possibilité de collaborer avec les parties prenantes pour comprendre les besoins et pour définir et faire évoluer la stratégie d'architecture des données, y compris la modélisation des données, le stockage et les schémas d'accès.
Vous travaillez avec une équipe agile interfonctionnelle pour intégrer des données provenant de divers systèmes et sources, effectuer des processus d'extraction, de transformation et de chargement (ETL) et maintenir l'intégrité des données tout au long du pipeline, en itérant sur les solutions et en communiquant sur les progrès réalisés.
Maîtrise des langages de programmation
Une solide compréhension des technologies de base de données (par exemple, SQL, NoSQL), des entrepôts de données et d'Azure
Expérience en architecture de données la conception de modèles de données, la définition de solutions de stockage et les schémas d'accès aux données.
Familiarité avec le Machine learning, les concepts de traitement du langage naturel (NLP) et la Génération Augmentée de Récupération (RAG) est un plus.
Vous avez d'excellentes compétences en matière de communication et de collaboration, et êtes capable de travailler efficacement dans un environnement d'équipe et dans le cadre de projets agiles.
Issu d'une Formation Bac +2 à Bac +5 en informatique, vous êtes polyvalent.
Vous détenez 3 ans d'expérience à minima sur un poste similaire.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning'], 'FrSoftSkills': ['Communication', 'Collaboration', 'Organisation'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Analyst F/H,Allianz France,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-f-h-at-allianz-france-3909179361?position=8&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=%2F%2BCWe%2FLugj4ZcvfLAHVgjw%3D%3D&trk=public_jobs_jserp-result_search-card,"AIM Paris (Allianz Investment Management Paris), l’unité Investissement d’Allianz France, a la charge de la gestion des investissements des différentes entités d’Allianz France tant sur les portefeuilles d’actifs généraux qu’en unités de compte (UC), sur toute la chaîne de valeur des investissements, qui couvre la gestion actif-passif, la stratégie d'investissement, ainsi que la planification des revenus des investissements, le suivi des risques, le reporting et les opérations.
Le rôle de l’équipe :
Pour soutenir ces activités, l’équipe AIM Finance & Opérations, en charge des données d’investissements, des reportings et du suivi des risques des investissements, est à la recherche d’un
Data Analyst
. Ce dernier, rattaché hiérarchiquement à AIM Finance & Opérations, couvrira l’ensemble des besoins des différentes équipes d’AIM (AIM Investment Strategy, AIM Asset-Liability Management, AIM Finance) et interviendra de façon opérationnelle sur des projets de transformation locaux et Groupe.
Voici vos principales missions :
·
L’automatisation
de sourcing des données, des contrôles et des réconciliations entre différents systèmes d’informations
·
Le développement
de macros/codes afin d’automatiser/optimiser les tâches
·
L’amélioration continue
des bases de données Investissements
·
La fiabilisation et l’amélioration
continue des bases de données
·
La mise en place
de nouveaux reportings (Power BI)
·
Le pilotage de projets
en lien avec les initiatives groupes et locales
Vous travaillerez en lien étroit avec les autres équipes d'AIM France ainsi que les équipes AIM centrales à Munich. La maitrise de
l’anglais
est alors indispensable.
Votre parcours :
Diplômé(e) en informatique/gestion des données ou d’une école d’ingénieur (
BAC +5),
vous disposez d’une expérience significative dans la manipulation de données
(5-10 ans).
Maîtrise de plusieurs langages de développement du type
Python, VBA, Access, SAS
Bonne maîtrise des outils bureautiques Powerpoint, Excel et VBA, et idéalement une première expérience significative dans l’utilisation d’outils de datavisualisation
(Power BI)
Capacité d’analyse et de synthèse, ouverture d’esprit, rigueur, organisation et autonomie,
Français et
anglais
professionnel courants indispensables, à l’écrit comme à l’oral.
Vous aimez travailler en équipe dans une entreprise qui met ses collaborateurs au cœur de sa stratégie de développement ! Alors rejoignez-nous chez Allianz ! Toute l’équipe de Marwen NAJAR vous y attend !
En tant qu’entreprise reconnue et internationale, nous vous offrons des avantages attractifs tels qu’une flexibilité du temps de travail, le télétravail (3 jours par semaine), 9 semaines de congés, une restauration sur site, des taux préférentiels collaborateurs, des congés maternité et paternité élargis, des places en crèche et bien plus encore.
En tant qu’employeur, nous mettons tout en œuvre pour vous assurer un bien être propice à votre épanouissement.
Allianz est la 1ère marque mondiale d'assurance présente dans 70 pays. Les #Allianzers, c'est 9.000 salariés en France qui accompagnent leurs clients tout au long de leur vie, des collaborateurs qui associent innovation, performance et agilité pour relever des défis permanents au quotidien. Parce qu'en nous préoccupant de l’avenir de nos collaborateurs, nous pouvons encore mieux nous préoccuper de l’avenir de tous nos clients. Pour cela, Allianz innove et met la technologie aux services des hommes pour préparer leur avenir, un avenir plus sûr.
En qualité d’employeur engagé, Allianz reconnaît que sa force se trouve dans la diversité de ses collaborateurs. Nous sommes fiers de promouvoir l’intégration et l’égalité des chances quel que soit le sexe, l’âge, l’origine, la nationalité, la religion, le handicap, ou l’orientation sexuelle de nos collaborateurs.
Allianz, We secure your future !
Informations complémentaires :
Poste base à PARIS, La Défense, Métro Esplanade
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Organisation', 'Flexibilité'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '10', '10', '10']}"
"Big Data Engineer Confirmé – Paris, France (H/F)",Astek,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/big-data-engineer-confirm%C3%A9-%E2%80%93-paris-france-h-f-at-astek-3839098103?position=9&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=NcjHVVoQEj9b2qUQVZUuKA%3D%3D&trk=public_jobs_jserp-result_search-card,"CDI
Paris - France
Publiée il y a 2 mois
Le Groupe Astek
Ce Que Nous Allons Accomplir Ensemble :
Nous rejoindre en tant que
Big Data Engineer Confirmé (H/F),
afin d’accompagner un opérateur télécoms, Leader en Europe dans l’assistance et le support applicatif de niveau 3 (résolution des problèmes utilisateurs, exploitation des environnements hors production).
Un challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !
Votre Mission, Si Vous L’acceptez :
Supervision et détection et résolution des problèmes utilisateurs (développeurs, exploitants et data exploreurs)
Développement de solutions de self-service ou d’une solution de résolutions automatiques des problèmes
Qualifier les données et les résultats
Conception technique des solutions
Assurer l’accompagnement et le déploiement des évolutions des processus et outils
Accompagner la phase de mise en production
Votre Future Équipe :
Vous intégrerez une équipe à la fois technique et fonctionnel, qui œuvre chaque jour pour développer et maintenir en conditions opérationnelles l’ensemble des solutions IT !
L’équipe est en interaction avec des clients à la fois internes et externes.
Votre stack de jeu
Système d’exploitation : Linux
Outils des distributions : HDP, HDF, ELK
Environnement Big data : Hadoop, Spark,
Langage : Scala, Shell, Python
Cloud computing : GCP ou AWS
Base de données : No SQL (Cassandra, Mongo DB), Shell, Ansible
Dataviz : Power BI ou Kibana
Des notions en Réseau et Systèmes feront la différence !
Les Petits Plus Du Projet :
Vous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.
Vous ?
Diplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.
Vous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?
Vous faite preuve de proactivité et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?
Alors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !
Astek
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de
ses 7800 collaborateurs
qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.
Tous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.
Rencontrons-nous
Notre projet commun vous plait ?
Postulez à cette annonce, et soyez transparent !
Maud, notre Talent Acquisition Referent, vous contactera pour un premier échange.
Puis vous rencontrerez Martin, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .
Enfin, vous rencontrerez Jérémy, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.
Nos Plus
Astek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo
Une politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)
Notre charte de la Diversité
Mots-clés :
ingénieur – ingénieure – consultant – consultante – Hadoop – Scala – Data
Caractéristiques de l'emploi
Catégorie Ingénieur
Job Industry Télécom / Média
Postuler en ligne
Nom *
Prénom *
Email *
Un email valide est requis.
Téléphone *
Un numéro de téléphone valide est requis.
Joindre un CV *
Mots-clés :
ingénieur – ingénieure – consultant – consultante – Hadoop – Scala – Data
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': ['Confirmé'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Lead Data Engineer,Ippon Technologies,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/lead-data-engineer-at-ippon-technologies-3851535052?position=10&pageNum=32&refId=FUgWcwWTfkSy1LADz3eV7w%3D%3D&trackingId=yY%2FhyCA7DHFhXuAJwymR4g%3D%3D&trk=public_jobs_jserp-result_search-card,"Cabinet de conseil et d'expertise en technologie, international et indépendant.
En quelques mots : 700 passionnés de tech, 12 agences dans le monde, 6 communautés d’expertise d’excellence, contributeur actif et sponsor de l'écosystème numérique, des publications soutenues et reconnues sur nos réseaux.
Rejoignez notre communauté de 70 experts en data, dont 30 à Paris, où la collaboration dynamique entre data engineers, data analysts et data architects est le moteur de notre succès. Avec une communication proactive sur des canaux internes, restez constamment informé des dernières tendances, participez à des discussions stimulantes et contribuez à l'organisation d'événements passionnants (dataday, datapéro, datalunch…).
Faites partie d'une équipe où l'innovation et l'engagement sont les clés de notre excellence collective !
Notre spécialité ? construire des data platforms dans le cloud public avec les meilleures technos du moment.
En tant que tech lead, tu interviendras sur la création d'un entrepôt de données pour les KPIs d’un grand groupe dans le secteur de l’énergie. Le but étant de leur permettre de superviser leurs activités afin de supporter leurs décisions stratégiques.
Ton rôle :
Intervenir sur l’architecture et le développement d’une pipeline d'alimentation de données
Travailler sur la modélisation et l’implémentation de l'entrepôt de données
Conseiller et accompagner les équipes dans la réalisation des dashboards de suivi des KPIs
DevOps: projet entièrement Terraformé (ressources + droits), CI/CD Gitlab, administration GCP
Faire une veille technologique active et partager tes connaissances en interne
Travailler en collaboration avec les métiers et les data analysts pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship etc…)
Si tu le souhaites, tu pourras également :
Participer aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups)
Capitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne.
Tes connaissances :
Tu maîtrises le développement en Python
Tu as de l’expérience dans la mise en place de pipeline de données jusqu’en production (CI/CD Gitlab, Terraform)
Tu as une expérience dans un environnement Cloud (GCP de préférence, AWS, Azure)
Tu as une bonne connaissance d’un outil de visualisation (Looker Studio, Power BI)
Tu accompagnes des data engineers dans la mise en place des bonnes pratiques
Tu es capable de proposer/challenger la stack technique
Ippon c’est aussi :
Travailler en équipe au sein d'une communauté data à la pointe des évolutions
Un suivi de proximité réalisé par ton manager (expert data)
Devenir ceinture noire en data grâce à notre programme d’accompagnement de carrière Blackbelt
Participer à nos apéros et divers évènements internes pour consolider la cohésion d’équipe
Notre process de recrutement :
Préqualification téléphonique - 20 min
Un entretien RH / Sales - 1H00
Un entretien technique avec 2 consultants data
Si le match est bon des deux côtés : Hadjimé ! Tu te lanceras sur le tatami Ippon !
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication', 'Collaboration', 'Organisation'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data & Software Engineer Intern,Ledger,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-software-engineer-intern-at-ledger-3884430248?position=1&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=vWJk2DfCNQCU7uPn3dV2bA%3D%3D&trk=public_jobs_jserp-result_search-card,"We're making the world of digital assets accessible and secure for everyone.
Join the mission.
Founded in 2014, Ledger is the global platform for digital assets and Web3. Over 20% of the world’s crypto assets are secured through our Ledger Nanos. Headquartered in Paris and Vierzon, with offices in UK, US, Switzerland and Singapore, Ledger has a team of more than 600 professionals developing a variety of products and services to enable individuals and companies to securely buy, store, swap, grow and manage crypto assets – including the Ledger hardware wallets line with more than 6 millions units already sold in 200 countries.
At Ledger, we embody the values that make us unique: Pragmatism, Audacity, Commitment, Trust and Transparency. Hear from our employees how they shape the work we do here.
Data at Ledger
As a data driven company, we ensure all major decisions are backed by data. To continue meeting that goal, we are looking for an intern to reinforce our data engineering team.
You will be responsible for optimizing our new data platform, building and maintaining reliable data pipelines. The ideal candidate is a software builder who will enjoy optimizing data systems and building them from the ground up in a fast paced environment and innovative space (blockchain is one of our key data sources!). You will also support data analysts and data scientists on data initiatives.
This internship will be a valuable opportunity to gain hands-on experience in all the aspects of a modern data stack: software engineering best practices, DevOps, SecOps concerns, Cloud infrastructure, legal compliance, etc.
Start date: As soon as possible
Length of internship: 6 months
Your mission
Work on data extractions from various sources (relational databases, API, flat files, etc.) and their transformations
Perform code reviews to guarantee code quality
Monitor data pipelines and investigate discrepancies
Contribute to migration of existing pipelines on AWS
Contribute to the improvement of the data engineering stack. We are an open-minded team and your opinion will be valued
Team tech stack
ELT: custom development (Python, SQL), Fivetran, Stitch, Adverity + dbt
Cloud: AWS
DWH: Redshift, migration to Snowflake underway
Orchestration: Airflow
BI: Tableau, Mixpanel
Versionning: Github
IaC: Terraform
What we're looking for
Fluent English
SQL & Python are among your best friends
You know how to fix conflicts in Git
Plus Points
Airflow, dbt, Snowflake, Github Actions
Knowledge about crypto
Experience working with Cloud infrastructure (ideally AWS)
What’s in it for you?
Flexibility: A hybrid work policy
Social: Frequent social events, snacks and drinks
High tech: Access to high performance office equipment and gadgets, including Apple products
Transport: Ledger reimburses 75% of your preferred means of transportation
Food: We offer lunch vouchers with Swile
Vacation: 1 day off for every full month worked
We are an equal opportunity employer for all without any distinction of gender, ethnicity, religion, sexual orientation, social status, disability or age
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Flexibility', 'Initiative']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer Talend (confirmé/sénior) - H/F - CDI,Talan,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-talend-confirm%C3%A9-s%C3%A9nior-h-f-cdi-at-talan-3902901378?position=2&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=py96T5xv7qfymnc5Nn1EyA%3D%3D&trk=public_jobs_jserp-result_search-card,"Talan est un groupe international de conseil en transformation et en innovation par la technologie, créé en 2002.
Nos 5000 consultantes et consultants partagent à travers le monde l’audace d’innover, le goût de l’excellence, et l’envie de relever les défis les plus complexes.
Nous accompagnons les entreprises dans des secteurs variés : énergie, industrie, transport, finance, luxe… à travers 3 grandes expertises :
Le Conseil en Management et Innovation (320 Consultants en France)
La valorisation des données, leurs structurations, et leurs usages (Data et Technologies)
L’intégration de solutions logicielles (Cloud et Applications Services)
Nos valeurs : engagement, respect, partage, esprit d’équipe et optimisme.
Talan est une entreprise responsable, reconnue par ses collaborateurs et attachée à la diversité. Des aménagements peuvent être proposés si vous êtes en situation de handicap.
Retrouvez nos engagements RSEiciet nos actions en faveur de la diversitéici
Job Description
VOTRE ROLE SUR NOS PROJETS :
Vous interviendrez sur des projets de:
Mise en place de Modern Data Platform
Interconnexion d’applications opérationnelles en temps réel
Au sein d’équipes projet, votre rôle sera de:
Concevoir et mettre en place des flux d’intégration de données
Garantir la qualité des développements
Rédiger des spécifications fonctionnelles et techniques
Modéliser l’entrepôt de données
Mettre en place des solutions de suivi et pilotage des flux de données
Proposer des solutions d’optimisation
Mettre en place ou faire évoluer des chaînes CI/CD
VOTRE ROLE CHEZ TALAN :
Au sein du pôle Tech for Data, vous contribuerez à la croissance et la prospérité de la communauté au travers des activités suivantes:
Benchmark de solutions et conseil auprès de nos clients sur les solutions technologiques à adopter, en lien avec leurs besoins
Réalisation de POC (Proof Of Concept)
Partage de connaissances et formations internes
Passage de certifications
Veille technologique
Participation à la rédaction de réponse à appel d’offre
Qualifications
VOTRE PROFIL:
Diplômé d’un Bac+5 en informatique/data, vous justifiez d’une expérience d’au moins 3 ans sur des problématiques d’intégration, traitement et mise en qualité de données en ayant mis en œuvre des flux de données sur un Talend (la maîtrise d’autres solutions de traitement de données est un plus).
Vous maîtrisez les concepts de modélisation de données et les architectures de type DataLake, DWH, Datamarts (la connaissance de la modélisation Datavault est un plus).
Vous savez évoluer dans un environnement avec un modèle de données complexe et évolutif.
Vous disposez d’un très bon relationnel et vous êtes reconnu pour votre capacité à évoluer efficacement avec des interlocuteurs aussi bien techniques que non-techniques.
Force de proposition, vous savez mobiliser autour de vos idées et de vos projets.
Vous savez évoluer dans un contexte data ops et avez une connaissance des chaînes CI / CD (gitlab, Azure Devops, ...)
La maîtrise de la méthodologie Agile est un plus.
Ensemble réalisons de nouveaux projets Talantueux!!
VOTRE SOUHAIT D’EVOLUTION:
Si vous êtes passionné par l’innovation, et souhaitez élargir vos compétences techniques dans la data, accéder à des fonctions de management de projet et d’équipe, participer au développement commercial et organisationnel, ou tout simplement pouvoir valoriser vos prises d’initiatives et développer de nouveaux terrains de jeux, alors rejoignez-nous!
Additional Information
AVANTAGES
:
Plan de formation pour accompagner votre carrière (formations éditeurs, certifications) grâce à nos partenariats nous accordant une position de partenaire privilégié, et management de proximité par des experts
Locaux modernes en centre-ville
Top 5 du Palmarès Great Place to Work
Télétravail jusqu’à 5 jours selon les missions, prime d’équipement de 100€
Mobilité en France et à l’étranger
Top 1% des entreprises évaluées par Ecovadis dans le domaine social, environnemental et éthique
Tickets restaurant, prime vacances, 50% transport (abonnement transport public), mutuelle
Permanence handicap (consultant dédié aux collaborateurs en situation de handicap et aux proches aidants)
Actionnariat salarié
Prime de cooptations
RTT
PROCESS RECRUTEMENT
:
L’équipe recrutement s’engage à vous proposer un processus de recrutement rapide et fluide
1 entretien RHpar Teams (45min)
1 entretien opérationnel avec le responsable de domaine, au siège (1heure)
1 entretien avec le directeur de pôle, au siège(1heure)
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Cloud', 'CI/CD', 'CI / CD'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data engineer F/H,DOCAPOSTE,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-docaposte-3879674310?position=3&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=4DW5b8oYCvw49FhS2q2jkg%3D%3D&trk=public_jobs_jserp-result_search-card,"Intitulé du poste
Data engineer F/H
Contrat
CDI
Télétravail
Oui
Description de la mission
Au sein du pôle, vous avez la charge de manipuler les données de la plus grosse base de données médicale du monde, transmises par l'Assurance Maladie. Votre mission est de préparer et de mettre à disposition les données pour les différents acteurs de l'entreprise. Vous industrialisez les flux de données de centaines de milliers de patients, pour les suivre sur une dizaine d'années et repérer certains événements de leur prise en charge. Ainsi, les analyses pourront mettre en évidence les différents cycles des parcours de soin. Ce travail collaboratif se déroule en mode projet pluridisciplinaire.
Ainsi vous devez :
À partir de bases de données complexes et volumineuses, réaliser les opérations de contrôle d'intégrité, d'extraction, de nettoyage, et programmer leurs automatisations pour constitution des bases de données d'études
Coder des algorithmes spécifiques de sélection de soins (hospitalisations, délivrance de médicaments, consultations …) et de séquences de traitement en utilisant toutes les particularités des données,
Réaliser des jointures complexes entre les différentes tables de données,
Optimiser les temps de traitement et de l'espace de stockage : millions de patients sur des Téra de données, en automatisant au maximum les opérations de traitement,
Travailler en mode collaboratif pour la rédaction de macros/fonctions génériques pour que votre travail puisse servir à toute l'équipe,
Votre nouvel environnement
Filiale du groupe DOCAPOSTE SANTE, nous sommes leader en France dans le traitement et l’analyse des données de santé en vie réelle, et plus particulièrement celles issues des bases du Système National des Données de Santé (SNDS) et des bases en OPEN DATA. Nous élaborons et produisons pour nos clients (industries pharmaceutiques, fabricants de dispositifs médicaux, institutionnels) des études pharmaco-épidémiologiques, médico-économiques et de parcours de soins à partir de solides méthodes statistiques et de solutions innovantes s'appuyant sur l'Intelligence Artificielle (Machine Learning, Deep Learning, …).
2jours de Télétravail
Nous vous accompagnons
Un programme de formation et d'accompagnement est prévu en fonction de vos compétences précédemment acquises et de votre expériences
Localisation du poste
Europe, France, Auvergne-Rhône-Alpes, Rhône (69)
Lieu
Niveau d'études min. requis
Diplôme
Niveau d'expérience min. requis
Langues
Profil : Pour l’égalité des chances, Docaposte fait vivre la diversité. Nos postes sont ouverts à tous.
LYON
Critères candidat
Profil : Pour l’égalité des chances, Docaposte fait vivre la diversité. Nos postes sont ouverts à tous.
Rigueur
manipulation de données de santé
Analytique
Données riches et complexes
Référence
2023-4471D
Entité qui recrute
HEVA
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Statistiques'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data engineer - Dev - Alternance H/F,Assystem,"Courbevoie, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-dev-alternance-h-f-at-assystem-3888473931?position=4&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=aJ6OHI9E7yPzGHW%2BOkjCjQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Trouver des solutions au dérèglement climatique est la priorité du 21ème siècle, et implique de switcher à l’énergie bas-carbone. Chez Assystem, on s’est donc donné pour mission d’accélérer la transition énergétique partout dans le monde. Et pour y parvenir, nos 7500 Switchers couplent leur expertise historique en ingénierie et en management de projet aux technologies digitales.
Présent dans 12 pays (Europe, Moyen-Orient, Asie), nous travaillons sur la production et la distribution d'électricité bas-carbone, à travers le développement des énergies nucléaires et renouvelables. Nous participons également à modernisation des réseaux électriques et l'électrification des usages, à travers l'hydrogène pour décarboner les secteurs des transports et de l'industrie.
Description du poste
Vous serez intégré sur notre plateau de data science à la Défense.
En tant qu'alternant(e) en gestion des données, vous serez chargé(e) de :
Participer à la conception, au développement et à la maintenance des processus de gestion des données.
Collaborer avec l'équipe pour résoudre les problèmes techniques et proposer des solutions.
Contribuer aux revues des projets, en comprenant les défis liés aux données et en proposant des solutions appropriées.
Effectuer des tests et des analyses pour assurer la qualité des applications.
Suivre les bonnes pratiques de développement logiciel et contribuer à l'amélioration continue des processus.
Participer à la mise en place des pratiques MLOPS à travers l'intégration et le déploiement continu.
Votre rôle inclura également de proposer des solutions technologiques pour répondre aux contraintes de productivité et de sécurité.
«Pourquoi réaliser votre alternance chez Assystem? On a 3 bonnes raisons pour vous convaincre!
🥐 Travailler au sein d’une équipe engagée qui ramène expertise et croissants le matin!
😎 Découvrir pourquoi 92% de nos stagiaires/alternants apprécient l’ambiance et leur environnement de travail
🏆 Gagner en compétences et développer votre expertise métier en échangeant au quotidien avec les collaborateurs Assystem, ainsi que le client en direct pour plus de proximité !»
Qualifications
Nous recherchons un candidat titulaire d'un diplôme de
niveau BAC+2
avec des compétences dans les domaines de SQL, NoSQL, UML, Python, React.js, Node.js et l'architecture des systèmes d'information.
La maîtrise du français et de l'anglais est nécessaire.
Nous valorisons également des qualités telles qu'un bon relationnel, un esprit de synthèse, une autonomie et un esprit innovant chez nos candidats.
Informations supplémentaires
Nous nous engageons au respect de l’égalité de traitement entre les candidats, et célébrons toutes les formes de diversité. Chez Assystem, seules les compétences comptent!Si vous souhaitez porter à la connaissance d’Assystem une quelconque situation ou des besoins spécifiques, n’hésitez pas vous serez accompagné(e)!
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
"Senior Data Engineer (Spark, Hive, Impala) - Paris - CDI",METEOJOB by CleverConnect,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-spark-hive-impala-paris-cdi-at-meteojob-by-cleverconnect-3862148207?position=5&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=%2FRXOfDy2If%2BTy%2BVQXX5s8w%3D%3D&trk=public_jobs_jserp-result_search-card,"Entreprise
Chez LJE Solutions, nous plaçons l’humain au cœur de chaque projet. Au-delà des compétences, nous valorisons les
aspirations
et les
valeurs
de chaque individu.
Nous intervenons dans tous les secteurs d'activité en France et en Suisse.
Description Du Poste
LJE Solutions recherche, pour un de ses clients, cabinet de conseil spécialisé dans la technologie, un/une Sénior Data Engineer pour compléter son équipe.
Notre client est un cabinet de conseil mixte qui allie la technologie, la data & l'IA, le CRM & le digital.
Le Poste
En tant que Senior Data Engineer, vous serez responsable du développement de nouveaux modèles de données et de pipelines. Vous aurez l'opportunité de tester les solutions les plus innovantes du marché pour améliorer nos capacités en matière de données. Vous jouerez également un rôle crucial dans l'assistance aux clients, le cadrage des projets et le coaching des consultants juniors.
Vos Responsabilités
Développement de modèles de données et de pipelines,
Test des solutions innovantes pour améliorer les capacités en données,
Assistance aux clients dans le cadrage des projets,
Coaching des consultants juniors,
Participation au développement de l'entreprise.
Rémunération Et Avantages
Tickets restaurant,
Mutuelle d'entreprise,
Environnement de travail dynamique et motivant,
Diversité de projets stimulants,
Perspectives d'évolution de carrière,
CDI avec rémunération fixe attractive et part variable, à partir de 50k€/an et définie en fonction du profil et de l'expérience,
Deux jours de télétravail par semaine,
Participation active à la vie de l'entreprise.
Description Du Profil
Profil recherché :
Diplôme d'une école d’ingénieur / génie informatique Bac+5,
Minimum 4 ans d'expérience dans le domaine de la data,
Capacité à travailler en équipe et à superviser plusieurs actions,
Excellentes compétences en communication, présentation et coordination,
Capacité d'analyse et de résolution de problèmes,
Aptitude à assimiler rapidement de nouvelles technologies,
Mindset positif et volonté de contribuer au développement de l'entreprise.
Compétences Techniques
Technologies Big Data (Spark, Hive, Impala...),
Services Cloud (AWS / Azure / GCP),
Langages de programmation : Python, Java, Scala,
Mise en production de cas d'usage Data, notamment en Machine Learning,
Bases de données SQL,
DevOps et développement de flux de données (data pipelines) avec Docker/Kubernetes et chaînes CI/CD.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication', 'Résolution de problèmes'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Junior', 'Senior'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Engineer (F/H) – en stage,Carrefour,"Massy, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-%E2%80%93-en-stage-at-carrefour-3884393188?position=6&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=oRZJtJ4lw6LlmZ%2BTwf0kow%3D%3D&trk=public_jobs_jserp-result_search-card,"Nous rejoindre, c’est rejoindre l’un des leaders mondiaux de la distribution qui met l'accent au quotidien sur la diversité, la RSE et le digital, pour satisfaire nos clients et nos collaborateurs. En tant que partenaire premium des Jeux Olympiques et Paralympiques de Paris 2024, nous partageons les valeurs du sport en permettant à nos équipes de se dépasser et encourageons une alimentation saine au juste prix pour tous.
Vous cherchez à travailler dans une entreprise dynamique où votre travail rime avec impact social et environnemental ? Bienvenue chez nous !
Porteuse de cette ambition, la Direction D&A Technology recrute un(e)
Data Engineer (F/H) – en stage
Au sein de Carrefour, la Direction D&A Technology a pour mission la construction d’un socle de données mutualisées d’Entreprise.Les équipes de la Direction travaillent conjointement à une plateforme unique pour collecter, structurer, stocker et diffuser les données du Système d’Informations Carrefour. Un socle Data Centric est mis en œuvre pour permettre des usages opérationnels, décisionnels et analytiques. Vous aurez un rôle de Data Engineer sur un scope fonctionnel, ce qui se traduira par le développement de traitements d’ingestion, stockage et exposition de données, que ce soit avec un framework Temps Réel ou avec un framework de type ETL
🎯 Les missions
Dans ce cadre, vous serez amené à
Comprendre les volets technique et fonctionnel du Produit Data en charge
Appliquer les bonnes pratiques et les normes de développement
Contribuer à l’automatisation du delivery
Délivrer les cas d’usages attendus sur le Produit Data
Proposer des axes d’amélioration sur la Plateforme
👥 Profil
Vous êtes étudiant dans une école d’ingénieurs informatique cherchant un stage de fin d’études et souhaitant évoluer dans un environnement BigData avec de forts enjeux pour l’entreprise.
Vous avez un intérêt pour l’architecture de systèmes distribués Big Data et des capacités d’analyse
Vous avez une bonne connaissance des méthodes de gestion de projets en mode agile (SCRUM)
Vous avez une bonne capacité à travailler en équipe, tout en étant très autonome
Vous avez des compétences techniques sur ces domaines
○ Scala/Java (Connaissance d’un des deux langages)
○ Ecosystème Big Data (Spark, Apache Kafka, Avro ...)
○ Outils de déploiement et orchestration (Kubernetes, Docker, Ansible …)
○ Google Cloud Plateform (GCS, BigQuery, GKE, Cloud Pub/Sub, Dataproc, …)
○ Bases de données NoSQL (Cassandra, BigTable…) & Moteur de recherche (Elastic Search...)
○ CI/CD (Git, Jenkins, Nexus, Docker Registry, …)
Vous êtes une personne passionnée, curieuse, autonome, et intéressée par le Software Craftsmanship.
Encore plus de bonnes raisons de nous rejoindre
Intégrer une équipe conviviale à taille humaine au sein d‘un grand groupe
Pour le site de Massy  Un campus attractif avec plusieurs restaurants d’entreprises, salle de sport avec cours, offres Comité d’entreprise.
12 % de remise sur achat
📝 Informations complémentaires
Date de début  02 janvier 2024
Durée  6 mois
Lieu  Massy (91) – RER B/RER C Massy-Palaiseau
Chez Carrefour, nous avons à cœur de ne passer à côté d’aucun talent et sommes fiers de compter des équipes représentatives de la société dans son ensemble. Nous encourageons ainsi tous types de profils à postuler à cette offre et garantissons un processus de recrutement dénué de toutes formes de discriminations.
Show more
Show less","{'ProgLanguage': ['Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': ['Avro'], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker', 'Jenkins'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': ['Apache Kafka'], 'Automation': ['Ansible', 'Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Intermediate/Senior Data Engineer - Scientific Engine - CDI,Descartes Underwriting,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/intermediate-senior-data-engineer-scientific-engine-cdi-at-descartes-underwriting-3863474083?position=7&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=KFHl0gad8reCli%2FEGp2aVQ%3D%3D&trk=public_jobs_jserp-result_search-card,"ABOUT DESCARTES UNDERWRITING
Descartes was born out of the conviction that the ever-increasing complexity of risks faced by corporations, governments and vulnerable communities calls for a renewed approach in insurance. Our team brings together industry veterans from the most renowned institutions (AXA, SCOR, Swiss Re, Marsh, Aon, ...) and scientists on top of their field to bring underwriting excellence. After 5 years of existence, Descartes has secured a leading position in parametric insurance for weather and climate-related risks utilizing machine learning, real-time monitoring from satellite imagery & IoT. After a successful Series B raise of $120M USD, we launched Descartes Insurance, a 'full stack' insurer licensed to underwrite risk by the French regulator ACPR. With a growing corporate client base (350+ and counting), our diverse team is headquartered in Paris and operates out of our 13 global offices in North America, Europe, Australia, Singapore, Hong Kong and Japan. Descartes is trusted by a panel of A-rated (re)insurers to carry out its activities.
ABOUT YOUR ROLE
Due to our consistent growth, we are expanding our Data, Software and DevOps team. We are seeking profiles dedicated to data engineering. At the core of the development of our scientific engine modeling climate phenomena, your main missions will be to create, improve and maintain the data pipelines used to train our model and infer the different scenarios to make a climate risk assessment. You will have to take initiative and assess the viability of proof-of-concept projects.
You will have to work with data scientists and software engineers to run and develop our models. You will be working alongside DevOps engineers to reliably put models in production and selected the compute/store instance needed to perform these tasks. Your secondary mission will be to automate the flow of information between the tech and business to monitor climate events.
🔔
KEY MISSIONS 🔔
Design, setup, and maintain:
Data pipelines and associated datalakes;
Connections to external and internal APIs;
Associated CI/CD and release pipelines;
Notification tools to inform the team of the status of the operations.
Propose and setup data storage, data processing and data visualizing tools including:
Assessing the pains and needs of the teams;
Benchmarking different solutions;
Assessing the security, price and reliability of data architecture;
Following the development the evolution of technologies on the topic;
Forecasting and tracking cloud spend.
Participate in:
Tech stack evolution;
Discussions with tech partners;
Training of other tech teams;
Support and debug of internal users.
TECH STACK 🖥️
Cloud provider: GCP
Code versioning tool: Git + Gitlab
OS: Linux
Container: Docker
Container orchestrator: Kubernetes
Code base: Python
Notification tool: Slack
DATA STACK
Types: images, time series, data frames, etc.
Pipeline orchestrator: Apache Airflow
Data stores: Cloud SQL, FireStore, BigQuery
In our project, data is collected by sensors (satellite, weather station, IoT). We don’t work with personal or sensitive data, in most cases the data is publicly available (earthquake magnitude, cyclone track, precipitation …).
ABOUT YOU
EXPERIENCE & QUALIFICATIONS 💻🖥️
[Hard skills]
Knowledge of the tech stack and demonstrated proficiency in production environments;
Minimum 3 years’ experience in Python object-oriented programming;
Experience converting Python code to efficient data engineering tools;
Production experience with Docker;
Production experience with a cloud provider (GCP, AWS or Azure);
CI/CD and release pipelines;
Good knowledge in English and fluency in French.
[Soft skills]
Excellent communication skills, in both formal and informal settings, and in English and French;
Contribute to a rigorous data engineering culture;
Propagate Data Engineer best practices to other tech teams;
Well versed in Agile;
Mentoring junior engineers.
[Nice-to-have]
Prior experience working in data science or scientific computing projects;
Working knowledge of DevOps;
Contribution to an open source project.
MINDSET 💥
Strong interest with climate issue (it’s not a hoax, many people suffer from it);
Being comfortable to work alongside corporate insurers (some still wear suits 👔);
You enjoy CI/CD automation (or at least appreciate the elegance of a well-crafted pipeline);
Strong team spirit and ability to work (you’ll have to review code and have your code reviewed);
Rigorous, creative and meticulous mind (we handle large insurance, we take our time);
Strong desire to learn (there’s no limitation to the tech used, we’re happy to test and learn new tools);
Eagerness to work in a multi-cultural environment (policies and teams are from all around the world 🗺️).
WHY JOIN DESCARTES UNDERWRITING?
Opportunity to work and learn with teams from the most prestigious schools and research labs in the world, allowing you to progress towards technical excellence;
Commitment from Descartes to its staff of continued learning and development (think annual seminars, training etc.) ;
Work in a collaborative & professional environment ;
Be part of an international team, passionate about diversity ;
Join a company with a true purpose – help us help our clients be more resilient towards climate risks;
A competitive salary, bonus and benefits;
You can benefit from a punctual home office days.
If you want to develop your skills and work in a friendly startup atmosphere, don’t hesitate and send us your application!
At Descartes Underwriting, we cherish value of diversity whatever it may be. We are committed to fighting against all forms of discrimination and for equal opportunities. We foster an inclusive work environment that respects all differences.
With equal skills, all our positions are open to people with disabilities.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': ['Linux'], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': ['Slack', 'Teams'], 'Other': ['DevOps', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': ['Full', 'Junior'], 'TypeContract': [], 'Salary': ['Salary'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer H/F,Davidson consulting,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-davidson-consulting-3913991399?position=8&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=r1M1AAERTfnl0evdu2IjpQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Rejoindre Davidson, ce n'est pas seulement intégrer un groupe de 3000 consultants dans 8 pays et 3 continents, c'est intégrer LA société qui a été élue par ses salariés Great Place To Work France et Europe pendant 4 ans ainsi que la plus grande B Corp (Benefit Corporation) de France !
Les « B Corp » formant une communauté de sociétés qui ont décidé d'être non pas les meilleures du monde mais les meilleures POUR le monde.
Parce que notre développement repose sur des principes forts :
Un profond respect de l'ensemble de nos parties prenantes : consultants, clients et fournisseurs. Car si le travail ne fait pas le bonheur, il peut cependant faire le malheur. Nous nous engageons donc à écouter, agir avec honnêteté et promouvoir l'équité.
Une empreinte environnementale minimale, et sociétale maximale. C'est pourquoi, au-delà des missions que vous réaliserez, vous pourrez également contribuer à des projets que Davidson soutient : missions de solidarité internationale (avec Planète Urgence), accompagnement d'étudiant(e)s issus de milieux peu favorisés (avec Article 1), investissement dans des startups développant des solutions innovantes !.
Un Management adhocratique basé sur la mise en oeuvre des principes de l'entreprise horizontale et du management tribal.
Sur ce dernier point une précision d'importance : le bien-être au travail est un luxe qu'il faut pouvoir s'offrir en étant une entreprise solide. Ceci induit pour les davidsonien(ne)s d'allier prises d'initiative, engagement et professionnalisme. Car sans travail, le talent n'est qu'une sale manie. Et nous incite à chercher à recruter des éléments meilleurs que nous. Dans une organisation classico-hiérarchique, il peut être bénéfique d'avoir une armée de gens qui travaillent pour vous. Dans une adhocratie, ils causent des dégâts.
Pour Le Compte D'un De Nos Clients Grand Compte, Et Sous La Responsabilité Du Manager Davidson, Vous Apporterez Votre Expertise En Tant Que Data Engineer. Vos Missions Seront Entre Autres
Assurer la collecte, le stockage et l'exploitation des données.
Développer des applications distribuées à grande échelle.
Construire des modèles de stockage performants.
De formation Bac +5, vous êtes diplômé d'une Ecole d'Ingénieurs ou de l'Université, avec une spécialisation en informatique et développement.
Vous avez une bonne maîtrise des différentes technologies/outils suivants :
Spark / Hadoop.
Scala, Java.
SQL (MariaDB), HBase, Hive, Impala.
Vertica, Elastic Search, HDFS (Parquet, ORC).
Kafka.
Airflow.
Vous avez un très bon niveau d'anglais et idéalement une expérience de travail en méthodes agiles.
Vos capacités d'analyse et votre rigueur vous permettent de mener à bien les attendus techniques de vos projets dans le respect des délais impartis.
Votre ouverture d'esprit et votre disponibilité vous permettront de réussir et d'apprécier le métier de consultant au sein d'une entreprise en forte croissance.
Show more
Show less","{'ProgLanguage': ['Java', 'Scala', 'R'], 'DataBase': ['SQL', 'HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Consultant Microsoft Data & BI (H/F),EXAKIS NELITE,"Clermont-Ferrand, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/consultant-microsoft-data-bi-h-f-at-exakis-nelite-3905645675?position=9&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=IJwuqp4QuWYYzre07ojdbQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Exakis Nelite, entité du groupe
Magellan Partners
, est le
1er partenaire pure-player Microsoft
indépendant en France avec l’ambition de devenir le premier partenaire Européen et en Afrique Francophone avec sa forte présence au Maroc.
Nés du rapprochement de 2 leaders spécialistes de l’intégration des solutions Microsoft, nous allions expertise technique et fonctionnelle pour répondre concrètement aux enjeux de demain : accélération de la transformation digitale,
CyberSécurité, Intelligence Artificielle, IoT, Data, transformation vers le cloud Azure, Services Cognitifs …
En rejoignant la communauté Exakis Nelite dans l’une de nos 12 agences (680 collaborateurs), vous intégrerez une équipe passionnée et impliquée dans les projets les plus innovants. Vous rejoindrez une structure construite autour de valeurs tournées vers ses collaborateurs : intelligence collective, convivialité et bienveillance.
Exakis Nelite a reçu la certification
Great Place to Work
et se place
3ème de sa catégorie
en 2024 parmi les entreprises où il fait bon travailler en France !
Poste et missions:
En tant que Consultant(e) Microsoft Data / BI, vous intervenez dans le cadre de projet d’informatiques décisionnelles, en forfait ou en régie auprès de nos clients. Les missions sont les suivantes :
• Accompagner le client dans une démarche de mise en œuvre d'un système décisionnel classique ou d'un outil de Modern BI
• Analyser et qualifier le besoin
• Rédiger des spécifications fonctionnelles, techniques
• Concevoir et développer les solutions BI
• Modéliser et enrichir des bases de données
• Mettre en place des outils de restitution BI pour la production de tableaux de bord et de reporting
• Modéliser et implémenter des Datamarts et Datawarehouses
• Préparer et animer des formations et des séances de coaching BI en entreprise
• Modéliser des algorithmes de calculs statistiques permettant de faire diverses analyses (prédiction,clustering,…)
Profil:
De formation BAC+5 idéalement en ingénierie, vous justifiez d’un minimum de 2 ans d’expérience sur des technologies de Business Intelligence et/ou d’outils statistiques.
Vous avez une réelle expertise sur un ou plusieurs environnements et langages suivants :
SQL server (2012 et suivants) et développement de requêtes SQL complexes
Maîtrise de la suite Microsoft BI (SSIS, SSAS, SSRS)
Maîtrise de l'outil de Modern BI (Power BI)
Préparation de la donnée
Modélisation de la donnée (conception d'un modèle de donnée optimal)
Création de rapports interactifs et tableaux de bords
Connaissance des problématiques de connexion aux données (une expérience en développement de Custom Connectors est un plus)
Mise en œuvre de serveur Power BI Report Server
Vous êtes à l’aise avec les syntaxes DAX ou MDX
Bonne connaissance des services Data de Microsoft Azure (SQL Database, SQL Datawarehouse, Cosmos DB, Azure Data Lake Store, Azure Data Lake Analytics, Azure Data Factory, Azure Databricks, Azure HDInsight)
Bonne connaissance des services IA de Microsoft Azure (Azure Machine Learning, Azure Cognitive Services)
Bonne connaissance d’un langage de programmation (R,Python)
Maitrise de l'anglais.
Des expériences dans les domaines du Big data et du machine learning sera un réel avantage (profils Data Scientists ou Data Citizens appréciés). Maitrise de l’anglais indispensable.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': ['Statistiques'], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning', 'Statistiques', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Engineer en Alternance,Archery Data & Analytics,"Tours, Centre-Val de Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-en-alternance-at-archery-data-analytics-3907527785?position=10&pageNum=35&refId=dZ1gMrOpD8AXyzg1FiBMxg%3D%3D&trackingId=fuMbh5r3Mmp72P1Au75CNg%3D%3D&trk=public_jobs_jserp-result_search-card,"Description de l'entreprise
Archery Data & Analytics est un cabinet d’expertise Data, filiale d’Archery Strategy Consulting, cabinet de conseil en direction générale spécialisé dans 3 secteurs : aéronautique/spatial/défense, transports/logistique et énergie.
Nous intervenons chez nos clients sur des problématiques :
De stratégie et de transformation data/digitale (gouvernance des données, roadmap data, digitalisation…)
De maîtrise d’œuvre et gestion de projets Data à forts enjeux (utilisation des méthodes Agiles, Scrum, DevOps…)
De réalisation de projets Data & Analytics (Business Intelligence, MDM, BIG DATA, Data Sciences, Low Coding…)
Enfin, nous développons et commercialisons des solutions métiers à haute valeur ajoutée (ingénierie, gestion de projets, outils grands programmes, suivi de production, maintenance…).
Le cabinet a été constitué en 2021 et est aujourd’hui représenté par 13 consultants Data & Analytics répartis sur nos sites de Tours, Paris et Toulouse.
Description de la mission
Archery Data & Analytics à la recherche d’un.e alternant.e en Data Engineering en Master 1 (4ème année) afin de compléter notre équipe.
Au sein du cabinet, vous évoluerez dans l’environnement de travail suivant :
Fonctionnel : Industrie, supply-chain, logistique, fonctions transverses (qualité, finance, informatique…)
Plateformes Cloud : Microsoft AZURE
Plateformes Data Onpremise : Hadoop/Spark
Outils de Reporting : Microsoft Power BI ou QlikView ou Tableau.
Plateformes d’intégration : SQL Integration Services, Informatica, Azure Data Factory
Langages de programmation : SQL, DAX, Python, .NET
Outils de Data Science : Data Bricks
Des connaissances dans cet environnement sont un véritable plus, aucune maitrise n’est demandée.
Vos Missions au quotidien :
Apporter conseils et expertises à nos clients lors des missions Data.
Concevoir, développer et mettre en place des solutions Data & Analytics de bout en bout :
Participer à la captation et l’analyse des besoins de nos clients avec l’aide de nos consultants Data.
Concevoir les solutions à mettre en œuvre de façon à répondre aux exigences et attentes des utilisateurs.
Contribuer à la conception, puis à la mise en place des infrastructures que ce soit en IaaS, SaaS ou PaaS…
Concevoir et mettre en place des flux de données automatisés, des modèles de données…
Concevoir et développer des reporting d’aide à la décision (calculs complexes et algorithmes inclus).
Documenter les solutions développées et en assurer le maintien en conditions opérationnelles (MCO).
Assurer le suivi du projet et la relation avec le client final.
3. Participer aux projets de développements de produits internes du cabinet Archery.
4. Assurer la veille technologique et contribuer à étendre nos expertises.
Profil recherché
Niveau du diplôme visé : Bac+5 avec des bases en systèmes d’informations
Autonomie, rigueur, esprit d’équipe et force de proposition
Très bon relationnel, bonnes capacités d’analyse, de synthèse et de rédaction.
Localisation du poste : Tours (37)
Prise de fonctions : Septembre 2024
Types de contrats : Contrat d’apprentissage ou contrat de professionnalisation selon profil sur 24 mois
Processus de recrutement
Un premier entretien avec Meghann, notre chargée RH et Paul, notre manager du pôle Date engineering
Un deuxième entretien avec Julien, notre Président
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
Développeur Big Data - Spark,NEXTON,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/d%C3%A9veloppeur-big-data-spark-at-nexton-3911787310?position=1&pageNum=37&refId=3scIT4h7LcxHkowem3t5kA%3D%3D&trackingId=lNIo4gI17gKIgLn8o6MPpw%3D%3D&trk=public_jobs_jserp-result_search-card,"Mission (fiche métier)
NEXTON recrute un
Développeur Big Data - Spark
, en CDI, à
Lyon
!
Qui sommes-nous ?
NEXTON c’est avant tout une entreprise qui accompagne ses clients dans leur transformation digitale. Tous les jours, nous travaillons avec des grands comptes et des pures players (SNCF, Orange, BNP PARIBAS…).
Nous sommes experts du digital aussi bien sur de l’accompagnement stratégique qu’opérationnel.
Fort du succès, Nexton connaît aujourd’hui un développement significatif, autour de ses valeurs piliers : cohésion, confiance et performance.
Et pour toi ? Notre politique de développement des compétences dynamique saura te séduire avec un programme de suivi de carrière sur-mesure.
Le contexte :
Pour l'un de nos clients, dans le secteur de l'énergie, nous sommes à la recherche d'un développeur Big Data.
Les missions :
Apporter une expertise
Big Data
pour faciliter la manipulation des données.
Définir les solutions techniques permettant le traitement massif des données.
Mettre en place des
solutions de stockage de données
(SQL, NoSQL etc.)
Veiller la sécurisation et la clarté des pipelines de données pour faciliter
l'analyse
et la
transformation
.
Assurer la création, la maintenance, l'optimisation et la sécurité des bases de données.
Assurer le support aux équipes de
développement
afin d'identifier et proposer des solutions performantes.
Profil (fiche métier)
De formation supérieure, tu justifies d'une expérience d'au moins
4 ans
dans le domaine.
Tu maitrises
Spark
,
Python
et
SQL
.
Tu es
autonome
,
rigoureux
et
force de proposition
.
De plus, tu as acquis une
capacité d'analyse
et de
synthèse
grâce à tes différentes expérience.
Tu maitrises également les fondamentaux de
l'agilité
.
Enfin, ton
esprit d'équipe
te permet de communiquer et de travailler dans les meilleures conditions.
NEXTON c’est aussi et surtout de nombreux moments de rencontres tout au long de l’année :
- Des communautés : 2 Meet Up par mois pour partager et échanger avec des experts
- De nombreux moments de rencontres professionnels et extra professionnels tout au long de l’année
- Des moments privilégiés avec ton manager
Prêts à nous rejoindre ? Rencontrons-nous !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Engineer Snowflake (confirmé/sénior) - H/F - CDI,Talan,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-snowflake-confirm%C3%A9-s%C3%A9nior-h-f-cdi-at-talan-3890985747?position=2&pageNum=37&refId=3scIT4h7LcxHkowem3t5kA%3D%3D&trackingId=7fDmrg36TdmiPiOJV45ifA%3D%3D&trk=public_jobs_jserp-result_search-card,"Talan est un cabinet de conseil en innovation et transformation par la technologie.
Depuis 20 ans, Talan conseille les entreprises et les administrations, les accompagne et met en œuvre leurs projets de transformation et d’innovation en France et à l'international. Présent sur cinq continents, le groupe prévoit de réaliser un chiffre d'affaires de 600 millions d'euros en 2022 pour plus de 6000 consultant·e·s et vise à dépasser la barre du milliard d’€ de CA à horizon 2024.
Le Groupe met l'innovation au cœur de son développement et intervient dans les domaines liés aux mutations technologiques des grands groupes, comme le Big Data, l'IoT, la Blockchain et l'Intelligence Artificielle.
Présent dans les événements incontournables du secteur, comme Viva Technology, Talan prend régulièrement la parole sur les enjeux de ces technologies révolutionnaires aux côtés d'acteurs majeurs du secteur et de parlementaires (Syntec Numérique, Forum de l'intelligence artificielle, French Fab Tour, Forum de Giverny…).
Talan est une entreprise responsable, attachée à la diversité. Des aménagements de poste peuvent être organisés pour tenir compte des personnes en situation de handicap.
Retrouvez nos engagementsRSEiciet nos actions en faveur de la diversitéici
Job Description
Nous sommes à la recherche d’un Big Data Engineer Snowflake qui sera en charge des modélisations data et de l’intégration des données: acquisition, préparation, modélisation et stockage, exposition, . Vous devrez faire preuve d’un état d’esprit à la fois innovant, méthodique, orienté solution (et non problème!), et communiquant.
Responsabilités
Analyse des besoins techniques métiers, participation à la définition des architectures solution SQL, développement et optimisation, code review, maintenir les pratiques Devops “You build IT, You run IT”, support à recette et mise en production, documentation,…
Modélisation de la Cloud Database Snowflake
Benchmark de solutions et conseil auprès de notre client sur les solutions technologiques à adopter, en lien avec leurs besoins
Partage de connaissances et formations interne
Qualifications
Issu d’une formation supérieure (école d’ingénieur, master…) avec une expérience dans le domaine du conseil (orienté satisfaction client et vision partenariale)
Vous disposez d’au moins 6 années d’expérience dans le domaine du SQL/ETL et ayant une expérience d’au moins 1 an sur Snowflake
Maîtrise du développement data (SQL, Python, …) et vous disposez de solides expériences dans la mise en place de pipeline de données
Maîtrise d’au moins une technique de modélisation: Star Schéma, DataVault, DataMesh,…
Expérience sur une plateforme Cloud (idéalement AWS)
Expérience sur des flux temps réel
La connaissance de concepts comme les suivants serait un +: DataOps, FinOps,..
Expérience de l’Agilité
Autonomie, organisation, sens du partage
Bonne communication
Orientation produit et solution
Additional Information
AVANTAGES
:
Plan de formation pour accompagner votre carrière (formations éditeurs, certifications) grâce à nos partenariats nous accordant une position de partenaire privilégié, et management de proximité par des experts
Locaux modernes en centre-ville
Top 5 du Palmarès Great Place to Work
Télétravail jusqu’à 5 jours selon les missions, prime d’équipement de 100€
Mobilité en France et à l’étranger
Top 1% des entreprises évaluées par Ecovadis dans le domaine social, environnemental et éthique
Tickets restaurant, prime vacances, 50% transport (abonnement transport public), mutuelle
Permanence handicap (consultant dédié aux collaborateurs en situation de handicap et aux proches aidants)
Actionnariat salarié
Prime de cooptations
RTT
L’équipe recrutement s’engage à vous proposer un processus de recrutement rapide et fluide :
Un premier échange de 30 min en visio avec le recruteur pour vous présenter le poste et comprendre votre projet professionnel
2 entretiens métier, dont au moins 1 dans nos locaux, pour entrer dans les détails du poste et rencontrer votre futur manager
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Organisation'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '20', '20', '20']}"
Data engineer - développeur C# H/F,Reboot Conseil,"Strasbourg, Grand Est, France",https://fr.linkedin.com/jobs/view/data-engineer-d%C3%A9veloppeur-c%23-h-f-at-reboot-conseil-3909657286?position=3&pageNum=37&refId=3scIT4h7LcxHkowem3t5kA%3D%3D&trackingId=Bu0VwR8Z3%2FIHjniwIutgYA%3D%3D&trk=public_jobs_jserp-result_search-card,"En 2020, dans un climat défavorable et démoralisant, nous avons décidé d’être cette jeune pousse porteuse d’espoir que l’on voit après un incendie ayant décimé toute une forêt, d’être ce vent nouveau qui veut faire évoluer les choses, d’être ce bouton Reboot qu’on presse pour corriger ce qui n’allait pas et repartir sur une bonne base. Voilà comment notre société est née.
Jour après jour, nous portons nos ambitions et nos rêves pour nous réaliser individuellement et collectivement avec un socle culturel atypique issu du livre « Reinventing organizations » dont découle des valeurs fortes de bien-être au travail, de partage, de liberté et de transparence.
Chez Reboot Conseil, nous ne nous contentons pas de fournir des prestations de conseil, nous créons de la valeur pour nos clients, que ce soit par des conseils en présentiel ou à distance, l’accompagnement de candidats en immersion dans les équipes de nos clients, la conception de projets innovants depuis notre centre d'expertise, le recrutement de talents exceptionnels ou même la commercialisation de nos propres produits.
Notre mission est de ne jamais s'ennuyer et de toujours repousser les limites de ce que nous pouvons réaliser. Rejoignez-nous pour une aventure passionnante où chaque membre est une source inépuisable de nouveautés.
Nous recherchons actuellement un/e Data Engineer - analyste développeur
Contexte_:
L'avancement des solutions informatiques permettant le traitement automatique du langage naturel et l'apprentissage par l'exemple ouvre de nouvelles perspectives pour améliorer le soutien aux collaborateurs et mieux servir les clients.
Le client a ainsi choisi d'investir considérablement dans le domaine de l'informatique cognitive, en étant parmi les premières en France à déployer la technologie. Plusieurs solutions dites ont été mises en place, comprenant des Assistants Virtuels (chatbots), des Analyseurs d'emails et des Assistants Vocaux téléphoniques.
Au sein du département dédié, dans un environnement innovant, vous aurez l'opportunité de rejoindre une équipe spécialisée dans l'exploitation des données et les architectures.
Cette équipe a pour mission de définir l'architecture applicative des solutions, de développer des outils d'analyse des données collectées et d'enrichir les processus métiers de l'entreprise.
Selon les projets en cours, les technologies utilisées incluent notamment C#, SQL Server, NOSQL, Kibana, Kafka et d'autres technologies émergentes. En tant que membre de cette équipe dynamique, vous serez impliqué dans diverses tâches liées à la gestion des données.
Vous serez amené à concevoir et rédiger des spécifications fonctionnelles, applicatives et techniques, à développer des applications intranet pour l'exploration et la restitution des données, à modéliser les données des solutions dans le système d'information décisionnel, à créer des tableaux de bord et à fournir un support aux autres équipes. Vous serez également chargé d'organiser la recette des solutions applicatives, de gérer les demandes d'évolution et de suivre la production, tout en travaillant en mode projet conformément aux normes de qualité en vigueur.
Profil
Titulaire d'une formation supérieure en informatique de niveau bac +4/5, vous avez acquis une expertise dans les ETL, les bases de données orientées Analytique, les solutions BI ainsi que dans les langages de programmation objet comme le C#.
Ce qui retiendra notre attention chez vous, c'est avant tout votre personnalité ! Nous recherchons des candidats curieux, ouverts aux innovations technologiques, prêts à relever des défis techniques tout en étant à l'écoute et rigoureux, car ces qualités vous permettront de mener à bien votre mission.
Vous disposez également de capacités d'analyse et de synthèse, vous attachez de l'importance au respect des normes et standards qualité, et vous êtes enthousiaste à l'idée d'apprendre de nouveaux outils et langages.
Vous appréciez la communication et le travail en équipe, mais vous savez également collaborer efficacement avec d'autres équipes.
Informations contractuelles
Poste en CDI basé à Strasbourg
Salaire: 32-42K€
Déplacements: non
Les avantages chez Reboot
Une prime de mobilité durable de 200€/an, 2 jours de télétravail/semaine, 2 charity days par an, un abonnement à Gymlib pris en charge par Reboot à 50%, une évolution salariale annuelle, automatique, une bonne mutuelle, abonnement transports en commun pris en charge à 50%, des BSPCE...
Soit au total, un Package Salarial de 3.5k€ en complément de votre salaire annuel brut.
➕des tech party days 1x/trimestre
➕un Hackathon 1x/an
Show more
Show less","{'ProgLanguage': ['C#', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Organization']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['32', '3.5k'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,Societe Generale,"La Défense, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-societe-generale-3889843774?position=4&pageNum=37&refId=3scIT4h7LcxHkowem3t5kA%3D%3D&trackingId=K%2BeDgWn%2FwNWyqx33YjAVSA%3D%3D&trk=public_jobs_jserp-result_search-card,"240006K4
Vos missions au quotidien
Vous souhaitez exercer votre talent et exprimer votre créativité au sein de notre équipe (Feature Team) en charge des services Big Data en tant que Data Engineer ? Rejoignez-nous !
En tant qu'alternant(e) Data Engineer, vous rejoindrez l'équipe Métrologie, en charge le service Big Data hébergeant les données techniques logs, métriques…du cloud privé de la Société Générale. Ces données sont exposées pour des utilisations diverses : monitoring, maintenance prédictive, capacity planning, améliorations des services...
Concrètement, vous serez amené(e), sous la supervision de votre tuteur et/ou votre manager, à :
Participer à l'évolution des services offerts par cette équipe en enrichissant son offre
Participer à la production du Cloud Privé Société Générale qui est au service des DSI métiers
Travailler sur des technologies Big Data très récentes, hébergées sur le Cloud Privé de la banque
Et si c’était vous ?
Vous préparez un Bac +4/5 en école de Commerce, d'Ingénieur ou Université avec une spécialisation en Développement, Informatique.
Vrai(e) team player, vous vous épanouissez dans un environnement collaboratif et favorisez la réussite de votre équipe
Vous avez des connaissances sur certaines des technologies suivantes : Python, Kafka, Hadoop, Bigdata, Elastic Search, Druid
Vous avez de bonnes bases autour de la CI/CD (Jenkins)
Rigoureux(se), autonome et organisé(e), vous avez un bon relationnel
You're fluent in english ! Vous êtes notre candidat(e) idéal(e) !
Pensez à accompagner votre CV de votre planning de formation
Plus qu’un poste, un tremplin
Dès votre arrivée, vous serez intégré dans nos équipes et apprendrez chaque jour aux côtés de nos experts qui vous accompagneront dans vos missions. Progressivement, vous gagnerez en autonomie sur vos projets pour faire de cette expérience un vrai accélérateur de carrière. Vous découvrirez également toute la diversité de nos métiers, dans un secteur qui évolue et innove en permanence.
A la fin de vos études, diverses opportunités pourront s’offrir à vous, en France et à l’international.
Pourquoi nous choisir ?
Attentif à votre qualité de vie et conditions de travail, vous bénéficiez d’avantages :
Prime* de participation et d’intéressement
Jours de télétravail (selon le rythme de votre service et celui de votre alternance)
Prise en charge de 50% de votre titre de transport
Billetterie à prix réduits de notre Comité d’Entreprise (concerts, cinéma, sport…).
Offre variée de restaurants d’entreprise et de cafétérias à tarifs compétitifs ainsi que des titres restaurants dématérialisés quand vous êtes en télétravail
Si vous avez 3 mois d’ancienneté sur l’exercice de référence
Créer, oser, innover, entreprendre font partie de notre ADN. Si vous aussi vous souhaitez être dans l’action, évoluer dans un environnement stimulant et bienveillant, vous sentir utile au quotidien et développer ou renforcer votre expertise, nous sommes faits pour nous rencontrer !
Vous hésitez encore ?
Sachez que nos collaborateurs peuvent s’engager quelques jours par an pour des actions de solidarité sur leur temps de travail : parrainer des personnes en difficulté dans leur orientation ou leur insertion professionnelle, participer à l’éducation financière de jeunes en apprentissage ou encore partager leurs compétences avec une association. Les formats d’engagement sont multiples.
Nous sommes un
employeur garantissant l'égalité des chances
et nous sommes fiers de faire de la diversité une force pour notre entreprise. Le groupe s’engage à reconnaître et à
promouvoir tous les talents
, quels que soient leurs croyances, âge, handicap, parentalité, origine ethnique, nationalité, identité de genre, orientation sexuelle, appartenance à une organisation politique, religieuse, syndicale ou à une minorité, ou toute autre caractéristique qui pourrait faire l’objet d’une discrimination.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Créativité', 'Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,MS Amlin,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-ms-amlin-3910914411?position=5&pageNum=37&refId=3scIT4h7LcxHkowem3t5kA%3D%3D&trackingId=k4225ovXcJ%2FeYbTYQOOjbQ%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for a highly skilled and motivated Data Engineer to join our dynamic team. As a Data Engineer you will play an important role in designing, building and maintaining the systems and architecture that enable MS AISE to collect, store and analyze large volumes of data. The ideal candidate is passionate about data, possesses strong analytical skills and has a proven track record of implementing robust and scalable data solutions in the cloud.
As an MS AISE Data Engineer you will reportin into MS AISE’s Data Engineering Team Manager and will work closely with the MS AISE’s Reporting & Analytics team as well as other key stakeholders with a strong interest in data, such as technical pricing, actuarial and finance.
Key Responsibilities
Data architecture design: design and implement scalable and robust data architectures to support MS AISE’s data needs. Collaborate with stakeholders to understand data requirements and translate them into technical specifications.
Data Integration: Develop and implement ETL (Extract, Transform, Load) processes to integrate data from various sources into a unified data lake. Ensure data integrity, accuracy and reconciliation throughout the integration process
Data Management: Manage and optimize database / datamart design including schema design, indexing and performance tuning as well as data security and access controls to ensure protection of sensitive data.
Data Modeling: Create and maintain data models to represent the structure and relationships within the data of MS AISE.
Data Pipeline Development: Build and maintain data pipelines to ensure automated collection, processing and storage of data. Monitor and troubleshoot data pipeline issues to ensure smooth and reliable data flow and availability.
Collaboration with cross-functional teams: Collaborate with the MS AISE Reporting & Analytics team and other key stakeholders to understand data requirements and contribute to the delivery of solutions that meet business objectives.
Documentation: contribute to creating and maintaining comprehensive documenatation for data engineering processes, workflows and data dictionaries.
Technological Evaluation: Ensure you stay informed about emerging technologies and evaluate their applicability to the organization’s data architecture and processes.
Key Skills / Experience
Minimum 3+ years of Experience as a Data Engineer
Strong Experience with relational data structures, theories, principles and practices
Strong hands-on experience with SQL Language, data transformation and modelling tools.
Proven experience with building Data Lake, Delta lake and large scale Data Warehouses
Strong experience in implementing large scale cloud data architecture using Azure, Synapse, Azure Databricks, Data Fabric etc
Proficient in Azure Data Factory, Azure Data Lake, Azure DevOps, Azure Databricks, Azure SQL Database, Stream Analytics. Knowledge of Microsoft Fabric is a strong plus
Good knowledge of tools associated with data ingestions & data transformations (e.g. SQL, Spark, Python)
Good knowledge or experience with Python Programming Language
Good knowledge or experience with other Cloud platforms is a strong plus (Google Cloud, Databricks, AWS, Snowflake, etc)
Knowledge of insurance market business applications & procedures and/or from an equivalent industry
Competencies
Strong analytical & conceptual mindset
Strong teamplayer
Ability to deliver from start to finish with minimal oversight
Ability to work in a structured and efficient manner
Ability to work in line with standard development framework & practices
Pragmatic, self-starter and solution oriented
Ability to identify and anticipate on problems early and communicate timely
Good communication skills
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Organization']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
"Data Engineer F/H - Système, réseaux, données (H/F)",HelloWork,"Rennes, Brittany, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-syst%C3%A8me-r%C3%A9seaux-donn%C3%A9es-h-f-at-hellowork-3900073980?position=6&pageNum=37&refId=3scIT4h7LcxHkowem3t5kA%3D%3D&trackingId=M433s5A%2B6fw3NFQES954wQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Cette offre d’emploi est fournie par Pôle emploi
Description
Descriptif du poste: Participez au développement d'hellowork.com au sein de l'équipe Data Ingé d'Antoine ! hellowork.com, c'est : * Plus de 4 millions de visites par mois * Plus de 500 000 candidats uniques par mois Vous évoluerez dans un environnement où la qualité, la performance et l'accessibilité sont au cœur de tous nos développements ! Et bien sûr, tout ça se déroule dans un cadre accueillant, motivant et bienveillant pour vous permettre de mener à bien la multitude de sujets actuels ou à imaginer pour faire avancer le produit ;) Plus concrètement, votre quotidien sera composé de : * Faire évoluer et maintenir l'infrastructure recommandation principalement en Python en collaboration avec l'équipe Data Science ; * Assurer la haute disponibilité de la donnée ; * Collaborer avec les équipes expertes et produit pour résoudre les problèmes, remettre en question et améliorer les pratiques de travail sur la donnée, les process, les architectures et les technologies ; La stack technique : * Python sur les services liés à la Data Science, NodeJs sur les services orientés API ; * Des services scalables déployés sur Kubernetes via une CI Gitlab aux petits oignons créée en collaboration avec l'équipe DevOps ; * GPU ou CPU ? Il y a qu'à tester ! * Du monitoring clé en main Prometheus / Grafana / Log Elastics ; * Des challenges sur des DB variées plutôt NoSQL : Elastics, MongoDB, Redis. * Broker de message via Azure Event Hubs Profil recherché: Pour être très simple, voici les points importants pour nous dans le profil recherché : * Vous avez une expertise sur le langage Python et êtes prêt à utiliser d'autres langages (principalement NodeJs). * Vous êtes curieux et passionné par les nouvelles technologies ; * Vous justifiez de minimum 2 ans d'expérience sur un poste similaire. Une expertise Python est un vrai plus mais nous cherchons avant tout quelqu'un sachant s'adapter ! * Vous avez un esprit d'équipe, aimez collaborer avec des profils différents du vôtre, et êtes familier des méthodes agiles ; Si vous êtes intéressé, alors rencontrons nous ! Promis, on est très sympa. Et si vous hésitez à postuler parce que vous ne cochez pas toutes les cases, surtout venez nous en parler ! Nous favorisons la diversité et nous formons et accompagnons les personnes qui nous rejoignent tout au long de leur évolution.
PROFIL SOUHAITÉ
Expérience
Expérience exigée de 1 An(s)
Source: Pole emploi (https://www.pole-emploi.fr)
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', ' MongoDB'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data engineer SQL server / Dev SSIS / C# (H/F) 👨🏻💻,XRAYS TRADING,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-sql-server-dev-ssis-c%23-h-f-%F0%9F%91%A8%F0%9F%8F%BB%E2%80%8D%F0%9F%92%BB-at-xrays-trading-3813760880?position=7&pageNum=37&refId=3scIT4h7LcxHkowem3t5kA%3D%3D&trackingId=PpuTStCeaf%2BEecpbgYcOag%3D%3D&trk=public_jobs_jserp-result_search-card,"XRAYS TRADING
est depuis 19 ans la structure la plus
atypique
du monde de la bourse !
Nous faisons en sorte pour que ton quotidien soit bourré d'
adrénaline
et de
moments inoubliables
, le tout en t’aidant à construire ta
carrière
au sein d'un groupe de travail sans équivalent 👩🏼‍💻
Pour résumer : Tu vas intégrer une boite d’
ingénieurs
totalement barrés qui savent s’éclater pendant et après le boulot ! 😉
NOTRE BESOIN :
Participer aux projets liés aux Stress Tests Risques de Marchés en tant que data engineer et développeur (MS SQL, packages SSIS, cubes SSAS et composants C#)
Fournir des solutions pour des nouveaux besoins
Assurer le support et la maintenance de l'outil
Traitements ETL complexes (SSIS) pour fournir des données via des cubes OLAP (Multidimensionnel). Ecosystème d'outils en C#. Gestion du code dans Git/ BitBucket. Pipe de livraison Jenkins & XL Release.
Une connaissance des produits financiers et de mesures de risque de marché serait un plus sur ce poste ainsi qu'un Anglais opérationnel obligatoire.
TOI
:
Tu es diplômé(e) d'une belle école d'ingénieur.
Tu es éveillé(e) et tu communiques sans difficulté tant en Français qu'en Anglais.
Tu es passionné(e) par la finance de marché, tu es donc quelqu'un de curieux qui cherche à comprendre comment fonctionne notre écosystème.
Tu recherches une portée internationale dans tes attributions🌍
Ton tempérament te mène à sortir de ta zone de confort afin de gagner en autonomie tout en découvrant un immense périmètre dont on ne fait jamais le tour : la finance de marché !
NOTRE OFFRE :
Avec une expérience minimale de 5 ans, tu peux partir du principe où nous te proposons un salaire minimal de 3800 euros nets.
Ce salaire est évidement très bien revalorisé en fonction de ton diplôme mais surtout de ton expérience !
Nous te formerons à nos outils, métier et méthodes, nous sommes souples, ouverts et humains. Nous souhaitons t'accompagner dans ta montée en compétences.
Passe nous faire un check sur www.xrays.fr ! 🚀
Show more
Show less","{'ProgLanguage': ['C#', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['3800', '5'], 'Level': [], 'Experience': ['a', 'n', 's', '19', '19', '19']}"
Data Analyst H/F,Inetum,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-h-f-at-inetum-3843955768?position=8&pageNum=37&refId=3scIT4h7LcxHkowem3t5kA%3D%3D&trackingId=iHtnLky7lKLcRpf5Mbva9Q%3D%3D&trk=public_jobs_jserp-result_search-card,"Détail de l'offre
Informations générales
Entité de rattachement
Inetum est un leader européen des services numériques. Pour les entreprises, les acteurs publics et la société dans son ensemble, les 28 000 consultants et spécialistes du groupe visent chaque jour l'impact digital des solutions qui contribuent à la performance, à l'innovation et au bien commun.
Présent dans 19 pays au plus près des territoires, et avec ses grands partenaires éditeurs de logiciels, Inetum répond aux enjeux de la transformation digitale avec proximité et flexibilité.
Porté par son ambition de croissance et d'industrialisation, Inetum a généré en 2023 un chiffre d'affaires de 2,5 milliards d'€.
Pour répondre à un marché en croissance continue depuis plus de 30ans, Inetum a fait le choix délibéré de se recentrer sur 4 métiers afin de gagner en puissance et proposer des solutions sur mesure, adaptées aux besoins spécifiques de ses clients le conseil (Inetum Consulting), la gestion des infrastructures et applications à façon (Inetum Technologies), l'implémentation de progiciels (Inetum Solutions) et sa propre activité d'éditeur de logiciels (Inetum Software). Inetum a conclu des partenariats stratégiques avec 4 grands éditeurs mondiaux - Salesforce, ServiceNow, Microsoft et SAP et poursuit une stratégie d'acquisitions dédiée afin d'entrer dans le top 5 européen sur ces technologies et proposer la meilleure expertise à ses clients.
Tous nos postes sont ouverts aux personnes en situation de handicap.
Description du poste
Métier
Applications Delivery - Software Development
Intitulé du poste
Data Analyst H/F
Contrat
CDI
Description De La Mission
Dans le cadre de la croissance de notre agence lilloise, nous développons notre Practice Data et recrutons des profils Data de divers horizons Data Analyst, Data Engineer, Data Scientist et Data Gouv. Les besoins métiers de nos clients et la multitude des technologies font qu'il nous faut nous appuyer sur une diversité de compétences. Vous pourriez être l’un d’eux et rejoindre Inetum.
En tant que Data Analyst, vos principales missions consistent à
Analyser et retranscrire le besoin client
Identifier, extraire et exploiter les sources d'acquisition de données les plus pertinentes
Valoriser de la donnée
Développer l'outil de data visualisation pour accompagner les équipes métiers dans leurs aides à la décision
Être le lien entre les équipes métier pour les accompagner dans la mise en œuvre des nouveaux outils
Profil
Pour mener à bien votre rôle, il vous faut
parler SQL couramment
un niveau avancé sur Excel et/ou Google Spreadsheet
une maîtrise d'un outil décisionnel comme PowerBI, Qlik, Tableau ou encore Google Data Studio
Vous vous reconnaissez dans ces quelques lignes, alors rencontrons-nous !
Notre plus
Rejoindre la région Nord-Est, c’est bénéficier des avantages d’un Grand Groupe tout en gardant la proximité régionale.
Nous mettrons tout en œuvre pour vous apporter un équilibre vie perso / vie pro. C’est pourquoi nous vous proposons un rythme hybride (selon les contraintes clients)
Une trajectoire de carrière personnalisée et adaptée à vos souhaits d'évolution grâce à une implantation à l’international (26 pays, 7 Fablab), des formations ciblées et des projets couvrant l’ensemble de la chaîne de valeur IT (+25 filières métiers)
Intégrer un collectif d’experts partageant des valeurs de solidarité et d’excellence
Intégrer une entreprise ayant une stratégie affirmée de certifications de ses collaborateurs
Localisation du poste
Localisation du poste
France, Nord, 59 Nord
Ville
Lille
Critères candidat
Niveau d'études min. requis
Bac+5
Niveau d'expérience min. requis
Plus de 2 ans
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': ['Hybride'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '30', '30', '30']}"
Data engineer,Societe Generale,"La Défense, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-societe-generale-3918754036?position=10&pageNum=37&refId=3scIT4h7LcxHkowem3t5kA%3D%3D&trackingId=C%2Fki0HYve2YtGbtwssrtHQ%3D%3D&trk=public_jobs_jserp-result_search-card,"24000COX
Vos missions au quotidien
Nous recherchons un(e) alternant(e) en Data Engineering dans le cadre d'un de nos projets au sein de notre direction informatique Corporate Functions Technologies.
Vous contribuerez à l’exécution d'un chantier majeur de la chaîne Credit Risk.
Concrètement, vous serez amené(e) à :
Concevoir des solutions pour collecter, nettoyer, organiser et synthétiser de gros volumes de données (pour alimenter bases de données, datalakes et projets Big Data) ;
Co-élaborer le design technique du produit délivré ;
Apporter votre soutien sur les développements de votre équipe ;
Développer, entretenir et utiliser votre expertise sur la stack Scala, Spark, Hadoop ;
Participer à l’industrialisation du procédé pour les données les plus pertinentes dans le cadre du projet pour produire les analyses les plus opérationnelles, assurer une veille technologique et démontrer un intérêt pour le domaine bancaire.
Et si c’était vous ?
Vous êtes un(e) étudiant(e) de niveau Bac+4/5 en Université, école de Commerce, d’Ingénieur, avec une spécialisation en Big Data.
Vous maitrisez les technologies Big Data, en particulier l'écosystème Hadoop.
Vous possédez de solides connaissances en Spark et Scala.
Vous êtes attaché(e) au respect des bonnes pratiques de développement.
You're fluent in english ! Vous êtes notre candidat(e) idéal(e) !
Pensez à accompagner votre CV de votre planning de formation!
Plus qu’un poste, un tremplin
Dès votre arrivée, vous serez intégré dans nos équipes et apprendrez chaque jour aux côtés de nos experts qui vous accompagneront dans vos missions.
Progressivement, vous gagnerez en autonomie sur vos projets pour faire de cette expérience un vrai accélérateur de carrière. Vous découvrirez également toute la diversité de nos métiers, dans un secteur qui évolue et innove en permanence.
A la fin de vos études ou de votre VIE, diverses opportunités pourront s’offrir à vous, en France et à l’international.
Pourquoi nous choisir ?
Attentif à votre qualité de vie et conditions de travail, vous bénéficiez d’avantages :
Prime de participation et d’intéressement
Jours de télétravail (selon le rythme de votre service et celui de votre alternance)
Prise en charge de 50% de votre titre de transport
Billetterie à prix réduits de notre Comité d’Entreprise (concerts, cinéma, sport…).
Offre variée de restaurants d’entreprise et de cafétérias à tarifs compétitifs ainsi que des titres restaurants dématérialisés quand vous êtes en télétravail
Créer, oser, innover, entreprendre font partie de notre ADN. Si vous aussi vous souhaitez être dans l’action, évoluer dans un environnement stimulant et bienveillant, vous sentir utile au quotidien et développer ou renforcer votre expertise, nous sommes faits pour nous rencontrer !
Vous hésitez encore ?
Sachez que nos collaborateurs peuvent s’engager quelques jours par an pour des actions de solidarité sur leur temps de travail : parrainer des personnes en difficulté dans leur orientation ou leur insertion professionnelle, participer à l’éducation financière de jeunes en apprentissage ou encore partager leurs compétences avec une association. Les formats d’engagement sont multiples.
Nous sommes un
employeur garantissant l'égalité des chances
et nous sommes fiers de faire de la diversité une force pour notre entreprise. Le groupe s’engage à reconnaître et à
promouvoir tous les talents
, quels que soient leurs croyances, âge, handicap, parentalité, origine ethnique, nationalité, identité de genre, orientation sexuelle, appartenance à une organisation politique, religieuse, syndicale ou à une minorité, ou toute autre caractéristique qui pourrait faire l’objet d’une discrimination.
Show more
Show less","{'ProgLanguage': ['Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data engineer GCP Sénior,Apside,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-gcp-s%C3%A9nior-at-apside-3825028887?position=1&pageNum=40&refId=PlhdJ%2BvRBuQPyL9EJS3cmw%3D%3D&trackingId=OFv5GAwoVnXsBllV5IoOZA%3D%3D&trk=public_jobs_jserp-result_search-card,"💥
Découvrez la Vie Apsidienne
📹
et vous aussi, devenez Apsidien
On aurait pu demander à Chat GPT de vous démontrer en quoi
Apside est l’ESN qu’il vous faut,
mais on préfère que vous le découvriez vous-mêmes 👇😏
🔥
Découvrez votre future mission
👉
Contexte
Rejoignez notre Practise Cloud/Data, afin d’intervenir sur des sujets à haute valeur ajoutée !
Secteur
: Télécom
Méthode de travail
: Agile Safe
Notre client a besoin d’un accompagnement sur leurs projets métiers
Data/IA
accostant sur un cloud public et à la construction d'outils pour accélérer et faciliter cet accostage.
Cela sera réalisé dans un environnement
GCP
et en grande majorité sur des
technologies innovantes
pour des services Data & IA. La mission sera partagé entre le ""build"" des cas d'usage et outils, et le ""run"" de ces derniers.
😎 Mission
Etude et définition des architectures GCP, ainsi que leur implémentation
Mise en application des exigences opérationnelles (sécurité, exploitabilité et industrialisation)
Aiguillage sur nos outils transverse et préconisations à l'usage du cloud public
Construction d'outillages facilitant l'accostage de ces des projets métiers DATA-IA
…
Environnement technique
:
GCP
Git
Gitlab
Bash
Docker
Kubernetes
GitlabCI
💰
Le package salarial que nous vous proposons
Contrat :
CDI
Avantages groupe :
carte ticket restaurant Swile, prime de mobilité, RTT, accord télétravail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle à 100% etc…
Avantages agence :
intégration de la Practise Cloud/Data, afterworks, communauté techlead
Formation :
certifications techniques, cours particuliers d’anglais en interne, accès à un catalogue de formations grâce à notre plateforme e-learning (
Academy by Apside
) ou via nos organismes partenaires.
🔮
Ô vous futur Apsidien, qui êtes-vous ?
Au moins 5 ans d'expérience en tant que Data Engineer
Maitrise de l’environnement cloud GCP
Force de proposition, bon relationnel et autonome
😏
Apside a suscité votre curiosité ?
Dans un environnement marqué par une accélération des évolutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients à créer de la valeur et à adresser leurs enjeux stratégiques en leur mettant à disposition des expertises technologiques (
Data / IA, Cloud, Cyber
) et une expérience sectorielle (
Industrie, Banque, Assurance, Service, Secteur Public
). Pour un accompagnement global, le groupe propose des offres transverses autour du
Handicap
(Apsid’EA), du
Digital Learning
, et du
Conseil
.
🤔
Et votre place dans tout ça ?
👉 Notre volonté
est de vous accompagner dans la construction et l’épanouissement de votre carrière
en nous appuyant notamment
sur 3 piliers :
Une
rémunération
à hauteur de vos investissements et de vos compétences
Une
trajectoire professionnelle
stimulante sur mesure
Un
engagement
autour des valeurs Apsidiennes : la qualité de vie et des conditions de travail au cœur de nos enjeux
Engagée pour
un monde plus inclusif et plus responsable
, Apside réinvente l’ESN et propose l’Engagement Sociétal et Numérique. Découvrez notre démarche RSE ainsi que notre vision de l’Entreprise Engagée.
Convaincu ? A vous de jouer, envoyez-nous votre CV !
Rejoignez l’aventure Apsidienne et découvrez notre vision d’une ESN singulière et résiliente
🚀
Show more
Show less","{'ProgLanguage': ['R', 'Bash'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
"Big Data Engineer Confirmé – Lille, France (H/F)",Astek,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/big-data-engineer-confirm%C3%A9-%E2%80%93-lille-france-h-f-at-astek-3839095323?position=2&pageNum=40&refId=PlhdJ%2BvRBuQPyL9EJS3cmw%3D%3D&trackingId=nK8OX75hvZYpbs8abogtpA%3D%3D&trk=public_jobs_jserp-result_search-card,"CDI
Lille - France
Publiée il y a 2 mois
Le Groupe Astek
Ce Que Nous Allons Accomplir Ensemble :
Nous rejoindre en tant que
Big Data Engineer Confirmé (H/F),
afin d’accompagner un opérateur télécoms, Leader en Europe dans l’assistance et le support applicatif de niveau 3 (résolution des problèmes utilisateurs, exploitation des environnements hors production).
Un challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !
Votre Mission, Si Vous L’acceptez :
Supervision et détection et résolution des problèmes utilisateurs (développeurs, exploitants et data exploreurs)
Développement de solutions de self-service ou d’une solution de résolutions automatiques des problèmes
Qualifier les données et les résultats
Conception technique des solutions
Assurer l’accompagnement et le déploiement des évolutions des processus et outils
Accompagner la phase de mise en production
Votre Future Équipe :
Vous intégrerez une équipe à la fois technique et fonctionnel, qui œuvre chaque jour pour développer et maintenir en conditions opérationnelles l’ensemble des solutions IT !
L’équipe est en interaction avec des clients à la fois internes et externes.
Votre stack de jeu
Système d’exploitation : Linux
Outils des distributions : HDP, HDF, ELK
Environnement Big data : Hadoop, Spark,
Langage : Scala, Shell, Python
Cloud computing : GCP ou AWS
Base de données : No SQL (Cassandra, Mongo DB), Shell, Ansible
Dataviz : Power BI ou Kibana
Des notions en Réseau et Systèmes feront la différence !
Les Petits Plus Du Projet :
Vous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.
Vous ?
Diplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.
Vous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?
Vous faite preuve de proactivité et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?
Alors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !
Astek
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de
ses 7800 collaborateurs
qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.
Tous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.
Rencontrons-nous
Notre projet commun vous plait ?
Postulez à cette annonce, et soyez transparent !
Maud, notre Talent Acquisition Referent, vous contactera pour un premier échange.
Puis vous rencontrerez Martin, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .
Enfin, vous rencontrerez Jérémy, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.
Nos Plus
Astek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo
Une politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)
Notre charte de la Diversité
Mots-clés :
ingénieur – ingénieure – consultant – consultante – Hadoop – Scala – Data
Caractéristiques de l'emploi
Catégorie Ingénieur
Job Industry Télécom / Média
Postuler en ligne
Nom *
Prénom *
Email *
Un email valide est requis.
Téléphone *
Un numéro de téléphone valide est requis.
Joindre un CV *
Mots-clés :
ingénieur – ingénieure – consultant – consultante – Hadoop – Scala – Data
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': ['Confirmé'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
"Data Engineer – Bordeaux, France (H/F)",Astek,"Bordeaux, Nouvelle-Aquitaine, France",https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-bordeaux-france-h-f-at-astek-3839091989?position=3&pageNum=40&refId=PlhdJ%2BvRBuQPyL9EJS3cmw%3D%3D&trackingId=y4Kk%2Bf4%2BPQLFNzvLUNrwvw%3D%3D&trk=public_jobs_jserp-result_search-card,"CDI
Bordeaux - France
Publiée il y a 2 mois
Le Groupe Astek
Ce Que Nous Allons Accomplir Ensemble :
Vous êtes passionné par la Data ?
Vous avez à cœur d’aider les entreprises à mieux gérer leurs données ?
Vous souhaitez contribuer à la croissance et à la réussite de nos projets clients ?
Nous aussi !
Rejoignez-nous et intervenez sur toute la chaîne de traitement de la donnée dans un environnement grand compte : de la récupération, en passant par le traitement, pour en faire l’affichage et l’analyse.
Votre Mission, Si Vous L’acceptez :
Réalisation d’opérations de collecte de données
Concevoir et développer des pipelines de données réutilisables pour collecter, nettoyer, extraire et transformer les données.
Industrialisation et optimisation des jobs data
Réalisation de contrôles qualité tout au long des opérations,
Réalisation d’opérations d’intégration de données et de traitements de masse
Collaborer avec les équipes multidisciplinaires pour intégrer les solutions de données dans les applications existantes.
Participer à la veille technologique et recommander des solutions innovantes pour améliorer nos pratiques de data engineerin g.
Votre Future Équipe :
Au sein d’une équipe composée de 4 Data Engineers Astek, vous rejoindrez les équipes métiers et techniques de notre client afin de l’accompagner dans le traitement de ses données.
Votre stack de jeu
Langages : Python, Java
Analyse de données : Talend, Power BI, Qlik
BDD : SQL, PostgreSQL, MongoDB, Oracle
Les Petits Plus Du Projet :
Vous évoluerez dans un contexte data innovant dans lequel vous progresserez tout en étant accompagné sur les nouveaux outils.
Vous ?
De formation Ingénieur ou équivalent (Bac+5), vous avez une première expérience dans un contexte autour de la Data et vous avez pour objectif de continuer votre montée en compétences.
Vous aimez développer en Python et êtes intéressé par les sujets d’IA générative.
Ce projet n’est pas le seul que nous pourrions vous proposer. Prenons le temps d’échanger afin de vous présenter les sujets les plus adaptés à vos ambitions.
Astek
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de
ses 7800 collaborateurs
qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.
Tous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.
Rencontrons-nous
Notre projet commun vous plait ?
Postulez à cette annonce, et soyez transparent !
Mathilde, notre Talent Acquisition Officer, vous contactera pour un premier échange téléphonique.
Puis vous rencontrerez Thomas, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission.
Enfin, vous rencontrerez Guillaume, notre Directeur de département, avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.
Nos Plus
Astek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo
Une politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)
Notre charte de la Diversité
Caractéristiques de l'emploi
Catégorie Ingénieur
Job Industry Aérospatial / Défense / Sécurité, Distribution / Services Internet, Energie / Sciences de la Vie / Autres industries, Finance / Gouvernement, Télécom / Média, Transports Terrestres
Postuler en ligne
Nom *
Prénom *
Email *
Un email valide est requis.
Téléphone *
Un numéro de téléphone valide est requis.
Joindre un CV *
Les Petits Plus Du Projet :
Vous évoluerez dans un contexte data innovant dans lequel vous progresserez tout en étant accompagné sur les nouveaux outils.
Vous ?
De formation Ingénieur ou équivalent (Bac+5), vous avez une première expérience dans un contexte autour de la Data et vous avez pour objectif de continuer votre montée en compétences.
Vous aimez développer en Python et êtes intéressé par les sujets d’IA générative.
Ce projet n’est pas le seul que nous pourrions vous proposer. Prenons le temps d’échanger afin de vous présenter les sujets les plus adaptés à vos ambitions.
Astek
Créé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.
Depuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de
ses 7800 collaborateurs
qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.
Rejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.
Tous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.
Rencontrons-nous
Notre projet commun vous plait ?
Postulez à cette annonce, et soyez transparent !
Mathilde, notre Talent Acquisition Officer, vous contactera pour un premier échange téléphonique.
Puis vous rencontrerez Thomas, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission.
Enfin, vous rencontrerez Guillaume, notre Directeur de département, avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.
Nos Plus
Astek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo
Une politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)
Notre charte de la Diversité
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL', ' MongoDB'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['PostgreSQL', 'Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
Alternant - Data Engineer H/F,ALLIANCE EMPLOI,"La Couture, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/alternant-data-engineer-h-f-at-alliance-emploi-3913991795?position=4&pageNum=40&refId=PlhdJ%2BvRBuQPyL9EJS3cmw%3D%3D&trackingId=D4Pqp4d97XFIilzjFbjMyQ%3D%3D&trk=public_jobs_jserp-result_search-card,"La campagne "" alternance2024 "" est lancée ! Êtes-vous prêt(e) à monter en compétences et acquérir de l'expérience ? Avec 25 ans d'expertise, 2000 salariés et notre réseau de 400 entreprises issues des secteurs industriel, agroalimentaire, automobile, métallurgie, pharmaceutique, sidérurgie ou encore logistique, nous sommes la destination idéale pour celles et ceux qui cherchent une alternance.
Lauréat 2021 des Pépites de l'alternance, notre mission est simple : apporter la bonne compétence au bon moment. Et c'est là que vous entrez en jeu !
Nous sommes à la recherche d'un Alternant - Data Engineer (H/F) en alternance pour une entreprise partenaire spécialisée dans la chimie pour une durée de 12 à 24 mois au sein du Utilités qui a en charge la fourniture d'utilités pour l'ensemble du site.
Le poste et les missions ?
Informatique
Modernisation des pratiques de collecte de données industrielles,
Création d'interface saisie d'encours de production
Refonte des rapports d'exploitation
Algorithme de numérisation des rapports PDF
Automatisation des archivages
Gestion des datas consolidées
Statisitques
Intégration des statistiques au coeur des systèmes de production
Cartes de contrôles
Prévisions statistiques
Etude d'implantation de machine learning
Algorithme de traitement et nettoyage des données (analyse de la dérive)
Organisationnel
Déploiement des solutions de power BI au sein du service
Outils d'affiche, de partage et d'analyse de données
Analyse fonctionnelle des flux de données
Rédaction des logigrammes de gestion de données
Rédaction des bonnes pratiques d'archivage et de traitement de donénes
Formation des utilisateurs et propriétaires
Ce que nous allons aimer chez vous ?.
Vous avez le sens de l'organisation et du service, une capacité à s'intégrer dans un milieu technique de terrain, vous avez le sens de l'organisation et des priorités, vous êtes rigoureux
Mais aussi :
Vous préparez un diplôme d'ingénieur Bac +4 / 5 en informatique et analyse de données
Vous maîtrisez le développement informatique (base de données, Python, Java)
Les avantages de rejoindre Alliance Emploi
Contrat : ALTERNANCE de 12 à 24 mois
Début : Septembre 2024
Lieu : LESTREM [ site non accessible en transport en commun]
Rémunération : Selon le barème de l'alternance
Et aussi : intéressement, mutuelle, prévoyance, CSE, formations qualifiantes
Et ce n'est pas tout ! En choisissant Alliance Emploi, vous vivrez une expérience basée sur la confiance, la solidarité et l'engagement. Vous développerez vos compétences à travers une diversité de missions et notre réseau d'entreprises, et nous nous engageons à vous proposer un accompagnement personnalisé pour booster votre carrière.
Alors convaincu(e) ?
N'attendez plus pour postuler et venez découvrir la différence Alliance Emploi !
La diversité est une force. Nous sommes engagés pour l'inclusion en offrant des opportunités de carrière à toutes les personnes, indépendamment de leur genre ou de leur situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Statistiques'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '25', '25', '25']}"
Data engineer & BI (IT) / Freelance,Free-Work,"Gennevilliers, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-bi-it-freelance-at-free-work-3891617843?position=5&pageNum=40&refId=PlhdJ%2BvRBuQPyL9EJS3cmw%3D%3D&trackingId=ofPaeu5UIMHyncbiJE4gfQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Data engineering : mise à disposition des données dans le datalake au format
delta lake (Azure Databricks)
▪ Conception des flux data
▪ Développement des pipelines de données avec ADF et
Databricks (ingestion Bronze/ Implémentation Silver)
▪ Mise en œuvre avec le superviseur des flux du monitoring de ceux-ci
▪ Intégration des tests unitaires
Analyse & BI :
▪ Analyse des besoins
▪ Conception, élaboration et déploiement des rapports et tableaux de bord
sous Power BI à partir du Gold
▪ Modélisation des datamarts
▪ Analyses de données et Requêtages
▪ Maintenance évolutive et corrective
Coordination des intervenants SI
Rédaction de spécifications & Documentation
Mise en place des exigences RGPD/SSI
Profil candidat:
Profil confirmé Technico-Fonctionnel :
Incontournable :
Expérience de mise en œuvre de data platform Databricks & de projets BI
Connaissances et expériences sur Azure Data Factory et Azure Databricks
SQL (niveau confirmé)
Power BI, DAX
Force de proposition
Rigoureux et autonome
Appréciables :
SQL Server SSIS, SSRS
Dev Python
Git Azure Devops
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Alternance Data Engineer (H / F),Servier,"Suresnes, Île-de-France, France",https://fr.linkedin.com/jobs/view/alternance-data-engineer-h-f-at-servier-3906489264?position=8&pageNum=40&refId=PlhdJ%2BvRBuQPyL9EJS3cmw%3D%3D&trackingId=gFDTjOYYRhDTsVSpW5mpfw%3D%3D&trk=public_jobs_jserp-result_search-card,"Description du poste :
Le Groupe Servier mène une transformation digitale ambitieuse avec un objectif clair de devenir « best in class Digital Performer ». La Data Factory joue un rôle central dans cette ambition et a un impact fort à chaque étape de la chaine de valeur. Depuis la recherche de nouvelles molécules, la prédiction de leur comportement, l’analyse d’efficacité des traitements en passant par l’optimisation des processus de production des médicaments, la gestion des stocks et la prévision des ruptures jusqu’à l’approche omnicanale et personnalisée auprès des professionnels de santé, le suivi des patients… : tous ces enjeux s’appuient sur la Data et sa puissance transformante.
Rattachée à la Direction Digital, Data & IS, la Data Factory œuvre pour rendre les données accessibles, les valoriser à travers des produits data métiers à base d’IA et d’Advanced Analytics, et transformer Servier en un groupe orientée data (« Data-driven ») où tous les collaborateurs connaissent les enjeux de la data.
La création des produits Data s’appuie sur une « plateforme Data » centrale, cloud-native, sécurisée et performante avec les technologies les plus avancées. Servier a établi un partenariat stratégique de cinq années avec Google Cloud, lui donnant accès à des technologies innovantes et des liens privilégiés avec ses experts, et permettant de disposer d’une puissance de calcul augmentée, d’accélérer l’analyse et de développer l’innovation sur de nombreux défis business et technologiques.
Au sein de la Data Factory, et en lien avec les autres pôles de la Data Factory, le Chapter TechLead a pour mission de pourvoir en ingénieur les différentes équipes produits et les équipes enablers afin de développer les data pipeline (ingestion, Exposition), outils, software nécessaire au produit. Les ingénieurs sont aussi responsables de tester, de documenter le code ainsi que le maintien en condition opérationnel de ce code.
Afin d’accompagner le Global Data Office, vous serez chargé(e) de :
Ecrire le code nécessaire au produit
Tester et documenter le code
Déployer le code dans les différents environnements
Faire de la veille autour des enjeux et opportunités de la Data dans un groupe pharmaceutique,
Contribuer à votre façon à la transformation digitale du groupe, et participer à la construction dynamique de la Data Factory
Description du profil :
Votre formation :
Ecole d’ingénieur en informatique ou/et data science
Vos compétences :
Maitrise des langages de programmation python et SQL
Curiosité pour les nouvelles technologies et l’innovation au sens large
Maîtrise des outils collaboratifs et de la suite office (Word, Excel, PowerPoint, et SharePoint)
Rigueur, votre sens du détail, capacités d'analyse et de synthèse, autonomie et bon relationnel
Excellentes aptitudes interpersonnelles, écoute, empathie, assertivité, esprit d’équipe
Indépendance, organisation et capacité à gérer plusieurs sujets en même temps
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Empathie', 'Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Staff Data Ops Engineer (x/f/m),Doctolib,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/staff-data-ops-engineer-x-f-m-at-doctolib-3824234041?position=9&pageNum=40&refId=PlhdJ%2BvRBuQPyL9EJS3cmw%3D%3D&trackingId=RX46B7dYVz9kKnrZFyVlog%3D%3D&trk=public_jobs_jserp-result_search-card,"About Doctolib
Doctolib is a purpose-led company that strives for a healthier world with more than 2,800 employees across France, Italy, and Germany. Since 2013, Doctolib has been improving the daily lives of more than 340,000 healthcare professionals by providing them with new-generation technology and services. Doctolib also serves more than 80 million Europeans, offering a fast, frictionless and secure journey for all their care needs.
We are currently looking for a Data Ops to help us build and develop the data architecture that will enable Doctolib to offer its users an ever more efficient service.
An important point at Doctolib: personal data is fully encrypted and anonymized, and security requirements are extreme.
Your tasks
Build and maintain Doctolib's data architecture;
Guarantee the maintainability and scalability of the various components, relying on the services of Doctolib's cloud provider and all Devops best practices;
Adapt the technical stack to anticipate user needs and the limits of existing systems;
Evaluate new market technologies and their potential within the stack;
Collaborate with Data Engineers, Data Scientists, Security teams and developers to develop future Data-driven functionalities within Doctolib products;
Participate in team improvement and agile rituals.
Who you are :
If you don’t meet all the requirements below but believe this opportunity matches your expectations and experience, we still encourage you to apply!
You have significant experience in the Data world, including at least 3 years in Data Ops / DevOps.
You have an interest in Big Data technologies
You're familiar with Terraform and Docker
You have an interest in code and Python
You have a good knowledge of data transformation tools and AWS services
You like to innovate and make Data architectures evolve towards greater scalability
What we offer
A stock-option program for each Doctoliber
A competitive health insurance paid 100% by the company
A dedicated onboarding program - the Doctolib Academy
Mental health and wellbeing offer in partnership with moka.care
The Doctolib Parent Care Program, including extended parental leave, meet-ups and inspiring conferences
A subsidy from the work council to refund part of the membership to a sport club or a creative class
Subsidy for lunch and various food offers in our offices
A flexible workplace policy offering both hybrid and office-based mode
Flexibility days allowing to work in EU countries and the UK 10 days per year
The interview process
Recruiter Interview
Case study to do at home + technical interview with the team
Team Meeting
Reference check
Offer !
If you would like to find out more about tech life at Doctolib, feel free to read our latest Medium blog articles!
At Doctolib, we believe in improving access to healthcare for everyone - regardless of where you come from, what you look like. This translates into our recruitment process: Doctolib is an equal opportunity employer. We don't just accept diversity at Doctolib, we respect and celebrate it!
The more diverse ideas are heard, the more our product will truly improve healthcare for all. You are welcome to apply to Doctolib, regardless of your gender, religion, age, sexual orientation, ethnicity, disability, or place of origin. If you have a disability, let us know if there's any way we can make the interview process smoother for you!
All the information transmitted via this form is processed by Doctolib for the purpose of managing applications. For more information on how Doctolib processes your application data, click
here
.
If you wish to exercise your rights or if you have any questions about the processing of your data, you can write to us at
hr.dataprivacy@doctolib.com
.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Flexibility']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer - H/F,MARGO,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-margo-3816671828?position=10&pageNum=40&refId=PlhdJ%2BvRBuQPyL9EJS3cmw%3D%3D&trackingId=dizlXreGGcB%2BlipDGTUbAg%3D%3D&trk=public_jobs_jserp-result_search-card,"Entité experte
de Margo Group des problématiques
Data, Cloud et DevOps
créée en 2020 par leurs fondateurs Raphaël et Mounir. Aujourd’hui
60 consultants
ont intégré l'entité et nous avons commencé à travailler avec
18 nouveaux clients
(Banque, Industrie, Assurance, Énergie, E commerce, Santé). A leurs côtés, vous pourrez évoluer rapidement et développer de nouvelles compétences.
Deux ADN fondateurs forts
et spécifiques à Margo Analytics à l’origine de l’entité :
- Toujours se positionner sur
les plus beaux sujets
et sur les
missions à fortes valeurs ajoutées
- Recruter
des
consultants passionnés
et
curieux
qui cherchent à être
challengés
Aujourd’hui, Margo Analytics possède 4 communautés
de compétences :
- Data engineer
- Data Science/ IA
- Galaxy OPS (devOps, dataOps, cloudOps)
- Architecte Big Data
Ainsi en rejoignant Margo Analytics vous aurez le choix des missions (#consultantfirst) sur lesquelles vous souhaitez travailler. Vous serez accompagné par les deux fondateurs ainsi que par le leader de votre communauté, dont les rôles sont de rechercher le projet qui correspondra le plus à vos attentes et de vous accompagner dans votre carrière.
🎯Les missions Margo Analytics :
Au sein de la communauté Data Engineer vos missions
seront
:
-
Développer en mode agile
les cas d’usages métier
- Mettre en place des
processus de collecte, d’organisation, de stockage et de modélisation des données
- Développer des traitements de transformation et de production de données
- Assurer la
mise en production des modèles de prédiction
créés par les Data Scientists
- Participer à l’
amélioration continue
et au refactoring de code
Besoin de projection ? Voici un exemple de mission :
Camille accompagne un grand compte dans le domaine de l’industrie sur son projet de mise en place d’un nouveau datalake en Azure databricks. L’objectif de cette mission est d’assurer la distribution de la donnée de manière optimisée pour créer une couche de distribution et permettre aux Data Scientists d’implémenter les use cases. Camille apporte son expertise sur les technologies suivantes : Spark, Scala, Azure, Databricks.
Nos stack Technique :
- Langage : Python/Scala/Java
- Framework : Spark/Hadoop
- Cloud: Azure/ AWS/ GCP
🙌 Les avantages :
- Tickets restaurants Swile
- Mutuelle Alan prise en charge à 100%
- Pass Navigo pris en charge à 100%
- Télétravail
- Formations illimitées
- Locaux en plein coeur de Paris
- Places en crèches
🤝Notre processus de recrutement :
Notre processus de recrutement se fait en 3 étapes, réparties sur 7 à 15 jours maximum :
- Première rencontre !
Vous échangez avec un RH et un dirigeant sur votre parcours, vos aspirations professionnelles ainsi que sur Margo Analytics et les opportunités que nous proposons
-
Challengez-vous
dans le cadre d’un entretien technique avec l’un de nos experts. C’est également l’occasion pour vous d’avoir son retour d’expérience
- Dernier entretien de motivation
: pour finir, vous rencontrez un membre du board de Margo Analytics pour un entretien final
🔍 Vous êtes un(e) futur(e) Margo Analytics si :
Must-Have
Vous êtes issu(e) d’une école d’ingénieur ou d’un cursus universitaire équivalent niveau Bac + 5
/ Master
Vous aimez coder et vous êtes passionné(e) d’informatique et de Data
Vous êtes curieux(se) et vous vous intéressez aux dernières technologies du marché
Vous justifiez d’une première expérience en tant que Data Engineer
Nice to Have
Vous êtes ambitieux(se) et n’avez pas peur de travailler sur des projets challengeants dans des environnements à fortes contraintes techniques . Vous parlez et comprenez l’anglais.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Backend Data Engineer | Python - AWS | Plateforme SaaS BtoB - Culture et sport | Paris (1 jour sur site/semaine)),Octopus IT - Expert du recrutement tech,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/backend-data-engineer-python-aws-plateforme-saas-btob-culture-et-sport-paris-1-jour-sur-site-semaine-at-octopus-it-expert-du-recrutement-tech-3837199239?position=1&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=rw9KnhS4AN95GIKeUl9F8g%3D%3D&trk=public_jobs_jserp-result_search-card,"La société
Cette Start-up fondée par une équipe de Data Scientists, a développé un logiciel SaaS BtoB au service des différentes structures culturelles et sportives.
Les clients (théâtres, musées, clubs de sport professionnels, salles de spectacles,...)
utilisent la solution afin de mieux comprendre et valoriser leurs donnés et de développer leurs publics de demain.
Novateur, cette solution complète, centralise et automatise les données venant de la billetterie, cashless, boutique en ligne ...
Elle permet d’analyser finement les publics (BI) et de mettre en place des stratégies de marketing innovantes. Notre solution, à la pointe de la technologie, ainsi que l’accompagnement de nos clients au quotidien par notre équipe d'experts dédiés, nous positionnent comme leader sur le marché français.
Mieux connaitre son client, ses envies et goûts est primordial pour s'adresser de manière personnalisé et percutante à lui !
La société, est rentable et en forte croissance en 2020 et 2021.
Le poste
Une équipe de 20 personnes dont 6 à la tech passionnés qui ne demande qu'à en accueillir d'autres autour de valeurs fortes et assumées : bienveillance, transparence et combativité.
Le Backend Engnineer travaillera au sein de l’équipe technique, en synergie avec le CTO.
Les missions du poste
:
Améliorer la maintenabilité et le monitoring des ETL
Mettre en place et développer des API
Développer de nouvelles features
Développement de nouveaux pipelines de données pour aller récupérer la Data n'importe où
Optimiser et challenger l'environnement technique existant
Stack : Python (Django, Flask) Vue.js, PostgreSql, AWS
Les évolutions à venir dépendront majoritairement de vous et de vos préconisations.
Votre profil
Vous avez un esprit vif et êtes capable d’apprendre vite de nouvelles technologies ou de nouveaux langages
Vous êtes à l'aise avec du Python et du Sql et vous avez à partir de 2 ans d'expérience
Vous travaillez avec beaucoup de rigueur, vous recherchez l'efficacité tout en faisant les choses dans les règles de l'art et avec les bonnes pratiques du développement logiciel
Vous avez l'habitude de travailler avec des API
Vous avez une parfaite connaissance de l’architecture d’une plateforme web
Le plus : connaissance des ETL et technologie de scraping de données.
Le salaire & avantages
50-55K€ selon expérience
4 jours de remote/ 1 jours de présentiel
Carte de transport
Carte Swile & mutuelle
Et plus encore…
Ce qu’on préfère
Ambiance start-up, humaine et vivante avec de nombreux événements organisés
De nombreuses responsabilités sont confiées à tous les postes
Méthodes Agiles (Scrum) dans une équipe qui a une vraie vision agile
Ce poste a été soigneusement choisi par votre coach. Powered by Octopus IT, cabinet d’Experts en Recrutement Tech (CDI et clients finaux uniquement) – Visitez nous pour plus d’opportunités :
www.octopusit.fr
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': ['PostgreSQL'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote'], 'TypeContract': ['CDI'], 'Salary': ['50'], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Analyst (F/H),FBD Group,"Tremblay-en-France, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-f-h-at-fbd-group-3886652484?position=2&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=7TfIusgG6zvJO9lRTIEq%2Bw%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous aimez :
Conduire le changement dans le secteur de la distribution
Partager les challenges d’une entreprise agile en forte croissance
L’esprit d’équipe et l’ambiance qui règnent dans un groupe à taille humaine (250 collaborateurs)
L’innovation, le numérique et les nouvelles technologies au service de l’expérience client
La gestion de projet
Vous aimerez accompagner l’entreprise dans son évolution en étant un agitateur d'idées.
Rattaché(e) au Responsable Performance, vous aurez pour objectif principal le recueil, l’analyse et l’interprétation de la donnée de l’entreprise. Ces données peuvent être liées aux clients (CRM), aux produits et à leurs performances, ou même aux concurrents.
En collaboration étroite avec les autres pôles de la Direction Digitale, les équipes Commerce, les équipes Marketing, et les équipes IT, ainsi que les autres Directions support du Groupe, vous construirez, exploiterez et interpréterez les données pour en dégager des observations business utiles.
Missions Principales :
Echanger avec les différentes équipes internes concernées pour comprendre leurs besoins et leurs enjeux
Définir les indicateurs clés de performance (KPI) avec les parties prenantes pour mesurer la tenue des objectifs
Mettre en place ou optimiser les outils de suivi et de prévision avec la DSI
Analyser les bases de données
Synthétiser les résultats de ces analyses pour les communiquer aux bons interlocuteurs
Proposer des plans d’action (mise en place d’outils, définition des processus…)
Identifier les problèmes et proposer des solutions appropriées
Assurer la qualité et la fiabilité des données
Etablir une cartographie des données utilisées au sein des analyses pour une meilleure compréhension des interlocuteurs
Missions Détaillées :
Recueil des besoins
Echanger avec les différentes équipes internes concernées pour comprendre leurs besoins et leurs enjeux
Définir les indicateurs clés de performance (KPI) avec les parties prenantes pour mesurer la tenue des objectifs
Modéliser les données nécessaires à l’analyse
Collecte des données
Recueillir et extraire les sources de données pertinentes (savoir où les trouver) et de qualité, à traduire ensuite en données statistiques
Assurer la qualité et la fiabilité des données
Identifier les problèmes (données manquantes, qualité insuffisante, stockage, centralisation, etc.) et proposer des solutions appropriées en exprimant le besoin associé aux équipes IT
Traitement
Proposer des plans d’action :
Mettre en place des process/requêtes et des automatisations
Mettre en place ou optimiser les outils de suivi et de prévision avec la DSI
Trouver de nouvelles façons de traiter la donnée avec l’appui de nouveaux outils, au vu de l’accroissement considérable du nombre de données
Analyse
Définir et gérer des outils d’analyse et d’aide à la décision : prévisions et objectifs de vente, tableaux de bord, indicateurs de performance, analyse de la conjoncture et des marchés, suivi de la performance et appui aux équipes commerciales
Analyser les bases de données, produire des analyses métiers et proposer des recommandations aux managers
S’assurer de la cohérence des résultats par rapport au commerce et à l’activité réelle en point de vente
Communication
Créer des Dashboard, mettre en place des KPIs et des reportings de performance
Construire et faire évoluer les rapports issus de la Business Intelligence (BI) & Web Analytics
Synthétiser les résultats de ces analyses pour les communiquer aux bons interlocuteurs
Etablir une cartographie des données utilisées au sein des analyses pour une meilleure compréhension des interlocuteurs
Diffuser et assurer la bonne interprétation et la bonne compréhension des rapports d’analyse
Transverse
Faire preuve de proactivité pour explorer les données disponibles et proposer des analyses non sollicitées
Réaliser une veille marketing et commerciale sur la concurrence et proposer des analyses associées
Contribuer à l’analyse des parcours omnicanaux et à la recherche de leviers de croissance
Explorer les opportunités d’application de l’IA dans la Data Analyse
Réaliser des études qualitatives sur différentes problématiques business
Participer à la mesure économétrique des campagnes d’acquisition ainsi qu’à la mesure de l’attribution/contribution des différents leviers marketing actionnés
Expérience : 5 ans minimum en Data analyse, avec des connaissances en data engineering idéalement dans le secteur du retail
Formation : Bac+5, cursus en mathématiques, statistiques, économie, marketing ou en informatique (idéalement Master en statistiques / économétrie ou Master spécialisé en Big Data)
La maitrise de l’anglais courant est nécessaire
Savoir-faire & compétences :
Maîtrise des bases de données business (BigQuery, Snowflake, Redshift…) et du langage SQL
Maitrise de la construction d’un Datawarehouse
Maîtrise d’un outil d’orchestration (DBT, Airflow…) et du langage Python
Maîtrise d’au moins un outil de dashboarding (Looker, Power BI, Tableau…)
Maîtrise des librairies de manipulation de données en Python (NumPy, Pandas, Matplotlib, SciPy, Scikit-learn)
Une connaissance des outils tels que Hadoop ou Spark serait un plus
Compréhension des enjeux liés à la traçabilité des données (data lineage)
Une connaissance métier, notamment en marketing et relation client est nécessaire
Savoir être :
Grande aisance écrite et orale pour optimiser la compréhension et la communication avec les équipes
Passion pour les chiffres et les statistiques
Orientation business pour faire des recommandations pertinentes et actionnables
Aisance relationnelle pour emporter l’adhésion des interlocuteurs
Esprit de synthèse et analytique
Esprit ouvert et compétences transverses pour appréhender les problématiques techniques et les restituer de façon compréhensible aux différents publics
Rigoureux, organisé et réactif
Force de proposition
Prise de recul
Pourquoi nous rejoindre ?
La brigade
Une Direction qui accorde une attention particulière au bien être des collaborateurs : Nous sommes Great Place to Work !
Les ingrédients
Vous aurez tous les ustensiles nécessaires pour votre recette (ordinateur, téléphone portable professionnel etc.)
Une rémunération fixe + Prime annuelle liée à vos objectifs + Prime de participation
Les assaisonnements
Une mutuelle familiale prise en charge à 100% par le Groupe FBD
L’accès à un Restaurant Interentreprises
2 jours de télétravail par semaine sans condition d’ancienneté
Des locaux à Roissy à 200m du RER B et une annexe à Paris 9ème
Un réseau social Entreprise pour vous mettre au courant et participer à la vie d’entreprise
La carte
Notre menu est composée d’ingrédients divers et variés, ce qui en fait la richesse de ses plats.
Nous accordons une attention particulière à lutter contre toute formes de discriminations, raison pour laquelle nous avons adhéré à l'association A Compétences Egales qui œuvre dans ce domaine.
La cerise sur le gâteau
Boissons chaudes et fruits Bio à disposition pour bien commencer la journée !
Des activités tous les mois pour les collaborateurs, préparés par notre service Com Interne + 2 évènements groupe par an.
En cuisine, le meilleur reste à inventer ! Nous vous attendons !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['Pandas', 'NumPy', 'R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': ['Scikit-Learn'], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI', 'Matplotlib'], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Statistiques'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Engineer,Lyreco France,"Valenciennes, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-lyreco-france-3918198650?position=3&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=HJ%2Fkp3pwyyjqSMpJSYgSXQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Responsible for building systems that collect, manage, and convert raw data into usable information for
data scientists and business analysts to interpret.
The data engineer aims to make data accessible so that the organization can use it to evaluate and
optimize its performance
Are you excited for your new career adventure?
At Lyreco, we offer more than just a job, but a career! Our Data Storage Team is looking for a talented and ambitious new Data Engineer to join in HQ (Marly).
Lyreco is the European leader and the third largest distributor of workplace products and services in the world. A privately owned company since 1926, Lyreco has constantly adapted to the evolutions of workplace thanks to its focus on excellence in customer experience, strong partnerships with renowned suppliers, and efficient logistics.
With more than 12,000 employees, Lyreco directly operates in 25 countries in Europe and Asia and covers 17 additional markets on 4 continents through a network of distribution partners.
YOUR MISSIONS:
Industrialize data integration, data cleansing, data analytics programs or data management processes.
Contribute to the design, development, testing, deployment, performance in production and maintenance of the data-centric software including APIs, cloud-based architectures, libraries, toolbox.
Liaise with the Business Analysts, Data Scientists, Analytics specialist or Architects to understand how data needs to be: extracted, converted/transformed, loaded and exposed; consumed, calculated and updated.
Help the Data Architect to create an overview of the Data Lineage (from data flows, data transformations inside applications).
Provide clear documentation of the business rules embedded in the systems and potentially manage or help solving Data Quality issues.
Adapt to the central/ spokes’ ecosystem and local context and standards
YOUR PROFILE:
Degree in Computer Science or relevant data field
Experience in the area of trade working in projects… working in projects in Data Engineering, Big Data and/or Cloud environment using Apache Spark
Working experience with Cloudera Data Platform is a plus
Structured yet agile approach – with good knowledge of the scrum agile methodology
Languages: SQL; Python
Big Data & Streaming technologies (Hadoop/HDFS, Spark…)
NoSQL Databases (e.g. Hbase, MongoDB…)
Fluency in written and spoken English
REASONS TO JOIN LYRECO :
A full- time job in a dynamic, passionate, international team
Possibility to join internal mobility program
Competitive salary (bonus, benefits)
You will work in hybrid model in Valenciennes/Marly/France
If the above job description interests you and you think you are a good fit, apply now! We look forward to receiving your application in English.
Êtes-vous enthousiaste à l'idée de vivre une nouvelle aventure professionnelle ?
Chez Lyreco, nous offrons plus qu'un emploi, une carrière ! Notre équipe Data Storage est à la recherche d'un Data Engineer H/F talentueux et ambitieux pour rejoindre notre équipe au siège social.
Lyreco est le leader européen et le troisième distributeur mondial de produits et services pour l’environnement de travail. Société privée depuis 1926, Lyreco s'est constamment adaptée aux évolutions du monde du travail grâce à son souci d'excellence en matière d'expérience client, à ses partenariats solides avec des fournisseurs renommés et à sa logistique efficace.
Avec plus de 12 000 employés, Lyreco opère directement dans 25 pays d'Europe et d'Asie et couvre 17 autres marchés sur 4 continents par le biais d'un réseau de partenaires de distribution.
Lyreco se préoccupe des personnes.
Nous nous engageons à offrir Une Excellente Journée de Travail à nos clients et à nos employés.
Cela signifie que chez Lyreco, nous cultivons activement les talents et veillons à ce que tous nos employés aient une Une Excellente Journée de Travail. Afin de garantir le développement de nos employés, nous leur offrons une formation continue et une mobilité interne lorsque nous recrutons pour un poste vacant - Chez Lyreco, tout est possible !
Chez Lyreco, nous valorisons l'excellence, la passion, le respect et l'agilité. Nous savons que les personnes sont au cœur de tout ce que nous réalisons, c'est pourquoi chez Lyreco nous faisons de notre mieux pour soutenir nos employés où qu'ils travaillent, quelle que soit leur mission, en leur offrant la meilleure expérience professionnelle possible pour atteindre l'excellence.
CE QUE NOUS PROPOSONS:
Industrialiser les programmes d'intégration de données, de nettoyage de données, d'analyse de données ou de gestion des données.
Contribuer à la conception, au développement, aux tests, au déploiement, à la performance en production et à la maintenance des logiciels axés sur les données, y compris les API, les architectures basées sur le cloud, les bibliothèques et les boîtes à outils.
Collaborer avec les analystes métier, les data scientists, les spécialistes de l'analyse ou les architectes pour comprendre comment les données doivent être : extraites, converties/transformées, chargées et exposées ; consommées, calculées et mises à jour.
Aider l'architecte de données à créer une vue d'ensemble de la lignée des données (à partir des flux de données, des transformations de données à l'intérieur des applications).
Fournir une documentation claire des règles métier intégrées dans les systèmes et potentiellement gérer ou aider à résoudre les problèmes de qualité des données.
S'adapter à l'écosystème central ou décentralisé et aux normes locales.
CE QUE NOUS RECHERCHONS:
Formation supérieure en informatique (Bac+4/5) ou en Big Data
Expérience dans le domaine du e-commerce travaillant sur des projets de Data Engineering, Big Data et/ou en environnement Cloud en utilisant Apache Spark Bonnes connaissances de la plateforme de données Cloudera *
Approche structurée mais agile, avec une bonne connaissance de la méthodologie agile Scrum
Bonnes connaissances des langages : SQL ; Python Technologies Big Data & Streaming (Hadoop/HDFS, Spark…) , bases de données NoSQL (par exemple, Hbase, MongoDB…)
Maîtrise de l'anglais écrit et parlé
Les « + » chez Lyreco Management
Une expérience dans un environnement international.
Prime sur objectifs.
Prime de participation et prime d’intéressement.
Des actions en faveur de l’équilibre vie professionnelle/ vie personnelle (conciergerie d’entreprise, associations sportives, plateforme de covoiturage, crèche d’entreprise).
« Lyreco est signataire de la Charte de la diversité. Nous garantissons le respect des règles de non-discrimination à l'embauche et nous nous engageons à aider les personnes éloignées de l'emploi. »
Télétravail.
Envie de rejoindre l’aventure Lyreco Management ? N’attendez plus, transmettez-nous votre candidature !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', ' MongoDB', 'HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Organization']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': ['1926'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (F/H) - Nantes,SFEIR,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-nantes-at-sfeir-3477971987?position=4&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=vCFQNCficndPACYOQbG7LA%3D%3D&trk=public_jobs_jserp-result_search-card,"C'est quoi SFEIR ? 🤔
SFEIR, c'est avant tout une communauté de 900 techs en France, en Belgique et au Luxembourg.
Nous aidons nos clients à :
🔹 Être compatible avec le futur en développant leurs architectures SI, Cloud et Data ;
🔹 Donner de la valeur à leurs données, innover avec l'IA ;
🔹Créateur de valeur grâce aux API & microservices;
🔹 Développer des applications web et mobiles pour améliorer leur expérience client et se connecter partout.
Notre culture d'entreprise est résolument tournée vers l'expertise technologique, la transmission de connaissances et le respect de nos valeurs phares :
bienveillance, inclusivité, excellence, liberté, responsabilité.
Et Nantes dans tout ça ?
SFEIR Nantes c'est :
🔹 Une agence crée en 2018 par @Jean-François Garreau , co-fondateur du DevFest Nantes
🔹 Une trentaine de consultant(e)s qui se connaissent toutes et tous
🔹 4 Managers qui sont aussi des consultants @Hoani , @Adrien , @Frédéric , @Valentin
🔹 Une dizaine de clients actifs en local et une centaine au niveau du groupe
🔹 Des événements en interne et en organisés chaque mois (afterworks, meetup, formations)
🔹 Une communauté active : une cinquantaine de conférences externes données en 2022 (Devfest, Devoxx, communauté JS…)
🔹 Des locaux sur l'île de Nantes dans le bâtiment totem du numérique nantais.
Concrètement, quel sera mon job ?
En tant que Data Engineer en mission chez un client, ton rôle est d’appréhender son contexte et d'implémenter une plateforme pour valoriser sa donnée.
Tu es à l’aise sur l’un des langages suivants :
Python, Java, Scala et SQL
; et tu as déjà utilisé des
frameworks de calculs distribués
(Spark, Beam,…).
Tu connais et utilises les différentes
solutions de stockage
(SQL, DWH, NoSQL, Search engine,Streaming...).
Tu connais les principes du développement Cloud, IaaC et des
chaînes CI/CD
et tu as des connaissances en
machine learning.
Tu es curieux(se) et fais de la veille technologique.
@Arthur et @Paul-Antoine , nos super commerciaux, seront à ton écoute pour définir avec toi ta mission idéale et tu pourras en changer lorsque tu en auras fait le tour.
Voici des exemples de projets réalisés par des sfeiriens pour te donner une idée :
🔹
Oussama
a une mission au sein d'un acteur digital native, il s'agit de la mise en place d'une plateforme data (batch et stream) sur Google Cloud pour de l'analytics et du reporting interne (mises en relations, performances des assureurs, comportements utilisateurs...). La stack technique est composée de : Cloud Storage, Dataflow, Bigquery, Cloud Composer (Airflow), Datastudio, Terraform, GitLab.
🔹
Pascal
a une mission dans le domaine de la santé, il s'agit de la mise en place d'une plateforme s'adaptant automatiquement à la charge de travail sur un Cloud provider pour analyser les données issues du séquençage du génome de patients atteints d'un cancer afin d'aider au diagnostic. La stack technique est composée de : Kubernetes, DataFlow, BigQuery, MongoDB, elastisearch.
Et si je souhaite évoluer ?
Nous proposons des possibilités d'évolution verticales et horizontales. Nous pourrons t'accompagner pour te certifier ou évaluer sur une autre spécialité, ou encore devenir
Lead Data ou Data Architecte.
Tu auras également la possibilité de prendre des rôles en interne si tu le souhaites :
🔹
Évaluateur(trice)
dans le processus de recrutement
🔹
Formateur(trice)
aux Sfeir Schools ou au Sfeir Institute
🔹
Speaker(euse)
lors de conférences, meetups, talks internes, auprès des écoles
🔹
Engineering manager
si tu veux manager une équipe tout en restant dans la technique
Et si tu as d'autres envies, on en discute, chacun est différent et on fait au cas par cas.
Je suis intéressé(e)🙂 , comment vous rejoindre ?
Si cette annonce a fourni ton attention, il ne te reste plus qu'à postuler.
@Justine se font un plaisir de t'en dire plus 🙂. Tu pourras ensuite te frotter à nos célèbres
PlayOffs
: 3 tests d'évaluation technique en pair-programming (algorithmie, langage, framework). Ne t'en fais pas, nos évaluateur(trice)s sont bienveillant(e)s !
Enfin, tu échangeras avec Arthur et Paul-Antoine au commerce et Jean-François et @Arnaud , nos directeurs.
Chez Sfeir, la confiance, la bonne humeur et l'inclusion font partie de notre ADN.
En rejoignant notre communauté, tu deviendras un(e) Sfeirien(ne) bienveillant(e), libre et responsable.
#L’inclusionEstUneForce: Notre process de recrutement inclusif assure une égalité de traitement et de chance aux candidats de tous horizons.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', ' MongoDB'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Software Engineer,Orange Logic,France,https://fr.linkedin.com/jobs/view/software-engineer-at-orange-logic-3862540191?position=5&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=T5fgB%2B5g8k%2Bi3gxd161Chg%3D%3D&trk=public_jobs_jserp-result_search-card,"For more than two decades, Orange Logic has empowered a wide range of clients with its digital asset management system, Cortex. We’ve worked with almost every industry, from Finance to NGOs, Media giants to educational institutions, securing and organizing their assets. The Software Engineer will enhance Orange Logic’s software by participating in the design, development, maintenance and testing process.
What you can expect in your role:
Taking ownership of projects and having the opportunity to further your knowledge by exploring machine learning, security, DevOps, and more.
Developing scalable new features for our software product that exceeds our customer’s needs.
Building architecture for our platform to ensure optimal performance.
Obtaining requirement feedback from internal teams/clients to maintain/support the product development.
Write the Unit Tests for robust development.
Performing code reviews on other team member’s work.
You are:
Proficient with English (both verbal and written).
Have 3+ years’ practical experience on a web-based application.
Proficient with any backend programming languages (e.g. .NET, Java, Python, etc.).
A strong fundamental understanding of software development.
An understanding of complex algorithms and data structures, as well as a passion for intellectual challenges.
Strong self-discipline for delivering well-tested, complete features/modules under a tight schedule and the capability for rational thinking.
Experience with the database management tool SQL is a plus, but not mandatory.
Obtained bachelor’s degree in any relevant major (e.g. Information Technology, Computer Science, etc.).
Perks of joining the team:
Competitive compensation & benefits package
Remote Work Environment
How to get started:
If you’re up for the challenge to be part of a growing engineering team we’d like to hear from you. Apply today!
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Machine Learning'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F/NB) - Paris,Keyrus,"Levallois-Perret, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-nb-paris-at-keyrus-3799373891?position=6&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=FLAgTMnItXbsCuBfCiaawA%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes-nous ? Une
success story
dans la
Data
et le
Digital
!
Notre mission ?
Des projets à forte valeur ajoutée pour accroître la performance et la compétitivité des entreprises, faciliter et accélérer leur transformation.
Notre Expertise Depuis Plus De 20 Ans ? Le Conseil Et L'intégration De Solutions Innovantes Autour De Trois Domaines
Data Intelligence
Business Intelligence, Big Data & Analytics, Intelligence Artificielle
Digital Experience
Conseil, Stratégie & Performance Digitales
Conseil en Management & Transformation
Stratégie & Innovation, Pilotage de la Performance & Accompagnement des Projets
Nous sommes plus de 3000 talents sur plus de 20 pays et 4 continents. Notre ADN ? Innover et entreprendre.
Le Job
Les principales missions qui vous seront confiées seront les suivantes:
Mettre en œuvre divers outils de développement, de test, d'automatisation et d'infrastructure informatique.
Définir et paramétrer les processus de développement, de test, de publication, de mise à jour et de support pour les opérations DevOps.
Avoir les compétences techniques pour examiner, vérifier et valider le code logiciel développé dans le cadre du projet.
Surveiller les processus tout au long du cycle de vie pour leur adhésion et mettre à jour ou créer de nouveaux processus pour l'amélioration et la minimisation du gaspillage.
Encourager et créer des processus automatisés dans la mesure du possible.
Identifier et déployer des mesures de cybersécurité en effectuant en permanence une évaluation des vulnérabilités et une gestion des risques.
S'efforcer d'améliorer continuellement et de construire une intégration continue, un développement continu et un pipeline de déploiement constant (CI/CD Pipeline)
Gestion des rapports périodiques sur les progrès à la direction et au client.
Le Profil
Vous avez 5 ans d'expérience.
Vous parlez anglais couramment.
Pourquoi nous rejoindre ?
Pour intégrer une communauté d’experts curieux et passionnés et évoluer dans un environnement multiculturel, formateur et favorisant la mobilité internationale.
Parce que vous êtes #DataGeek, #DigitalAddict, #InnovationLover !
#KeyrusRocks #YouRock
Keyrus est une entreprise où il fait bon vivre et travailler !
Découvrez
La vie chez Keyrus en 60 sec
Keyrus en 3 mots
Nos animations pour nos collaborateurs sur Facebook et sur Instagram.
Notre vidéo par Welcome To The Jungle
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '20', '20', '20']}"
Ingénieur DATA (H/F),Haute Autorité de Santé,"St.-Denis, Île-de-France, France",https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-h-f-at-haute-autorit%C3%A9-de-sant%C3%A9-3887884957?position=7&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=Et2SHnCydBT9YAzruviS9A%3D%3D&trk=public_jobs_jserp-result_search-card,"Date : 13/02/2024
Poste à pourvoir Ingénieur Data (H/F)
Emploi-repère Chef de projet
Catégorie d’emploi Catégorie 1
Type de contrat Contrat à durée indéterminée / Temps complet
Localisation Saint-Denis (93)
Rémunération Selon expérience et niveau de formation, par référence aux grilles indiciaires des agences sanitaires en application du décret n°2003-224 du 07 mars 2003 ou selon statut particulier si fonctionnaire (détachement)
Description Du Poste a Pourvoir
Missions générales du poste à pourvoir Depuis 3 ans, la mission data a mis en place de nombreux projets mobilisant des traitements de données. Parmi ceux-ci, on peut citer des projets open sources tels que ADEX qui est utilisé au quotidien par les agents de la HAS et d’autres agences sanitaires pour analyser les conflits d’intérêt des experts, open SNDS qui permet de visualiser rapidement des informations sur les consommations de soins, ou la base de données sur la qualité et la sécurité des soins qui alimente l’espace QualiScope du site internet de la HAS. D’autres projets ambitieux sont en cours, mobilisant notamment du traitement automatique du langage naturel pour mieux exploiter de gros volumes de retours d’expériences de patients ou la littérature scientifique.
Ces projets se basent sur une plateforme data, développée en interne, hébergée chez un clouder français, et homologuée pour traiter les données sensibles de la HAS. Elle comprend notamment une forge logicielle GitLab, un stockage objet MinIO, et des ressources de calcul importantes selon les besoins, notamment des GPU.
Au sein de la mission data, votre mission sera de développer et maintenir des traitements de données automatisés, pour répondre efficacement aux besoins data de la HAS. Vous travaillerez pour ce faire en synergie avec le responsable technique de la mission data, en binôme par projet avec les autres membres de l’équipe, et de façon régulière avec les data managers et statisticiens des différents services métiers de l’institution.
Au Fil Des Projets Vous Serez Amené à
Développer des traitements de données et maintenir en condition opérationnelle l’existant
Participer à la rationalisation et l’amélioration constante de la qualité technique de nos productions (choix technologiques, promotion des bonnes pratiques, relecture de code)
Contribuer à des projets open source permettant d’améliorer l’autonomie des utilisateurs et des citoyens dans l’exploitation des données issues de, ou utilisées par la HAS
Participer à la publication en open-data des données produites par la HAS
Travailler au sein d’une équipe transverse d’une dizaine de personnes, composée de profils pointus dans leurs domaines (Data, Visualisation, NLP, épidémiologie, etc.)
En tant qu’ingénieur data, vous occuperez une position clé au sein la mission data, et plus largement des projets data de la HAS. Vous saurez créer une dynamique autour de vos missions, et participer à la réussite des projets data de la HAS.
Direction et service d’affectation
Direction générale
Mission Data
La collecte massive de données entraîne actuellement des transformations majeures dans tous les secteurs d’activités. Les systèmes de santé commencent à être traversés par cette (r)évolution, qui touche en particulier la production de connaissances et leur usage quotidien. Pour prendre pleinement ce virage, la HAS s’est dotée en 2021 d’une stratégie pluriannuelle dédiée.
Vous travaillerez dans la mission data, rattachée au directeur général, dont le rôle est de mettre en euvre cette stratégie, par la réalisation de projets et d’études au service des métiers et missions de l’institution.
La mission data est à la fois un laboratoire d’innovation, un centre d’expertise, et un catalyseur de transformations dans l’usage des données par la HAS. Elle promeut les dynamiques de connaissance ouverte (open source, open data, open knowledge), conformément aux valeurs de transparence, d’expertise et d’indépendance de l’institution.
Profil Recherché
Formation Titulaire Bac+5 (Master, diplôme d’ingénieur ou diplôme équivalent).
Spécialité en informatique, data ou DataOps.
Expérience Vous justifiez d’une expérience professionnelle significative dans le domaine (5 ans minimum) avec la réalisation de projets de traitement de données d’envergure, et une appétence pour les infrastructures de données sous-jacentes.
Compétences De nature autonome, vous savez faire preuve d’initiative et avez un réel sens de l’organisation.
Vous êtes expert des langages, librairies et outils de traitement et de flux de données (ETL). La pile technologique de la HAS, en évolution, utilise centralement Python et ses librairies (pandas, dask), mais aussi R, SQL, Spark, GitLab-CI. Une connaissance de plusieurs langages et frameworks de programmation est donc appréciée.
Bonne maitrise généraliste des systèmes de stockage de données : formats de stockage fichier, bases de données relationnelles et documents, systèmes de fichiers local et distribué, stockage objet adapté au cloud.
Bonne maîtrise de git, des systèmes de tests automatisés, d’intégration et de déploiement continu.
Connaissance des systèmes LINUX, de virtualisation et de conteneurisation (Podman, Docker).
Participation à des projets open source et open data appréciée.
Enfin, vous êtes tourné vers l’action, pragmatique, rigoureux, aimez travailler en équipe et faire progresser le collectif en partageant vos compétences.
LA HAUTE AUTORITÉ DE SANTÉ
Autorité publique indépendante à caractère scientifique, la Haute Autorité de santé (HAS) vise à développer la qualité dans les champs sanitaire, social et médico-social, au bénéfice des personnes.
Elle travaille au côté des pouvoirs publics dont elle éclaire la décision, avec les professionnels de santé pour optimiser leurs pratiques et organisations, et au bénéfice des usagers dont elle renforce la capacité à faire des choix.
Elle Exerce Trois Missions Principales
évaluer les médicaments, dispositifs et actes en vue de leur remboursement ;
recommander les bonnes pratiques professionnelles, élaborer des recommandations vaccinales et de santé publique ;
mesurer et améliorer la qualité dans les hôpitaux, cliniques, en médecine de ville et dans les structures sociales et médico-sociales.
La HAS exerce son activité dans le respect de trois valeurs : la rigueur scientifique, l'indépendance et la transparence.
Elle Est Organisée Autour
d’un Collège de huit membres dont un président ;
de commissions spécialisées présidées par des membres du Collège :
de services réparties en cinq directions.
Ref : C157O73948
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['Pandas', 'R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Chef'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Analyst H/F,Wewyse,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-h-f-at-wewyse-3811756415?position=8&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=f7%2B8eg2EVVvTYWtksqC2LQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Wewyse
est un cabinet de conseil spécialisé en
Data et en Intelligence Artificielle
. C'est aussi et surtout une communauté de passionnés partageant l'ambition de grandir ensemble et d'ouvrir le champ des possibles dans leurs domaines.
Si vous pensez que la Data et l'Intelligence Artificielle ont beaucoup à offrir au monde de demain, et si vous souhaitez apporter votre contribution à ce monde, avec humilité et enthousiasme, alors vous êtes un Wyser en puissance.
Être
Data Analyst
chez Wewyse c'est :
Intégrer une communauté d’experts Data passionnés,
Recevoir et partager des connaissances, des savoirs-faire lors de nombreux évènements internes et acquérir constamment de nouvelles compétences,
Intervenir sur des projets à la fois techniques et business, souvent dans un contexte international, avec des forts enjeux stratégiques auprès de nos clients dans tous les secteurs (retail, énergie, médias, transports, banque et assurance …)
Viser l’excellence dans la qualité et l’actionnabilité des analyses, grâce à la maîtrise technique des outils en Data Analytics (développement, notebooks, visualisation, techniques de statistiques avancées…) et la bonne compréhension des enjeux business de nos clients,
Participer à des projets innovants, impactants et positifs au sein de notre Datalab, en équipe, avec des partenaires académiques (CentraleSupelec, …) et parfois avec des starts up, afin de laisser exprimer vos valeurs et vos idées,
Avoir la possibilité de s’impliquer dans le développement de Wewyse (participer au recrutement, concevoir des formations, donner des masterclass en école d’ingénieur, participer à l’avant-vente business, piloter des projets au sein du Datalab, développer de nouveaux partenariats …)
Être encouragé, conseillé et accompagné dans un parcours de formation adapté à vos ambitions professionnelles et avec un budget dédié,
Faire partie de la famille Wemanity, créer rapidement un réseau d’experts et de personnes reconnues dans leur domaine, participer aux évènements communs (voyage annuel, talks, afterworks ...) et bénéficier des multiples opportunités de carrière au sein du Groupe,
Avoir l’opportunité de travailler avec une équipe dynamique, dans une ambiance start-up et à taille humaine dans le centre de Paris (Opéra 9ème arrondissement)
Ce que nous aimons :
Les personnalités ouvertes, curieuses, ambitieuses
Un excellent niveau en
SQL
Une première expérience en
Python
La maîtrise d’un outil de Data Visualisation
(PBI, Tableau, Qlik, Looker, Data Studio …)
Une expérience dans l’utilisation de
Jupyter Notebook / Databricks Notebook
est fortement appréciée
L’aisance en communication
L’esprit de synthèse
L'anglais
Vous vous reconnaissez ? Alors n'hésitez pas à postuler !
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Statistiques'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),Euro Information Developpements / EID,"Villeneuve-d’Ascq, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-euro-information-developpements-eid-3821163420?position=9&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=UMhN2ZF6jgNU3v6cad79RQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes nous
Euro-Information, filiale technologique de Crédit Mutuel Alliance Fédérale, conçoit, réalise, maintient et exploite un système d’information commun utilisé par le Groupe.
Les activités de développement et de production informatique au niveau national et international sont assurées par environ 4000 salariés répartis sur plusieurs sites géographiques au niveau national : Strasbourg, Nancy, Dijon, Orléans, Lyon, Lille, Cergy, Val de Fontenay, Paris et Nantes.
Première Banque à adopter le statut d’entreprise à mission, le Crédit Mutuel Alliance Fédérale s’investit et s’engage dans différentes missions sociales et environnementales :
L’accompagnement de tous par notre organisation coopérative et mutualiste reste au cœur de notre ADN.
La technologie au service de l’humain est une référence dans notre monde connecté.
La solidarité et l’éco-responsabilité deviennent des axes clés dans notre développement.
Notre raison d’être : Ensemble, Ecouter et Agir.
Vos missions
Vous souhaitez intégrer une équipe dynamique et à taille humaine au sein d’une entreprise solide et d’un grand groupe en développement constant ? Vous souhaitez travailler sur un enjeu d’aujourd’hui et plus encore de demain : la donnée ? Euro-Information, la Fintech de Crédit Mutuel Alliance Fédérale structure une Data Factory pour :
Accélérer la valorisation des données au travers d’analyse de masse et de modèles allant jusqu’au prédictif.
Répondre à la confiance de nos clients en garantissant la sécurité de leurs données et une utilisation responsable et encadrée, respectueuse de leur vie privée.
La Data Factory EI se positionne comme un fournisseur de solutions pour les équipes informatiques et pour l’ensemble des entités du groupe et comme un facilitateur d’échanges et de mutualisation. Pour mener à bien ces missions, la relation métier s’avère essentielle. Elle réunit :
Des Data Architects. Ils fournissent l’environnement, les outils et s’assurent de leur performance et de leur constante évolution.
Des Data Engineers. Ils ont la maitrise des données, de leur récolte à leur mise à disposition adaptée aux besoins des métiers. Ils conçoivent les modèles d’enregistrement des données.
Des Concepteurs Business Intelligence. Ils simplifient, mutualisent et industrialisent les reportings.
Des Data Scientists. Ils accompagnent les métiers et les entités sur la Data Science, ils réalisent à façon sur les sujets à fort enjeu.
Des Data Officers. Ils coordonnent et mutualisent les énergies sur les projets Data comme dans le cadre de la Gouvernance et de l’administration des données.
Nous recherchons un(e) Data Engineer.
Vous participerez à la mise en œuvre et au maintien du Système d’information Décisionnel du groupe
Pour les banques, les crédits à consommation et les filiales, en France et à l’international :
Compréhension de l’activité et des besoins de vos clients, en dialogue avec la MOA
Compréhension du SI de production, en dialogue avec les équipes MOE
Modélisation du Système d’Information Décisionnel
Conception et réalisation
Diagnostic des dysfonctionnements rencontrés
Maintenances correctives et évolutives
Support auprès des différents métiers
Documentation technique
Suivi des traitements
Pilotage de projets de petite taille en autonomie
Vous travaillerez sur une grande variété de projets sur des métiers divers et sur des dizaines d’entités gérées sur le SI d’Euro-Information : crédit, assurance, commercial, leasing, prévention de la fraude, sujets liés à l’Intelligence Artificielle en lien avec la Cognitive Factory, sujets liés à la lecture automatique des documents en lien avec l’OCR Factory…
Vous intervenez sur des projets à dimension internationale sur des environnements Vertica, Hadoop, Stambia, SAS, Business Objects, QlikSense…
Vous participez, coordonnez et menez à bien des projets de valorisation de données sur des cas d’usages concrets :
Participez aux projets de bout en bout : de l’expression du besoin jusqu’à la réalisation et au suivi de sa performance en lien avec la MOA et nos Data Officers
Mobilisez l’ensemble des acteurs : Business, Data scientists, Data engineer, sources de données …
Eclairez et participez aux prises de décision.
Mutualisez les travaux et les bonnes pratiques entre les acteurs et les différents métiers du groupe.
Accompagnez le changement. Au sein de vos projets et au-delà, vous portez l’acculturation Data au sein du groupe
Ce que vous allez vivre chez nous
Télétravail (2 jours par semaine)
Rémunération fixe versée sur 13 mois
RTT
Intéressement, participation et abondement
Plan épargne entreprise et PERCO
Contrat de santé collectif
Prévoyance
Retraite supplémentaire prise en charge à 100% par l’employeur
Conditions bancaires et assurances préférentielles
Politique parentale avantageuse
Ce que nous allons aimer chez vous
De formation bac +4/5, vous disposez idéalement, d’une expérience significative sur un poste équivalent.
Connaissance du monde OPEN
Vous avez la maitrise d’un ETL, d’une base de données orientée Analytique, d’une solution BI. La connaissance de l’outil de modélisation PowerDesigner serait un plus
La maitrise de l’anglais serait également appréciée.
Ce qui nous plaira le plus chez vous :
C’est vous-même ! Alors on vous attend ouvert(e), force de proposition, doté(e) d’un certain sens critique, autonome et respectueux(se) de la confidentialité des informations détenues car c’est ce qui vous permettra de mener au mieux votre mission.
On dit de vous que vous avez une certaine aptitude à communiquer et le sens du travail en équipe.
Vous êtes motivé(e) et vous souhaitez vous investir fonctionnellement et techniquement, n’hésitez plus l’offre est faite pour vous.
Autres informations
Le poste basé à Villeneuve-d’Ascq est à pourvoir en CDI dès que possible.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),Extia,"Sophia Antipolis, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-extia-3675382393?position=10&pageNum=42&refId=hZyb1K8CoeqwCyZyTWnF7w%3D%3D&trackingId=7G%2FuGfBxDl2QaW7f%2FCkTIg%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous souhaitez rejoindre une entreprise qui place l’humain au cœur de ses préoccupations ? On vous attend chez
Extia
!
Société de conseil spécialisée dans les métiers de l’IT, de l’ingénierie et du digital, Extia privilégie depuis sa création en 2007 une approche qui allie performance et bien-être au travail. Une vision de l’entreprise partagée aujourd’hui par plus de 2 500 Extiens en France et à l'international et récompensée par le label Great Place to Work® depuis 13 ans, notamment en
2024 où les Extiens se hissent à la première place du palmarès Best Workplaces France
!
Chez Extia, c’est « D’abord qui, ensuite quoi » alors, allons-y !
D'abord qui
D'abord qui ?
Ingénieux, votre imagination débordante vient à bout de chaque problématique,
Attentif aux détails, vous avez un œil de Lynx pour repérer les incohérences,
Efficace, vous ne remettez pas à demain ce qui peut être fait dès aujourd’hui.
Ensuite quoi
Ensuite quoi ?
Vous aurez le rôle de support technique aux équipes d’analyse : structurer les données, réaliser des analyses « statistiques » ou « techniques » sur les données, développer des outils d’analyse… Vous mènerez des études afin d’identifier les solutions les plus pertinentes. Vous serez en charge de :
Participer à la définition des besoins,
Mettre en place des Pipelines de traitement de données,
Développer de nouvelles features,
Maintenir en condition opérationnelle du system legacy de test,
Collaborer avec les Data Scientists au développement des modules d’analyse de donnée,
Intégrer des sources de données.
**Profil recherché **
Disposant de minimum 2 à 3 ans d'expérience, vous êtes habitué à travailler aussi bien avec des méta-données qu’avec des données non-structurées. A cet effet vous maitrisez Spark et vous connaissez le langage de programmation Scala. Vous possédez également une affinité avec le cloud.
Enfin, vous avez une bonne maitrise de l'anglais, à l'écrit comme à l'oral !
Show more
Show less","{'ProgLanguage': ['Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Statistiques', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '13', '13', '13']}"
Data Engineer,Energy Jobline,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-energy-jobline-3892672213?position=1&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=upuV45Ei8yGufim4hykR%2Fg%3D%3D&trk=public_jobs_jserp-result_search-card,"Do you have Data Engineering experience, and are you seeking a new contract role in Paris? NES Fircroft is helping a collaborative company recruit an agile and passionate Data Engineer for a 12-month contract. Hybrid based, must have 5+ years experience. Please note this position requires you to be fluent in French and English.
As a Data Engineer, you will be responsible for the data architecture's design, development and production. You will also collect business and user requirements, design data pipelines, and push the architecture into production.
In your first few weeks in this Data Engineer role, you can expect to:
Collect business and user requirements
Design the data architecture
Design data pipelines
Put the architecture into production
Ensure the maintenance and evolution of the architecture
Collaborate and participate in activities within the chapter
Attend Communities of Practice (CoPs) meetings.
To apply for this Data Engineer role, your soft skills, expertise, and experience should include:
Experience working with Python/PySpark/Databricks under an Azure Cloud environment
Experience creating data pipelines and putting the architecture into production
Experience in working with data pipelines, maintaining and evolving the architecture.
If you want to positively impact and create change, you'll be rewarded with an excellent contract per day rate for your inclusive and committed approach.
To apply for this contract Data Engineer job in Paris, please contact NES Fircroft today. Please refer any friends or colleagues for this role or direct them to our Careers page on our website.
With over 90 years of combined experience in delivering workforce solutions to the global energy industry, NES Fircroft is proud to be the world’s leading engineering staffing provider spanning the Oil & Gas, Power & renewable, Infrastructure, and Life Sciences, Mining, Automotive and Chemicals sectors worldwide. We provide tailored staffing solutions, sourced from a global talent pool by a dedicated, discipline-specific team of consultants.
With over 90 years' combined experience, NES Fircroft (NES) is proud to be the world's leading engineering staffing provider spanning the Oil & Gas, Power & Renewables, Chemicals, Construction & Infrastructure, Life Sciences, Mining and Manufacturing sectors worldwide. With more than 80 offices in 45 countries, we are able to provide our clients with the engineering and technical expertise they need, wherever and whenever it is needed. We offer contractors far more than a traditional recruitment service, supporting with everything from securing visas and work permits, to providing market-leading benefits packages and accommodation, ensuring they are safely and compliantly able to support our clients.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer (H/F),Extia,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-extia-3841318856?position=2&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=XzXy52KW1ipP%2BVsyy2ii8A%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous souhaitez rejoindre une entreprise qui place l’humain au cœur de ses préoccupations ? On vous attend chez
Extia
!
Société de conseil spécialisée dans les métiers de l’IT, de l’ingénierie et du digital, Extia privilégie depuis sa création en 2007 une approche qui allie performance et bien-être au travail. Une vision de l’entreprise partagée aujourd’hui par plus de 2 500 Extiens en France et à l'international et récompensée par le label Great Place to Work® depuis 13 ans, notamment en
2024 où les Extiens se hissent à la première place du palmarès Best Workplaces France
!
Chez Extia, c’est « D’abord qui, ensuite quoi » alors, allons-y !
D'abord qui
Rigoureux, vous ne laissez rien au hasard,
Efficace, vous ne remettez pas à demain ce qui peut être fait dès aujourd’hui,
Autonome, vous savez mener vos missions à bien sans aide.
Ensuite quoi
Vous aurez le rôle de support technique aux équipes d’analyse : structurer les données, réaliser des analyses « statistiques » ou « techniques » sur les données, développer des outils d’analyse…
Vous mènerez des études afin d’évaluer les nouvelles technologies dans le domaine du Big Data, Data Mining ou Machine Learning afin d’identifier les solutions les plus pertinentes.
Vous serez en charge de :
Participer à la définition des besoins et à la rédaction des User Stories,
Collaborer avec les Data Scientists au développement des modules d’analyse de donnée,
Concevoir et construire des architectures de données,
Intégrer des sources de données,
S'assurer que les données sont facilement accessibles et que leur exploitation fonctionne comme demandé, même dans des circonstances hautement évolutives,
Exécuter des processus ETL (extraire / transformer / charger) à partir d'ensembles de données complexes et / ou volumineux.
Profil recherché :
Maitrise de l’écosystème Microsoft Azure Data Factory, Azure Data Lake est un plus
Maitrise Technologie autour de la data : Power BI, Spark, Airflow, Python, Scala…
Maitrise des bonnes pratiques de développement et méthodes agiles
Base de données : SQL, Postgré SQL (Paas) et modélisation de la donnée
Connaissance des systèmes de gestionnaire de conteneur (Kubernetes,...)
Connaissance des outils de déploiements : Jenkins, Git, maven, Ansible
Qualités relationnelles et capacité à gérer nombreuses interactions
Dynamisme, autonomie et envie de découvrir des manières différentes/innovantes de faire
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': ['Statistiques'], 'CloudComputing': ['Azure'], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible', 'Kubernetes', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning', 'Statistiques'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '13', '13', '13']}"
Data Engineer Talend (F/H),Pact&Go,"Montpellier, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-talend-f-h-at-pact-go-3914234996?position=3&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=7lqBbN6xVdGsLiePECBTYA%3D%3D&trk=public_jobs_jserp-result_search-card,"PACT&GO est un Cabinet de Conseil en Recrutement, indépendant et de proximité.
Notre objectif est de démocratiser les services des cabinets de recrutement auprès des Start-Up, TPE & PME, en proposant un service de qualité, pour une facturation plus équitable : 2x moins chère en moyenne que les prix en vigueur sur le marché !
De plus, il n'existe aucun risque pour nos clients de solliciter Pact&Go : nous ne demandons pas d'acompte et n'appliquons pas de clause contraignante (ex : clause d'exclusivité) : Nous fonctionnons uniquement au succès !
En parallèle, nous souhaitons nous mettre pleinement aux services de nos candidats, afin de comprendre réellement leurs projets professionnels et présenter LA bonne opportunité qui correspondra à leurs attentes.
Nous sommes spécialisés sur les secteurs d'activités suivants dans les Régions Occitanie & PACA :
Informatique
Tertiaire
Bureau d'études & Ingénierie
Nos Valeurs
Engagement
Confiance
Persévérance
Transparence
Bienveillance
Pour tout complément d'information, vous pouvez consulter notre site internet : www.pact-go.com
Sécurisez vos recrutements : Confiez vos recherches à Pact&Go !
Le Poste
Nous recherchons un(e) Data Engineer (F/H) compétent(e) sur Talend pour l'un de nos clients, acteur incontournable dans le secteur IT à Montpellier.
Il s'agit d'un poste à pourvoir au sein d'un centre de services d'une ESN réputée sur Montpellier (depuis + 20 ans), à taille humaine, et qui prend soin de ses salariés (suivi personnalisé et plan de formation tout au long du parcours).
Missions
Conception et développement de processus ETL en utilisant Talend pour intégrer et transformer les données en vue de leur utilisation dans un environnement de middleware.
Écriture et optimisation de requêtes SQL pour la manipulation et l'analyse des données.
Travail en étroite collaboration avec l'équipe de développement pour intégrer les solutions.
Collaboration avec les équipes de projet pour comprendre les besoins métiers et traduire ces besoins en solutions techniques efficaces.
Réalisation de tests pour garantir la qualité et la fiabilité des données.
Documentation technique pour assurer une bonne compréhension et une maintenance efficace des développements.
Profil
De formation Bac+3 à Bac+5, vous détenez une 1ère expérience significative sur une fonction similaire.
Vous avez déjà participé à un ou plusieurs projets BI, sur des problématiques de Data Integration (mise en place des bonnes pratiques pour des flux otpimisés) ou plus généralement dans la mise en place de Datawarehouses/Datamarts.
Vous attachez une importance particulière à la qualité de vos développements (respect de l'architecture, normes de codage, tests unitaires,…).
Vous avez une très bonne maîtrise de Talend.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5', 'Bac+3'], 'Experience': ['a', 'n', 's', '20', '20', '20']}"
Data Analyst (H/F),Wemanity Lille,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-analyst-h-f-at-wemanity-lille-3890979963?position=4&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=Ng5MnIpePveHtR6h3PYHtg%3D%3D&trk=public_jobs_jserp-result_search-card,"Fondée en 2018, l’agence
Wemanity Lille
se développe d’année en année grâce aux talents qui font de cette aventure une réussite ! 🌈​
3 mots pour la décrire ? Pétillante, à taille humaine et audacieuse !
Pour suivre cette dynamique nous recherchons des talents ayant l'envie de progresser et de se lancer dans une belle aventure professionnelle et humaine à nos côtés. 🌳
Nous recherchons un(e)
Data Analyst (H/F)
avec un minimum d'expérience de 3/4 ans, qui a envie d'apprendre et de dévoiler tout son potentiel à nos côtés !
Qui sommes-nous
?
💡
🎢 Cabinet de conseil spécialisé en
transformations digitales et agiles.
🌍 Présent à l’international :
Paris, Lille, Belgique, Pays-bas, Maroc et Luxembourg.
👥
600
coopérateurs passionnés.
🍃Un pôle RSE débordant d’idées qui propose des projets à
impact
sociaux
et
environnementaux.
💼​ Présence du Wemanity Learning Center permettant
formations & certifications.
🦸 Ton profil :
Tu es Data Analyst (H/F)
, tu as une expérience de plus de 3/4 ans et tu souhaites rejoindre une équipe de passionnés ambitieux?
Tu aimes mettre en valeur les bonnes pratiques, apprendre et partager ? Cette offre est faite pour toi !
🚀 Tes missions* :
Maîtrise des outils d'analyse de données.
Expérience avec les outils de visualisation de données (Tableau, Power BI).
Connaissance des techniques d'analyse statistique et des modèles de machine learning.
Utilisation de méthodologies Agile/Scrum.
Collaboration avec des équipes interfonctionnelles.
*Liste non exhaustive
🌈Pourquoi Wemanity ?
Un salaire attractif et de nombreux avantages (RTT, formations & certifications, primes vacances, CE, carte Swile …) ☀️
Des projets variés à forte valeur ajoutée 🎈
Une vraie montée en compétences avec notre pôle formation (formations, certifications) 🪄
L’opportunité d’intégrer une communauté d’agilistes soudée 🚀
La possibilité d’intégrer des squads agiles qui fonctionnent en mode startup pour intervenir sur des projets from scratch en interne ou chez nos clients 🤹‍
La possibilité de travailler en réelle autonomie (télétravail) ☀️
Une carrière évolutive au sein d’une entreprise pleine de projets 💪
Une entreprise avec un réel engagement RSE 🌳
📌
Notre processus de recrutement en 3 étapes :
1ère étape, ton profil nous intéresse ! Premier échange avec l’un de nos recruteurs passionnés pour faire connaissance et en savoir plus sur tes souhaits d’évolution.
2ème étape, l’entretien technique : c’est le moment d’échanges sur la partie technique avec un membre de Wemanity (coopérateurs).
Entretien final : Tu rencontreras un commercial afin de déterminer le projet et le client chez qui tu pourras évoluer et t’épanouir ! Suite à ces 3 échanges, je te souhaite d’obtenir ton billet d’entrée pour rejoindre l’aventure Wemanity !
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['3'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Analyst (H/F),moOngy Digital Lab,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-analyst-h-f-at-moongy-digital-lab-3890974953?position=5&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=C6F4wu8iMd3zYQGH%2FQrEKg%3D%3D&trk=public_jobs_jserp-result_search-card,"Web Transition, c’est qui ?
Fondée en 2011,
Web transition
est une entreprise de services numériques opérant sur le marché de l’IT/Digital !
Constituant une part essentielle de
MoOngy Digital Lab
, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et également en Data !
Notre objectif : nous implanter comme un acteur principal sur le marché de la Transformation Digitale en accompagnant et valorisant les compétences de nos collaborateurs !
Nous sommes convaincus que le succès de MoOngy Digital Lab réside dans la somme des potentiels de nos équipes 🤝
Ton équipe : La tribu Data
Parce qu’il est indispensable que tu puisses partager tes connaissances mais aussi en acquérir de nouvelles, tu feras partie de l’une de nos tribus : celle de la Data. De plus, cela te permettra d’être acteur dans le développement et la stratégie de Web Transition. Ce système de co-réflexion et co-construction est un fondement essentiel chez nous !
Dans cette aventure, tu :
Optimises
les requêtes SQL ;
Construis
les modèles de régression ;
Analyses
la rentabilité des actions marketing ;
Réalises
des études ad hoc sur les comportements clients ;
Produits
et
automatises
le reporting.
Rejoins-nous si tu as :
Une expérience de 3 ans minimum,
Une maitrise technique de GCP / DBT / PowerBI,
Une expérience similaire dans une startup ou dans l’industrie serait un plus !
Ton savoir-être :
Ouvert d’esprit
Respectueux des différences de chacun
Curieux
Proactif
Par où on commence ?
Un premier entretien RH d’1h pour comprendre ton parcours et tes aspirations
Un second entretien de 45 minutes avec l’un de nos Business Manager afin de valider tes compétences et qu’il se projette sur l’une des missions qu’il pourrait te proposer
Un troisième entretien de quelques minutes avec notre responsable d’agence pour te proposer d’intégrer notre superbe Team Web !
3 entretiens en peu de temps, si ton profil correspond tu intègreras très vite nos équipes 😉
Prêt pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant à cette offre ! Voici les avantages qui t’attendent en tant que Weber :
🤩 Des collègues incroyables
🏆 Certifiée Great Place to Work
🎮 Des bureaux sympas (où vous serez toujours les bienvenus)
🎉 Des teambuilding et évents tous les mois
💻 Des tributs métiers pour échanger entre Weber du même métier
Des missions chez le client qui sont accompagnées et coachées par ton manager
Un accompagnement dans ton plan de carrière et tes envies de re skilling
🤓 Un catalogue de formations certifiantes ouvert à tous les salariés
🍽️ Une carte tickets restaurant MyEdenred
❤️ Une mutuelle GrasSavoye
🚎 Une prise en charge des frais de transport à 100%
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Analyst,Volvo Group,"Vénissieux, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-analyst-at-volvo-group-3860811035?position=6&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=1Xhk0m4PORbq0Y47g5nYQQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Transport is at the core of modern society. Imagine using your expertise to shape sustainable transport solutions for the future? If you seek to make a difference on a global scale, working with next-gen technologies and the sharpest collaborative teams, then we could be a perfect match.
What You Will Do
At DIGITAL&IT DATA you will contribute to the transformation of our company, the transport industry and society at large. You will:
Drive workshops with the business stakeholders to understand their business context and to collect their needs.
Become a Data expert on one or more business domains, to draw conclusions and to provide answers to business stakeholders questions.
Access, manipulate, query, and analyze data using different software, tools and techniques.
Be part of the data modeling process (design and implementation) as well as in the definition of business rules that will ensure data consistency and quality.
Pre-process, clean and structure data to facilitate data exploration and advanced analytics activities.
Design and implement dashboards using advanced visualization tools.
Work in cross functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Be an active member of the Data Analyst Chapter, sharing knowledge and best practices with your peers.
Benefit from dedicated trainings to maintain and develop your Data Analyst skills.
Your Future Team
As Data Analyst, you will be part of A&BI (Analytics & Business Intelligence) Trucks Dealer agile team. You will have opportunity to work on heterogenous topics such as (non exhaustive list):
Paperless NLP: Support Paperless project thanks to NLP (Natural Language Processing) & AI (Artificial Intelligence) processes,
Dealer Quality program: Set of Key Performance Indicators on all Dealer processes to support Sales Area organization,
Set up Data Product on Azure to support smart business services,
Be part of the maintenance team to ensure business process continuity, ...
Who are you?
Do you dream big? We do too, and we are excited to grow together. Are you the one?
Having a master degree or equivalent in IT.
Data & Data Analysis has been your world for 3-5 years now.
You are an experienced Data Analyst looking for new challenges in the transport industry. You are proactive problem solver with innovative thinking with a strong sense of teamwork.
Writing SQL and doing data modelling are natural activities for you.
You are experienced working on data manipulation and data modeling.
You are experienced designing and developing dashboard using at least one visualization tool (such as PowerBI for example).
You are familiar with Azure Data Analytics Stack (Storage accounts, Databricks, Azure Data Factory, SQL Server, SQL Data base, etc...).
You are familiar writing scripts on Python to do data preparation/cleaning.
You have excellent communication skills in French and English (C1 level)
Ready for the next move?
Are you excited to bring your skills and disruptive ideas to the table? We can’t wait to hear from you. Apply today!
We value your data privacy and therefore do not accept applications via mail.
Who We Are And What We Believe In
Our focus on Inclusion, Diversity, and Equity allows each of us the opportunity to bring our full authentic self to work and thrive by providing a safe and supportive environment, free of harassment and discrimination. We are committed to removing the barriers to entry, which is why we ask that even if you feel you may not meet every qualification on the job description, please apply and let us decide.
Applying to this job offers you the opportunity to join
Volvo Group.
Every day, across the globe, our trucks, buses, engines, construction equipment, financial services, and solutions make modern life possible. We are almost 100,000 people empowered to shape the future landscape of efficient, safe and sustainable transport solutions. Fulfilling our mission creates countless career opportunities for talents with sharp minds and passion across the group’s leading brands and entities.
Group Digital & IT
is the hub for digital development within Volvo Group. Imagine yourself working with cutting-edge technologies in a global team, represented in more than 30 countries. We are dedicated to leading the way of tomorrow’s transport solutions, guided by a strong customer mindset and high level of curiosity, both as individuals and as a team. Here, you will thrive in your career in an environment where your voice is heard and your ideas matter.
‘
Data
’ is a new function within Volvo Digital & IT with the goal to unlock the power of data for the whole Volvo Group to become a fully data-driven company! With data, the core component of our transformation journey, we will, together with our data talents, make the Volvo Group 2030 vision happen. We will take care of all the aspects of Data, how it is quality assured, documented, made available, prepared and consumed through BI, Analytics and Machine Learning. We have an ambitious transformation in front of us, with an implementation of the Data Layer in Azure as well as the reinforcement of Data Governance and Data Management in the full Group.
The ‘Data’ function is a large multi-cultural organization with 600+ employees and contractors located mainly in 7 countries - Sweden, Poland, India, Belgium, Brazil, USA, and France. In this role you will be joining the Data Analyst Chapter team in France.
We collaborate with other parts of the organization, both in Volvo Digital & IT, with Truck Divisions, Business Areas and Group Functions. We foster an environment where ideas, thoughts and opinions can be shared. We are team players with clear common ambitions, and we win together.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['Machine Learning'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Teamwork', 'Organization']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,DXC Technology,"La Défense, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-dxc-technology-3754144852?position=7&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=%2BvXftRJJ6zT01L49fbFMwQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Job Description
Chez DXC Technology, nous agissons en tant que conseiller indépendant et de confiance dans la numérisation de l'entreprise du client. Nos employés sont responsables de l'optimisation des processus industriels de nos clients dans de nombreux domaines de marché différents ainsi que du développement, de la mise en œuvre et de la gestion de leurs mécanismes liés aux données informatiques.
DXC Technology conseille, développe et met en œuvre des solutions d'analyse et d'IA modernes en appliquant l'expertise de l'industrie, des solutions et services technologiques et commerciaux, des méthodologies éprouvées et des personnes expérimentées. Grâce à nos offres, nos capacités et notre portée mondiale, les organisations accélèrent la transformation numérique et produisent en toute confiance des résultats commerciaux reproductibles et alimentés par l'IA. Nous travaillons avec nos clients pour convertir les données en informations, les informations en idées et les idées en meilleurs résultats commerciaux.
Dans notre équipe DXC Technology, nous recherchons actuellement un Data Engineer. Les domaines typiques dans lesquels vous travaillerez sont : les solutions et services d'analyse, les plates-formes Big Data, l'intelligence artificielle et l'IoT.
Fonctions Professionnelles Essentielles
Concevoir et mettre en œuvre des pipelines d'ingestion de données par lots et en temps réel pour Azure Data Lake ou similaires sur autres Cloud Solution Providers (AWS, GCP)
Mettre en œuvre des contrôles de qualité des données
Mettre en œuvre des mécanismes de lignage, d'agrégation et de réconciliation des données (y compris des tableaux de bord et des alertes)
Mise en œuvre du catalogage des données, de l'archivage et de la reprise après sinistre
Travailler avec les équipes de source de données et de consommation de données pour s'aligner sur les structures et les schémas de données
Compétences
Familiarité avec les technologies modernes basées sur le cloud (data lakes, data warehouses, ETL/ELT, Spark)
Expérience de la mise en œuvre de pipelines d'ingestion de données / ETLs / ELTs
Connaissance générale d'Azure ou d'une plate-forme et d'un écosystème comparables – Storage Account, CosmosDB (SQL & Gremlin API), Event Hub, App Services
Expérience avec au moins 3 des technologies de données Azure suivantes ou des technologies Cloud similaires:
Azure SQL
Azure Data Lake
Azure Databricks (SQL ou Python, Delta Lake)
Azure Data Factory
Azure Analysis Services
Expérience avec SQL, Python
et
Baccalauréat ou diplôme supérieur en informatique, en ingénierie commerciale, en informatique ou dans une discipline technique connexe
1-3 ans d'expérience sur le terrain dans un rôle similaire
Maîtrise du français et de l'anglais
Personnalité
Concentré sur l'avancement du client avec des livrables de qualité dont vous pouvez également être fier
Attention aux détails et aux objectifs
Aimer faire partie d'une équipe DXC et client
Démontrer l'appropriation
Un état d'esprit positif
Ce Que Vous Obtiendrez
Un paquet salarial motivant, comprenant des avantages sociaux, des horaires de travail flexibles et la possibilité de travailler à domicile
Faire partie d'une équipe ambitieuse et en pleine croissance
Formation en cours d'emploi et accès à notre plateforme en ligne “DXC-University”
L'opportunité de travailler avec les dernières technologies pour un large éventail de clients
Recruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here
.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer (F/H),Groupe SII,"Montpellier, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-groupe-sii-3906253305?position=8&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=WCihcTF8zDatDHW40GOocw%3D%3D&trk=public_jobs_jserp-result_search-card,"SII Montpellier
accompagne ses clients dans l’intégration des nouvelles technologies, procédés et méthodes de management de l’innovation pour contribuer au développement de leurs futurs produits ou services et faire évoluer leurs systèmes d’information. Nous conjuguons de manière durable et vertueuse la satisfaction de nos clients avec le bien-être et l’épanouissement de nos collaborateurs tout en délivrant un haut niveau de performance.
Au travers de notre entité composée d’une quarantaine de consultants et de leurs expertises liées au développement applicatif, à la gestion de projets et d’infrastructures, au test et aux enjeux autour de la data et cyber sécurité. Nous intervenons aujourd’hui sur des projets à forte valeur ajoutée, ambitieux et à l’international autour des secteurs du numérique, du télécom, de la banque/assurance, de l’industrie et des services en assistance technique et/ou en engagement (centre de services, centre de compétences, ...). Nous accompagnons de nombreux acteurs locaux (Montpellier et agglomération) vous permettant de trouver plusieurs opportunités proches de chez vous ou depuis chez vous (flexibilité / mode de fonctionnement hybride).
Vous rejoindrez une équipe soudée, talentueuse et engagée qui priorise le bien-être au travail, l’humilité et le partage. Vos compétences techniques et votre savoir-être seront mis en valeur au sein de missions sur des sujets variés allant de la conception et développement d’applications web, mobile, IHM à la gestion d’infrastructure traditionnelle et Cloud en passant par la gestion de projets agiles.
Rencontrons-nous et valorisons ensemble nos savoir-faire.
Dans le but de répondre aux demandes de nos partenaires, nous ouvrons un poste en qualité de
Data Engineer
(F/H), sur
Montpellier
(34).
Vos Missions
Collecter et stocker les données
Comprendre les besoins des utilisateurs
Déterminer la cohérence des données
Implémenter des outils de pointe pour faciliter l’usage des données dans l’entreprise et améliorer l’efficacité opérationnelle.
Diplômé(e) d'un Master en Informatique, d'une école d'Ingénieur ou équivalent, vous disposez d'une expérience de
minimum 5 ans
sur des compétences similaires en DATA. Vos compétences techniques sur SSAS, SSIS, SSRS seront un vrai plus !
La rigueur, la patience et la curiosité sont vos points forts. Vous disposez d'un important esprit d’équipe et faites preuve de bienveillance au quotidien.
Le Groupe SII est une société d’ingénierie et de conseils en technologies (ICT) et une entreprise de services numériques (ESN).
Nous sommes au cœur de l'innovation au service de grands comptes dans des secteurs d'ingénierie variés.
En 2023, pour la
6e année consécutive
, SII France a obtenu le label
Great Place To Work
®.
Nous avons été reconnus
3e entreprise de « + de 2500 salariés »
où il fait bon vivre.
Nous sommes très fiers d’obtenir cette reconnaissance de nos salariés !
Ce succès est le reflet de notre culture basée sur notre volonté de proposer à tous nos salariés un cadre de travail épanouissant pour le développement de leurs compétences et carrières.
En fonction de la mission, il est possible de réaliser jusqu'à
50 % de télétravail
grâce à notre accord dédié. Rejoignez le
mouvement #fungenieur
dans lequel la passion pour la technologie, la créativité, la proximité et l’esprit d’équipe sont mis à l’honneur.
Le Groupe SII est une société handi-accueillante, signataire de la Charte de la diversité en entreprise.
Alors si ces valeurs vous parlent, rejoignez-nous !
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Créativité', 'Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': ['Hybride'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Senior Data Engineer,Harnham,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-harnham-3903984738?position=9&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=7ERt5GvD3YB7YI%2FhPxHcXQ%3D%3D&trk=public_jobs_jserp-result_search-card,"SENIOR DATA ENGINEER
PARIS (75)
75-80K EUR
Cette entreprise spécialisée dans le marketing et la publicité mobile recherche activement un(e) Senior Data Engineer. Le Machine Learning constitue le cœur de son expertise, avec des solutions technologiques novatrices visant à améliorer la performance publicitaire sur smartphones et tablettes.
VOTRE MISSION :
Assurer le bon fonctionnement de l’infrastructure de données.
Superviser le traitement, le stockage, et l’agrégation des données.
Proposer de nouvelles idées pour améliorer les outils et les flux de travail.
Gérer les algorithmes de data science.
Collaborer étroitement avec l’équipe SCRUM.
Rationaliser les processus de données pour une efficacité maximale.
VOTRE PROFIL :
Diplome d’une école d’ingénieur / Master
Au moins 4 ans d’expérience (minimum hors stage et alternance)
Compétences solides en Spark, Scala, Kafka et Python (obligatoire)
Miatrise de la gestion des systèmes Big Data.
Connaissance d’AWS ou GCP
Anglais: Fluent obligatoire
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Data Analyst H/F,GECO RECRUTEMENT,"Pontivy, Brittany, France",https://fr.linkedin.com/jobs/view/data-analyst-h-f-at-geco-recrutement-3915127710?position=10&pageNum=45&refId=helkV5fb1RK2O%2B2D3vOHXQ%3D%3D&trackingId=oZ0%2FOs6rk3MLsRzAQbCLcQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Geco est un cabinet de recrutement indépendant.
Nous intervenons auprès des cabinets et des entreprises dans la recherche de profils ciblés.
Nous accompagnons les candidats dans leurs recherches d'emploi et s'assurons que leurs souhaits professionnels se réalisent.
En lien permanent avec l'entreprise et le candidat tout au long du processus de recrutement, l'écoute et la disponibilité vont guider nos interventions afin de répondre au mieux à l'intérêt des deux parties.
Nous recherchons actuellement un Data Analyst H/F pour rejoindre une équipe dynamique au sein d'une industrie en pleine croissance située à proximité de Pontivy.
En Tant Que Data Analyst, Vous Serez Responsable De L'analyse Des Données Afin D'optimiser Les Processus Et De Fournir Des Informations Clés à L'entreprise. Vos Principales Missions Seront Les Suivantes
Collecter, nettoyer et organiser les données provenant de sources diverses
Analyser les données pour identifier des tendances, des corrélations et des insights
Créer des tableaux de bord et des rapports pour présenter les résultats de manière claire et compréhensible
Collaborer avec les différents départements de l'entreprise pour répondre à leurs besoins en termes d'analyse de données
Participer à l'amélioration des outils et des processus liés à l'analyse de données
Veiller à la sécurité et à la confidentialité des données
Issu(e) d'une formation Bac +3, vous justifiez d'une expérience sur des fonctions similaires d'au moins 1 an.
Vous avez une expertise dans l'utilisation, l'administration et le développement de QlikView / QlikSense ainsi que de Power BI.
Possibilité de télétravail 2 jours par semaine.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Ingénieur Data SPARK/SCALA (H/F),DHM IT,"Neuilly-sur-Seine, Île-de-France, France",https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-spark-scala-h-f-at-dhm-it-3664561715?position=1&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=MDVOaMAeHhMiqmI8Ki21tA%3D%3D&trk=public_jobs_jserp-result_search-card,"Dans le cadre de notre croissance et Afin de renforcer notre Squad Data, nous recherchons plusieurs Ingénieurs Data Spark/Scala
Intégré.e au sein de nos équipes basées à Neuilly Sur Seine et/ou la région parisienne, vous pourrez mettre en pratique vos acquis et développer de nouvelles compétences techniques.
MISSIONS:
Au sein de l'équipe projet Business Intelligence & Big Data, l'ingénieur SPARK/SCALA aura les activités suivantes :
Conception détaillée, fonctionnelle et technique.
Développements SPARK / SCALA
Contribution à la formalisation des plans de tests, exécution des tests unitaires et d'un premier niveau de tests d'intégration.
Résolution des anomalies pendant les phases d’intégration fonctionnelle et de recette utilisateur.
Packaging et déploiement en pre-production et production.
Optimisation des traitements afin de garantir la qualité de service standards du DWG.
Mise en œuvre des solutions industrielles et de réentrance permettant une reprise optimum des traitements en cas d’incident en production.
Mise en service et accompagnement au déploiement.
Suivi des environnements.
PROFIL:
Une connaissance d'Oracle est nécessaire
Une expérience >5 ans sur des technologies SPARK & SCALA est nécessaire
Une connaissance de contexte technologique similaire est un plus
SOFT SKILLS:
Passionné.e par les technologies innovantes;
Désireux.se de t’investir dans des projets challengeants et gagner rapidement en responsabilités;
Pédagogue, emphatique et ""Problem Solver"";
Doté.e d’un excellent relationnel, d’un sens prononcé du service et de la qualité;
Excellent niveau de Français à l'oral et à l'écrit, l'Anglais serait un véritable plus.
AVANTAGE / CONTEXTE :
Entreprise de nouvelle génération à taille humaine, DHM IT s’engage fortement dans la mouvance RSE en proposant à ses consultants et salariés un cadre de travail sain où chacun pourra s’épanouir, se former et progresser à son rythme et selon ses aspirations.
Rémunération attractive;
Un fort esprit Startup nouvelle génération;
Participation dans notre stratégie RSE;
Formation, certification, Workshop, DHM University;
Carte resto, Titre de transport, Mutuelle et prévoyance;
Team buildings, Events, Sport Collectif;
Travail remote friendly;
Un environnement de travail 100% Agile / Cloud;
Des challenges, des nouvelles expériences et des projets innovants;
Une hiérarchie Smart.
Show more
Show less","{'ProgLanguage': ['Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Pricing Data Engineer,Luko,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/pricing-data-engineer-at-luko-3914821636?position=2&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=00Usttp0bRUbWMGJLkCsxw%3D%3D&trk=public_jobs_jserp-result_search-card,"Your mission in the team:
Luko x Allianz Direct has a mission to create insurance products which meet each user needs, transparent and quickly available, with a fair price.
As a Pricing Data Engineer, your job is build & transform the data and develop the tooling required for pricing analysis, strategies, and operations.
You will work closely with the Pricing team as well as with the Product team.
Your main missions will be:
Create and refine our pricing data infrastructure, laying the groundwork for continually improving analytics capabilities and helping create the best data models for modern insurance pricing
Develop and maintain our pricing monitoring and testing tools, including automating processes and integrating interfaces with product teams to enhance testing efficiency and streamline rollout productivity
Implement live reporting mechanisms to effectively communicate pricing monitoring results, while actively contributing to steering the company towards its profitability objectives through data-driven insights and strategic pricing adjustments
Design and implement scalable data models and control structures to encompass all products and channels, ensuring seamless integration and adaptability across the entire spectrum of our operations
Participate on developments related to the pricing engine
Your profile:
📍 Role based in Paris
To excel in this role, we recommend the following qualifications and attributes:
Professional Experience: 2 to 5 years of experience as a Data Engineer or Analyst.
Education: A Master's degree in Data Science, Statistics, or Computer Science is preferred but not mandatory.
Specialization: Prior experience in Data & Analytics is highly desirable.
Analytical Skills: Exceptional analytical and synthesis abilities.
Technical Proficiencies:
Proficient in data processing platforms, specifically DBT.
Strong programming skills in Jinja and SQL.
Familiarity with version control systems, particularly GitLab.
Languages: Proficiency in English is required; French proficiency would be an asset.
Personal Attributes: Must be a collaborative team player, detail-oriented, and well-organized. Should also demonstrate proactivity and the ability to work independently.
What is in it for you?
This is an opportunity to join a company in the process of scaling up, a team on a human scale and to work with Allianz Direct's European subsidiaries to exchange best practices and deal with cross-functional issues, and why not benefit from group mobility in Europe?
International and friendly team & Office in Paris
Group mobility in Europe
Meal vouchers
Health Insurance
Remote-friendly policy
Free book policy
Our recruitment process:
First interview/ Introduction call with your future manager Alexis (Lead Pricing Manager)
Case study + debrief with Alexis and Julien (Chief Actuary)
Technical interview with Gautier (Backend Developer)
Lukofit interview with a member of the People team
Allianz Direct France x Luko, Who are we?
We created Luko with a simple vision.
50% of consumers don’t trust their insurer, yet it is mandatory in many cases and it is the
only stakeholder you can turn to when damage occurs in your home. So we decided to
reinvent insurance to make it finally customer-centric like we believe it was meant to be.
We started by creating the most transparent and useful insurance company out there:
one that you can finally trust, because its business model was designed for positive impact and because its product experience is meant to empower users rather than mislead them.
We created the most intuitive and user-obsessed insurance experience you can wish
for:
subscription in 2 minutes, a simple offer, customisable coverages, no hidden fees, policies without any engagement, termination in 3 clicks.
In 6 years, we have convinced over 400,000 users to trust us. We rapidly became the online home insurance market, putting technology at the service of customer satisfaction and operational efficiency: by 2022, 25% of home insurance policies sold online in France were Luko policies; and nearly one in two new Luko customers took out a policy on the
recommendation of friends and family.
In February 2024, Luko joins Allianz Direct, the digital pan-European subsidiary of the
Allianz Group, with the aim of becoming the market leader in direct home insurance in
France. This makes Luko emerging the strongest neo-insurer.
Together, and because Luko and Allianz Direct share a common vision and DNA, we will continue our mission to shake up the market by offering the best value for money and even more simplicity and transparency.
Allianz Direct DNA
Becoming part of the Allianz Direct organisation means that you have a match with our DNA. We have the following 6 core values:
Customer obsessed
Communicative
Data-driven
Agility
Team player
Open-minded
You’ve read all the way, you may as well apply!
Our company-wide communication language is English (written & spoken).
We would therefore appreciate it if you could send us your application’s content (CV, cover letter, portfolio…) in English.
Belonging: Diversity, Inclusion & Equity
We believe inclusion is the result of ongoing commitment. We want to build an authentic environment, open to everyone regardless of nationality, physical ability, family structure, age, socio-economics, civil status, sexual orientation, gender identity, ethnic origin, religion, belief or anything else that makes your life experience unique. We are continuously working to create a diverse, equitable and inclusive environment, where everyone has a space to thrive.
Show more
Show less","{'ProgLanguage': ['Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistics'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML'], 'FrSoftSkills': ['Communication', 'Organisation'], 'EnSoftSkils': ['Communication', 'Adaptability']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
CDI - Ingénieur Data (H/F),Hermès,"Pantin, Île-de-France, France",https://fr.linkedin.com/jobs/view/cdi-ing%C3%A9nieur-data-h-f-at-herm%C3%A8s-3850308968?position=3&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=aDeaStLnXo4DASVbz%2FiWIA%3D%3D&trk=public_jobs_jserp-result_search-card,"Contexte
Au sein de la DSI Groupe de la Maison Hermès, la Direction des Infrastructures et des Opérations fournit et supporte l’ensemble des services d’infrastructure avec les niveaux d’expertise requis. Elle construit et sécurise le fonctionnement des plates-formes applicatives afin de fournir une infrastructure technologique stable et efficace pour les clients internes au niveau du groupe et en local.
Activités principales
En tant qu'Ingénieur Data (H/F), vous êtes responsable d’un ou plusieurs services technologiques sur les plateformes DATA (BDD, Data Lake, Data Streaming).
Ce domaine désigne l’ensemble des services visant à assurer la collecte, le stockage, la valorisation et la mise à disposition des données. Le périmètre Data intègre notamment la gestion des bases de données (relationnelles et non relationnelles), solutions de données non structurées (Data cloud), flux temps réel (Data Streaming) ainsi que les outils de réplication et de synchronisation.
Vos principales responsabilités sont de :
Assurer le Build et le Run des services de votre périmètre
Définir le planning de développement, les exigences et les prérequis au déploiement des services de votre périmètre
Être responsable du développement global et du packaging d'un service
Construire et gérer les socles et les services technologiques de votre périmètre et en définir la stratégie d’évolution en collaboration avec l’équipe Architecture & Innovation
Piloter et monitorer la performance et la disponibilité des socles et services
Assurer la gestion sur l’ensemble du cycle de vie de vos socles et services (évolution, licences, maintenance, décommissionnement, etc.), gérer leur obsolescence et réduire la dette technique.
Fournir un support technique N2/N3 sur votre périmètre
Profil souhaité
Diplômé(e) d'un Bac + 5, vous disposez d'au moins 5 ans d'expérience dans la construction, l'intégration, le déploiement d'infrastructure système/réseau dans des environnements Cloud et hybrides.
Vous maitrisez les technologies de bases de données relationnelles (MSSQL, Oracle, PostGreSQL) et non relationnelles (MongoDB). Vous possédez de solides compétences dans le développement et la mise en place de pipeline de données, notamment le Data Streaming. Les technologies de plateformes Data Modernes, Cloud et On Premise, n'ont plus de secrets pour vous.
Orienté(e) client et esprit d'équipe, vous êtes capables de traduire et analyser les exigences business en exigences techniques en gardant un discours impactant. Vous savez appréhender et gérer les risques et recherchez à monter rapidement en compétences sur de nouvelles technologies et procédures.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['PostgreSQL', 'Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Hybride'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Data Analyst Junior,Unlimitail,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-junior-at-unlimitail-3912999630?position=4&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=DFtU9h6mBh4XTShonIKYVQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Unlimitail
is a joint venture between
Carrefour
Group and
Publicis
Group, intending to become the leader of the
retail media market in Europe and Latin America
. It combines the power of Carrefour Links' in-store and e-commerce assets and traffic with Publicis
Citrus Ad and Epsilon's
on-site and off-site retail media technologies. The goal of Unlimitail is to become the premier retail media player in Continental Europe and South America for both in-store and digital channels.
Missions
Within the Product team, your mission will be to participate in the development of Unlimitail’s analytics capabilities. Your environment will be millions of transactional data coming from major retailers on the market to analyze and improve the performance of online and offline marketing campaigns.
As we work with both retailers (providing media inventory) and brands (buying media inventory), you will have the opportunity to gain a deep understanding of the retail media ecosystem.
You will work with different teams (Ad Operations, Marketing, Sales) in different countries and on a wide variety of topics:
- Perform e-commerce campaigns analysis to provide detailed insights
- Conduct end-to-end ad hoc analysis to help retailers best monetize their inventory
- Participate in the design and maintenance of databases, using SQL to store and manage data efficiently
- Work on automating data analysis and data visualization capabilities
- Work on Unlimitail’s customer lifetime value calculation capability
- Participate in the development of new, cutting-edge abilities to create the best
Requirements
:
· Master's degree in Data Analysis, business analytics, Computer Science, Statistics, Mathematics, Engineering, or a related field.
· Data Analysis: Proven ability to analyze large datasets, identify trends, and extract actionable insights through using strong SQL skills for querying and data extraction.
· Programming Languages: Python and SQL are mandatory. Any other is a plus
· Data Visualization: previous experience in creating interactive and insightful visualizations, using tools like Power BI or Looker is a big plus
· Knowledge of GCP or Azure environment is a plus
· Problem-solving: Adept at approaching analytical problems and proposing effective solutions.
· Good communication skills in English. Another language (Spanish, Portuguese) is a plus.
· Strong team player with the ability to collaborate effectively
Benefits
:
· 2 days of WFH
· 90% reimbursement of public transportation fees
· Office located in central Paris (Chatelet)
· Swile card of 10€/working days
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': ['Statistics'], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': [], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Consultant Data Engineer / Data Management F/H,VISEO,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/consultant-data-engineer-data-management-f-h-at-viseo-3111089632?position=5&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=t3x0qD06L2z9%2FHONHHn2cA%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous avez une forte appétence pour
l'intégration de donnée et le développement
?
Vous souhaitez travailler sur des technologies du
Cloud et des solutions innovantes
?
Vous êtes adepte de la
veille technologique
?
Au sein de VISEO nous recherchons un
Consultant Data Engineer / Data Management F/H
pour intervenir sur nos projets et contribuer au développement de notre Practice « Data Management »
VISEO organise et sponsorise de nombreux événements techniques en interne et en externe : MixIT, Webinar, TechAnHour, Snowcamp... Participer à ces évènements sont pour nous un moyen d’animer la communauté et de conserver un haut niveau d’expertise.
Vos missions :
Analyser
et
comprendre
les besoins des utilisateurs en termes de
collecte
,
stockage
,
valorisation de la donnée
Proposer une architecture adaptée
(On Premise, Cloud, hybrides, etc.)
Concevoir les modèles de données
et
les processus
(data flows, Workflows, etc.)
Mettre en œuvre
la plateforme de
Data Management
et les
développements nécessaire pour l’intégration
(ETL),
la valorisation
(Data Quality),
l’administration
(Data Governance),
la distribution
(Data Hub) et
le stockage de la donnée
Tester l’architecture et
les développements
.
Vérifier
la cohérence
fonctionnelle
des données en collaboration avec les
référents métiers
Déployer la solution
en environnement de recette utilisateurs et éventuellement, en environnement de
production
Dans le cadre de projets
Data
, vous accompagnerez nos clients autour de la
mise en place de solutions
qui auront pour
objectif la valorisation des données
comme
capital stratégique
de l’entreprise.
Pour cela vous serez amené(e) à utiliser les
plateformes de Data Management modernes
de nos
partenaires
tel que
Microsoft, Alteryx, Informatica, Talend, SAP, Microsoft Azure, Boomi, etc.
Ces activités pourront être menées dans des modes de
collaboration différents
(forfait, régie, mode Agile, méthodologie spécifique…).
Votre profil :
Vous possédez une expérience sur tout ou partie des activités décrites précédemment, en particulier sur les
phases de conception et de développement de projets Data
.
Une expérience acquise autour
d’actions concrètes d’optimisation
(technique ou fonctionnelle) serait un plus.
Vous maîtrisez tout ou partie d’une ou plusieurs
des solutions ETL classiques
(Informatica Power Center, Talend Studio, Microsoft SSIS, Alteryx, etc.)
et\ou IPaaS
(Azure Data Factory, Informatica IICS, Talend Cloud, Boomi, etc…)
Vous maîtrisez tout ou partie d’une ou plusieurs solutions de
stockages de données
(Snowflake, Redshift, PostgreSQL, Oracle, etc.)
L’utilisation
d’outils de reporting / data visualisation
(Tableau, Power BI, QlikSence...) est également appréciable.
Vous
appréciez la relation client
(interlocuteur technique, contact utilisateur…) et possédez un véritable
sens du service
avec cette double compétence technico-fonctionnelle.
Vous êtes
rigoureux(se), dynamique et doté(e) d’une curiosité, d’une agilité et d’un esprit d’équipe
vous permettant de vous adapter à différents contextes.
Intégrer nos équipes au quotidien, ça veut dire quoi ?
Vous ferez partie de la
communauté Data
: la proximité et la taille humaine de notre organisation vous permettront de rendre visible vos initiatives et d’évoquer facilement vos projets. En parallèle, le dynamisme de l’entreprise et sa croissance perpétuelle multiplieront vos opportunités d’évolution.
Vous bénéficierez d’un management de proximité par votre mentor tout au long de votre parcours chez VISEO : Votre mentor, consultant expérimenté de votre practice, viendra régulièrement échanger avec vous sur les challenges de votre mission, faire chaque semestre le bilan de vos réalisations et évoquer vos ambitions futures et les moyens de les réaliser.
Vous disposerez
de multiples moyens pour monter en compétences et découvrir de nouveaux domaines : formations, certifications, Brown Bag Lunchs, ateliers, meet-ups, rencontre d’experts, séminaires techniques…
#VISEO SPIRIT :
Un programme d’apprentissage en e-learning : accès digital academy et 7-speaking
Un accompagnement pour le bien-être : Sophrologie, Yoga, Gymlib, IKVélo …
Deux jours de télétravail par semaine
Du matériel informatique puissant et un double écran
Des locaux agréables proches du parc de la Cité de l'Espace
N’attendez plus, rejoignez VISEO. Devenez un #PositiveDigitalMaker !
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['PostgreSQL', 'Oracle', 'Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration', 'Initiative']}","{'JobDetail': ['Hybride'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer & Architect 100% Télétravail avec Quelques Déplacements à Lyon H/F,Proxiel,"Montpellier, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-architect-100%25-t%C3%A9l%C3%A9travail-avec-quelques-d%C3%A9placements-%C3%A0-lyon-h-f-at-proxiel-3913995274?position=6&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=LrBkKjeQvUt5UD52y8KOhQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Depuis 1999, PROXIEL accompagne des entreprises dans leur développement en assurant des prestations de conseil et d'ingénierie dans le domaine des technologies.
Proxiel : C'est plusieurs pôles d'activités.
Nous mettons un point d honneur à associer votre bien-être - adaptabilité en fonction de vos contraintes (possibilité de télétravail). Des solutions alternatives, peuvent être envisagées, dans la mesure où elles sont compatibles avec le business que nous entreprenons. Nous souhaitons que chacun de nos salariés s investissent dans nos projets et que notre entreprise soit animée par un projet commun : la réussite de chacun !
Notre approche est simple alors restons transparents dans nos échanges.
Notre siège est implanté à Montpellier PROXIEL. Nous disposons également d une agence sur Paris
Vous présentez des compétences dans les nouvelles technologies en qualité de techniciens développeurs ingénieurs, coté développement ou réseau, vous êtes basés ou mobile sur MONTPELLIER PARIS LYON TOULOUSE MARSEILLE AIX EN PROVENCE, NICE rejoignez-nous !
Bonjour,
Nous recherchons pour notre partenaire un Data Engineer & Architect (100% télétravail) avec quelque déplacement à Lyon :
Vous êtes en charge de la conception et de la construction de pipelines de données évolutifs permettant de récupérer, d'agréger et de prétraiter efficacement les données provenant de différentes sources, tout en garantissant une fiabilité et des performances élevées.
Vous êtes en charge de la conception des modèles de données, des solutions de stockage et des schémas d'accès.
Vous avez la possibilité de collaborer avec les parties prenantes pour comprendre les besoins et pour définir et faire évoluer la stratégie d'architecture des données, y compris la modélisation des données, le stockage et les schémas d'accès.
Vous travaillez avec une équipe agile interfonctionnelle pour intégrer des données provenant de divers systèmes et sources, effectuer des processus d'extraction, de transformation et de chargement (ETL) et maintenir l'intégrité des données tout au long du pipeline, en itérant sur les solutions et en communiquant sur les progrès réalisés.
Maîtrise des langages de programmation
Une solide compréhension des technologies de base de données (par exemple, SQL, NoSQL), des entrepôt de données et d'Azure
Expérience en architecture de données la conception de modèles de données, la définition de solutions de stockage et les schémas d'accès aux données.
Familiarité avec le Machine learning, les concepts de traitement du langage naturel (NLP) et la Génération Augmentée de Récupération (RAG) est un plus.
Vous avez d'excellentes compétences en matière de communication et de collaboration, et êtes capable de travailler efficacement dans un environnement d'équipe et dans le cadre de projets agiles.
Bac +5 ou équivalent
minimum : 3 ans d'expérience
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning'], 'FrSoftSkills': ['Communication', 'Adaptabilité', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
ALTERNANCE - DATA Engineer (F/H) - LILLE,BPCE Solutions informatiques,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/alternance-data-engineer-f-h-lille-at-bpce-solutions-informatiques-3913916322?position=7&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=FY8M2DqNOomcT8cTtN9c0g%3D%3D&trk=public_jobs_jserp-result_search-card,"Description de l'entreprise
BPCE Solutions informatiques, c’est un acteur IT de référence implanté au cœur des territoires. Ce sont près de 2 600 femmes et hommes au service des Banques Populaires, des Caisses d’Epargne et de plusieurs métiers spécialisés du Groupe BPCE, 2e acteur bancaire en France.
Nous imaginons et développons des solutions IT innovantes pour faciliter le quotidien de nos utilisateurs. Chaque jour, ce sont près de 1 Français sur 2 qui utilisent nos solutions !
Entreprise inclusive et engagée, ce sont vos compétences qui font la différence.
Le télétravail et les horaires flexibles vous permettent de préserver votre équilibre de vie. Et en présentiel, vous bénéficiez d’un environnement innovant et responsable.
Notre vision de l’alternance ? 🚀
Vous êtes accompagné(e) et encouragé(e) par une équipe bienveillante
Vous travaillez sur des projets d’envergure au service de nos clients
Vous apprenez du savoir-faire de personnes passionnées par leur métier
Vous déployez votre potentiel et consolidez votre projet professionnel
Alors, rejoignez-nous pour profiter de la diversité du terrain de jeu que propose BPCE Solutions informatiques !
⚡
Poste et missions
Intégrez la Plateforme Distribution et Data au sein de BPCE Solutions informatiques !
Au sein de
l'équipe Produit DATA ""Socle Décisionnel MYSYS""
de BPCE-SI, sous la direction du Scrum Master d'une des squads du produit, nous recherchons notre alternant(e).
Le patrimoine technologique du produit est Oracle, AIX/Linux/Windows, VTOM, PowerBI/Business Object, semarchy, Angular, JSP, Java, python, shells, VBA, Power Designer.
Gestion en mode Agile (Scrum/Kanban)
En tant que
DATA Engineer
(F/H) en alternance, vous aurez pour missions :
Prendre en charge des induits projet pour alimenter des entrepôts de pilotage DATA du SI MYSYS des Caisses d'Epargne avec l'ETL Semarchy. Vous assurez l'intégration de données, qui seront exploitées par les établissements bancaires. Vous mettrez en place les flux pour répondre au besoin des etablissements.
Il sera demandé également de compléter des informations fonctionnelles concernant le patrimoine des tables du produit.
Lors De Votre Alternance, Vous Vous Formerez Aux Techniques D'alimentation De La Squad Basées Sur L'écosystème Suivant
ELT SEMARCHY
SGBD Oracle/Exadata
Ordonnanceur VTOM
Système AIX/Linux
Profil et compétences requises
Vous recherchez une alternance dans le cadre de vos études supérieures en informatique de niveau
Bac+3 à Bac+5
.
Vous avez envie d’apprendre et d’intégrer une entreprise dynamique et agile qui vous permettra d’acquérir un réseau et une première expérience solide.
Vous avez à cœur d'accompagner vos clients dans des projets d'envergure.
Vous êtes rigoureux(se) avec une bonne capacité d'analyse et de synthèse.
Vous avez un bon relationnel et un esprit positif, vous êtes curieux(se) et vous appréciez travailler en équipe.
Compétences Et Connaissances Techniques
Exigé :
Connaissance du SQL
compréhension du fonctionnel
Souhaité
principes architecturaux DATA.
Connaissance d'un Système d'exploitation ""Unix-like""
Informations Complémentaires
Date de démarrage souhaitée :
Rentrée 2024
Rythme d'alternance souhaité :
Semaines complètes
Durée de l'alternance souhaitée :
1 an ou 2 ans
Notre processus de recrutement se compose de deux entretiens : un entretien RH avec Dana, puis un entretien opérationnel avec Martial et Paul-Aziz.
Informations complémentaires sur le poste
🤝BPCE SI, c'est un cadre de travail agréable avec un accompagnement de proximité, des formations, de multiples évolutions possibles au sein du groupe.
🚉 80% de prise en charge des frais de transports en commun !
☀️ Environ 9 semaines de congés annuels (congés et RTT).
💻 Télétravail hybride ! 30 jours de télétravail par trimestre et flexibilité des horaires.
🎳 Un CSE actif ! Des locaux attractifs et des comités d'animations par site.
🎖 Entreprise Partenaire premium des JO2024 et engagée sportivement !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': ['Linux', 'Windows'], 'DBMS': ['Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': ['Hybride'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5', 'Bac+3'], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
Data Analyst F/H,SOFTEAM,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-f-h-at-softeam-3587086987?position=8&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=EqEhNZbVAIsxTMGqS%2BjVhA%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous évoluez dans le domaine de la Data et souhaitez intégrer
un leader de la transformation numérique spécialisé dans les secteurs de la Banque, du Luxe, de l'Assurance, de la Finance, de l'Energie et la possibilité d'évoluer au sein du Groupe Docaposte !
Softeam est labellisé ""HappyIndex® AtWork "" 2022 pour la 5ème année consécutive !
Nos collaborateurs
travaillent en « mode projet » et accompagnent de bout en bout nos clients sur des problématiques de Gouvernance.
CE QUE NOUS RECHERCHONS
Data Analyst
pour intervenir sur nos projets et contribuer au développement de notre Practice « Augmented analytics »
CE QUE NOUS ATTENDONS DE VOUS
Vos missions :
Maîtriser les outils statistiques et les informations nécessaires à la mise en place d'une base de données,
Gérer parfaitement les différentes technologies spécifiques au big data,
Extraire les données du système source,
Extraire et traduire des données business en données statistiques,
Modéliser et assurer les mises à jour régulières de la base de données,
Contrôler la qualité des données,
Mettre en œuvre une data warehouse
Synthétiser et vulgariser les informations pour les rendre accessibles aux utilisateurs.
VOUS ETES
Ingénieur(e) de formation avec une spécialité Business Analytics, Big Data, Applications des Masses de Données (IAMD), Systèmes Décisionnels..
vous disposez d'une expérience de
3 ans minimum
en tant que
Data Analyst/Data Miner
..
Vous avez une expertise sur un ou plusieurs des outils d'alimentation et de traitement suivants :
SQL, Alteryx, Dataiku, Python
..
Vous avez une expertise sur un ou plusieurs des outils de data visualisation suivants :
Power BI, Looker/GCP, Qlik, Tableau Software,
Bon(ne) communicant(e), curieux(se) et adaptable, v
ous êtes doté(e) d'une grande capacité d’analyse et de synthèse et d'un bon esprit d'équipe,
Votre maîtrise de l'
anglais
vous permet d'évoluer dans un contexte international.
NOUS VOUS OFFRONS
Des
missions engageantes
auprès des grands acteurs du marché.
Un management de proximité avec
Gilles SALVADOR, Directeur du Centre d'Expertise Data
, toujours bienveillant et à à l'écoute et avec qui vous pourrez échanger au quotidien sur les enjeux de votre mission et évoquer vos futurs projets afin que nous puissions vous aider à les réaliser.
La possibilité d’évoluer et de monter en compétences grâce à des
formations et à des certifications auprès de nos clients et de nos consultants
, des
12@13
, notre Entité
Softeam Institute, Organisme de formation interne
de renommé qui délivre des formations auprès de nos clients...
QUI SOMMES-NOUS ?
SOFTEAM DATA
est une marque de
DOCAPOSTE
spécialisée dans l'informatique décisionnelle et les nouvelles technologies. Nous apportons notre expertise à nos clients, principalement des Grands Comptes de la place financière française, dans des projets de transformation digitale et cognitive.
2000 Softeamien.nes
sont dédié.es à la transformation métier et digitale de nos clients et ont généré 200 M€ de chiffre d’affaires en 2020.
SOFTEAM SPIRIT
Des
communautés d'expertises sur les sujets de la Data
De
super nouveaux locaux qui sont en plus accessibles facilement
Une
école de formation intégrée
Des
évènements : des soirées avec les consultants, des 12@13...
Une entreprise
labellisée ""Happy at Work"" pour la 5ème année consécutive
.
N’attendez plus, rejoignez SOFTEAM et venez nous rencontrer dans nos nouveaux locaux situés à la Défense #DevenezSofteamien !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Statistiques'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
"Consultant Data Engineer, PYTHON, JAVA & SCALA – H/F",Cat-Amania,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/consultant-data-engineer-python-java-scala-%E2%80%93-h-f-at-cat-amania-3888905331?position=9&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=8jIqzUqg0HTQbf6m4t9bGw%3D%3D&trk=public_jobs_jserp-result_search-card,"Cat-Amania
Créée en 1999, Cat-Amania fait partie des ESN les plus compétitives dans les régions où elle est implantée et se spécialise sur des projets d’envergure orientés métiers : Banque, Assurance, Grande Distribution et Protection sociale mais aussi Energie, Industrie, Secteur Public.
En 2023, Cat-Amania c’est 90,8 millions d’€ de chiffre d’affaires, 1100 collaborateurs répartis sur 15 agences en France et 6 agences à l’international : Maroc, Luxembourg, Canada et Suisse.
Aujourd'hui, l’objectif est de conforter le développement de nos agences et d’ouvrir de nouveaux pays en Europe, conserver notre croissance pour atteindre 100 millions d'€ de CA en 2024.
L’Agence de Paris
Fort de nos référencements et présente depuis 1999 l’agence Cat-Amania Paris bénéficie d’une croissance solide et dynamique permettant d'offrir de nombreuses opportunités à nos collaborateurs. Avec plus de 200 consultants, nous avons su gagner la confiance de nos clients et la conserver grâce à l’investissement de chacun, car Cat-Amania c’est avant tout une approche humaine avec la volonté d’entretenir des liens étroits au sein de ses équipes.
Le poste
Participer à la réalisation de projets décisionnels et au développement des processus d’intégration de Data Hub, de DataWarehouses ou Data Lakes en architecture On Premise ou Cloud & Hadoop.
Accompagner les Métiers dans l’expression de leurs besoins en termes de conception de flux de données et modélisation des traitements.
Définir et produire les dossiers de conception fonctionnelle et technique des flux de données et des traitements.
Définir et produire les scripts de traitements de données structurées ou NO-SQL: Data Prep, Data Quality Management, Calculs et traitements de données en mode batch et/ou real-time, interfaces exports/imports, interfaces API …
Définir et produire les processus d’exploitation IT des traitements de données : supervision, scheduling, sécurité, gestion des log …
Concevoir les protocoles de tests. Organiser et réaliser les plans de tests. Réaliser des tests unitaires et de recettes
Rédiger des manuels utilisateurs et former les utilisateurs
Développer des requêtes PYTHON et/ou JAVA et/ou SCALA
Être en support aux Data Analystes pour réaliser des études statistiques et des échantillons : profil et segmentation client, analyse de questionnaires …
Votre profil
Diplômé·e d’un cursus ingénieur en Informatique, vous justifiez d’une première expérience de plus de 3 ans dans les activités de Data Intégration, Data Ingénierie et de développement PYTHON, JAVA, SCALA
Vous maîtriser le framework HADOOP, SPARK
Vous maîtriser les outils Big Data suivants : YARN, PIG, HIVE, KAFKA ; FLINK, SPLUNK
Vous avez conçu, développé et déployé en production des traitements de différentes natures : Processus d’export, Processus d’import, Migration/reprise d’historique
Vous maîtriser les problématiques de Data Intégration dans un contexte multi-technologie : DB2, ORACLE, SQL SERVER, HDFS …
Vous maîtriser les problématiques de Data Intégration dans un contexte multi-technologie NOSQL : MONGODB, NEO4J, COUCHBASE, HBASE, CASSANDRA
Une première expérience des méthodes agiles & Devops serait un plus
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', ' MongoDB', 'Cassandra', 'Neo4j', 'HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['Oracle', 'SQL Server'], 'SoftBigDataProcessing': ['HBase'], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Statistiques', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Stagiaire Data Engineer (H/F),V and B,"Château-Gontier-sur-Mayenne, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/stagiaire-data-engineer-h-f-at-v-and-b-3863792436?position=10&pageNum=47&refId=ulU0mGztWnF4llUcE%2BYHkw%3D%3D&trackingId=hhvsUc9971uvM8ZELL4SlQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Rejoignez la team V and B !
Mission
V and B a créé un nouveau concept basé sur des lieux, des produits et une communauté intégrant tous ses acteurs (siège, franchisés et clients). Notre mission : Proposer et développer le goût des bons moments à partager de manière simple et responsable, avec des événements de plus en plus grands comme le V and B Fest’ ou notre participation au Vendée Globe. Notre réseau compte aujourd’hui 278 magasins qui œuvrent chaque jour à se démarquer pour offrir la meilleure satisfaction client, le tout avec une touche d’imprévu.
Le/la Stagiaire Data Engineer Sera Intégré/ée à L'équipe En Charge Du Projet De Gestion Des Erreurs Des Flux D'intégration De Données, De L'analyse Des Logs, Et Du Monitoring De L'état Des Flux. Sous La Supervision Du Responsable DATA, Vos Principales Missions Sont Les Suivantes
Aider à identifier, analyser et résoudre les anomalies et erreurs dans les flux d'intégration
Proposer des solutions techniques pour optimiser la qualité et la fiabilité des flux
Participer à la mise en place des procédures d'analyse des logs pour détecter les tendances, les anomalies et les opportunités d'optimisation
Collaborer avec l'équipe pour interpréter les informations tirées des logs et recommander des actions correctives
Participer à la conception et la mise en œuvre un système de monitoring en temps réel pour suivre l'état des flux d'intégration
Aider au développement des alertes automatisées pour signaler les dysfonctionnements potentiels
Profil
Vous êtes étudiant/te d’une école d’ingénieur ou d’un équivalent universitaire ?
Vous montrez un intérêt pour le domaine des projets Data, à travers vos expériences professionnelles, stages, cours ou projets personnels impliquant les flux d’intégration de données par API, l’utilisation d’un ETL, l'automatisation et l’orchestration des pipelines de données ?
Vous avez une bonne connaissance des technologies de l’API REST
Vous avez déjà pratiqué un ETL (Informatica est un plus)
Vous avez une forte appétence pour l'optimisation et l’ingénierie des flux de données
Vous avez une très bonne compréhension des bases de données et de SQL
Vous maîtrisez l'anglais technique
Vous souhaitez vous impliquer et vous épanouir dans une ambiance motivée et conviviale ? Si rigueur, esprit d'équipe, organisation et dynamisme sont des qualités qui vous caractérisent, alors venez révéler et développer votre talent au sein de notre équipe !
Stage d’une durée de 6 mois et basé au Siège social situé à Château-Gontier (53).
A compétences égales, une attention particulière sera apportée aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Analyst H/F,Lincoln France,"Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-h-f-at-lincoln-france-3827765148?position=1&pageNum=50&refId=feC9wifmdVTgZ6xVoBIeng%3D%3D&trackingId=A0JQT3D0MGPHTe%2BVb2xN7Q%3D%3D&trk=public_jobs_jserp-result_search-card,"DATA ANALYST H/F
CDI
3 ans minimum
Chez Lincoln
, nous formons une communauté d'innovateurs passionnés qui redéfinissent l'analyse de données depuis
plus de 30 ans
. En tant que
Pure Player Data
, notre expertise est reconnue dans les domaines
de la Modern BI, du Big Data et de la Science des données
.
Notre mission ?
Transformer les données en solutions concrètes pour nos clients grands comptes dans divers secteurs tels que la banque, le retail, les télécoms, l'industrie, la santé, etc.
Description de poste
Nous recherchons un
Data Analyst H/F
pour accompagner nos clients dans leurs projets stratégiques.
Vos missions
Collecter, nettoyer et traiter les données provenant de différentes sources.
Analyser les données pour identifier des tendances, des corrélations et des anomalies.
Concevoir et développer des tableaux de bord et des rapports pour présenter les résultats de manière claire et concise.
Collaborer avec les équipes clients pour comprendre leurs besoins et recommander des solutions adaptées.
Prérequis :
Maîtrise avancée des langages de programmation
(SQL, Python, R, etc.).
Connaissance approfondie des bases de données relationnelles et non relationnelles.
Expérience pratique avec des outils d'analyse de données
(Tableau, Power BI, Looker, Qlik etc.).
Fortes compétences analytiques et capacité à transformer les données brutes en insights exploitables.
Expérience de travail en
méthode Agile
pour la gestion de projet et le développement de solutions.
Capacité à travailler de manière autonome et en équipe.
Excellentes compétences en communication et en présentation.
Les plus du poste
Environnement Collaboratif
: projets innovants favorisant le partage des connaissances.
Accompagnement individualisé et de proximité
: formations certifiantes, attribution d’un Career Manager pour vous orienter dans votre trajectoire professionnelle, opportunités d’évolution de carrière.
Flexibilité du Travail
: Télétravail et horaires flexibles pour votre équilibre vie professionnelle-personnelle.
Rémunération Compétitive
: Salaire compétitif avec des avantages sociaux attrayants.
Mobilité
: Possibilité de mobilité à Paris, Lyon ou Aix-en-Provence offrant des expériences diversifiées au sein de Lincoln.
Notre processus de recrutement :
Un entretien RH (1h) et entretien technique (1h)
Cette annonce n’est pas faite pour vous si :
Vous êtes freelance et vous comptez le rester !
Toujours là ? Postulez et rejoignez nos
400 experts en Data
😉.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Communication', 'Flexibilité'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['400'], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Cloud Engineer,SELLIA,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/cloud-engineer-at-sellia-3902686769?position=2&pageNum=50&refId=feC9wifmdVTgZ6xVoBIeng%3D%3D&trackingId=oCyxHxImDX7MWux2XkmZZw%3D%3D&trk=public_jobs_jserp-result_search-card,"Nous recherchons un Ops Senior en environnement Kubernetes sur du cloud Azure pour prendre en charge l’automatisation du déploiement de nos applications, le suivi de notre infrastructure et l’optimisation des coûts des environnements.
Les missions :
et déployer des infrastructures pour applications cloud (y compris sur les services de CI/CD)
en place des outils de sécurisation, monitoring, backup, répartition des charges, etc
les processus et architectures
au processus de certification ISO (documentation, tests, etc)
à la conception et au développement des architectures cloud en utilisant les meilleures pratiques et les services adaptés (Azure).
en place et gérer des clusters Kubernetes pour le déploiement d'applications et de micro-services.
avec les équipes de data science pour intégrer les données et modèles selon une approche MLOps.
les bases de données relationnelles (postgreSQL) et MongoDB dans les solutions architecturales, en tenant compte des exigences de performances et de disponibilité.
Votre profil :
années d’expériences minimum sur des projets similaires
certifications Azure MS devops engineer expert serait un plus
moins une expérience significative sur Kubernetes.
des bases de données relationnelles et expérience avec MongoDB.
approfondie des principes d'architecture logicielle, de la conception de systèmes évolutifs et de la sécurité des applications.
à communiquer efficacement et à travailler en équipe, tout en faisant preuve d'autonomie.
compréhension des méthodologies Agile et des pratiques DevOps.
Les technologies suivantes n’ont pas de secret pour vous :
: Windows, Linux (ubuntu, WSL2), réseaux
: JavaScript/TypeScript, Python, Shell
de données : postgreSQL , mongoDB
: Docker/Podman, Kubernetes (AKS/EKS) : Helm, ISTIO,
: Cloud functions
GitOps, Serverless, Terraform, Helm, Ansible, Packer
Deployment: Cloud CI/CD
: Github, Consul, NGINX, WebPack, AWS Kinesis, Keycloak, Azure Devops
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go', 'JavaScript'], 'DataBase': ['SQL', ' MongoDB'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker'], 'Os': ['Linux', 'Windows'], 'DBMS': ['PostgreSQL'], 'SoftBigDataProcessing': [], 'Automation': ['Ansible', 'Kubernetes'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Engineer,L-Acoustics,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-l-acoustics-3826015735?position=3&pageNum=50&refId=feC9wifmdVTgZ6xVoBIeng%3D%3D&trackingId=h701WLuvM55ngg0K4lj6rQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Join our passionate and dedicated teams who are shaping the future of sound!
Membre de l’équipe IT & Digital, vous intervenez au cœur des différents projets de développement du Groupe sur un périmètre international (3 BU Business EMEA, AMERICAS & APAC – 3 centres R&D France, Royaume-Uni & Allemagne – 4 sites industriels France & Allemagne).
Pour accompagner notre niveau 2 du modèle data-drive, avec l’accompagnement d’un manager et un architecte donnée. Un plan de formation et d’accompagnement est prévu pour intégrer la ressource et l’accompagner dans son développement.
Dans le respect de la stratégie IT établie par le DSI, votre mission consiste à :
Responsabilités
Analyser les besoins fonctionnels.
Localiser les données de production.
Déterminer les spécifications techniques.
Modéliser les datawarehouse (entrepôts de données) et les datamarts (magasins de données) dédiés à une fonction particulière dans l’entreprise.
Accompagner le client dans la réalisation du projet.
Maintenir les sources de données à jour
Mettre à disposition les données aux métiers
Développer le dictionnaire de données
Stack Technique
Power BI / DAX
Azure Pureview
Azure Data Factory
Azure Synapse Datawarehouse
Data Lake, Logical Datawarehouse
Microsoft Fabric
Intégration et la livraison continue (CI/CD)
Savoir-faire
Rigueur et respect de planning
Esprit de synthèse
Capacités d’analyse
Autonomie
Maîtrise de l’anglais
Envie d’apprendre et d’évoluer
Veille technologique
Rédiger la documentation technique
Bonne communication écrite et orale
Votre Profil
Diplômé d’un Bac + 5 (école d'ingénieur ou universitaire), vous justifiez d’une expérience de 5 ans minimum dans des fonctions similaires.
Vous avez une expérience significative sur les domaines :
Plateforme données de Microsoft Azure (Microsoft Fabric/Lakehouse/Synapse)
Expérience sur des projets d'intégration et déploiement
Développement et intégration des API
Développement et gouvernance des rapports dans Power BI
Connaissance de base des technologies d’intelligence artificielle
Join our passionate and dedicated teams who are shaping the future of sound!
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
Senior data engineer,Harnham,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-harnham-3917047907?position=4&pageNum=50&refId=feC9wifmdVTgZ6xVoBIeng%3D%3D&trackingId=w%2BSECXFbLwwSLeezM5MFkw%3D%3D&trk=public_jobs_jserp-result_search-card,"Senior Data Engineer
Paris - 3 Jours de télétravail
Up to 80K€ fixe
CDI - Pas ouvert au freelance
Scala - Spark - Kafka
💡 L'entreprise
Une entreprise spécialisée dans le marketing et la performance des publicités mobiles recherche un Senior Data Engineer. Gérant un volume important de données, le Machine Learning est au coeur de son offre. L'entreprise propose des solutions technologiques innovantes pour améliorer la performance des campagnes publicitaires sur smartphone et tablettes tout en proposant une stratégie et des algorithmes sur mesure à ses clients.
🎯 Le poste
En tant Senior Data Engineer, vous rejoindrez l'équipe reporting en charge de tous les sujets liés à la data. Vos missions seront les suivantes :
Assurer le bon fonctionnement de l'infrastructure de données : gérer les magasins de données, les processus et la planification
S'assurer que les données sont toujours disponibles : superviser le traitement, le stockage, l'agrégation et le développement d'API
Apporter des nouvelles idées pour améliorer les outils
Suggérer des solutions nouvelles et intéressantes pour mettre à niveau l'infrastructure de données
Gérer les algorithmes de data science (garantir leur évolutivité et leur bon fonctionnement)
Travailler en étroite collaboration avec l'équipe SCRUM
Garder un œil sur l'échelle des systèmes de données, en s'assurant qu'ils peuvent répondre aux besoins croissants
Rationaliser les processus de données en intégrant des tests, une surveillance, des alertes et une récupération d’erreur efficaces
🔍 Profil Recherché :
Diplôme d'ingénieur en data engineering
Minimum 4 ans d'expérience sur un poste similaire
Solide connaissance du code en Scala/Spark
Spécialiste de la data streaming (maîtrise de Kafka pour gérer des données en temps réel)
Connaissance d'au moins un cloud provider (AWS, GCP ou Azure)
Connaissance des pipelines et les outils CI/CD (un vrai plus)
Pro de Python, avec une connaissance de Redis et Cassandra
Anglais courant
Pour postuler
Merci de me faire part de votre CV et je vous recontacterai au plus vite.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Senior'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
Senior Data Engineer,Pentalog,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-at-pentalog-3747758363?position=5&pageNum=50&refId=feC9wifmdVTgZ6xVoBIeng%3D%3D&trackingId=K2snjFw5ATMJtJNoo%2B38cQ%3D%3D&trk=public_jobs_jserp-result_search-card,"France
(Paris)
Full time
Job perks: Professional team; AI software
About The Project
How we hire:
At Pentalog, excellence is what you'll do. We're guided by a mission to positively impact the software development world.
Are you ready to be part of a revolutionary team that is reimagining content creation and using cutting-edge technology to drive the future of the creator economy? Our client is using technology, data and expertise to turn today's talented video creators into tomorrow's digital icons.
As a
Senior Data Engineer
, you'll be working on one or more key projects that drive innovation in content creation. Your role will empower creators to reach their full potential across social platforms such as Facebook, Instagram, Pinterest, Snapchat, TikTok, YouTube and beyond.
The majority tech stack is: Python/FastAPI and React with some projects in Node.js, Rust, React Native. Mandatory knowledge: Git, Docker. Nice to haves: AWS, Terraform.
Job Requirements
At least 6 years of experience in similar environments;
Proficiency in Python and familiarity with the data engineering tool landscape, particularly tools such as Airflow and DBT;
Knowledge of SQL and NoSQL databases;
Streaming pipeline experience with Kafka, Kinesis, Beam or Flink;
Big data infrastructure deployment experience, with proven ability to use tools such as Terraform on platforms such as AWS;
Familiarity with managed services such as S3, Redshift, EMR or others;
Architecture and systems design experience;
Experience with platforms such as Spark and Hadoop is a plus;
Rigorous, resourceful and curious, with a flair for problem solving;
Good communicator, with the ability to teach others;
Good team working skills;
Fluent in English.
Amazon Web Services (AWS) AWS Kafka AWS Kinesis AWS Redshifht Hadoop Node.Js NoSQL Python React js React native Rust SPARK SQL
Responsibilities
Develop, deploy, and maintain ETL processes in Python and Spark;
Implement and manage data pipelines utilizing tools such as Airflow, AWS Glue, and Kinesis;
Oversee and manage the data stores, including Redshift and RDS;
Implement and maintain data lake archiving strategies using S3, Parquet, and Iceberg;
Drive data modeling initiatives and implement schema governance tooling to ensure data integrity and consistency.
Benefits
Foreign language classes;
Competitive salary and bonuses;
Free pass to learning platforms;
A multicultural, friendly work environment;
Working in a company with an Agile mindset: continuous knowledge sharing and validated learning;
The possibility to bring your own creative and innovative ideas to life;
Mentorship programs that encourage and enable your professional development;
Great career development opportunities;
Improvement of your hard and soft skills through workshops, knowledge sharing sessions and presentations on multiple IT-related topics.
About Pentalog
As a leading European Software Services company operating internationally in France, Romania, Germany, Moldova, UK, Vietnam, Mexico, Morocco and USA, we employ over 1,300 engineers and IT experts who work in a very dynamic, multicultural working environment.
At Pentalog, your talents & ambitions are recognized and rewarded; we offer plenty of opportunities to develop, both individually, as well as a professional, and we reward our collaborators who understand the importance of self-improvement.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker'], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': [], 'EnSoftSkils': ['Problem Solving', 'Initiative']}","{'JobDetail': ['Full', 'Senior'], 'TypeContract': [], 'Salary': ['1,300'], 'Level': [], 'Experience': ['a', 'n', 's']}"
Data Analyst H/F,Lincoln France,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-analyst-h-f-at-lincoln-france-3892483893?position=7&pageNum=50&refId=feC9wifmdVTgZ6xVoBIeng%3D%3D&trackingId=Vqm8X77go1MCC4UqQc%2FGVg%3D%3D&trk=public_jobs_jserp-result_search-card,"CDI
📊
3 ans minimum
Chez Lincoln
, nous formons une communauté d'innovateurs passionnés qui redéfinissent l'analyse de données depuis
plus de 30 ans
. En tant que
Pure Player Data
, notre expertise est reconnue dans les domaines
de la Modern BI, du Big Data et de la Science des données
.
Notre mission ?
Transformer les données en solutions concrètes pour nos clients grands comptes dans divers secteurs tels que la banque, le retail, les télécoms, l'industrie, la santé, etc.
Description du poste
Nous recherchons un
Data Analyst H/F
pour accompagner nos clients dans leurs projets stratégiques.
Vos missions
Collecter, nettoyer et traiter les données provenant de différentes sources.
Analyser les données pour identifier des tendances, des corrélations et des anomalies.
Concevoir et développer des tableaux de bord et des rapports pour présenter les résultats de manière claire et concise.
Collaborer avec les équipes clients pour comprendre leurs besoins et recommander des solutions adaptées.
Prérequis :
Maîtrise avancée des langages de programmation (
SQL, Python, R
, etc.).
Connaissance approfondie des bases de données relationnelles et non relationnelles.
Expérience pratique avec des outils d'analyse de données (
Tableau, Power BI
, etc.).
Fortes compétences analytiques et capacité à transformer les données brutes en insights exploitables.
Expérience de travail en
méthode Agile
pour la gestion de projet et le développement de solutions.
Capacités à travailler de manière autonome et en équipe.
Excellentes compétences en communication et en présentation.
Les plus du poste
Environnement Collaboratif
: projets innovants favorisant le partage des connaissances.
Accompagnement individualisé et de proximité
: formations certifiantes, attribution d’un Career Manager pour vous orienter dans votre trajectoire professionnelle, opportunités d’évolution de carrière.
Flexibilité du Travail
: Télétravail et horaires flexibles pour votre équilibre vie professionnelle-personnelle.
Rémunération Compétitive
: Salaire compétitif avec des avantages sociaux attrayants.
Mobilité
: Possibilité de mobilité à Lille, Lyon ou Aix-en-Provence offrant des expériences diversifiées au sein de Lincoln.
Notre processus de recrutement :
Un entretien RH (1h) et entretien technique (1h)
Cette annonce n’est pas faite pour vous si :
Vous êtes freelance et vous comptez le rester !
Toujours là ? Postulez et rejoignez nos
400 experts en Data
😉.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Communication', 'Flexibilité'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['400'], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
Data Engineer - data factory (H/F),Euro Information Developpements / EID,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-data-factory-h-f-at-euro-information-developpements-eid-3878343308?position=8&pageNum=50&refId=feC9wifmdVTgZ6xVoBIeng%3D%3D&trackingId=PMsSi4Y%2BeHJQ9msDAp5EBw%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes nous
Euro-Information, filiale technologique de Crédit Mutuel Alliance Fédérale, conçoit, réalise, maintient et exploite un système d’information commun utilisé par le Groupe.
Les activités de développement et de production informatique au niveau national et international sont assurées par environ 4000 salariés répartis sur plusieurs sites géographiques au niveau national : Strasbourg, Nancy, Dijon, Orléans, Lyon, Lille, Cergy, Val de Fontenay, Paris et Nantes.
Première Banque à adopter le statut d’entreprise à mission, le Crédit Mutuel Alliance Fédérale s’investit et s’engage dans différentes missions sociales et environnementales :
L’accompagnement de tous par notre organisation coopérative et mutualiste reste au cœur de notre ADN.
La technologie au service de l’humain est une référence dans notre monde connecté.
La solidarité et l’éco-responsabilité deviennent des axes clés dans notre développement.
Notre raison d’être : Ensemble, Ecouter et Agir.
Vos missions
Vous souhaitez intégrer une équipe dynamique et à taille humaine au sein d’une entreprise solide et d’un grand groupe en développement constant ? Vous souhaitez travailler sur un enjeu d’aujourd’hui et plus encore de demain : la donnée ?
Euro-Information, la Fintech de Crédit Mutuel Alliance Fédérale structure une Data Factory pour :
Accélérer la valorisation des données au travers d’analyse de masse et de modèles allant jusqu’au prédictif.
Répondre à la confiance de nos clients en garantissant la sécurité de leurs données et une utilisation responsable et encadrée, respectueuse de leur vie privée.
La Data Factory EI se positionne comme un fournisseur de solutions pour les équipes informatiques et pour l’ensemble des entités du groupe et comme un facilitateur d’échanges et de mutualisation. Pour mener à bien ces missions, la relation métier s’avère essentielle. Elle réunit :
Des Data Architects. Ils fournissent l’environnement et les outils et s’assurent de leur performance et de leur constante évolution.
Des Data Engineers. Ils ont la maitrise des données, de leur récolte à leur mise à disposition adaptée aux besoins des métiers. Ils conçoivent les modèles d’enregistrement des données.
Des Concepteurs Business Intelligence. Ils simplifient, mutualisent et industrialisent les reportings.
Des Data Scientists. Ils accompagnent les métiers et les entités sur la Data Science, ils réalisent à façon sur les sujets à fort enjeu.
Des Data Officers. Ils coordonnent et mutualisent les énergies sur les projets Data comme dans le cadre de la Gouvernance et de l’administration des données.
Nous recherchons un(e) Data Engineer.
Vous participerez à la mise en œuvre et au maintien du Système d’information Décisionnel du groupe
.
Pour les banques, les organismes de crédits à la consommation et les filiales, en France et à l’international :
Compréhension de l’activité et des besoins de vos clients, en dialogue avec la MOA
Compréhension du SI de production, en dialogue avec les équipes MOE
Modélisation du Système d’Information Décisionnel
Conception et réalisation
Diagnostic des dysfonctionnements rencontrés
Maintenances correctives et évolutives
Support auprès des différents métiers
Documentation technique
Suivi des traitements
Pilotage de projets
Vous travaillerez sur une grande variété de projets et à l’échelle de l’ensemble des entités gérées sur le SI d’Euro-Information .
Vous intervenez sur des projets à dimension internationale sur des environnements Vertica, Semarchy, WebFOCUS, SAP Business Objects, QlikSense…
Vous participerez, coordonnerez et mènerez à bien des projets de valorisation de données sur des cas d’usages concrets :
Des projets de bout en bout : de l’expression du besoin jusqu’à la réalisation et au suivi de sa performance en lien avec la MOA et nos Data Officers
En mobilisant l’ensemble des acteurs : Business, Data Scientists, Data Engineer, sources de données …
Avec participation aux prises de décision
En mutualisant les travaux et les bonnes pratiques entre les acteurs et les différents métiers du groupe
Dans vos projets et au-delà, vous porterez l’acculturation Data au sein du groupe.
Ce que vous allez vivre chez nous
Télétravail (2 jours par semaine)
Rémunération fixe versée sur 13 mois
RTT
Intéressement, participation et abondement
Plan épargne entreprise et PERCO
Contrat de santé collectif
Prévoyance
Retraite supplémentaire prise en charge à 100% par l’employeur
Conditions bancaires et assurances préférentielles
Politique parentale avantageuse
Ce que nous allons aimer chez vous
De formation bac +4/5, vous disposez idéalement d’une expérience significative sur un poste équivalent.
La maitrise de l’anglais serait un plus.
Connaissance du monde OPEN
Vous avez la maitrise d’un ETL, d’une base de données orientée Analytique, d’une solution BI.
La connaissance de l’outil de modélisation PowerDesigner serait un plus.
Connaissance du monde Host
Une expérience mettant en œuvre des technologies mainframe (MVS, JCL, Cobol, SGBD DB2, OPC) serait un plus.
Ce qui nous plaira le plus chez vous :
C’est vous-même ! Alors on vous attend ouvert(e), force de proposition, doté(e) d’un certain sens critique, autonome et respectueux(se) de la confidentialité des informations détenues car c’est ce qui vous permettra de mener au mieux votre mission.
On dit de vous que vous avez une certaine aptitude à communiquer et le sens du travail en équipe.
Vous êtes motivé(e) et vous souhaitez vous investir fonctionnellement et techniquement, n’hésitez plus l’offre est faite pour vous.
Autres informations
Le poste est à pourvoir dès que possible sur Nantes.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
ALTERNANCE - Data Engineer H/F,Crédit Agricole Group Infrastructure Platform,"Montpellier, Occitanie, France",https://fr.linkedin.com/jobs/view/alternance-data-engineer-h-f-at-cr%C3%A9dit-agricole-group-infrastructure-platform-3903710929?position=9&pageNum=50&refId=feC9wifmdVTgZ6xVoBIeng%3D%3D&trackingId=n3SsDOZU2EIJMkzwnoo%2Fww%3D%3D&trk=public_jobs_jserp-result_search-card,"Crédit Agricole – Group Infrastructure Platform (CA-GIP),
acteur majeur de la production informatique du groupe Crédit Agricole, recherche ses Jeunes Talents de demain en alternance et en stage !
L’équipe Data souhaite intégrer son futur
alternant Data Engineer H/F
à
Montpellier (34)
.
🚀 Vos missions :
Le socle Natif Data Cloud conçoit et opère des plateformes « as a service » facilitant la mise en place d’architectures modernes, distribuées et hautement résilientes pour l’ensemble des entités du groupe Crédit Agricole.
Au sein de cette équipe, vous aurez les missions suivantes :
Mettre en œuvre opérationnellement et techniquement la plateforme Streaming ;
Participer au daily meeting animé par le Squad Lead, à l’ensemble des rituels agiles de la Squad (Rétro, Démos, PI Planning) ainsi qu’aux réunions de co-construction de la roadmap et de mise en œuvre de celle-ci ;
Mettre en œuvre les éléments de la Backlog.
Les + de cette mission :
monterez en compétences au sein d’une
équipe de 40 personnes
soit de 20 clusters Kafka et serez accompagné par
Paul BERNARD
, responsable squad Streaming.
Si vous souhaitez acquérir de
véritables compétences en DevOps et en Kafka
, alors cette mission est faite pour vous.
✅ Votre profil :
Vous intégrerez à partir de septembre 2024 un cursus de niveau
Bac+4 / M1
en informatique,
de préférence en école d’ingénieurs.
Vous justifiez d'un
niveau d'anglais
professionnel
.
Compétences attendues :
Force de proposition ;
Rigueur ;
Animation et facilitation ;
Capacité de synthèse ;
Autonomie.
Environnements techniques:
Kafka ;
Ansible ;
Flink.
🏆 Pourquoi devenir un Jeune Talent CA-GIP ?
Pour devenir partenaire des grandes évolutions technologiques et mettre vos compétences au service des 53 millions de clients du groupe Crédit Agricole ;
Pour évoluer dans un environnement à la pointe de la technologie (DevOps, Cloud Hybride, Digital Workplace, Cybersécurité, Télécommunications, Réseau...) ;
Pour rejoindre une entreprise certifiée Top Employer, et donc reconnue comme employeur de référence ;
Pour bénéficier de missions responsabilisantes et développer vos compétences au sein d’un environnement de travail aussi bienveillant que challengeant, une expérience idéale pour lancer votre carrière (46% de nos Jeunes Talents en alternance et stage embauchés en CDI en 2023).
⭐ Nos petits plus !
Vous bénéficierez d’un remboursement de vos frais de transport en commun de 90% !
Vous souhaitez vous rapprocher de votre lieu de travail ? CA-GIP a conclu un partenariat avec l’organisme ViaHumanis ! Pour en savoir plus, cliquez ici.
Vous aurez accès à toutes les prestations de notre CSE !
CA-GIP est une entreprise handi-accueillante
: vous avez des besoins spécifiques, nous sommes là pour vous accompagner.
CA-GIP est signataire de la Charte de la Diversité depuis 2023
: en agissant chaque jour dans l’intérêt de la société, nous sommes un Groupe engagé en faveur des diversités et de l’inclusion. Pour en savoir plus sur la Politique des Diversités.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Hybride'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
Ingénieur data Spark (F/ H),Thales,"Vélizy-Villacoublay, Île-de-France, France",https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-spark-f-h-at-thales-3831274678?position=10&pageNum=50&refId=feC9wifmdVTgZ6xVoBIeng%3D%3D&trackingId=U%2BQniftly41O7LDODp8s1g%3D%3D&trk=public_jobs_jserp-result_search-card,"QUI SOMMES-NOUS ?
Au sein du site de Vélizy, nos équipes hautement qualifiées conçoivent et produisent des amplificateurs de puissance (tubes à ondes progressives, klystrons, gyrotrons, sous-systèmes pour les Grandes Infrastructures de Recherche, etc.) à destination des marchés Défense, Sécurité, Spatial et Scientifique. Chaque jour nos cadres, ingénieurs, techniciens et opérateurs mettent en commun leurs savoir-faire unique au service de l’innovation.
QUI SOMMES-NOUS ?
Thales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.
Thales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.
QUI ETES-VOUS ?
Diplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.
CE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :
En tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.
VOS MISSIONS :
Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Spark.
Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.
Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.
Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.
Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.
Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.
Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.
Innovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.
Innovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
