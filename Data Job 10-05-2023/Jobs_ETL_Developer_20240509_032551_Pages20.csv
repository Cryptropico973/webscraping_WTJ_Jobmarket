srouce,title,company,location,link,description,skills,details
LinkedIn,SQL Developer,MCI,New Caledonia,https://nc.linkedin.com/jobs/view/sql-developer-at-mci-3912049201?position=2&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=acYZPwwJr44VgyKwDppebA%3D%3D&trk=public_jobs_jserp-result_search-card,"Full-Time
Job Title
Global Sales Director
Job Type
Full - Time
Location
Angeles City, Remote US
MCI is a leading Business Process Outsourcing (BPO) company that specializes in delivering tailored solutions to meet the diverse needs of our clients. With a commitment to excellence and a focus on innovation, we have established ourselves as a trusted partner in the industry.
We are seeking an SQL Developer, who will be responsible for designing, developing, and maintaining databases and database applications. The role will involve working closely with software developers, database administrators (DBAs), and other stakeholders to design database solutions, write SQL queries, optimize database performance, and ensure data integrity and security.
To be considered for this role, you must complete a full application on our company careers page, including all screening questions and a brief pre-employment test.
Key Responsibilities
Design and develop database schemas, tables, views, stored procedures, and functions to support application requirements and business needs.
Collaborate with software developers and architects to integrate database functionality into application architecture and design.
Write and optimize complex SQL queries for data retrieval, manipulation, and analysis, ensuring efficient performance and minimal resource consumption.
Identify and resolve performance bottlenecks, query inefficiencies, and indexing issues to enhance database performance and scalability.
Perform routine database maintenance tasks, such as backups, restores, and data migrations, to ensure data availability, integrity, and recoverability.
Monitor database performance, storage utilization, and resource utilization, and implement proactive measures to optimize performance and prevent downtime.
Design and implement data integration processes, ETL (Extract, Transform, Load) workflows, and data migration scripts to facilitate seamless data flow between different systems and platforms.
Validate and cleanse data during the ETL process to maintain data quality and consistency across the organization.
Implement database security policies, access controls, and encryption mechanisms to protect sensitive data and comply with regulatory requirements (e.g., GDPR, HIPAA).
Conduct regular security audits and vulnerability assessments to identify and mitigate security risks and ensure compliance with data protection standards.
Document database designs, data models, and technical specifications to facilitate system maintenance, troubleshooting, and knowledge sharing.
Provide technical support and guidance to development teams, DBAs, and other stakeholders on database-related issues, best practices, and performance optimization techniques.
WONDER IF YOU ARE A GOOD FIT FOR THIS POSITION?
All positive, and driven applicants are encouraged to apply. The Ideal candidates for this position are highly motivated and dedicated and should possess the below qualities
Bachelor's degree in Computer Science, Information Technology, or related field.
Proven experience as an SQL Developer or Database Developer, with expertise in SQL query writing, database design, and performance tuning.
Proficiency in SQL programming languages (e.g., T-SQL, PL/SQL) and database management systems (e.g., Microsoft SQL Server, Oracle, MySQL).
Strong understanding of database architecture, relational database principles, and data modeling concepts.
Must be authorized to work in the country where the job is based.
Must be willing to submit up to a LEVEL II background and/or security investigation with a fingerprint. Job offers are contingent on background/security investigation results.
Must be willing to submit to drug screening. Job offers are contingent on drug screening results.
WANT AN EMPLOYER THAT VALUES YOUR CONTRIBUTION?
We offer competitive compensation packages, professional development opportunities, and a collaborative work environment that values diversity and inclusion.
This job operates in a professional office environment. While performing the duties of this job, the employee will be largely sedentary and will be required to sit/stand for long periods while using a computer and telephone headset. The employee will be regularly required to operate a computer and other office equipment, including a phone, copier, and printer. The employee may occasionally be required to move about the office to accomplish tasks; reach in any direction; raise or lower objects, move objects from place to place, hold onto objects, and move or exert force up to forty (40) pounds.
It is the policy of MCI and affiliates to provide reasonable accommodation when requested by a qualified applicant or employee with a disability unless such accommodation would cause undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment. If reasonable accommodation is needed, of Human Resources.
At MCI and its subsidiaries, we embrace differences and believe diversity is a benefit to our employees, our company, our customers, and our community. All aspects of employment at MCI are based solely on a person's merit and qualifications. MCI maintains a work environment free from discrimination, one where employees are treated with dignity and respect. All employees share in the responsibility for fulfilling MCI's commitment to a diverse and equal opportunity work environment.
MCI does not discriminate against any employee or applicant on the basis of age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations, and ordinances. MCI will consider for employment qualified applicants with criminal histories in a manner consistent with local and federal requirements.
MCI will not tolerate discrimination or harassment based on any of these characteristics. We adhere to these principles in all aspects of employment, including recruitment, hiring, training, compensation, promotion, benefits, social and recreational programs, and discipline. In addition, it is the policy of MCI to provide reasonable accommodation to qualified employees who have protected disabilities to the extent required by applicable laws, regulations, and ordinances where an employee works.
MCI (www.mci.world) helps customers take on their CX and DX challenges differently, creating industry-leading solutions that deliver exceptional experiences and drive optimal performance. MCI assists companies with business process outsourcing, staff augmentation, call center services, customer services, and IT Services needs by providing general and specialized hosting, software, staff, and services.
In 2019 Marlowe Companies Inc. (MCI) was named by Inc. Magazine as Iowa’s Fastest Growing Company in the State of Iowa and was named the 452nd Fastest Growing Privately Company in the USA, making the coveted top 500 for the first time. MCI’s subsidiaries had previously made Inc. Magazine's List of Fastest-Growing Companies 15 times respectively. MCI has fifteen business process outsourcing service delivery facilities in Iowa, Georgia, Florida, Texas, Massachusetts, New Hampshire, South Dakota, New Mexico, California, Kansas, and Nova Scotia.
Driving modernization through digitalization, MCI ensures clients do more for less. MCI is the holding company for a diverse lineup of tech-enabled business services operating companies. MCI organically grows, acquires, and operates companies that have a synergistic products and services portfolios, including but not limited to Automated Contact Center Solutions (ACCS), customer contact management, IT Services (IT Schedule 70), and Temporary and Administrative Professional Staffing (TAPS Schedule 736), Business Process Management (BPM), Business Process Outsourcing (BPO), Claims Processing, Collections, Customer Experience Provider (CXP), Customer Service, Digital Experience Provider (DXP), Account Receivables Management (ARM), Application Software Development, Managed Services, and Technology Services, to mid-market, Federal & enterprise partners. MCI now employs 10,000+ talented individuals with 150+ diverse North American client partners across the following MCI brands GravisApps, Mass Markets, MCI Federal Services (MFS), The Sydney Call Center, OnBrand24, and Valor Intelligent Processing (VIP).
The purpose of the above job description is to provide potential candidates with a general overview of the role. It's not an all-inclusive list of the duties, responsibilities, skills, and qualifications required for the job. You may be asked by your supervisors or managers to perform other duties. You will be evaluated in part based upon your performance of the tasks listed in this job description.
The employer has the right to revise this job description at any time. This job description is not a contract for employment, and either you or the employer may terminate employment at any time, for any reason.
Show more
Show less","{'ProgLanguage': ['Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['MySQL', 'Oracle', 'SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML'], 'FrSoftSkills': [], 'EnSoftSkils': ['Organization']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': ['40'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer,digiRocks recrute ✅,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-digirocks-recrute-%E2%9C%85-3903481080?position=3&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=u1YGRyud%2BVE0ms1zi0jDYQ%3D%3D&trk=public_jobs_jserp-result_search-card,"😎 Envie d'accompagner des organisations dans leurs stratégies, Fan de data?
Rejoins un jeune cabinet de conseil en stratégie spécialisé en data. Le cabinet a été créé il y a 4 ans pas des anciens de grands cabinets de conseil en stratégie qui ne se reconnaissaient plus dans ce qu'est devenu le ""consulting"". Cependant ils n'ont pas perdu espoir de pouvoir apporter du conseil à haute valeur ajoutée dans une ambiance friendly, façon start-up, sans sacrifier l'excellence.
Jean-Patrick recrute un(e) Consultant Data Engineer à Paris en CDI
✅ MISSION :
Vous serez responsable de la mise en œuvre de bout en bout de la pile de données, de la collecte au reporting, avec un accent sur l'infrastructure et les processus techniques. Vous travaillerez avec des Consultants en Stratégie & Data et les soutiendrez dans la résolution des défis liés aux données de leurs clients. Vous contribuerez à la définition des stratégies de données, à la mise en œuvre des systèmes de données et vous soutiendrez l'exploitation des données dans des projets transformationnels. En général, vous serez responsable de comprendre intimement les problèmes, de concevoir une stratégie technique pour les adresser et de faciliter une exécution technique de haute qualité.
✅ RÉSULTATS ATTENDUS :
🚀 Résultat 1: Unificateur de Données : Architecturer, assembler, assimiler, nettoyer et conformer de grands ensembles de données complexes pour livrer des insights commerciaux et alimenter les expériences de produits de données.
🚀 Résultat 2: Agent de Sécurité des Données : Concevoir et construire une infrastructure de données fiable et évolutive avec les techniques de confidentialité et de sécurité de pointe pour protéger les données.
🚀 Résultat 3: DataOps : Posséder la pile de données de bout en bout, y compris la collecte d'événements, la gouvernance des données, les intégrations de données et la modélisation.
🚀 Résultat 4: Gardien des Données : Assurer la cohérence et la qualité de l'environnement technique et de la structure des données à travers des métriques, de la documentation, des processus, des tests de données et de la formation.
Requirements
✅ PROFIL RECHERCHÉ :
Diplômé d'une Grande Ecole de Commerce ou d'ingénieur, avec une première expérience réussie comme Data Engineer, idéalement dans un contexte similaire au Conseil,
Connaissance des services de Data Warehouses Cloud. Expérience avec Google BigQuery, Snowflake, AWS Redshift/Athena, Looker, Azure SQL DWH, ou Azure Databricks est très souhaitable.
Connaissance des architectures de données relationnelles et de grandes données, de l'entreposage de données, de l'intégration de données, de la modélisation de données, de l'optimisation de données et des techniques d'analyse de données.
Expérience dans la construction de pipelines de données de bout en bout en utilisant des plateformes de données sur site ou basées sur le cloud.
Expérience pratique dans la livraison de solutions comprenant des bases de données, SQL avancé et développement logiciel dans des langues telles que Python.
Intéressé et connaissant les technologies Big Data et les technologies de l'écosystème Apache telles que Beam, Spark, Kafka, Airflow, bases de données, intégration, gestion des données de référence, assurance qualité, manipulation de données et technologies de gouvernance des données.
Expérience avec les plateformes cloud publiques et l'infrastructure cloud qui est essentielle.
Exposé aux outils ETL/ELT et de gouvernance.
Intéressé par les technologies et principes IA et ML.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data Engineer H/F,Thales,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-thales-3903089036?position=4&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=5BTSlqnHIPJ1%2Bp4q6XNZcA%3D%3D&trk=public_jobs_jserp-result_search-card,"📢 Nous recherchons un(e) Data Engineer, basé(e) à Lyon
👉Quelques mots sur les activités numériques de Thales Lyon :
Les activités numériques représentent une entité rattachée au groupe Thales, spécialisée dans l’IT et présente au national.
L’agence de Lyon adresse divers sujets d’expertise : ingénierie logiciels, cybersécurité, infogérance des infrastructures et transformation digitale.
🎯
Votre rôle et missions
En nous rejoignant, vous intégrerez le centre de compétences
Augmented data
,
spécialisé dans la conception, le développement et l’évolution d’applications data centrées. Vous y boosterez votre carrière en travaillant sur des technologies telles que
Spark, Elasticsearch, Kube ...
le plus souvent dans un environnement
Agile
.
Dans le cadre des projets que nous opérons aujourd’hui :
- Vous contribuerez à la conception, au maintien, à la scalabilité des plateformes d’analyse de données au travers de votre expertise sur les sujets data (base de données, gestion de flux, ETL …)
- Vous contribuerez à la conception et à la mise en production des pipelines d’analyses et de transformations de données en veillant à leur bonne adaptation aux besoins métiers et aux contraintes techniques du client
- Vous pourrez intervenir sur des sujets de visualisations, dans le but de notamment accompagnées nos clients sur la conception de Dashboard métier intelligent …
- Vous serez également amenées à échanger directement avec des DevOps/Datascientist pour la mise en place, l’intégration des pipelines et l’élaboration des algorithmes de traitements de données.
- A l’échelle du département, Vous serez un acteur majeur du développement de notre activité et du lancement de nouveaux projets de valorisation de données.
🙋‍♀️ 🙋‍♂️
Votre profil
De formation Bac +5 en informatique (école d’ingénieur, Master ou équivalent), vous justifiez d’une première expérience réussie sur un projet data ? Vous souhaitez participer à la conception et intervenir sur des solutions de récupération et d’exploitation de données métiers dans des contextes critiques et hautement sécurisés ?
Autonome, dynamique, organisé(e) et proactif(ve), vous souhaitez évoluer au sein d’équipes passionnées par l’exploration et l’intégration des technologies nouvelles au service des métiers de nos clients ?
Vous avez des compétences qui couvrent les domaines suivants :
Mise en place et gestion de base de données (SQL, Elasticsearch, Clickhouse ...)
Langages de programmations (Java, Python)
Gestion de flux (Kafka, flink, logstash …)
Environnements big Data (Spark/hadoop )
Principes et outils de type ETL
Vous êtes de plus intéressé(e):
Par les environnements containerisés (docker, kubernetes, helm ...)
Les concepts DevOps (Ansible, CI/CD...)
Les sujets de Datavisualisation (Vega, Kibana, python librairies...)
Vous aimez travailler en équipe ? Vous êtes reconnu(e) pour vos qualités relationnelles et vos capacités de vulgarisation ?
Alors notre poste d’Ingénieur(e) Data(H/F) est fait pour vous !
🙌
Votre carrière chez Thales
Différentes opportunités vous permettront de découvrir d'autres domaines ou sites. Vous pourrez évoluer et développer vos compétences dans différents domaines.
Explorez un espace attentif au développement personnel.
Développez vos talents dans un autre domaine du groupe Thales, en découvrant de nouveaux produits, de nouveaux clients, un nouveau pays ou en vous orientant vers une solution plus complexe.
Choisissez entre une expertise technique ou un parcours de leadership.
Vous travaillerez dans une entreprise résolument humaine avec des valeurs fortes comme la sécurité au travail, l’égalité Homme/Femme et l’équilibre vie personnelle/professionnelle (Accord Télétravail).
Rattaché(e) à la Convention métallurgie, vous bénéficierez aussi de ses multiples avantages (…)
Vous souhaitez en savoir plus ?
N’hésitez pas à contacter notre équipe de recrutement ou nos équipes directement.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible', 'Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'CI/CD'], 'FrSoftSkills': ['Leadership'], 'EnSoftSkils': ['Leadership']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer / Développeur Big Data # H/F,Air France,"Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-d%C3%A9veloppeur-big-data-%23-h-f-at-air-france-3900080172?position=5&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=NRt5Zl9%2FQscrskQoCBvcMA%3D%3D&trk=public_jobs_jserp-result_search-card,"Description du poste
Intitulé du poste
Data Engineer / Développeur Big Data # H/F
Métier
Systèmes d'informations - Développement
Catégorie socio-professionnelle
Cadre
Présentation du contexte
Vous avez peut-être déjà voyagé avec nous, mais que connaissez-vous de nos métiers et de la richesse des données qu’ils génèrent au quotidien ? Comment le traitement et l’exploitation de ces données peut contribuer à notre stratégie de Revenue Management, ou encore aux multiples opérations à réaliser pour permettre à un vol de partir à l’heure ?
Air France-KLM fait rêver 104 millions de passagers par an, en les emmenant vers plus de 250 destinations, grâce à une flotte de plus de 500 appareils. Le Groupe emploie 80 000 collaborateurs partout dans le monde :les opportunités sont vastes pour mettre à profit ses compétences, apprendre et se développer !
Le département de développement DATA, OR & AI d’Air France, au sein de la direction des Systèmes d’Information, intervient dans toute la chaîne de captation et de traitement des données du groupe pour délivrer à nos métiers des solutions applicatives clés en main.
Le département est également en charge de l’ensemble des outils techniques (ETL, DataLakes, DataWarehouses, Data visualisation) et du développement des talents et compétences de Data Engineering.
Notre mission ? Transformer la donnée brute en décision intelligente, pour mieux optimiser les métiers d’Air France – KLM !
Pour cela, nous avons chacun un rôle essentiel à jouer, pourquoi le vôtre ne serait pas celui de Data Engineer et de développeur Big Data ?
Description de la mission
Au sein de notre département, vous travaillerez main dans la main avec d’autres Data Engineers et développeurs Big Data ainsi qu’avec des spécialistes des métiers.
Intégré au sein d’une product team agile passionnée et dynamique :
Vous participez à l’analyse des besoins métiers du commercial, des opérations aériennes, de l’exploitation sol en aéroport, de la maintenance aéronautique ou encore du Cargo.
Vous contribuez à la définition, au développement, à l’industrialisation et à la maintenance d’applications Big Data ou en Business Intelligence
Vous présentez la restitution de vos travaux et accompagnez les utilisateurs d’un point de vue fonctionnel ou méthodologique
Vous serez en contact avec les directions métier du groupe Air France KLM.
Nous attachons beaucoup d'importance au développement des compétences de nos collaborateurs ainsi qu’à leur offrir des conditions de travail favorables à l’autonomie et aux missions à forte valeur ajoutée. L'ouverture, le respect, la bienveillance et le partage sont des valeurs humaines portées par l'entreprise.
Profil recherché
Vous êtes diplômé de niveau Master ou Ingénieur dans les domaines informatiques, vous avez acquis une expérience professionnelle dans le développement d’applications.
Vous disposez d’une expérience du développement indispensable en Backend / Java
Vous maîtrisez les bases de données relationnelles et le langage SQL
En Complément, Vous Avez Une Connaissance Ou Une Expérience Dans Tout Ou Partie Des Concepts Ou Outils Suivants
Environnement Big Data (Spark, Hadoop, Elasticsearch, Kafka, ...)
Base de données noSQL (MongoDB, HBase, REDIS) ou Data Warehouse Teradata
Outil de Datavisualisation (Spotfire, PowerBI, Qlik ou Kibana)
Solutions de Cloud (GCP) et hybride (GCP / AZURE)
(Ces compétences complémentaires ou manquantes pouvant aussi s'acquérir à travers un parcours de reskilling et de formations aux outils du data engineering dispensé en interne).
Vous avez participé à des projets organisés en Scrum ou Kanban, et avez peut-être même œuvré comme Scrum-Master, ce qui vous permettra de vous intégrer aisément au sein d’une Product Team. Votre esprit de synthèse, votre force de conviction et votre maîtrise de la communication facilitent les décisions avec l’ensemble des collaborateurs de l’équipe, éventuellement en langue anglaise, à l’écrit comme à l’oral.
Vous êtes autonome, rigoureux(se), responsable et curieux(se), vous aimez travailler en équipe. Vous possédez de bonnes capacités d'écoute, d'analyse, de synthèse et de communication.
Et bien sûr, vous êtes passionné(e), enthousiaste et ingénieux(se)
Ce que nous vous offrons
De la création de valeur pour l’ensemble des métiers d’Air France KLM
Des challenges et problématiques complexes à résoudre
L’opportunité de déployer des solutions Data industrielles à l’échelle !
Une grande part de responsabilité dans une structure hiérarchique horizontale
Un important degré de liberté pour apprendre et développer son expertise au sein de l’équipe
On vous attend le plus rapidement possible ! Et pour une durée indéterminée ;)
Type de contrat
CDI
Temps partiel possible
Non
Type d'horaires
Administratif
Profil candidat
Niveau d'études min. requis
Bac + 5 et plus
Langue
Anglais (4 - Confirmé / C1)
Localisation du poste
Localisation du poste
France, Provence-Alpes-Côte d'Azur, Alpes Maritimes (06)
Site
Valbonne
Show more
Show less","{'ProgLanguage': ['Java', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'HBase', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Hybride', 'Confirmé'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer – SQL & GCP - F/H,Orange Business,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-sql-gcp-f-h-at-orange-business-3916557264?position=6&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=4BBfpaE2gtnp4tmuT%2BVqng%3D%3D&trk=public_jobs_jserp-result_search-card,"Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?
Nous l’avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les étapes du voyage de la donnée.
Depuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’ESN d’Orange Business alliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …
Description du poste
Orange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.
Votre quotidien ?
En intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :
Au démarrage du projet :
Recueillir et analyser les besoins du client
Rédiger les spécifications fonctionnelles et techniques
Estimre les charges
Pendant la phase de réalisation :
Modéliser des datawarehouses et datamart (intégration de flux et consolidation des données)
Développer les procédures d’alimentation (ETL)
Développer en SQL
/ PLSQL / Shell
Garantir la qualité des données et leur disponibilité
Concevoir et développer des solutions frontend BI à des fins analytics & dashboarding
Réaliser la recette et les tests
Suivre et mettre en production
En fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !
Qualifications
Vous possédez 3 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels et ingénierie ou analyse data.
Vous avez de
solides compétences en développement SQL
(job, scripting, déploiement), vous avez l’habitude de travailler dans un
environnement Google Cloud Plateform
ainsi qu’avec
Power BI
.
Envie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?
Outre l’aspect technique, c’est une personnalité qui est recherchée !
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '30', '30', '30']}"
LinkedIn,Data Engineer - Modélisation SQL - F/H,Orange Business,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-mod%C3%A9lisation-sql-f-h-at-orange-business-3916551577?position=7&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=vVVscyPLDbH99Q%2BXCr5Ruw%3D%3D&trk=public_jobs_jserp-result_search-card,"Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?
Nous l’avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les étapes du voyage de la donnée.
Depuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’ESN d’Orange Business alliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …
Description du poste
Orange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.
Votre quotidien ?
En intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :
Au démarrage du projet :
Recueillir et analyser les besoins du client
Rédiger les spécifications fonctionnelles et techniques
Estimer les charges
Pendant la phase de réalisation :
Modéliser des datawarehouses et datamart (intégration de flux et consolidation des données)
Développer les procédures d’alimentation (ETL)
Développer en SQL / PLSQL / Shell
Garantir la qualité des données et leur disponibilité
Réaliser la recette et les tests
Suivre et mettre en production
En fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !
Qualifications
Vous possédez 5 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels et en modélisation.
Vous avez de s
olides compétences en développement SQL
(job, scripting, déploiement) ainsi que sur Python.
Envie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?
Outre l’aspect technique, c’est une personnalité qui est recherchée !
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '30', '30', '30']}"
LinkedIn,Data Engineer H/F,Inetum,"St.-Ouen, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-inetum-3843966639?position=8&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=BlCfRho2hVXsr1gigd79gQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Détail de l'offre
Informations générales
Entité de rattachement
Inetum est un leader européen des services numériques. Pour les entreprises, les acteurs publics et la société dans son ensemble, les 28 000 consultants et spécialistes du groupe visent chaque jour l'impact digital des solutions qui contribuent à la performance, à l'innovation et au bien commun.
Présent dans 19 pays au plus près des territoires, et avec ses grands partenaires éditeurs de logiciels, Inetum répond aux enjeux de la transformation digitale avec proximité et flexibilité.
Porté par son ambition de croissance et d'industrialisation, Inetum a généré en 2023 un chiffre d'affaires de 2,5 milliards d'€.
Pour répondre à un marché en croissance continue depuis plus de 30ans, Inetum a fait le choix délibéré de se recentrer sur 4 métiers afin de gagner en puissance et proposer des solutions sur mesure, adaptées aux besoins spécifiques de ses clients le conseil (Inetum Consulting), la gestion des infrastructures et applications à façon (Inetum Technologies), l'implémentation de progiciels (Inetum Solutions) et sa propre activité d'éditeur de logiciels (Inetum Software). Inetum a conclu des partenariats stratégiques avec 4 grands éditeurs mondiaux - Salesforce, ServiceNow, Microsoft et SAP et poursuit une stratégie d'acquisitions dédiée afin d'entrer dans le top 5 européen sur ces technologies et proposer la meilleure expertise à ses clients.
Tous nos postes sont ouverts aux personnes en situation de handicap.
Description du poste
Métier
Applications Delivery - Software Development
Intitulé du poste
Data Engineer H/F
Contrat
CDI
Description De La Mission
Le pôle BFA de la branche Application Services du groupe INETUM, recherche plusieurs Data Engineers afin d'intervenir auprès de clients grands comptes au sein des marchés bancaires et de l'assurance.
Au sein de l'équipe Data, en tant que Data Engineer, vous participez à la réalisation de divers projets et vos missions sont
Apporter votre connaissance en Big Data permettant la manipulation des données
Concevoir les plateformes permettant de traiter des volumes de données importants
Mettre en place des bases de données
Préparer le pipeline de données pour que les données déployées soient sécurisées et claires afin d'être analysées et transformées.
Profil
De formation ingénieure en informatique Bac + 5 informatique ou scientifique
Bonne communication orale et écrite en français et niveau d’anglais professionnel
Savoir- être Bon esprit d'analyse et de synthèse, sens de l'organisation et de la qualité, force de proposition, rigueur, travail en équipe, adaptabilité.
Si vous vous reconnaissez, n'hésitez pas à postuler !
Localisation du poste
Localisation du poste
France
Ville
Saint-Ouen
Critères candidat
Niveau d'études min. requis
Bac+5
Niveau d'expérience min. requis
Plus de 2 ans
Compétences
SQL
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Communication', 'Adaptabilité', 'Organisation', 'Flexibilité'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '30', '30', '30']}"
LinkedIn,Data Engineer (H/F),Scalian,"Valbonne, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-scalian-3819563847?position=9&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=ls0HHJwQgfpq8HfBIbSMQA%3D%3D&trk=public_jobs_jserp-result_search-card,"Ingénieur DATA / Data engineer (H/F)
Valbonne/Sophia-Antipolis
Type : CDI
Lieu : Locaux Scalian Sophia-Antipolis
Télétravail : En fonction des possibilités
Date de prise de poste : immédiatement ou en fonction de votre préavis
Salaire : en fonction du profil - entre 40 et 48K Brut annuels (hors avantages Scalian)
Avantages Scalian : Accord d'entreprise télétravail, Tickets restaurants, Mutuelle groupe, accord aménagement temps de travail, compte épargne temps, accord de participation et intéressement groupe, programme cooptation et apports d'affaires, accompagnement parentalité, avantages CSE
Vous êtes data engineer ou vous souhaitez le devenir !
Quel sera votre rôle ?
La portée de la mission comprend (sans toutefois s'y limiter) :
Science des données
Ingénierie des données
Analyse des données
Génie logiciel
Ce que cette expérience va vous apporter
Vous êtes autonome, vous avez le sens du service et de l’analyse, vous êtes impliqué, nous vous offrons une ouverture sur des projets complexes et une rapide évolution de carrière. Vous rejoignez notre business unit à Sophia Antipolis composée d'environ 50 consultants, avec possibilité de télétravail en fonction des sujets.
Nous co-construisons votre trajectoire professionnelle et assurons votre montée en compétences.
Nous nous inscrivons ensemble dans la durée, nous assurons votre montée en compétences et disposons d'une variété de sujets passionnants.
Ce que nous recherchons chez vous
De formation supérieure (Bac+5, école ou université), vous possédez idéalement une première expérience réussie dans ce domaine (débutants acceptés), vous aimez le travail en équipe.
Compétences requises
:
Etape d’analyse : Comprendre l’architecture technique, les sources de données, les objectifs fonctionnels.
Etape de conception : Solution de conception avec un fort centrage sur les pipelines de données et les modèles ML et l’exposition des KPI via API
Mise en œuvre : Après les phases d’analyse et de conception, procéder à a mise en œuvre dans des technologies sélectionnées (Java,Scala,Python,Spark)
Créer un code testé et documenté
Techno : Linux, Shell, Hadoop, Scrum, Python, Spark, Scala
Pourquoi feriez-vous le grand saut ?
Parce que Scalian vous accompagne dans le développement de votre carrière :
Programme d'onboarding complet sur 1 an avec votre manager et votre RH
Programme de formation (Scalian Academy, e-learning, webinaires et formations externes)
Communautés techniques (Squads, Practices) afin de valoriser et développer votre expertise
Événements internes (Afterworks, Awards Dinner, Kick Off, Live Event du COMEX, Stand Up) et externes (participation à des salons et forums spécialisés dans nos domaines d’activités…)
Dispositif d’accélération d’accès à la mobilité interne et à des échanges internationaux type Erasmus
Parce que Scalian favorise la Qualité de Vie au Travail :
Certifications Great Place to Work® et Best Workplaces for Women®
Prime de cooptation, prime vacances, prise en charge par l’employeur de 60% des titres-restaurant, Accord télétravail (jusqu’à 2,5 jours par semaine indemnisés), RTT (dont une partie monétisable), CSE (activités ludiques, chèques-cadeaux, chèques vacances)
Berceaux en crèches inter-entreprises
Don ou réception de jours de congés en cas de difficultés personnelles
Parce que Scalian développe une politique RSE concrète et ambitieuse :
Mobilité durable (indemnité kilométrique vélo, leasing de vélos à assistance électrique)
Actions environnementales (Fresque du Climat, Reforest'Action, Clean Up Day, mécénat ONF)
Postes ouverts aux personnes en situation de Handicap
Diverses politiques de diversité, d’inclusion et d’intégration mises en place
Scalian c’est aussi :
Une entreprise en très forte croissance qui, créée en 1989, compte aujourd’hui plus de 5500 personnes
Des références clients à forte valeur ajoutée auprès de grands industriels français (du CAC40) et internationaux
Un terrain de jeu où l’expertise se conjugue avec audace, liberté d’entreprendre et convivialité
Si vous aspirez à un environnement de travail qui valorise autant votre bien-être que votre développement professionnel,
rejoignez-nous et exprimez pleinement votre talent !
Envie d'élargir le cadre ?
Je suis Liza Djehel, Talent Acquisition Officer.
Si votre CV est retenu, je vous contacte pour un premier échange téléphonique de 15 à 20 minutes.
Nous déterminons ensemble si ce poste est en adéquation avec vos compétences et surtout, avec vos attentes.
L'échange est positif ? Nous convenons d'un entretien de 1h (en présentiel ou en visio) avec Lucas Daunar, Business Manager à Sophia-Antipolis. Cet échange permet de revenir en détail sur vos compétences, vos attentes, de vous présenter le poste plus en détail, et d'évoquer d'autres opportunités.
Nous prévoyons ensuite un rendez-vous technique de 1h (en présentiel ou en visio) avec un de nos responsable technique.
Enfin, nous vous présentons notre proposition d'embauche.
Notre processus de recrutement dure entre 15 et 30 jours
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['40'], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
LinkedIn,DATA ENGINEER,Action for Market Transformation - A4MT,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-action-for-market-transformation-a4mt-3910049004?position=10&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=TK2pmr%2BnPCY9%2FL7EfnBCng%3D%3D&trk=public_jobs_jserp-result_search-card,"A4MT – Action pour la Transformation des Marchés
A4MT conçoit et implémente des programmes d’engagement et de « Market Transformation » qui visent à généraliser des pratiques vertueuses – au sens environnemental et sociétal – en modifiant la donne du marché, en reconfigurant le jeu d’acteurs, généralement via des actions collectives.
Ces programmes agissent sur la demande en suscitant de nouvelles pratiques individuelles et collectives. A4MT assure le rôle de pilote, orchestrant les plans d’action des parties prenantes grâce à une équipe de qualité à caractère international, un savoir-faire sur la mise en œuvre des programmes, une connaissance technico-économique experte des sujets traités, et une capacité à interpeller les décideurs à bon niveau.
Championnat de France des économies d’énergie
A4MT avec ses partenaires opère l’ensemble des concours CUBE en France (Championnat de France des Economies d’Energies) et assure son développement international (Europe, Asie, etc.). CUBE est un concours original d’économies d’énergie et de CO2 pour les bâtiments tertiaires et résidentiels qui accélère fortement l’action de terrain grâce à une intelligence collective sur le terrain.
Le concours est aujourd’hui présent dans 8 pays et se développe encore. Au-delà des économies les plus faciles, il s’agit de mettre en œuvre la trajectoire de gestion immobilière et d’investissement qui permettra, au-delà des avancées dans ce programme à faible investissement, de progresser sur la trajectoire de la neutralité carbone.
https://championnatdefrancedeseconomiesdenergie.org/
MISSION
Rendant compte au directeur d’A4MT et en étroite collaboration avec le directeur technique A4MT, vous êtes Data Engineer, vous serez responsable de la conception, du développement et de la maintenance des bases de données, et des outils de reporting. Vous travaillerez en étroite collaboration avec l'équipe de développement (prestataire externe) et vous participez à la structuration d’une équipe IT interne pour créer des solutions innovantes répondant aux besoins de l'entreprise.
Votre mission s'articule autours des 3 axes ci-dessous:
1/ Pilotage et et développement
développer et déployer des reporting robustes et évolutifs.
le planning de développement et le budget alloué.
avec les équipes d’animation et le back office technique du programme et avec les prestataires externes pour comprendre les exigences et les spécifications du projet.
à la conception de l'architecture des bases de données et à la prise de décisions techniques.
la qualité des données en effectuant des contrôles qualité.
les performances des applications pour garantir une expérience utilisateur fluide.
la maintenance et les mises à jour régulières des applications existantes.
à l'affût des tendances et des technologies émergentes.
Vous serez responsable du process, de la maîtrise d’ouvrage liée à la Data et garant(e) de la qualité de service.
2/ Implication des équipes et de la sous-traitance
Vous serez impliqué dans une équipe informatique naissante et dans une équipe projet avec les différentes fonctions métiers. Vous devrez faire le suivi de votre implication avec le responsable de programme et le directeur technique d’ A4MT :
3/ Gestion de projet
Vous tiendrez le tableau de bord des outils : budgets, engagements, planning, résultats, développements.
PROFIL
Vous avez une expérience significative d’au moins 3 années dans l’écosystème de big data, des serveurs et bases de données dans des contextes de projets, d’exploitation de migration.
COMPETENCES
Bac +5 diplômé(e) d’une grande école d’ingénieur ou équivalent, vous êtes :
+5 diplômé (e) d’une école d’ingénieurs ou équivalent, en Data science, Informatique, génie logiciel ou domaine connexe.
professionnelle démontrée de 3 ans ou plus en tant que Data Engineer
des langages structurés (JavaScript, Scala, Python…),
avec les bases de données relationnelles (MySQL, PostgreSQL) et non relationnelles (MongoDB, Firebase).
au moins un outil de reporting (Power BI, Tableau …)
des services de déploiement et d'hébergement cloud comme AWS, Azure ou Google Cloud Platform.
compétences en développement back-end avec des technologies comme Node.js, Python, Ruby on Rails, ou Java. et notamment en PHP sont recommandées
des langages de programmation front-end tels que HTML5, CSS3 et JavaScript (notamment frameworks: comme React, Angular ou Vue.js).
à travailler en équipe, à communiquer efficacement et à résoudre les problèmes de manière autonome.
des principes de sécurité des applications web et des meilleures pratiques en matière de développement sécurisé ainsi que le respect du RGPD.
Date d’entrée et conditions
Le poste est à pourvoir immédiatement; il est basé au 54, rue de Clichy, Paris (IXème). Niveau de rémunération selon expérience.
Contact
Merci d’adresser votre candidature complète (CV, lettre de motivation, présentation du cursus en cours de conclusions et références éventuelles) à l’attention de M. Adrien Brunella sur le mail elisabeth.clement@a4mt.com
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go', 'JavaScript', 'HTML'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['MySQL', 'PostgreSQL'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'ML', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Database Developer,Selby Jennings,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/database-developer-at-selby-jennings-3900070880?position=11&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=MmyZ4WJAhpEun%2BieFAUZTQ%3D%3D&trk=public_jobs_jserp-result_search-card,"I am working on a role with a global quantitative and systematic hedge fund who are looking for a Database Developer to join their team in Paris to work closely with trading and research functions.
The ideal candidate would need to be proficient in Python, have System Architecture Experience and have proven professional work experience as a database developer.
Key Responsibilities
• Your role involves providing daily trading insights to multiple activities and asset classes.
• You will work with traders, quants and other quantitative developers to build robust reporting and analytical tools, thus providing constant feedback on investment strategy performance.
• Given the dynamic nature of the business, you will continually seek solutions that are both flexible and robust and which integrate seamlessly into their technology landscape.
• Design and maintain storage solutions to help automate daily processes to crunch trading data
• Take ownership of production processes to ensure constant alignment with business objectives
• Continually expand and upgrade the software infrastructure to accommodate the changing business needs
Your present skillset
• Expertise in
relational databases, NoSql databases
or other storage solutions for large data sets
• Proven professional work experience as a
database developer
•
Python, C#
experience nice to have
•
System Architecture Experience
• Ability to multitask, set priorities and work in a team
• Excellent communication skills and team player
Show more
Show less","{'ProgLanguage': ['Python', 'C#', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer,Ramify,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-at-ramify-3896146641?position=12&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=HexetsgIg2qZbxO1WiSWdQ%3D%3D&trk=public_jobs_jserp-result_search-card,"ABOUT
Ramify’s mission is to help people achieve financial freedom, no matter who they are and how much they have. We are revolutionizing the private wealth management industry by making smart and affordable financial products for everyone.
No more standardized solutions, hidden fees and complicated words, Ramify enables everyone to become a smart investor. The team combines elements of its research with technology to design customized investment portfolios composed of world-class financial products.
The team comprises around 15 talented individuals. Ramify is looking for talented people in all sectors, who want to have a huge impact, move fast and deliver.
JOB DESCRIPTION
The Quantitative Investment Solutions (QIS) Team is dedicated to designing innovative investment portfolio models and developing cutting-edge investment features within our product suite. Moreover, the QIS Team is at the forefront of driving AI-based solutions for Ramify. This involves conceptualizing and implementing transformative AI solutions tailored to meet the diverse needs of various teams within Ramify.
As a Data Engineer on our QIS Team, you will play a pivotal role in shaping the future of investment strategies through data-driven insights and AI-based solutions. Collaborating closely with our talented team of quantitative researchers, and investment experts, your responsibilities will encompass architecting and implementing robust data pipelines. These pipelines will facilitate the seamless integration of diverse data sources, empowering Ramify teams to make informed decision-making.
Key Responsibilities:
Design, build and launch data pipelines at scale to move data across Ramify platform with SQL technologies.
Design and implement processes and tools for data onboarding and quality, helping to deliver an industry best-practice solution for managing the data lifecycle.
Produce stand-alone tools that can be used by other teams to automate data quality and discover faults.
Build analytical tools that provide insight into business metrics across Ramify.
Architect and lead the implementation of AI based solutions within Ramify
PREFERRED EXPERIENCE
We're looking for people who:
Want to make a difference. We are a small team effectively reshaping how people look at the industry. We need people who 'get it' and want to play an integral part in helping us accomplish this mission and are persistent in getting the job done.
Skills we're looking for:
Master’s or upper-year undergraduate-level coursework in either Computer Science, Management Information Systems, Business Information Systems, Mathematics or Finance related field.
2+ years of professional experience in data engineering.
2+ years of experience with one or more coding languages such as Python (is a must), Java.
Experience with data modeling and ETL design, implementation and maintenance.
Demonstrable mastery of industry best practices in the data lifecycle, including data quality automation and tooling.
Excellent written and verbal communication skills with ability to communicate complex designs and solutions to non-technical and highly technical audiences alike.
Good attention to detail.
Strong analytics and strategic thinking skills
Nice-to-haves :
Understanding of ML/ Generative AI technologies and their applications.
Possess a passion, curiosity, and energy for finance + investing. You understand the ins and outs of the wealth management, trading, and more importantly - know how to explain these concepts simply
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (F/H),Renault Digital,"Boulogne-Billancourt, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-renault-digital-3911202728?position=13&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=qgGV8j9UkcjRkl8hja9q2A%3D%3D&trk=public_jobs_jserp-result_search-card,"Contexte :
Dans le cadre de son programme Industrie 4.0, Renault développe depuis 2017 sa propre plateforme pour connecter et agréger les données industrielles des 22 sites du Groupe et de plus de 2500 machines.
Fort de partenariats stratégiques signés avec Google Cloud (stack data full GCP), Renault Digital est à la recherche d’un(e) Data Engineer au sein du Pôle Architecture et Data pour mettre en place des chaînes de traitement de données répondant à de nouveaux besoins métiers.
Vous collaborerez au jour le jour avec les équipes métiers ainsi qu’avec les autres fonctions du Pôle Architecture & Data (Data Analysts et Scientists, architectes, …), exploitant des téraoctets de données (événements en mode streaming, traitements en batch et temps réels et les appels aux APIs) afin entre autres d’alimenter des modèles de machine learning (segmentation clients, détection automatiquement des pannes des véhicules, …).
Responsabilités principales :
Vous participez aux phases de framing, MVP et release des produits, services et APIs orientés data ;
Vous argumentez les choix d’architecture des projets et de la plateforme datalake sur GCP ;
Vous contribuez à la valeur métier des produits orientés Data s’appuyant sur le Datalake, en mettant en place des chaînes bout en bout de traitement de la data, de l’ingestion à l’exposition d’APIs et à la visualisation des données et des solutions ML/DS ;
Vous êtes garant de la qualité des données transformées dans le Datalake, du bon fonctionnement des chaînes de traitement et de l’optimisation de l’utilisation des ressources des ressources cloud ;
Vous proposez des standards d’architecture et de développement ;
Vous êtes force de proposition, innovant(e) et bienveillant(e).
Environement technique :
Spark, Scala, Python, Java, Airflow, SQL, Google Cloud Platform (BigQuery, Cloud Storage, PubSub, Beam, Dataflow, Cloud ML, TensorFlow, Kubernetes), Git, Docker, JSON, Bash, Spotfire
Profil recherché :
Vous avez minimum 5 ans d’expérience en tant que Data Engineer ;
Vous disposez d’une expérience en développement Spark, Scala, Python et requêtage SQL sur des gros volumes de données ;
Vous avez une appétence pour la data : validation, transformation, analyse, valorisation ;
Vous possédez une expérience de développement et orchestration de chaines ETL complexes via Airflow ou équivalent ;
Vous pratiquez la méthodologie agile (Agile Scrum et/ou Kanban) ;
Vous utilisez les services cloud (préférablement GCP) ;
Vous êtes capable d’échanger en anglais technique écrit et oral.
Informations complémentaires :
Votre poste sera basé à Boulogne-Billancourt (France) en CDI (temps plein)
Vous bénéficiez de 2 à 3 jours de télétravail par semaine
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go', 'Bash'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': ['TensorFlow'], 'DataSerialization': ['Json'], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['ML', 'Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Temps plein', 'Full'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer EMEA (F/M/D),Flowdesk,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-emea-f-m-d-at-flowdesk-3860942388?position=14&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=%2FKDndimAKY6wICIB4Jcdkw%3D%3D&trk=public_jobs_jserp-result_search-card,"Flowdesk is seeking a highly motivated Data Engineer to join the team and managed by the Quentin, the Lead Data!
The data engineer will be an integral part of the growing data team, working closely with our trading and quantitative departments to provide a platform and tools to answer their analytical needs. The data platform revolves around Dagster deployed on Kubernetes for the orchestration, dbt on BigQuery for the transformations, and Looker for the visualization.
Among our current challenges, we are currently focusing on the real-time ingestion and processing of terabytes of market data.
Responsibilities
Design, build, and maintain efficient, scalable, and reliable batch and streaming data pipelines to support Flowdesk's trading infrastructure.
Develop and maintain data warehouses, data lakes, and other data management systems with a strong focus on logical and physical modeling.
Contribute early on to the definition of the data team's data products to maximize ease of access, data quality, and related documentation.
Collaborate with the software engineering team to integrate data-related functionality into Flowdesk's trading infrastructure and other applications.
Leverage cutting-edge data engineering technologies to continually improve the speed, reliability, and scalability of Flowdesk's data processing capabilities.
Develop and maintain strong processes and procedures for data quality control, data validation, data documentation and, data integration.
Collaborate with cross-functional teams including traders, developers, and compliance officers to ensure that all data used by Flowdesk is accurate, timely, and compliant with relevant regulations and requirements.
Requirements
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
3+ years of experience in data engineering or related field.
Awareness of software, data, and analytics engineering best practices (e.g. programming standards, data modelization, code idempotency....)
General understanding of systems architecture and concepts (distributed computing, lake-house architecture, ci/cd workflows...)
Experience optimizing modern data warehousing platforms (BigQuery is a plus).
Strong communication skills and ability to work collaboratively in a fast-paced international environment.
Knowledge of the data engineering ecosystem (contribution to open source projects is a plus).
Strong analytical and problem-solving skills with a keen attention to detail.
Benefits
🌍 International environment (English is the main language)
🚃 50% of transportation costs & a sustainable mobility agreement
🍔 Swile lunch voucher (€9.25 per day, 60% covered)
🏥 100% Alan Blue covered for you and your children
💻 Top of the range equipment{{:}} Macbook, keyboard, laptop stand, 4K monitor & headphones
🎉 Team events and offsites
🔜 Coming soon {{:}} gym memberships, international mobility & lot of other cool benefits !
Recruitment process
👀 Are you interested in this job but feel you haven't ticked all the boxes? Don't hesitate to apply and tell us in the cover letter section why we should meet!
📝 Here's what you can expect if you apply{{:}}
HR interview (30')
Technical test
Technical interview (60')
Chat with the Head of People (30') and the Head of Department (30')
On the agenda{{:}} discussions rather than trick questions! These moments of exchange will allow you to understand how Flowdesk works and its values. But they are also (and above all) an opportunity for you to present your career path and your expectations for your next job!
Show more
Show less","{'ProgLanguage': ['Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': ['Teams'], 'Other': ['CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,DATA ENGINEER (H/F),SFR,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-sfr-3879318123?position=15&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=SFaLXT0NP%2F1pgJi2pSe0Og%3D%3D&trk=public_jobs_jserp-result_search-card,"En tant que Data Ingénieur expérimenté, vous occuperez un rôle essentiel dans notre équipe Data Science.
Vous serez responsable de la conception, du développement et de la maintenance des pipelines de données ainsi que de l'intégration de sources de données multiples.
Votre expertise sera cruciale pour garantir une gestion efficace des flux de données, ainsi que pour faciliter l'analyse et la visualisation des données en plus du support aux data scientists vos missions seront les suivantes :
Architecture projet des données
: Concevoir et développer des architectures projet de données robustes, évolutives et performantes pour intégrer et gérer de grandes quantités de données provenant de sources multiples. Assurer la fiabilité, l'évolutivité et la sécurité des flux de données entrant d’un projet Data Science.
Intégration des données
: Élaborer des pipelines de données efficaces pour l'extraction, la transformation et le chargement des données (via notre Framework ELT/ETL interne) provenant de différentes sources. Mettre en place des processus d'intégration automatisés et veiller à la qualité des données.
Gestion des bases de données
: Concevoir et optimiser des bases de données pour répondre aux besoins analytiques et de reporting. Assurer la performance, la disponibilité et la sécurité des bases de données, ainsi que la gestion efficace des requêtes.
Collaboration interfonctionnelle
: Support des Data Scientists, vous travaillerez avec les équipes business pour comprendre leurs besoins et fournir des conseils et des recommandations basés sur les données.
Optimisation des performances
: Surveiller et optimiser les performances des pipelines de données, des bases de données et des requêtes. Identifier les goulots d'étranglement et les points d'optimisation, et proposer des améliorations pour garantir des performances optimales.
Sécurité et conformité
: Veiller à ce que les données soient traitées et stockées conformément aux normes de sécurité et de confidentialité. Mettre en place des mécanismes de sécurité pour protéger les données sensibles et garantir la conformité aux réglementations en vigueur.
Votre profil :
Vous avez un
Diplôme universitaire en informatique, en génie logiciel, en science des données ou dans un domaine connexe et vous avez à minima 5 ans d'expérience en tant que Data Ingénieur.
Vous possédez également une solide maîtrise des technologies et des outils suivants :
Hadoop, Spark, SQL, Kafka, GCP BigQuery,
De plus vous avez une bonne compréhension des architectures, des modèles et des concepts de base de donnés avec une expérience avancée dans la mise en œuvre de pipelines ETL et dans la gestion de bases de données.
Vos connaissances en matière de sécurité des données, de conformité aux réglementations ainsi que vos compétences en programmation scripting et en développement logiciel seront un plus.
Vos excellentes compétences en communication seront des qualités appréciées et
un niveau d'anglais (appliquée au domaine technique) est un plus.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer,RSight®,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-rsight%C2%AE-3856216625?position=16&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=qLSCutHr%2FcwFsKvRnygi3g%3D%3D&trk=public_jobs_jserp-result_search-card,"Nous recherchons pour notre client, un
leader mondial des services et conseils en technologies
, un
ingénieur Databricks et Data Factory
qui rejoindra une équipe qui combine des compétences métiers avec une forte expertise data, analytique et d’intelligence artificielle pour mettre en œuvre des solutions qui visent à améliorer la gestion et la valorisation des données.
Descriptif des missions:
Vous êtes intéressé à travailler sur une solution ayant un impact direct sur les ambitions de notre client en matière de data (datadriven, data démocratisation) ? Alors devenez membre de l’équipe Corporate Data Lake de notre client ! Comme tout autre membre de l'équipe, vous :
Participer à la définition des composants informatiques supportant la fourniture de services
Développer, tester, industrialiser et déployer des composants en minimisant les impacts sur les utilisateurs (automatisation, 0 temps d'arrêt,...)
Documenter la bonne utilisation des services
Déployer et supporter nos fonctionnalités sur la plateforme
Apporter assistance et conseils aux utilisateurs métiers
Opérer la solution en opération courante (incluant le suivi de la qualité des services) et intervenir dans la résolution des incidents
Participer activement à l'amélioration continue des activités de l'équipe
Expliquer aux collaborateurs ce que le Corporate Data Lake peut faire pour eux
Configurer des espaces de travail pour eux
Fournir du coaching et de l'expertise lors de réunions en face à face ou sur les canaux communautaires
Participer à l'effort de support de la plateforme dans une approche ""vous la construisez, vous l'exécutez""
Contribuer aux premières phases de conception définissant l'avenir du Corporate Data Lake
Compétences:
1er expérience Azure (PaaS et IaaS)
Connaissance de Databricks et Data Factory
Maîtrise d'un ou plusieurs langages parmi : Python, Scala, Spark, PowerShell
Intégration et livraison continues (Jenkins, Azure Devops, GIT Lab CI, …)
Pratique des fondamentaux du génie logiciel (Gestion de Configuration, Tests,...)
Anglais : à l'aise pour assister à une réunion et rédiger de la documentation technique
Bonne capacité d'écoute, orientation client/utilisateur
Expression orale et écrite adaptée à l'interlocuteur
Curiosité et adaptation aux changements technologiques
Bénéfices:
Un processus de recrutement court, un accompagnement personnalisé, une évolution qui s'adapte à votre trajectoire de carrière.
En plus de votre quotidien lié à votre mission, vous pourrez entreprendre, être formé, passer des certifications.
Plan d'épargne pour la retraite collectif, mutuelle, tickets restaurant, des congés d'ancienneté, un catalogue CE, des accords d’entreprise relatifs au télétravail et à la parentalité et autres avantages.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (F/H),Thales,"Vélizy-Villacoublay, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-thales-3908228180?position=17&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=NNrRMFzlfyvjpi8zEBQidg%3D%3D&trk=public_jobs_jserp-result_search-card,"QUI SOMMES-NOUS ?
Thales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.
Thales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.
QUI ETES-VOUS ?
Diplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.
CE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :
En tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.
VOS MISSIONS :
• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Hadoop.
• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.
• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.
• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.
• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.
• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.
• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.
Innovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Data Engineer GCP (H/F),SQLI,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-gcp-h-f-at-sqli-3849296046?position=18&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=Lx91k5AJ27v1%2BCUIKqVSjw%3D%3D&trk=public_jobs_jserp-result_search-card,"Rejoignez
SQLI
et faites partie de l'équipe Data, au sein d’une société à taille humaine, mais avec de grandes ambitions. Nous sommes plus de 2200 talents sur 13 pays et 3 continents.​
Votre futur écosystème :
Au sein du pôle Data de SQLI
, pour rejoindre une équipe dynamique de +40 passionnés.
Des projets digitaux pour des clients grands comptes
: notamment dans les secteurs des services financiers, de l'industrie et du retail.
Une organisation orientée delivery
: vous travaillerez au sein de nos locaux ou en équipe intégrée chez nos clients en mode 100% agile, en mode projet.
Possibilité de télétravail jusqu'à 3 jours par semaine
Des communautés d’Experts
, pour vous aider à progresser​, avec des workshops et ateliers techniques favorisant le partage de connaissances.​
Des Managers de carrière
, pour être suivi par l'un de vos pairs sur l'entité Data, avec un
accompagnement dans l’expression de vos talents
(certifications et formations notamment via le partenariat Solutions de Microsoft, participation aux évènements/salons, publications dans la presse...).
Description du poste :
Un poste de
Data Engineer GCP (H/F)
est ouvert à
PARIS ou ROUEN
(selon votre localisation)
, pour faire partie de l’équipe Data chez SQLI et vous investir dans un environnement technique innovant.
Vos missions seront :
L'analyse et la compréhension des besoins métiers.
La participation à la définition et à la conception de l’architecture.
La réalisation des présentations, démonstrations, POC ou Pilotes pour mettre en lumière les recommandations technologiques.
Les développements de jobs d’alimentation (préparation, ingestion, traitement et contrôle qualité) et l'automatisation des flux d’alimentation du Data Lake et du Datawarehouse
Les tests de charge, tests unitaires…
La maintenabilité de la solution Big Data/BI : optimisation et performance des traitements.
Qualifications :
Ingénieur(e) de formation, avec minimum
3 ans d’expérience sur des projets Google Cloud Platform (BigQuery, Dataflow, ...)
Toujours en veille, à l’affut des nouveautés technologiques et vous aimez échanger (Events, conférences, meetups, etc…).
Force de proposition, vous vous sentez libre d’oser et de vous surpassez en partageant vos idées.
Compétences techniques requises :
Maîtrise d'un langage de programmation
(Python, Java, R, Spark, Scala).
Maîtrise de
SQL.
Une expérience sur au moins un ETL/ELT
(Talend, DBT).
Bonne connaissance des outils et framework d’industrialisation
CI/CD
et/ou gestion de version (Gitlab).
Serait un plus : une expérience sur Power BI, TIBCO EBX et/ou BO DS + la gestion de Conteners et Kubernetes (GKE).
Vous pensez que ce poste est fait pour vous ? Transmettez-nous votre profil !
Tous nos postes sont ouverts aux personnes en situation de handicap.
Des questions sur vos données personnelles ? Retrouvez notre politique de confidentialité concernant les candidats :
https://www.sqli.com/sites/default/files/2024-01/SQLI-PRIV-Politique-Confidentialite-Candidats-C0-29012024.pdf
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Data Engineer - Profils expérimentés H/F,LCL,"Villejuif, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-profils-exp%C3%A9riment%C3%A9s-h-f-at-lcl-3888403052?position=19&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=P1%2BGTBi8FCAuG8zwrP4fXA%3D%3D&trk=public_jobs_jserp-result_search-card,"🏦 LCL, c’est LA banque urbaine du Groupe Crédit Agricole - avec nous, accompagnez la transformation, le développement et le maintien technologiques de nos outils avec une vision business et de satisfaction de nos 6 millions de clients.
En tant qu’acteur majeur de la banque de détail, nous nous adaptons chaque jour aux nouveaux modes de consommation et les projets de nos de clients internes et externes tout en garantissant le besoin de sécurité et de développement technologique qu’impliquent nos activités.
💡Organisées en mode Agile, les 8 squads de la tribu DATA (6 squads Métier et 2 squads transverses) œuvrent au quotidien pour répondre à un enjeu majeur pour la banque : la collecte, le stockage, la gestion et l’usage de la donnée. En interaction permanente avec les autres tribus IT et les métiers, elles étudient et proposent les solutions et architectures à déployer pour répondre au mieux aux stratégies de développement et de pilotage de l’ensemble des métiers de la banque.
Rejoignez-nous si vous souhaitez participer aux réflexions et au développement de la trajectoire technique et DataCentric du SI LCL et plus largement du Groupe CA. Vous côtoierez et serez au cœur de l’implémentation de technologies variées telles que les plateformes Teradata, les solutions d’architecture applicative des technologies BigData ou IA, des environnements analytiques ou encore des solutions de datavisualisation. Vous assurerez le traitement de données en temps réel ou en batch et exposerez les données sous différentes formes.
Que vous souhaitiez devenir expert sur les socles technologiques ou relever le challenge de la gestion de projets Métier, nous vous aiderons à atteindre vos propres objectifs.
Vous rejoindrez une équipe pluridisciplinaire, clairement orientée vers le développement de ses collaborateurs à de nouvelles technologies !
🎯 En tant que Data Engineer :
· Vous aimez analyser les besoins avec les métiers, challenger, identifier les sources de données dans les différents univers technologiques, industrialiser des algorithmes, concevoir et développer des Datalab ou des Datamart sur les plateformes ? Vous saurez relever les challenges proposés par les squads métier !
· Vous préférez travailler à l’architecture et au déploiement de nouvelles plateformes, à la levée de la dette technologique ou encore réaliser de la veille au service de notre trajectoire ? La squad Socles Data est faite pour vous !
· Au-delà des projets que vous gérerez, garant du bon fonctionnement de votre parc applicatif, vous attacherez une grande attention à la mise en œuvre de solutions optimisées.
· La rigueur, la communication, l’esprit d’équipe mais aussi la curiosité et la créativité font partie de vos soft skills ! ils vous permettront de répondre aux enjeux de sécurité, de qualité, de transmission de la connaissance et contribueront à l’atteinte des objectifs de l’IT et plus largement de LCL, au service de ses clients.
💻 Voici les principales technologies utilisées au sein de la tribu, si certaines vous sont familières, nous vous aiderons à monter en compétence sur d’autres !
Langages utilisés : SQL, Python, Scala
SGBD : Teradata et utilitaires (TPT, BTEQ, …)
Streaming : Kafka
Search : ElasticSearch, SolR
Environnement : Unix
Solutions Big Data : Hadoop Cloudera, DataIku, HDFS, Hive, Impala,
Devops : GitLab, Jenkins, Nexus
Outils de visualisation : MS BI (SSIS, SSAS, SSRS) Qlik Sens, BO
Modélisation : MEGA
Outils collaboratifs : GIT, Jira, Confluence, Teams
⚡Si les nouveaux enjeux bancaires vous intéressent, que vous souhaitez intégrer une équipe Agile au service des métiers dans laquelle vous serez force de proposition et que vous aimez travailler dans un environnement motivant et dynamique, rejoignez-nous, cette offre est faite pour vous !
🔥 Les + de notre entreprise :
Accès au Plan d’épargne Groupe, intéressement et participation aux bénéfices de l’entreprise + abondement
Prix préférentiels bancaires et avantages CSE
Parcours évolutif dans l’entreprise et/ou dans le Groupe CA.S.A
Télétravail (jusqu'à 2 jours de télétravail par semaine)
De multiples commodités sur le campus (restaurants d'entreprise, salle de sport, crèche, centre médical, médiathèque...)
Forfait et avantages pratiques « mobilité durable » pour les velotafeurs
Des équipes aussi diversifiées que structurées dans une dynamique de transformation
LCL s’engage en faveur de la diversité et nous encourageons tout(e) candidat(e) ayant l’expérience requise à postuler à nos offres. Tous nos postes sont ouverts aux personnes en situation de handicap.
Nous avons encore de nombreuses raisons à vous présenter pour vous convaincre de nous rejoindre mais pour cela, il faudra postuler ici !
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Confluence', 'Teams'], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Créativité'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (H/F),iPepperGroup,"Valbonne, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ipeppergroup-3894091360?position=20&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=RFFhGqjJZXW%2B3KaXucElnA%3D%3D&trk=public_jobs_jserp-result_search-card,"iPepper recrute pour l'un de ses clients une PME éditeur de logiciel dans le domaine du voyage, un
Data Engineer (H/F)
passionné(e) et expérimenté(e) pour rejoindre une équipe dynamique.
En tant qu'Ingénieur(e) Data, vous serez en charge d'extraire et de transformer des données, de construire et d'optimiser des pipelines de données, ainsi que de concevoir des visualisations de données intuitives et informatives.
Responsabilités :
Concevoir, construire et maintenir des pipelines de données évolutifs et efficaces pour transférer des données entre des bases de données SQL et NoSQL.
Développer et mettre en œuvre des processus ETL pour extraire, transformer et charger des données à partir de différentes sources dans notre entrepôt de données.
Collaborer avec des équipes pluridisciplinaires pour comprendre les besoins en données et garantir la fourniture réussie de solutions de données.
Optimiser et ajuster les pipelines de données existants pour la performance et la fiabilité.
Concevoir et développer des visualisations de données et des tableaux de bord pour fournir des insights exploitables aux parties prenantes.
Surveiller et résoudre les problèmes de pipelines de données, en veillant à la qualité et à l'intégrité des données.
Profil recherché :
Diplôme universitaire en informatique, en ingénierie ou dans un domaine connexe.
Expérience avérée en tant que Data Engineer ou dans un rôle similaire, avec un accent particulier sur la construction de pipelines de données et de processus ETL.
Compréhension solide des bases de données
SQL
et
NoSQL
, y compris la modélisation des données et la conception de schémas.
Maîtrise des langages de programmation tels que
Python, Java ou Scala.
Expérience avec des outils de visualisation de données tels que
Tableau, Power BI.
Solides compétences en analyse et en résolution de problèmes, avec la capacité de traduire des données complexes en insights exploitables.
Excellentes compétences en communication et en collaboration, avec la capacité de travailler efficacement dans un environnement d'équipe pluridisciplinaire.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Résolution de problèmes', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer,PROXIAD,Greater Nice Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-at-proxiad-3901014428?position=21&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=JbkEw8N73f2WHgJMdBa5fg%3D%3D&trk=public_jobs_jserp-result_search-card,"Contexte
En tant que Data Engineer, votre rôle consistera à réaliser la conception, le développement, les tests unitaires, la qualification, l'intégration continue et la mise en production d'évolutions sur les projets du pôle produits scoring.
Ces projets Big Data GCP ont pour objet de développer des traitements de croisement de données, exploration data en mode agile (scrum et Safe), industriel (respect de normes), sur l'environnement Google Cloud Platform.
1 : Conception
Spécification et conception d'une solution se basant sur les développements existants.
Mettre en question les choix techniques dans le but de concevoir un logiciel répondant au mieux à la demande au moindre coût et avec la qualité demandée.
Conception de l'expression de besoins, de la réponse à l'expression de besoins à l'aide des besoins métiers remontés par le Product Owner.
2 : Réalisation
Développement de nouvelles fonctionnalités sur les composants des applications du pôle produits scoring en environnement GCP (DataProc, GCS, BigQuery, Airflow...)
Tests des développements réalisés
Revue de code des développements des autres développeurs
Mise en production via CICD des développements
3 : Suivi du RUN applicatif
Prendre en charge avec les autres membres de l'équipe le RUN des applications du pôle produits scoring. Cela inclus les tâches de rapport quotidien, la gestion des problèmes applicatifs, le soutien aux utilisateurs.
Compétences attendues
Maîtrise opérationnelle :
Confluence
Implémentation de l’intégration continue (Utilisation de la chaine CI/CD existante )
Connaissance des principes DevOps
Jira
Anglais (lu, écrit)
Maîtrise avancée :
Elaborer un cahier de recette
Big Query
Spécifications technique et documentation
Développement :Python, SQL, Scala, Javascript, GitLab
Expertise
GCP : Expérience significative en tant que Data Engineer Cloud. Mise en pratique des produits GCP et en particulier Dataproc, Big Query, composer, workflow, PubSub
Développement : Java
Compréhension générale des travaux BigData et du profiling
Informations complémentaires :
Télétravail 2 jours par semaines
Rémunération aux alentours des 45K€
Expérience requise : 6 ans
Localisation : Mougins
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go', 'JavaScript'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery', 'Big Query'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Confluence'], 'Other': ['DevOps', 'Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
LinkedIn,Data Engineer H/F,Chantelle,"Cachan, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-chantelle-3909775663?position=22&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=6A0p8tASG3TahyMB1aX0EA%3D%3D&trk=public_jobs_jserp-result_search-card,"La Direction des Systèmes d'Information et du Digital du groupe Chantelle recherche son/sa futur.e Data Engineer H/F, pour le lancement du grand chantier de rénovation de l'architecture Data : la bascule de l'intégralité de son Data Warehouse vers Google Big Query.
Nous souhaitons recruter un Data Engineer H/F confirmé.e, chargé.e de contribuer à la définition de la feuille de route de la Chantelle Data Plaform. En tant que Data Engineer vous travaillerez au sein de l'équipe Data Intégration en charge de la Chantelle Data Platform.
Vos Missions :
- Mettre en œuvre une infrastructure autour de Google Cloud Platform permettant de collecter (airbyte, API, ...) , transformer (dataform, Bigquery ...), exposer (dataviz, API, applications, ...) et historiser les données générées par l'entreprise.
- Travailler en étroite proximité avec les responsables des différents domaines fonctionnels (Référentiels, Supply Chain, Manufacturing, B2B, Retail & e-commerce, Finance, ...), avec notre équipe de Data Analysts ainsi qu'avec l'équipe technique en charge des infrastructures transverses
- Être force de proposition sur tous les sujets d'architecture et de modélisation (choix de mise en place de pipeline temps réels ou au contraire de flux de données en mode batch, ou bien encore stockage sur Big Query / Big Table en fonction des cas d'usage).
- Définir les éléments structurants, en justifiant vos choix, et les mettre en œuvre.
- Rationaliser et moderniser notre architecture d'intégration inter-applicative; se projeter sur la création d'un modèle de données de type Datamesh.
- Faire la refonte de la BI de nombreux use cases tels que le pilotage de nos stocks, personnalisation de nos sites e-commerce en temps réel en fonction de nos profils client, etc…
Stack technique : Google Cloud Platform, BigQuery, DataForm, DataFlow, PubSub, Airbyte, Github ...
Bonne maîtrise des langages Python et SQL
Pourquoi travailler chez Chantelle ?
Une flexibilité dans votre lieu de travail, selon la politique de télétravail de l'entreprise.
11 jours de RTT/an ainsi qu'un 13ème mois.
Une culture d'entreprise familiale basée sur des valeurs de respect, de créativité, de durabilité et de transparence
Une aventure dans laquelle vous pourrez vous épanouir, apprendre et entreprendre, avec une grande variété de missions et beaucoup d'autonomie
Des équipes ressources humaines et des managers à votre écoute pour vous accompagner dans votre parcours professionnel
Des réductions sur nos produits et des ventes au personnel
Des avantages dans votre qualité de vie au travail : une conciergerie complète proposant un large panel de services, des activités en interne, un CSE.
Vous souhaitez rejoindre un Groupe familial, innovant, engagé et leader dans son secteur en France comme à l'international et vous souhaitez apporter votre expertise et authenticité pour guider votre équipe vers le succès : postulez et rejoignez le Groupe Chantelle !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery', 'Big Query'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Créativité', 'Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer,EarthDaily Agro,"Balma, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-at-earthdaily-agro-3883708013?position=23&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=Z1Bn3iRSRmq3lgA1wQOI9w%3D%3D&trk=public_jobs_jserp-result_search-card,"About Us
EarthDaily Agro provides space age data and analytics to the organizations and people who feed the planet!
With 35 years of industry experience, EarthDaily Agro provides customers with the data, analysis and knowledge they need to make more efficient and effective decisions. B2B services range from global risk management and monitoring of agricultural commodities to the marketing of inputs and precision agriculture consulting, using the latest research in agronomy, information technologies and remote sensing.
EarthDaily Agro also develops highly customized business solutions for agricultural lenders, insurers, input suppliers and food companies, with easy-to-use analytics, that help reduce the daily risks of agriculture.
EarthDaily Agro is headquartered in Minneapolis, MN, USA, with offices in France, Brazil, Australia and Switzerland and is a division of EarthDaily Analytics Corp.
EarthDaily Analytics Corp., a vertically-integrated data processing and analytics company, is launching a new constellation of earth observation satellites. The EarthDaily satellite constellation will significantly enhance geospatial analytics capabilities in agriculture, forestry, environment, financial services, and intelligence, among many other verticals.
Main Job Tasks And Responsibilities
As a EarthDailyAgro Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and infrastructure specialized for geospatial and remote sensing applications. You will work closely with data scientists, geospatial analysts, remote sensing experts, software engineers, and DevOps teams to ensure the successful deployment and scaling of data pipeline to feed geospatial data machine learning models. Your role will be crucial in optimizing the geospatial machine learning ecosystem and ensuring the seamless integration of AI-driven geospatial solutions into real-world applications.
Your Responsibilities Include
Cloud-based data pipeline Conceptualization, Development and Scaling: Build up pipeline to ingest large volumes of geospatial data, pre-process them and meet data scientists’ requirements, in terms of accessibility, speed, format, quality.
Automation and CI/CD: Industrialization of pipeline deployment, orchestration, workflows, and versioning.
Cost & Speed Optimization: Collaborate with infrastructure team to develop, optimize, and fine-tune pipeline.
Cloud and Containerization: Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization tools.
Infrastructure Management: Utilize containerization technologies and cloud-based services to set up and manage infrastructure, enabling seamless deployment and scalability.
Monitoring and Anomaly Detection: Implement monitoring systems to track pipeline performance and identify anomalies.
Version Control and Data Version Control: Proficient with version control systems like Git and DVC.
Security and Compliance: Ensure the security and privacy of geospatial data, adhering to relevant data protection regulations and industry best practices.
Collaboration and Communication: Collaborate with interdisciplinary teams to integrate data pipeline into existing applications or develop new geospatial products.
Issue Resolution and Troubleshooting: Identify and resolve promptly technical issues related to geospatial data processing, performance, or infrastructure.
Education, Knowledge And Abilities
Requirements
Education: Master's degree in Computer Science, specialisation in Geomatics and/or Remote sensing would be a plus.
Experience: 3+ years experiences with data pipeline processes and deployment is a must-have. Proven hands-on experience in setting up pipelines and data processes with opensource tools (e.g., MLFlow, Argo, Kubeflow) is desirable.
Programming Skills: Proficiency in Python and with data manipulation frameworks (e.g., dataframe, numpy, pandas, xarray, rasterio) and librairies (e.g., Dask).
Problem-Solving Skills: Autonomous, and strong analytical and problem-solving abilities to address complex geospatial data and analysis challenges.
Communication Skills: Excellent communication and interpersonal skills to collaborate effectively with cross-functional teams and stakeholders.
French mandatory (job based in France). Fluent in English (oral and written): meetings with internal are mostly in English.
Preferred Additional Skills
Experience with Earth Observation (EO) data analysis and processing.
Experience with geospatial data formats (e.g., GeoTIFF, Shapefile, NetCDF).
Spatial Analysis Techniques: Understanding of spatial analysis techniques and algorithms commonly used in geospatial data manipulation.
Remote Sensing Integration: Knowledge of remote sensing data sources (e.g., STAC catalog, satellite imagery, LiDAR, SAR) integration into data pipelines for accurate and up-to-date geospatial analysis.
CONDITIONS
Full time job based in Balma, near Toulouse, France.
Fixed + Bonuses
TR / ""Family"" insurance / CSE
Powered by JazzHR
WrfSXQ5YJg
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['Pandas', 'NumPy', 'R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'ML', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Organization', 'Interpersonal Skills']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer,Digital Waffle,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-digital-waffle-3913824888?position=24&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=sYvCEinWvVGScYf9pk8tqA%3D%3D&trk=public_jobs_jserp-result_search-card,"Digital Waffle is proud to have partnered with an innovative tech startup in Paris, who are looking for a talented Data Engineer to join their growing team!
They are made up of a powerhouse of experts, combining
data engineers, business process gurus, and Project managers
who leverage the most advanced solutions available; utilising process mining, automation tools, and smart execution systems.
Looking for an experienced Data Engineer (3-5 years)
What You'll Do:
This is a full-time,
hybrid role (Paris-based)
where you'll wear many hats: data exploration, system integration, data prep, data modeling, and implementing data solutions.
Experience:
Expertise in data engineering, data modeling, and ETL (Extract, Transform, Load) processes
Data warehousing and data analytics skills
Experience handling large, complex datasets
Proficiency in SQL and programming languages like Python or Java
Stellar problem-solving and analytical skills
Top-notch communication and collaboration abilities
Bachelor's or Master's in Computer Science, Information Systems, or a similar field (a plus for process mining or intelligent process automation experience)
If you are an experienced and driven Data Engineer, please apply here!
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (Snowflake),MindPal,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896992742?position=25&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=lPc66SjHIzg51G%2BwgpRsRw%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for experienced
Data Engineers
with knowledge of
Snowflake
platform.
Responsibilities
Creating and managing data in the Snowflake environment
Designing and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms
Optimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately
Automating data processing workflows using tools such as Airflow or other workflow management tools
Deploying and configuring tools to monitor and report on the performance of the Snowflake system
Requirements
Minimum 1 year of experience as a Data Engineer
Ability to use Snowflake
Very good knowledge of SQL and programming in Python
Ability to work with databases, including the Snowflake platform
Knowledge of ETL tools and data integration
Ability to work in a team and good communication skills
Fluent English in speaking and writing
We Offer
B2B contract type
Full-time job
Remote and flexible working hours
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,DATA Engineer (H/F),Boulanger,"Lesquin, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-boulanger-3854554057?position=26&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=M3pdzGobBgvn0n9%2B6lfQ%2Bw%3D%3D&trk=public_jobs_jserp-result_search-card,"Au sein de la direction informatique, le pôle DATA a pour missions de maximiser la mise en valeur des données de BOULANGER ,ELECTRO-DEPOT et KREFEL/HIFI afin d’aider nos décideurs à agir sur les leviers de leur performance par des processus décisionnels efficients.
Au sein de ce pôle, tu prendras en charge un large domaine métier qu'il te faudra maitriser de bout en bout : de la données brutes, sa transformation jusqu'à son exposition dans les reporting. Cela afin d'en assurer le bon fonctionnement, les évolutions constantes et sa pérennité
Tes tâches principales portent sur :
Le pilotage et la mise en œuvre de projets DATA.
La collecte, le stockage et l’exploitation fluides des données par le développement de solutions
Missions
Maitriser les règles fonctionnelles et les KPI de ton domaine afin de challenger les métiers dans les évolutions et les nouveaux projets
Accompagner des équipes métiers dans leurs travaux d’identification et expression des besoins sur la data
Participer aux ateliers de conception et développement des applications data
Modéliser la solution à mettre en œuvre
Concevoir et mettre (ou faire mettre) en œuvre des flux les pipelines d’intégration (en mode batch ou fil de l'eau) de données structurées/semi-structurées
Transformer les données : consolider, enrichir et optimiser les données, qui seront exploitées par le métier
Créer, faire évoluer et optimiser les restitutions
Suivre et animer les développeurs (ETL, restitution, self-BI internes ou externes)
Gérer le RUN
Maitrise le SQL et la base de données (Oracle, Snowflake)
Maîtrise d’outils de restitution (tel que Business Object (BO), PowerBI…)
Capacité relationnelle, rigueur et dynamisme
Maîtrise un ou plusieurs outils de préparation et traitement de la donnée (DataStage, Stambia, ...)
Capacité à s’adapter à tout type d’interlocuteurs (technique, métiers, Direction)
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['Oracle', 'Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer H/F,Neosoft,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-neosoft-3877878521?position=27&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=n7iKAdkp4FW8O5QNXJuUyg%3D%3D&trk=public_jobs_jserp-result_search-card,"Tous nos postes sont ouverts au télétravail
Groupe indépendant de conseil en transformation digitale de près de 1800 collaborateurs, Néosoft s’est construit, depuis 2005, sur un modèle qui place l’excellence, le dépassement de soi et la RSE au cœur de sa stratégie.
En nous rejoignant, vous intégrez des communautés d’experts et de talents qui vous permettent de développer vos compétences et d’offrir à nos clients le meilleur accompagnement possible.
Notre savoir-faire s’articule autour de nos 6 domaines d’expertise :
Conseil & Agilité
Cybersécurité
Data
DevOps
Infrastructures & Cloud
Software Engineering
Nous recherchons pour intégrer notre
agence lilloise
un(e)
Data Engineer confirmé(e)
.
Nous aimerions vous voir rayonner au sein de notre communauté DATA (+100 collaborateurs) animée par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients à consolider un patrimoine Data responsable.
🎯
Vos missions :
Après une période d’intégration, en tant que
Data Engineer
, voici à quoi ressembleront vos activités dans des contextes clients Retail ou Banque / Assurance / Finance :
Analyser et s'approprier les cas d'usages
Analyser et valoriser les données du patrimoine
Mettre en place des flux de transformation de données
Réaliser les tests permettant de s'assurer la qualité du delivery
Continuer la mise au point de frameworks data
Créer et développer des modules de déploiement des solutions
Assurer l'industrialisation de moteurs basés sur l'IA
Assurer le niveau de performance des pipelines
Implémenter les outils de monitoring du socles de données
📝
Votre profil :
Nous vous imaginons avec au moins 4 ans d’expériences sur des projets autour de la
Data
, une maîtrise des
bases de données (SQL)
, des outils de transformation de la donnée
(Talend, BigQuery, Airflow)
, et un socle de compétences solides autours des langages
Python, Spark, Scala, Hadoop, Java.
👉
Votre carrière chez Néosoft
Depuis sa création, Néosoft place ses collaborateurs au cœur de sa stratégie. Notre culture pourrait se résumer en un mot : le collectif.
Nos communautés d’experts vous donnent la possibilité d’apprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.
Nous veillons à ce que chacun bénéficie d’un accompagnement de proximité et d’un suivi de carrière personnalisé auprès de votre manager dédié :
1 bilan d’activité trimestriel pour suivre le développement de vos compétences
1 entretien d’évaluation qui a lieu chaque année pour évaluer votre performance et déterminer vos nouveaux objectifs
1 entretien annuel auprès de votre RH dans le but de cartographier vos nouvelles compétences pour échanger sur vos projets professionnels et souhaits de formations
👉
Vos avantages
Formations et développement de l’expertise :
Vous disposez de temps alloué et rémunéré en contribuant au développement de votre expertise technique et de celle du groupe (Participations à des Tech days, animation d’une conférence à l’interne ou à l’externe, rédaction d’articles, rencontres avec nos candidats en processus de recrutement…)
Un abonnement illimité LinkedIn Learning offert
Bien-être au travail :
Un accord de télétravail flexible jusqu’à 100% de télétravail et personnalisable
Un partenariat avec Gymlib qui favorise le sport en entreprise
Des initiatives locales (afterworks, défis sportifs, team buildings, …)
Et bien plus encore :
Parce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive rémunérée dès l’arrivée du collaborateur
En plus de votre salaire : participation, compte épargne temps, actionnariat...
👉
Votre parcours candidat
Notre processus de recrutement se compose de deux étapes clés :
Un entretien de recrutement RH avec un Talent Acquisition Spécialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carrière possibles au sein de notre groupe
Un entretien d’évaluation technique pour réaliser un diagnostic de vos compétences techniques et identifier les compétences sur lesquels poursuivre votre évolution
Vous aurez également la possibilité de rencontrer pour compléter votre processus un acteur de notre pôle Business ou un pair de votre métier pour échanger sur son expérience collaborateur.
Nous avons hâte de vous rencontrer !
A bientôt,
L’équipe Néosoft 🖐
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': ['Salaire'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data Engineer - Bordeaux,Capgemini,"Bordeaux, Nouvelle-Aquitaine, France",https://fr.linkedin.com/jobs/view/data-engineer-bordeaux-at-capgemini-3889788624?position=28&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=nIragHs8rRPTeomiah0NXg%3D%3D&trk=public_jobs_jserp-result_search-card,"Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une
communauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la
valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.
Vos missions :
Vous êtes passionné par le domaine de la Data, vous souhaitez prendre part à des projets d'envergure, concevoir des solutions, les implémenter et les faire évoluer?
Alors rejoignez notre équipe Data Engineering Services au sein de Capgemini Cloud Infrastructure Services en tant que Data Engineer.
Vous avez acquis une expérience solide dans le développement, la mise en œuvre et l’optimisation de solutions pour le traitement d'un grand volume de données, vous êtes capable de créer des solutions qui répondent aux besoins métiers et IT, alors rejoignez notre équipe d’experts.
En qualité de Data engineer, vos missions sont les suivantes :
▪ Concevoir et développer des solutions Data/IA.
▪ Accompagner les Métier dans la compréhension et la mise en œuvre de solution orientées données.
▪ Collaborer avec les Dev, les Ops, les experts infrastructures dans la construction de solutions et d’infrastructures axées sur les données.
▪ Gérer un écosystème de partenaires data et assurer un haut niveau d'expertise
▪ Assurer un rôle de veille technologique sur tous les outils autours de la data, de l’IA et de la BI.
Votre profil :
Vous êtes issu d’une formation ingénieur ou équivalent bac+5 informatique spécialisée en DATA et vous justifiez d’une expérience de 3 à 5 ans dans un rôle similaire. Expert dans une technologie de base de données relationnelle (PostgreSQL, Oracle...)
Expert dans une technologie de base NoSQL (MongoDB, Cassandra...)
Vous maitrisez un framework de manipulation de données (Hadoop, Spark, Kafka...)
Vous maitrisez les concepts DevOps et avez de bonnes notions en scripting et développement
Vous avez une expérience des outils BI et de data visualisation (Kibana, PowerBI...)
La maitrise de l'anglais est nécessaire.
3 raisons de nous rejoindre :
Qualité de vie au travail :
accord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu :
certifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE :
plan actionnariat, tarif préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culture
Nos engagements et priorités :
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
À propos de Capgemini :
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['PostgreSQL', 'Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer,eXalt Value,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=29&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=5UkNpHa5sTYINw6lkcL30g%3D%3D&trk=public_jobs_jserp-result_search-card,"eXalt
est un cabinet de conseil IT
Pure player Data
& IA basé à Paris.
Notre offre s’articule autour de 4 piliers réunis au sein d’une même communauté pour un accompagnement à 360° alliant une expertise technique et méthodologique à une approche conseil métier:
Data Gouvernance & Project
Data Engineering & Big Data
Data Performance & Analytics
Data Science & IA
Filiale du groupe eXalt, créé en 2018,
regroupant plus de
950 collaborateurs en France
(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)
et à l’international
(Colombie, Etats-Unis, Espagne, Belgique),
eXalt Value
apporte une
expertise approfondie
dans le domaine de la Data & IA et conseille les entreprises dans le déploiement de leurs stratégies data-driven.
Bénéficiant du support du groupe eXalt
(1er dans la catégorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value
est en pleine croissance et regroupe aujourd’hui une communauté d’expertise de plus de 60 collaborateurs en région parisienne.
Nos consultants interviennent sur d
es projets d’envergure
dans divers secteurs d’activité,
Banque & Assurance, Médias, Transports, Retail, Tourisme, etc.
Nous recherchons un
Data Engineer Confirmé H/F (minimum 4 ans d'expérience dans la fonction)
pour rejoindre notre communauté sur le
pilier Data Engineering & Big Data.
Vos missions:
Concevoir et développer des pipelines et des flux de données.
Intégrer et transformer des données provenant de différentes sources.
Développer et mettre en œuvre des algorithmes de traitement de données avancés.
Collaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.
Assurer la qualité et la fiabilité des solutions développées.
Conseiller les équipes clients sur les solutions à mettre en place.
Les Prérequis :
Titulaire d'un Bac+5, Ecole d'Ingénieur
Maîtrise d'un ou plusieurs langages de programmation (
Python, Scala, Spark, etc
.).
Expérience approfondie des technologies
Big Data (Hadoop, Spark, Kafka, Talend, etc.)
Expérience avérée
en
environnement Cloud (AWS, GCP, ou Azure)
.
Solides compétences en conception et en optimisation de pipelines de données.
Expérience de travail en
méthode Agile
Capacité à travailler de manière autonome et en équipe.
Excellentes compétences en communication et en résolution de problèmes.
Maîtrise de l’anglais (oral & écrit dans un contexte international professionnel).
Votre environnement eXalté:
Un environnement de travail Collaboratif
favorisant les initiatives et projets transverses à la Practice Data & IA (Lab IA, Data Hub, etc.).
Un collectif de consultants passionnés,
s’intéressant aux tendances innovantes du secteur.
Une Practice de proximité,
privilégiant la montée en compétence de ses collaborateurs (formations, coachings, mentorats, etc.)
Un suivi individualisé et de proximité
par un.e Data Sales Manager référent du compte client, un.e Chargé.e RH et un.e Practice Manager
Une équipe ouverte et dynamique,
qui privilégie les moments de partage et de convivialité (séminaires, eXaltemps, meet-up, déjeuners d’équipe, etc.)
Notre processus de recrutement :
Un entretien RH avec Estelle,
à la suite duquel vous saurez tout (ou presque) d’eXalt Value,
Un entretien technique avec un Manager assorti d’un test technique,
lors duquel vous aurez l’occasion de démontrer vos talents mais aussi d’apprendre avant même de dire oui,
Un entretien final avec la Directrice Associée ou le Directeur Opérationnel,
pour finir de vous convaincre de nous rejoindre 😊
Nous avons hâte de recevoir vos CV, et de faire votre connaissance!
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Résolution de problèmes'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data engineer - F / H,United Robotics Group,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=30&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=zHBqRypzsrJd0gNx9KSZVA%3D%3D&trk=public_jobs_jserp-result_search-card,"Bienvenue chez
Aldebaran
, leader européen de la robotique
au sein du groupe
United Robotics Group
.
Nous concevons et industrialisons des robots innovants avec une vision sociétale ambitieuse pour façonner un monde plus humain. Depuis 2005, nous sommes à l'avant-garde de l'interaction homme-robot avec des produits emblématiques tels que NAO et Pepper.
Notre dernier-né,
Plato
,
incarne notre engagement envers la technologie de pointe et la sécurité,
fabriqué en France avec des composants européens.
Rejoignez nos équipes multiculturelles et dynamiques pour être au cœur de la révolution de la robotique.
Si vous êtes passionné.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer à façonner l'avenir, nous vous offrons une expérience enrichissante et stimulante.
En tant que membre de notre équipe, vous bénéficierez d'une culture d'entreprise axée sur le sens de ce que nous faisons et valorisant la responsabilité sociale et environnementale.
Chez Aldebaran, nous valorisons l'innovation, la diversité et l'égalité et encourageons chacun.e à être ouvert.e, authentique, courageux.se, responsable et engagé.e.
Finalité du poste
Au sein de l'équipe Cloud-Online Services, le Data engineer intégrera l'équipe Data, responsable du développement des produits destinés à la collecte, aux process et à l'exploitation des données de nos robots.
Il aura pour rôle de définir et d'implémenter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui gèrent les robots du groupe.
Missions principales
Le Data engineer aura pour responsabilités de :
évaluer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,
concevoir et développer des services Data en respectant la spécification fonctionnelle et la méthodologie agile,
agréger et stocker de grandes quantités de données,
mettre en place des solutions de data processing,
intégrer/développer des outils de visualisation de données et analyser les KPI,
développer, tester, sélectionner et mettre en production des algorithmes qui permettent de répondre aux besoins,
réaliser des analyses de données,
mettre en place des tests de charge et fonctionnels pour les solutions Data,
investiguer et corriger les bugs remontés par les utilisateurs,
contribuer à la mise en place de l'infrastructure et outil de déploiement (CI/CD)
Rejoignez-nous pour faire partie d'une aventure passionnante où Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !
Requirements
Pour la bonne exécution des missions confiées, vous témoignez d'au moins 6 ans d'expérience en tant que développeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.
Compétences demandées :
Bonne compréhension des technologies d'infrastructure et de déploiement,
Compétences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,
Bonne compréhension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,
Bonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)
Bonne connaissance et une expérience pratique de Scrum\Scrumban et des méthodes agiles,
Une certification AWS sera appréciée,
Un niveau de français et d'anglais courant est indispensable,
Des expériences dans des environnements fortement internationaux sont un plus
Benefits
Nos principaux avantages :
Une culture du bien-être en entreprise qui a fait ses preuves (budget célébration et moments de convivialité par équipes et directions, restauration collective de qualité, environnement de travail agréable)
Un engagement fort en matière de responsabilité sociale et environnementale (promotion de l'égalité professionnelle, performance de notre plan diversité et inclusion, référent handicap, fresque du numérique)
Une culture du télétravail encadrée de manière appropriée !
Tous nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
LinkedIn,Data Engineer,Coders Connect,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-coders-connect-3870419202?position=31&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=beZujM6hc33RzbbSXAlpIw%3D%3D&trk=public_jobs_jserp-result_search-card,"Coders Connect and Sanofi are joining forces to bring an electrifying twist to the biopharmaceutical world!
Work with a rhythm that suits your style (2 days remote and 3 days onsite magic).
Language
: Proficiency in English is required for this role to ensure effective communication within our diverse, global team.
About Sanofi:
We're not just a company; we're a global movement, focused on human health and making a real difference. Our mission? To battle pain, ease suffering, and sprinkle a little bit of magic in the process by developing life-changing medicines and vaccines through breakthrough science and wizard-level technology.
Digital & Data: The Pulse of Our Mission
At the heart of our quest lies our digital and data powerhouse. Think of us as the digital healthcare platform of your dreams, where innovation meets speed, and technology shakes hands with medicine. With our scale, deep-rooted connections in health ecosystems worldwide, and a knack for pushing boundaries, we're here to revolutionise medicine, one digital solution at a time.
The Role: Data Engineering Virtuoso
As our Data Engineering Virtuoso, you're tasked with designing and orchestrating the data pipelines that power our ambitious data analytics initiatives. You'll ensure our data's integrity and accessibility, laying the groundwork for groundbreaking insights and innovations.
Requirements
Cloud Platforms: Proficient in AWS services, with Azure & GCP knowledge a plus. Your work involves leveraging cloud solutions for scalable data processing and storage.
Data Modeling & Query Performance: Expertise in crafting data models and optimizing SQL queries to enhance performance. Experience with Snowflake or similar data warehousing solutions is highly valued.
Integration Services: Skilled in utilizing Integration Services like IICS and Tibco, you facilitate seamless data flow and integration across various platforms.
Scripting & Development: Proficient in scripting languages such as Python and R, enabling you to automate tasks and manipulate data efficiently. Familiarity with GitHub for source code management underscores your commitment to collaborative development and version control.
Visualization & Reporting: Knowledgeable in creating insightful data visualizations using tools like PowerBI, Tableau, or similar, turning complex data into actionable insights.
Data Governance & Compliance: A keen understanding of data quality, security, and governance standards, especially in healthcare environments subject to regulations like GxP, SOX, and data privacy laws.
Real World Data & Standard Models: Experience with Real World Data (e.g., EHR, Claims) and familiarity with standard data models (e.g., OMOP, FHIR) enhance your ability to contribute to our healthcare objectives.
Pipeline Frameworks: Proficiency in using frameworks like Apache Airflow or Kedro for crafting efficient, reliable data pipelines that support our AI and ML initiatives.
The Reward:
A chance to play a crucial role in a collaboration that's redefining healthcare through digital transformation.
A seat at the round table of diversity and inclusion, where different backgrounds and experiences conjure the richness of our culture.
An endless horizon of professional growth, learning opportunities, and the chance to contribute to a future where better health is a global reality.
The Call to Adventure:
If you're ready to join a quest for better – better treatments, better outcomes, and better science – and believe in the magic of bringing diverse talents together to make miracles happen, we want you. Let's embark on this journey together and transform the future of healthcare.
Better is out there. Are you ready to find it with us?
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Initiative']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (H/F),Web Transition,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=32&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=vLfroncagdB3hXjfmeFKmg%3D%3D&trk=public_jobs_jserp-result_search-card,"Web Transition, c’est qui ?
Fondée en 2011,
Web transition
est une entreprise de services numériques opérant sur le marché de l’IT/Digital !
Constituant une part essentielle de
MoOngy Digital Lab
, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et également en Data !
Notre objectif : nous implanter comme un acteur principal sur le marché de la Transformation Digitale en accompagnant et valorisant les compétences de nos collaborateurs !
Nous sommes convaincus que le succès de MoOngy Digital Lab réside dans la somme des potentiels de nos équipes 🤝
Ton équipe : La tribu Data
Parce qu’il est indispensable que tu puisses partager tes connaissances mais aussi en acquérir de nouvelles, tu feras partie de l’une de nos tribus : celle de la Data. De plus, cela te permettra d’être acteur dans le développement et la stratégie de Web Transition. Ce système de co-réflexion et co-construction est un fondement essentiel chez nous !
Dans cette aventure, tu :
T’assures
de la maîtrise de la donnée et est garant de la qualité de son utilisation (référencement, normalisation, et qualification)
Travailles
à la compréhension et l'intégration des données en provenance des différents formats
des interfaces de flux
également à la définition de la politique de la donnée et à la structuration de son cycle de vie dans le respect des réglementations en vigueur
la supervision et l'intégration des données de diverse nature qui proviennent de ces sources multiples et vérifie la qualité des données qui entrent dans le Data Lake
Garantis
l'accès qualitatif aux sources de données
Facilites
l’accès aux données pour tes collègues (data scientists, data analysts…)
Assistes
les autres équipes dans l'accès et la compréhension des données des socles.
Rejoins-nous si tu as :
Expérience d’au-moins 4 ans dans la Data
Appétence à la qualité des données.
Connaissance familière des Datawarehouses.
Maitrise de Python, Oracle SQL, GCP/Power BI
Aisance avec les indicateurs, tu as une bonne capacité d'analyse et de rédaction.
Ton savoir-être :
Ouvert d’esprit
Rigoureux
Autonome
Respectueux des différences de chacun
Curieux
Proactif
Agile
Par où on commence ?
Un premier entretien RH d’1h pour comprendre ton parcours et tes aspirations
Un second entretien de 45 minutes avec l’un de nos Business Manager afin de valider tes compétences et qu’il se projette sur l’une des missions qu’il pourrait te proposer
Un troisième entretien de quelques minutes avec notre responsable d’agence pour te proposer d’intégrer notre superbe Team Web !
3 entretiens en peu de temps, si ton profil correspond tu intègreras très vite nos équipes 😉
Prêt pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant à cette offre ! Voici les avantages qui t’attendent en tant que Weber :
🤩 Des collègues incroyables
🏆 Certifiée Great Place to Work
🎮 Des bureaux sympas (où vous serez toujours les bienvenus)
🎉 Des teambuilding et évents tous les mois
💻 Des tributs métiers pour échanger entre Weber du même métier
Des missions chez le client qui sont accompagnées et coachées par ton manager
Un accompagnement dans ton plan de carrière et tes envies de re skilling
🤓 Un catalogue de formations certifiantes ouvert à tous les salariés
🍽️ Une carte tickets restaurant MyEdenred
❤️ Une mutuelle GrasSavoye
🚎 Une prise en charge des frais de transport à 100%
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data Engineer,Airswift,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-airswift-3909165766?position=33&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=H6EwNhVemE1u%2FM3i1U3DQg%3D%3D&trk=public_jobs_jserp-result_search-card,"Data Engineer
Location
: Paris (Hybrid)
Contract type
: 12 months +
Years of Experience
: 4+
Recruitment Partner:
Airswift
Key Words:
Project Management | Jira | Digiboard | Banking | Stakeholder Management | Architecture | Cloud | Payments/Credit |ServiceNow | PPM |
Responsibilities
Design, develop, and implement data pipelines to collect, process, and store structured and unstructured data from various sources.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and translate them into technical solutions.
Optimize and tune data pipelines for performance, scalability, and reliability.
Ensure data quality and integrity throughout the data lifecycle, implementing data validation and monitoring processes.
Evaluate and implement new tools and technologies to enhance our data infrastructure and capabilities.
Requirements
:
Extensive experience in Python.
Strong experience with data processing frameworks and tools such as Apache Spark.
Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform.
Solid understanding of data modelling, database design, and SQL
French and English speaking
Freelancing opportunity
The next step
We have an exceptional team in place, and we are pleased to be able to appoint a further person to our growing business. We are aware that you may not ‘tick all the boxes’, but if you believe you can genuinely offer some valuable skills and experience to our business, please in the first instance contact our recruitment partner Airswift.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA'], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (H/F),Beelix,"Antibes, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-beelix-3838611420?position=34&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=SE7K53prpCanxUE8Nz3ejA%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes-nous ?
Depuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants :
🚗Automobile
⚡Energie
📡Médias & Télécoms
👗Luxe & Retail
💶 Banque, Finance & Assurance
✈️Défense
Aujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.
Dans le cadre de notre développement, nous recherchons un Data Engineer (H/F) pour l'un de nos clients.
Quelles missions au quotidien ?
Être le leader de la brique Datalakehouse
Développer les scripts de transformations de données et les pipelines d’alimentation
Proposer des évolutions architecturales ou de fonctionnalités pour améliorer le socle technique
Être le back-up du leader technique sur la partie reporting (Power BI)
Orientation satisfaction client et résultat final forte mais également sensibilité au « comment »
Innovation et proposition de nouvelles pratiques pour améliorer l’environnement et les conditions de travail des équipes
A propos de vous ?
5 + années d'expérience en tant que Data Engineer
Maîtrise des technologies suivantes : Microsoft Azure, Microsoft Azure Synapse Analytics (Spark / Python / Pipeline / Serverless), fichiers parquet / delta, Microsoft Power BI, Microsoft SQL Server, langage SQL, Datawarehousing / Modélisation de données
Analyses et export de données
Connaissance de l’ensemble du processus depuis la collecte jusqu’à la mise à disposition des données en ayant comme point fort la maitrise de sa transformation et mise en forme
Vous avez un bon niveau d’anglais
Localisation : Biot et/ou Carros
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (F/H),Aubay,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=35&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=RBe%2FSfh1XFULSgxcv2128A%3D%3D&trk=public_jobs_jserp-result_search-card,"Passionné par la Data, tu souhaites rejoindre une communauté d’experts dans le domaine afin de développer tes compétences en Data Engineering. Aubay renforce ses équipes Data et recherche des Data Engineers pour intégrer des dispositifs de projets pointus et variés.
Ton quotidien en tant que Data Engineer chez Aubay, :
Définition de la stratégie de stockage et mise en œuvre des technologie appropriées (base de données SQL, NoSQL, stockage distribué,…)
Ingestion des données (structurées, semi-structurées ou non-structurées) selon différentes fréquences : batch, micro-batch ou temps réel
Conception et mise en œuvre de pipelines de données afin de fournir des données prêtes à l’emploi aux consommateurs : uniformisation, mise en qualité, enrichissement, calcul d’indicateurs,…
Conception et développement d’API pour exposer les données auprès d’applications tierces
Appui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning
Préparation et animation d’ateliers de travail avec des interlocuteurs variés : recueil/approfondissement des besoins métiers, avancement/restitution des travaux, transfert de compétences,…
Ton profil :
Tu dispose d’une formation niveau BAC+5 (Master 2 ou école d’ingénieur) spécialisée en informatique
Tu as déjà une première expérience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data
Les technologies telles que Hadoop, Spark ou Kafka sont tes technologies de prédilection
La programmation n’a plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python
Tu maîtrises les tenants et aboutissants de la philosophie DevOps et des outils orientés CI/CD
Tu es soucieux de la qualité et de la performance de tes développements et tu t'intéresse à l’innovation frugale
Tu es un expert technique dans ton domaine sans pour autant oublier l’importance d’une communication orale et écrite de qualité et adaptée à chacun de tes interlocuteurs
Tu travaille au quotidien en mode agile et tu en maitrise les fondements
Ce qui nous caractérise :
Des missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs variés (Banque, Assurance, Telecom, Industrie,…) qui permettent à nos collaborateurs de monter en compétences et de devenir des experts Data reconnus
De l’apprentissage en continu avec des formations et des certifications sur les technologies Data d’aujourd’hui et de demain
Des experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets
Des communautés de savoir-faire Data proposant de manière régulière aux collaborateurs d’Aubay du contenu et des évènements de partage (webinar, meetup/afterwork, BBL,…) sur les thématiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,…
Aubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.
Ta carrière chez Aubay :
Tu auras la possibilité de développer et certifier tes compétences sur les dernières technologies Data avec un focus fort sur les plateformes Data Cloud telles qu’Azure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks
Tu pourras rejoindre la BU d’excellence Data et évoluer au sein d’un environnement humain et professionnel de haut niveau. Tu profiteras d’un management sur-mesure pour t'accompagner dans ta trajectoire de carrière
Au sein de la BU d’excellence, de multiples perspectives s’offriront à toi :
Rôle de « Lead » : Vous pourrez gagner en responsabilité sur le plan technologique et devenir un référent auprès de nos clients et des collaborateurs de la communauté Data Engineering
Rôle de « Champion » : Vous représenterez Aubay auprès d’un ou plusieurs de nos partenaires éditeurs stratégiques et vous participerez activement à l’animation de la relation sur le plan technologique
Rôle de « Head » : Vous pourrez prendre la responsabilité du savoir-faire Data Engineering et de ses offres et en assurer le développement au sens large (développement business, recrutement, management de collaborateurs, définition de la stratégie et animation de la communauté au sein du groupe Aubay,…)
Besoin d’en savoir plus sur le processus de recrutement ?
Un échange macro au niveau RH avec Doriane
Un entretien technique avec Marius ou Peter, deux de nos référents techniques
Un échange managérial avec le Directeur de la BU Modern BI & Data
A savoir que l’ordre des étapes peut varier selon tes envies (ex : échange managérial avec l’échange technique)
Aubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure', 'Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
LinkedIn,Data Engineer,ADVANCED Schema,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-advanced-schema-3886398270?position=36&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=0IzfiiIAtmjfmcIDT5XGsw%3D%3D&trk=public_jobs_jserp-result_search-card,"En tant que Data Engineer, vous aurez les missions suivantes :
Concevoir
des modélisations physiques
Construire
des mappings techniques et rédaction de spécifications d’alimentation.
Développer
des flux des données
Contribuer
au pilotage de projets, de proof of concepts
Participer
à des missions d’expertise
Compétences professionnelles & niveau d'études requis :
Vous êtes titulaire d'un diplôme
Bac +3
minimum dans le domaine de la
data
Vous possédez minimum
1 an d'expérience
dans le métier
Être
enthousiaste
à l'idée
d'apprendre de nouvelles technologies
Expérience de la méthodologie
Agile / Scrum
Capacité à
planifier et à prioriser
les
tâches
et les
activités confiées
en autonomie
Maîtrise
de l’anglais oral et technique obligatoire
Expérience
avérée dans l'écriture de code propre avec 2 ou plusieurs des technologies suivantes :
BASH, SQL, Java, Python, NoSQL
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Bash'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer,ADVANCED Schema,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-at-advanced-schema-3539059697?position=37&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=vCi%2FAHpg6Emgt0PR%2BhHtxA%3D%3D&trk=public_jobs_jserp-result_search-card,"ADVANCED SCHEMA
est une société de services informatiques
spécialisée dans la donnée.
Depuis 20 ans, nous créons des plateformes data sur mesure pour nos clients, orientées usages et alliant qualité, performance, sécurité et gouvernance.
ADVANCED SCHEMA
a développé de nouvelles activités pour réaliser l'ambition du groupe : devenir
une entreprise end-to-end,
en proposant une offre à 360° à nos clients pour les
accompagner à chaque étape de leurs projets.
À ce jour, nous sommes près de 220 passionnés répartis entre Paris, Lille, Nantes, Lyon mettant à profit leur expertises aussi bien dans le domaine du retail, de la finance/assurance, du luxe, des médias, de la santé et de l'industrie.
Aujourd’hui, nous souhaitons intégrer de nouveaux renforts dans nos équipes Lilloises.
En tant que Data Engineer, vous aurez les missions suivantes :
Concevoir des modélisations physiques
Construire des mappings techniques et rédaction de spécifications d’alimentation.
Développer des flux des données
Contribuer au pilotage de projets, de proof of concepts
Participer à des missions d’expertise
Compétences professionnelles & niveau d'études requis :
Vous êtes titulaire d'un diplôme Bac +3 minimum dans le domaine de la data
Vous possédez minimum 2 ans d'expérience dans le métier
Positif(ve), curieux(se), rigoureux(se) et doté(e) d'une bonne aisance relationnelle
Être enthousiaste à l'idée d'apprendre de nouvelles technologies
Expérience de la méthodologie Agile / Scrum
Capacité à planifier et à prioriser les tâches et les activités confiées en autonomie
Maîtrise de l’anglais oral et technique obligatoire
Expérience avérée dans l'écriture de code propre avec 2 ou plusieurs des technologies suivantes : BASH, SQL, Java, Python, NoSQL
Notre proposition :
Temps plein en
CDI
avec un
salaire attractif
+ participation aux bénéfices + prime(s) sur investissement personnel
Mode de
travail hybride
(agence, site, télétravail selon projets/clients)
Ticket restaurant (Sodexo)
Mutuelle financée à 50%
Prévoyance
Comité entreprise
5 jours d’onboarding plein temps via la
ADVANCED SCHEMA Academy
Notre investissement :
Chez
ADVANCED SCHEMA
, nous t’offrons un environnement de travail stimulant et collaboratif ainsi que des possibilités de croissance et de développement professionnel. Également un
accompagnement/support au quotidien
pour te faire grandir et monter en compétences, sur des projets qui répondent à de
vrais enjeux pour nos clients
. Si tu es passionné(e) par les données et prêt(e) à relever de nouveaux défis, alors nous aussi nous aimerions te rencontrer
Process de recrutement :
Si ta candidature retient notre attention, nous te proposons :
Un premier échange téléphonique/visio
Un entretien physique (+questionnaire d’évaluation) avec un senior manager
Un entretien final à notre siège Parisien afin de rencontrer le DG
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go', 'Bash'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Hybride', 'Temps plein', 'Senior'], 'TypeContract': ['CDI'], 'Salary': ['50'], 'Level': [], 'Experience': ['a', 'n', 's', '20', '20', '20']}"
LinkedIn,Data Engineer,AFD Technologies,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-afd-technologies-3899625641?position=38&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=bYpfT0MgYlzf9jo6i%2Fxwig%3D%3D&trk=public_jobs_jserp-result_search-card,"AFD.TECH part of Accenture
est le spécialiste du conseil en transformation digitale des grandes entreprises 🚀.
A ce jour, le Groupe est composé de 2.000 talents répartis dans 3 pays (France, Belgique & Maroc) 🌎 pour un chiffre d’affaires annuel de 125M€ !
Nos Talents d’abord 😎:
Les Talents d’AFD.TECH part of Accenture sont au cœur de la stratégie du Groupe et il est primordial pour nous que chaque collaborateur trouve du sens dans son travail.
Au-delà de proposer une carrière ambitieuse et personnalisée à nos Talents, nous avons à cœur de leur offrir un environnement de travail flexible (remote), inclusif et épanouissant dans tous nos bureaux (Paris, Bruxelles, Rabat, Lyon, Strasbourg, Lille, Nantes, Toulouse, Marseille, Bordeaux et Rennes)🌍.
Avec 20% de croissance par an et plus de 20 ans d’existence, AFD.TECH part of Accenture est devenu l’acteur incontournable du marché des infrastructures informatiques, réseaux et télécoms.
Notre proposition de valeur ? Intervenir sur l’ensemble du cycle de vie de projets complexes, pour des clients grands comptes, venant de secteurs tels que la banque, le ferroviaire, les médias, télécoms, etc (comme la Société Générale, Bouygues Telecom, Orange, Thales et bien d’autres encore !)👩🏻‍💻.
Nous rejoindre est une formidable aventure humaine : nous vous proposons un poste un poste de
Data Engineer en CDI
, au sein de notre agence Lilloise.
Vos missions ✅:
En tant que Data Engineer pour l'un de nos clients grands comptes, votre rôle s’articulera autour de différents axes :
Appréhender le contexte et les enjeux métier du client.
Collaborer avec les équipes métier pour comprendre les exigences en matière de données.
Définir des architectures data.
Concevoir et mettre en place des pipelines de données.
Construire des flux de données complexes.
Vous travaillerez dans une mission à forte valeur ajoutée et de longue durée (minimum 1 an et demi).
Votre profil✅:
Vous maîtrisez le langage SQL, les ETL et les ELT.
Vous aimez automatiser, mettre en place vos data pipelines et maîtriser les technologies: CI/CD, Terraform, Github, Python, Kafka.
Vous possédez des compétences en data visualisation : Business Objects, Qlikview, Qlik Sense, PowerBI ou Data Studio.
Vous connaissez Google Cloud Platform (GCS, BigQuery).
Vous êtes diplômé(e) d’une formation BAC + 5.
Vous avez une première expérience significative dans la data engineering (
minimum 3 ans
).
Vous projetez votre carrière dans un cabinet de conseil exigent et successful, qui vous permettra de développer votre esprit entrepreneurial et de répondre à vos ambitions.
Ce que nous offrons chez AFD.TECH part of Accenture 🤗:
Une politique de flexibilité dans votre organisation et un bon équilibre de vie 🏃‍♂️.
Des avantages plus que compétitifs 💰.
Un accompagnement et un suivi régulier durant tout votre parcours chez AFD.TECH (Launchpad, Linkers, rookies, etc…).
Un état d’esprit familial et de la proximité entre tous 👨‍👩‍👧‍👦.
Des moments de convivialité toute l’année 🍾 (évent en équipe, séminaire annuel, sports collectifs etc.).
Un parcours d’évolution sur mesure 🔼.
A très bientôt chez AFD.TECH part of Accenture!
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': ['Organisation', 'Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': ['Remote'], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '20', '20', '20']}"
LinkedIn,Data Engineer,Thales,"Ollioules, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-at-thales-3902424527?position=39&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=gUaUTnXX4O5aMuZNjUpt4A%3D%3D&trk=public_jobs_jserp-result_search-card,"QUI SOMMES-NOUS ?
Thales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.
Thales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.
QUI ETES-VOUS ?
Diplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez a
u moins 3 ans d'expérience
dans les technologies Big Data.
Passionné par le
secteur de la Défense et du Naval.
CE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :
En tant que
Data Engineer,
vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.
VOS MISSIONS :
• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies
Hadoop, Spark, Scala
.
• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.
• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.
• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.
• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.
• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.
Innovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.
Show more
Show less","{'ProgLanguage': ['Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Data Engineer - Lille,Capgemini,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-lille-at-capgemini-3914228495?position=40&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=p4ZL6Pvek5LSJf3xzYmxKw%3D%3D&trk=public_jobs_jserp-result_search-card,"Capgemini en quelques mots
Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.
Vos missions
Vous maitrisez au minimum un langage de programmation appliqué à l’analyse de données
(Java, Python, Scala et les environnements Spark et / ou Hadoop).
Vous êtes passionné par le Big Data et le Machine Learning et l’analyse de données
Vous concevez et mettez en œuvre des stratégies sécurisées d'acquisition et d'intégration de données
Vous configurez des référentiels de données à la pointe de la technologie dans des environnements distribués
Vous construisez des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée
Votre profil
Diplômé(e) de Bac+5 en informatique
4 ans d’expérience
(au sein d’une ESN ou chez un intégrateur) en conseil clientèle
Une solide culture technologique
Un bon niveau d’anglais
3 raisons de nous rejoindre
Qualité de vie au travail :
accord de télétravail en France et à l’international, accord sur l’égalité
professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu :
certifications et formations en libre accès, accompagnement sur mesure avec
votre carreer manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE :
plan actionnariat, activités à tarifs préférentiels, remboursement partiel
vacances, remboursement de votre abonnement sportif ou culturel
Nos engagements et priorités
Le groupe Capgemini encourage une
culture inclusive dans un cadre multiculturel et handi-accueillant.
En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des
initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
Capgemini
est un
leader mondial
, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de
55 ans d’expérience,
nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le
cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning', 'Cloud'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data Engineer (F/H) à Nantes,Siderlog Conseil,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-%C3%A0-nantes-at-siderlog-conseil-3858540683?position=41&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=HGGdfZPbjaOackab%2BATkIA%3D%3D&trk=public_jobs_jserp-result_search-card,"Rejoignez une Aventure Passionnante chez Nous !
🚀
Si vous recherchez une entreprise en pleine croissance où votre potentiel peut s'épanouir pleinement, vous êtes au bon endroit !
Chez nous, l'humain est au cœur de notre culture d'entreprise. Nous croyons en l'autonomie, la confiance et le partage comme des valeurs essentielles qui guident chacune de nos actions.
Ne perdez plus de temps, rencontrons-nous dès maintenant !
En tant que membre de notre équipe de consultants Siderlog, vous travaillerez en étroite collaboration avec nos clients. Voici un aperçu des missions qui vous attend :
Contribution à la fabrication de produits dans un environnement Cloudera 🛠️
Accompagnement sur la fabrication des modèles de Machine Learning sur des données énergétiques et plus largement ESG. 🤖
Attendu :
Contribuer au sein d'une équipe agile à répondre aux besoins des Caisses régionales.. 🌐
Définir les architectures des solutions avec le reste de l’équipe 🏗️
Fabriquer et tester les solutions 🧪
Déployer dans les différents environnements 🚀
Garantir le bon fonctionnement en production 💼
Accompagner l’évolution des pratiques de l’équipe dans une logique d’amélioration continue de la qualité du code 📈
Entrainer et tester des modèles de Machine Learning 🧠
Profil :
Une expérience entre 4 à 7 ans
Lieu : Nantes
Début : Dès que possible
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '7', '7', '7']}"
LinkedIn,Data Engineer,ASTRELYA,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-at-astrelya-3910760230?position=42&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=xS5BufbehfAwbLJ4GkTZBQ%3D%3D&trk=public_jobs_jserp-result_search-card,"ASTRELYA est un groupe de conseil et d’expertise IT fondé en 2017, présent en France (Paris et régions) et en Suisse (Genève). Aujourd'hui plus de 280 collaborateurs accompagnent nos clients dans l’accélération et la transformation de leurs organisations.
Dans le cadre de notre développement, nous recherchons un
Data Engineeer F/H
.
Vos rôles et responsabilités :
Développements Java Spark
Optimisation et gestion des évolutions de l&#39;architecture pour intégrer des calculs sur des volumétries de plus en plus importantes
Support technique auprès des équipes de développement et du responsable applicatif
Conception des solutions applicatives cohérentes avec l&#39;ensemble du SI et avec les normes et standards
Développer et garantir les pratiques de développement et de documentation associés (DevOps
L’environnement technique dans lequel vous évoluerez :
Java, Scala, Spark, écosystème Hadoop, environnement DevOps
Les compétences recherchées :
Formation : École d’ingénieur ou équivalent Bac+5
Expériences : Minimum 5 ans d’expérience
Langues : Anglais technique
Excellent relationnel, force de proposition, autonome
Pourquoi rejoindre ASTRELYA ?
Une gestion de carrière personnalisée et un management de proximité
Une politique active de formations / certifications (technique, métier, leadership)
Une offre variée de missions d’expertise
Un engagement RSE fort : Ecovadis Gold, Signataire de la charte pour la diversité, du Pacte des Nations Unies et mise en place du Mécénat de compétences
Un programme de cooptation attractif
Afterworks, conférences techniques et activités sportives réguliers
Cette annonce vous correspond ? Postulez !
🚀
Tous nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Java', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': ['Leadership', 'Organisation'], 'EnSoftSkils': ['Leadership']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer,Beelix,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-beelix-3865239426?position=43&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=X3Z%2FKDHugw6kJ06w9HTF%2Bg%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes-nous ?
Depuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants:
🚗Automobile
⚡Energie
📡Médias & Télécoms
👗Luxe & Retail
💶 Banque, Finance & Assurance
✈️Défense
Aujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.
Dans le cadre de notre développement, nous recherchons un Data Engineer en Île-de-France.
Quelles missions au quotidien ?
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le Datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le Datalake
Les exploiter pour atteindre la finalité business (exposition de Business View, réintégration des résultats dans le SI, service de scoring, …)
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Accompagner les Data Engineers sur son périmètre pour garantir la qualité des livrables
Expertise souhaitée
Expertise en SPARK et PySpark
Expertise sur Databricks
Une expérience sur un cloud provider public comme Azure (idéalement), AWS, ou GCP
Connaissances avancées d'outils de BI comme PowerBI (idéalement) ou Spotfire
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Etre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT
Maitrise des Traitements Big Data en mode Streaming
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
A propos de vous ?
Diplômé d'une école d'ingénieurs ou équivalent
Au moins 3 ans d'expérience en tant que Data Engineer
Expérience en mode de Delivery Agile (Scrum, Kanban, etc.…)
Vous avez un bon niveau d’anglais tant à l’écrit qu’à l’oral
Pourquoi nous rejoindre ?
Un suivi et un accompagnement au quotidien
Un organisme de formation certifié Qualiopi, un abonnement linkedin learning pour chaque salarié et des partenariats avec des spécialistes pour d’autres expertises
De nombreux événements : Afterworks, Communautés métiers, Happy talks…
une Expérience personnalisée basée sur vos besoins grâce au Prédictive Index
Notre package « unBeelievable » : 100% du titre de transport, Tickets restaurants, CSE, Prime de participation ...
Nombreux évènements (afterworks, sport, etc) et des communautés métiers dynamiques
Le processus de recrutement ?
Échange téléphonique (15 min)
Entretien 1 RH pour apprendre à vous connaître
Entretien 2 avec votre futur N+1 pour appréhender la relation managériale
Entretien 3 avec un Responsable commercial pour avoir la vision stratégique
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Data Engineer (H/F),MP DATA,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-mp-data-3908719610?position=44&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=2MIiyRrB12%2BwsWUba%2B9GJQ%3D%3D&trk=public_jobs_jserp-result_search-card,"MP DATA est une société spécialisée dans l’acquisition, le traitement, et la valorisation des données.
Depuis sa création en 2015, MP DATA accompagne ses clients, majoritairement industriels, dans le management de leur performance et l’exploitation de leurs données.
Les collaborateurs, tous issus de grandes écoles, incarnent au quotidien les valeurs d’Excellence, de Partage et d’Engagement.
Ils associent savoir-faire technique, méthodologie et passion et mettent leurs compétences au service de missions et projets au sein de grands groupes français.
MP DATA accompagne ses clients sur toute la chaine au travers de 3 pôles d’expertise : Conseil et Stratégie, Infrastructure & CloudOPS, Data Science.
Chez MP DATA, les équipes commerciales cherchent des missions en fonction des envies des collaborateurs et non pas l’inverse. Les consultants sont accompagnés dans tous leurs projets, de la mobilité géographique, au changement de secteur d’activité en passant par le développement de nouvelles compétences.
Rejoindre MP DATA, c’est la garantie de travailler sur des sujets passionnants avec un cadre technique fort.
Descriptif du poste :
Nous recherchons un Data Engineer expérimenté pour rejoindre notre équipe.
En tant que Data Engineer, vous serez responsable de la conception, du développement et de la mise en œuvre de pipelines de traitement de données en temps réel à grande échelle.
Vous travaillerez avec des technologies telles que Kafka, Flink, Kinesis et vous utiliserez les services du cloud AWS pour stocker et traiter les données.
Vos responsabilités :
Utiliser Kafka pour le traitement de flux de données en temps réel à grande échelle, en travaillant avec les producteurs, les consommateurs et les topics.
Mettre en œuvre des pipelines de traitement de données en streaming avec Flink, en appliquant des transformations complexes et en gérant les états.
Écrire du code efficace et maintenable en Java / Python pour manipuler et analyser les données en temps réel.
Utiliser Kubernetes pour déployer et gérer des applications conteneurisées à grande échelle, en assurant la résilience et l’évolutivité des services.
Utiliser les services AWS tels que Amazon S3, AWS Lambda, Elastic Kubernetes Service (EKS), Elastic Container Service (ECS) et Elastic Compute Cloud (EC2) pour le stockage, le traitement et le calcul des données en temps réel.
Suivre les meilleures pratiques pour une utilisation efficace du cloud, en assurant la gestion des coûts, la sécurité des données et la disponibilité des services.
Collaborer avec l’équipe de développement logiciel et la gestion de projets pour assurer un flux de développement fluide et une livraison efficace des fonctionnalités.
Bon à savoir :
CDI / ASAP / Toulouse
Profil recherché:
Nous recherchons un candidat diplômé d'une grande école d'Ingénieur avec une première expérience.
Compétences nécessaires :
Expérience significative dans un environnement industriel en mode DevOps, avec des outils tels que CICD, gitlab, Jenkins, Sonar, Nexus, XLdeploy, Camunda, etc.
Maîtrise des langages de programmation tels que Python, Java et expertise dans l’écriture et l’optimisation du code SQL
Maîtrise du français et bonne maitrise de l’anglais.
Capacité à travailler en équipe et esprit d’équipe.
Le processus de recrutement se déroule en 3 entretiens :
Prise de contact
1er entretien : Présentation et projet du candidat + présentation MP DATA
2ème entretien : Entretien de qualification technique
3ème entretien : Rencontre avec les équipes dans les locaux MP DATA + Proposition de collaboration
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer,Shippeo,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-at-shippeo-3908268236?position=45&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=DxKRCRuTeTU%2Bx7wYrAc6QQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Founded in 2014, Shippeo is a French based SaaS company providing supply chain visibility. Shippeo has grown from 70 to 220 employees during the last two years and is continuing to rapidly scale after an additional $40M fundraising round in October 2022.
Shippeo is an exceptionally diverse company with colleagues from 27 different nationalities and speaking 29 languages. With offices throughout Europe, North America and recently Asia, Shippeo provides global coverage to all of our clients.
Our product is composed of a mission critical SaaS web platform (API everywhere), with high traffic inbound/outbound integrations.
Our mission is to anticipate problems and proactively alert end-customers so they can efficiently manage exceptions. We achieve this by collecting and matching millions of theoretical and real data from different stakeholders.
The Data Intelligence Tribe is responsible for leveraging Shippeo’s data from our large shipper and carrier base, to build data products that help our users (shippers and carriers alike) and ML models to provide predictive insights. This tribe’s typical responsibilities are to:
get accurately alerted in advance of any potential delays on their multimodal flows or anomalies so that they can proactively anticipate any resulting disruptions
extract the data they need, get direct access to it or analyze it directly on the platform to gain actionable insights that can help them increase their operational performance and the quality and compliance of their tracking
provide best-in-class data quality by implementing advanced cleansing & enhancement rules
As a Data Engineer at Shippeo, your objective is to ensure that data is available and exploitable by our Data Scientists and Analysts on our different data platforms. You will contribute to the construction and maintenance of Shippeo’s modern data stack that’s composed of different technology blocks:
Data Acquisition (Kafka, KafkaConnect, RabbitMQ),
Batch data transformation (Airflow, DBT),
Cloud Data Warehousing (Snowflake, BigQuery),
Stream/event data processing (Python, docker, Kubernetes) and all the underlying infrastructure that support these use cases.
Qualifications
Required:
You have a degree (MSc or equivalent) in Computer Science.
3+ years of experience as a Data Engineer.
Experience building, maintaining, testing and optimizing data pipelines and architectures
Programming skills in Python and experience with asynchronous event processing (asyncio).
Advanced working knowledge of SQL, experience working with relational databases and familiarity with a variety of databases.
Working knowledge of message queuing and stream processing.
Knowledge of Docker and Kubernetes.
Knowledge of a cloud platform (preferably GCP).
Experience working with workflow management systems such as Airflow.
Desired:
Experience with cloud based data warehouse solutions (BigQuery, Snowflake).
Experience with Kafka and KafkaConnect (Debezium).
Experience with Infrastructure as code (Terraform/Terragrunt).
Experience building and evolving CI/CD pipelines with Github Actions.
Monitoring and alerting on Grafana / Prometheus.
Experience working on Apache Nifi.
Informations supplémentaires
We are looking for talents who share our values:
🚀 Ambition
💙 Care
🎯 Deliver
🤝 Collaboration
Find out more about our values in
Our Culture Book
If you identify with our values and enjoy working in a fast-paced and international environment, Shippeo is just the place for you!
We are committed to fostering diversity and inclusion within our workplace as we value the unique perspectives and experiences that individuals from all backgrounds bring to our team. We are dedicated to providing equal employment opportunities to all candidates, regardless of their background or abilities, and our commitment to inclusion is reflected in our policies, practices, and workplace culture.
We understand that candidates may have unique needs or questions related to disability inclusion. To facilitate this, you can reach our dedicated Disability Advisor at
inclusion@shippeo.com
with any inquiries or requests for accommodations during the application process.
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['ML', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Stage - Data Engineer - ML (H/F),Withings,"Issy-les-Moulineaux, Île-de-France, France",https://fr.linkedin.com/jobs/view/stage-data-engineer-ml-h-f-at-withings-3613476264?position=46&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=%2F8DZLqwtmjFyUqn3vcgywQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Chez Withings, nous développons des appareils de santé connectée : nos balances connectées, montres hybrides, tensiomètres, moniteurs de sommeil et tous les dispositifs de notre gamme sont aujourd'hui utilisés par des millions d'utilisateurs. Notre objectif est de permettre la prévention, le dépistage et l'accompagnement d'un certain nombre de maladies chroniques via des produits et des services innovants afin de révolutionner la manière dont on prend soin de notre santé.
Au sein de l'équipe Machine Learning, nous développons des algorithmes pour extraire des informations physiologiques et médicales pour nos utilisateurs tels que le SPO2, la fréquence cardiaque, la détection de diverses pathologies comme la fibrillation atriale, l'apnée du sommeil...
Intégré.e au sein de l'équipe Machine Learning, tu auras une ou plusieurs des responsabilités suivantes :
Développer un outil de monitoring de la dette technique, des mauvaises pratiques de code, des failles de sécurité ;
Construire des dashboards de visualisation ;
Construire un système d'alerte pour notifier les contributeurs d'éventuels problèmes ;
Développer des outils permettant de corriger les éventuels problèmes de façon automatisée ;
Requirements
À la recherche d'un stage d'une durée de 3 à 6 mois ;
Préparation d'un Master en école d'ingénieur ou équivalent / année de césure possible ;
Maîtrise de Python ;
Maîtrise de Debian ou de Ubuntu, de Shell et de l'environnement Linux ;
Première expérience sur du développement logiciel ;
Culture DevOps (omniprésence du monitoring, automatisation des tâches, ...)
Compréhension de la culture et des besoins des différents membres de l'équipe ;
Rigueur, autonomie, prise d'initiative, curiosité
Benefits
Rejoindre l'aventure Withings, c'est :
Intégrer un des pionniers et leaders mondiaux de la santé connectée, plusieurs fois primé au Consumer Electronic Show
Contribuer à des projets innovants et ambitieux pour la santé de demain dans un environnement agile et en constante évolution
Intégrer une entreprise internationale, membre de la FrenchTech 120, dont les équipes sont basées à Issy-les-Moulineaux, Boston, Hong-Kong et Shenzhen
Participer à l'amélioration continue de nos produits et services en les bêta-testant avant leur sortie, notamment lors de nos nombreuses sessions sportives entre collègues
Participer à la Withings Med Academy en assistant à des conférences de professionnels de santé afin de renforcer ses connaissances dans le domaine médical
Collaborer avec des collègues passionnés et célébrer ensemble chacune de nos réussites !
Toutes les candidatures reçues sont étudiées indépendamment de l'origine ethnique, des croyances, de la religion, du genre, de l'orientation sexuelle ou de la santé des candidats. Withings aspire à offrir et garantir l'égalité des chances aux candidats et seules les personnes habilitées (RH et Management) auront accès aux informations concernant votre candidature.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Machine Learning'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Hybride'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer Senior,AXA en France,"Hauts-de-Seine, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-senior-at-axa-en-france-3884386043?position=47&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=SDubjYBbL7PRNB7yLnRk%2Bg%3D%3D&trk=public_jobs_jserp-result_search-card,"Environnement
En tant que
Senior Data Engineer F/H
, vous allez contribuer directement aux projets des directions métier (ex : fraude santé, multiéquipements, pricing IARD, optimisation du lead management, fragilité auto, …) d’AXA France et à la construction du socle technique Big Data.
Vous allez intégrer une équipe d'une dizaine de personne composée de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus métier de la Direction Transformation Digital Tech et DATA (DT2).
La Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :
- Une organisation agile en feature teams : tribus, guildes, squads
- Des projets sur des applications innovantes à fort trafic (web, mobile…)
- Des méthodologies craft (TDD, BDD, clean code, code review…) et DevOps
- Une communauté de partage de bonnes pratiques (BBL, dojo, meetup, conf…)
Votre rôle et vos missions
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le data lake
Les exploiter pour atteindre la finalité business (exposition de business view, réintégration des résultats dans le SI, service de scoring, …)
De travailler à la création du socle technique Big Data et industrialiser le cycle de développement de l'équipe
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Votre profil
Vous justifiez de plusieurs expériences significatives (+ de 5 ans) sur du
développement big data, en particulier sur du PySpark.
Compétences techniques :
Connaissances avancées en développement en
PySpark
(Spark avec le langage Python)
Maitrise de l'environnement
Microsoft Azure
Connaissances avancées d'outils de BI comme
PowerBI
Compétences transverses :
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Expérience en mode de delivery Agile (Scrum, Kanban, etc...)
Driver et accompagner des Data Engineer junior sur les aspects technique
Et Idéalement :
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA
Maitrise des Traitements Big Data en mode Streaming
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
Qui sommes nous ?
AXA est un des leaders de l’assurance et de la gestion d’actifs dans le monde.
Nous aidons nos 108 millions de clients à traverser les petites et grandes difficultés de la vie.
Chaque jour, nous agissons ensemble pour inventer la meilleure manière de les protéger et voulons donner à chacun les moyens de vivre une vie meilleure.
Un challenge qui donne le sourire et envie de se lever le matin !
Chez AXA, nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C’est pour cette raison que nous menons une politique RH engagée qui favorise la diversité, qui préserve l’équilibre vie privée-vie professionnelle et accélère le développement des compétences et des carrières.
Ainsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une véritable culture d’expertise, accélérant le développement des compétences de chacun et proposant une rémunération attractive.
Pourquoi nous rejoindre ?
Vous êtes porteur d’idées et d’initiatives innovantes ? Vous proposez des solutions et êtes au service du client ? Faites partie de notre grande famille en rejoignant
Un leader mondial offrant des opportunités de carrières intéressantes
Une entreprise qui donne une place de choix à l’innovation, à l’initiative et aux actions solidaires (notamment via l’association AXA Atout Cœur)
Un environnement inclusif à tous les niveaux (mixité, handicap, initiatives pour favoriser l’insertion des jeunes, orientation sexuelle, etc.)
Un accès à de multiples avantages (congés, temps partiel, télétravail, etc.)
Un cadre stimulant, qui permet de rencontrer des collaborateurs performants et d’enrichir ses compétences
Victime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Teams'], 'Other': ['DevOps', 'Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Junior', 'Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Analytics Data Engineer (Internship),Equativ,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/analytics-data-engineer-internship-at-equativ-3901954759?position=48&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=YVUggz38DDZrNjhBOH0OuQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Your mission
Helping the company be fully data-driven is not just a buzzword, it is our mission at Equativ. We, the Data Analytics team at Equativ, maximize efficiency by enabling easy and permanent access to quality data, valuable insights & rigorous thinking. Our responsibility is to ensure our local and central teams, clients and partners, make informed business decisions. To do so, we leverage huge volumes of data (Equativ handles 150Bn Auctions per Day) in a state-of-the-art tech stack (AirFlow, Snowflake, Tableau…) in order to provide actionable insights to all teams at Equativ!
Reporting to the manager of the Analytics team, your mission will be to maintain and upgrade our data pipeline.
What you'll do
Day-to-day maintenance of our data pipeline:
Ensure data pipeline ingestion accuracy in due time
Follow-up on data quality issues raised by internal customers
Improvement of sourcing processes:
Migrate from Talend data flows to Python scripts for our sourcing jobs
Develop resources to make Data Analysts autonomous in sourcing data in our Snowflake database (through ready-to-use scripts or small interfaces)
Developing new projects on our main platforms (Tableau & Snowflake)
Leverage new resources to make the most out of Snowflake (Streamlit, Snowpark…)
Identify new ways to structure our data sources in Tableau while reducing the loading time for the user
Participate in the restructuring of our data marts (schemas, stages & permissions)
Communication:
Sync with Data Analysts to make sure that their requests are properly prioritized
Synchronize with other technical teams (Core-Data, Infra) to gather requirements of the migration and ensure a smooth transition
Understand business needs to suggest the most efficient technical solution
About You
Pragmatic & hands-on mindset is required: you’ll have latitude to explore different options, but you need to go for the most effective solution
Technical knowledge of Python & SQL is a must
Knowledge of collaboration platforms (Gitlab) & Agile processes is a plus
You can demonstrate your ability to solve problems end to end
You are fluent in English
👋 About us
Equativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus — four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.
Headquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.
The company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times’ FT 1000: Europe’s Fastest-Growing Companies.
Equativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.
Come and lead the charge with us in building a transparent ecosystem based on quality!
Equativ is an equal opportunity employer. Equal access to employment, services, and programs are available to everyone, regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you require reasonable accommodation throughout the application and/or interview process, please contact the recruitment team at ta-team@equativ.com
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': ['Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (H/F),Technology & Strategy,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-technology-strategy-3881556102?position=49&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=V5yopRJgKm1yv0Jrdf2Uag%3D%3D&trk=public_jobs_jserp-result_search-card,"Découvrez Novencia
:
Expert en Data et Intelligence Artificielle, nous aidons nos clients à exploiter et à valoriser leurs données sous toutes ses formes en les accompagnant sur des projets de Data Analyse, Data Gourvernance, Data Architecture, Data Science, et Data Engineering…
Vous avez une solide expérience de minimum 2 ans dans l'ingénierie des données et vous êtes à la recherche de nouveaux défis ? Bouclez votre ceinture, la suite est pour vous !
Type de contrat : CDI
Lieu : Lyon
En qualité de Data Engineer (H/F), votre rôle sera :
Concevoir et proposer les solutions de développement répondant aux besoins fonctionnels et techniques des projets big data.
Tu participes à la conception de solutions permettant le traitement de volumes importants de pipelines données.
Réaliser ces solutions par l’écriture de code, en respectant les méthodes et procédures qualités définies au sein du département Technique.
Mise à disposition sécurisé et lisible de la data.
S’assurer de la conformité fonctionnelle et technique de ces réalisations en effectuant les tests automatisés nécessaire et la mise en place de monitoring (système et qualité).
Assurer la maintenance des applicatifs / plateforme data science
Assurer une veille technologique
Vous disposez des compétences suivantes :
Maitrise des plateformes Cloud (AWS, GCP ou Azure), de Scala et de SQL.
Un.e touche à tout : possédant des compétences en langage Python/Spark, de bonnes capacités de modélisation, une forte appétence pour le Big Data
Fin.e connaisseur.euse : Data Engineer convaincu, très peu de secrets pour les clusters et pour les calculs parallèles
Explorateur.trice : découvre de nouvelles technos grâce à une veille régulière
Débrouillard.e : relève de nouveaux défis
Notre objectif commun est de co-construire votre carrière en fonction de vos aspirations et de vos compétences.
Contactez-moi en message privé ou par mail à s.ziki@technologyandstrategy.com !
Let's make it possible #together
*Nos postes sont ouverts aux personnes bénéficiant d’une Reconnaissance de la Qualité de Travailleur Handicapé (RQTH). T&S Groupe encourage la diversité et l’égalité sur le lieu de travail. Tous les candidats qualifiés H/F/* sont pris en considération pour un emploi sur un même pied d'égalité.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
LinkedIn,Stage - Data Engineer,Exotec,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/stage-data-engineer-at-exotec-3918170659?position=50&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=4yuUcRj%2F0x7kYt4sveJZSg%3D%3D&trk=public_jobs_jserp-result_search-card,"Chez Exotec, nous mettons l'excellence technologique au service de la redéfinition des relations entre humains et robots. A travers le monde, nos solutions révolutionnent la façon dont nos clients délivrent leurs produits aux consommateurs finaux. Nous contribuons au succès des plus grandes marques du commerce et de l'industrie, tout en améliorant les conditions de travail de leurs salariés.
Par l'alliance de l'intelligence artificielle et d'un hardware performant, nos robots sont désormais déployés dans le monde entier et leur succès a fait de nous la première licorne industrielle française.
Rejoindre Exotec, c'est l'opportunité de donner du sens à vos compétences. Grandissez avec plus de 800 ExoPeople dans le monde entier pour faire de vos idées des réalités.
La révolution robotique portée par Exotec ne fait que commencer, vous en êtes ?
Au sein du pôle Data, de la DSI d'Exotec, votre rôle sera de participer au développement de l'environnement et de l'infrastructure Data d'Exotec.
Pour cela :
Vous participez à la mise en œuvre des composants techniques de la plateforme de données d'Exotec
Vous travaillez sur la collecte dans la plateforme de données provenant de sources multiples : Salesforce, ERP, logiciels développés en interne
Vous nettoyez, mettez en qualité et préparez les données afin de les rendre disponibles pour les différents cas d'usage qui en ont besoin
Vous migrez des reportings existants vers la plateforme de données et mettez en œuvre de nouveaux cas d'usage pour répondre aux besoins de l'entreprise
Vous travaillerez au sein de l'équipe data et en étroite collaboration avec la software factory, ainsi qu'avec les utilisateurs des métiers qui ont besoin de rendre intelligibles les données disponibles
Requirements
Vous êtes étudiant(e) d'une école d'Ingénieur généraliste avec une spécialisation programmation ou informatique
Vous recherchez un stage de fin d'études d'une durée de 4 à 6 mois
Vous avez idéalement une première expérience en Data Engineering et le développement de pipeline de données
Vous maitrisez Python, l'ETL et SQL,
Curieux(se) et rigoureux(se), vous souhaitez rejoindre une équipe jeune et dynamique ainsi que vous investir dans des projets complexes et excitants
Vous avez un niveau d'anglais courant
Chez Exotec, nous garantissons l'égalité des chances dans notre processus de recrutement. L'ensemble des candidatures reçues sont étudiées indépendamment de l'âge, du genre, de l'origine, de la religion, de la couleur de peau, de la nationalité, du sexe, du handicap, de l'orientation sexuelle ou de toute autre distinction protégée par la loi. Nous mettons en place un environnement de travail inclusif et respectueux de toutes les différences. En rejoignant le Pacte Parité, Exotec s'engage pour un écosystème French Tech plus paritaire.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Solutions Engineer (Data & AI),LVMH,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=51&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=k9JSETDfWfypotKWZEAbAw%3D%3D&trk=public_jobs_jserp-result_search-card,"LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.
We are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.
If you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,
If you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,
If you like building tech solutions having direct impacts on billion-dollar-valued businesses,
If you have good communication skills and like sharing your knowledge,
Apply now, and join us!
The mission
The Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.
Our team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.
You will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).
Main responsibilities
You will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).
You will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.
You will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.
Applying the quality and security standards. Making them evolve if necessary.
Producing realistic, understandable and documented solutions following the group guidelines.
Sharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.
Taking responsibility as member of the team on the product performances (delivery and long term usage)
Required expertise and knowledge
Ability to build technical solutions answering concrete usage (User Stories) and communicate them to the team.
Dimension and evaluate complexity for technical solution productions.
Extensive knowledge and experience with good learning and sharing abilities.
Evaluate quickly risks and opportunities about technical choices.
Solid oral, written, presentation and interpersonal communication and relationship skills.
Problem-solving skills on Data and AI, coding and software development
Tech lover
Feedback taker and giver
Team player
Key benefits to join our team
Attractive packages
Offices in the 8th arrondissement near the Champs Elysées
Flexibility on the working hours
Remote work possible (~40%)
7 weeks of holidays (congés payés + RTT)
LVMH brands exclusive private sales
Great employee committee and health insurance (CE, mutuelle)
Last generation MacBooks
Part of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!
You’re eligible if
You have a strong experience (3+ years) in cloud data architecting or consultancy.
You graduated from an engineering (or equivalent) with a master’s degree. Computer Science knowledge is mandatory.
Experience on data stacks and/or Google Cloud (built in components) is a huge plus.
French and English both written and oral (Maisons are all over the world)
You’re thrilled to support the #1 luxury group to get even better.
Hiring Process
Call with our HR partner dedicated to the Tech Team
Technical interview with the Solution Engineering Manager
Technical test
Interview with the Head of Engineering
Still here? Apply now!!
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Flexibility']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['40'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer GCP (F/H),Apside,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-gcp-f-h-at-apside-2902806697?position=52&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=CwTgaf7bJVvvYa%2B8by%2FfjQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Envie de rejoindre une entreprise apprenante ? Engagée pour t’accompagner dans ton évolution professionnelle et dans tes projets personnels ?
Rejoins Apside pour travailler sur les projets de demain !
Le poste ?
Pour le compte de notre
client acteur mondial de la beauté et cosmétique,
tu interviendras dans la
transformation d’un projet worlwide,
où tu devras
développer la Data Platform et l'ensemble des services Data qui seront exposés aux différentes équipes du client. Aussi, tu seras amené à développer des use cases data.
Dans ce sens, tes missions seront les suivantes :
Designer l'architecture et développer la solution
Définir et développer les Data Model
Être garant de la qualité du code
Être DevOps (Utilisation/mise en place de chaine CI/CD et support niveau L3 des développements)
Environnement technique :
GCP (BigQuery, Cloud Run, Cloud Build)
SQL
Python
DevOps (Github)
API Development
Terraform
Méthodologie Agile
Toi ?
Tu as déjà travaillé sur
Google Cloud Platform (GCP)
?
Tu es
autonome
,
rigoureux
, et
bon communiquant
?
Tu souhaites participer à un
projet d’envergure associant cloud et Big Data
?
Et la suite ?
Tu rencontres d’abord l’équipe RH pour parler de tes attentes, ton projet, ton futur !
Puis les managers pour parler concret : missions, projets, parcours de carrière, et bien sûr salaire et avantages J
Et tu discutes avec un de nos Tech Leads, pour évaluer tes compétences/ te challenger.
Les infos en plus !
Télétravail ! 😊
Un salaire attractif en fonction de ton expérience + différents avantages
Un groupe en pleine croissance avec un management bienveillant
Et une évolution personnalisée avec la possibilité de se former via une plateforme interne
Tu souhaites donner un nouvel élan à ta carrière ? Rejoins la vie Apsidienne !
Pour en savoir plus à
www.apside.com
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['Salaire'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer / Developpeur Talend,Siderlog Conseil,"Niort, Nouvelle-Aquitaine, France",https://fr.linkedin.com/jobs/view/data-engineer-developpeur-talend-at-siderlog-conseil-3861714639?position=53&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=IHXQMJ2Ij4wKcXuVkn7anQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Siderlog est un cabinet de conseil spécialisé implanté à Niort depuis 2004 qui accompagne les directions métiers et SI sur des projets de:
- Business et Data Analyse
- Management de projets
- Conduite du changement
Pour soutenir notre croissance, nous prévoyons à Niort le recrutement de 20 consultants d'ici 2025.
Nos consultants bénéficient d'un modèle qui favorise l'épanouissement professionnel et le bien être:
🍃Un processus d'intégration spécifique et un suivi régulier
🍃Une écoute active des attentes, notamment en terme de formations, certifications
🍃Des déjeuners et évènements mensuels
🍃Un management et un accompagnement de proximité
🍃Un package salarial attractif
🍃La possibilité de contribuer aux projets d'entreprise ( RSE, communautés métiers, pôle conseil et expertise)
🍃Entreprise labellisée Happy At Work, charte Télétravail...
🍃De nombreux autres avantages que nous vous invitons à venir découvrir
Siderlog recherche pour renforcer son équipe, à Niort un(e) consultant(e) Data Engineer / Developpeur Talend.
Dans ce cadre vous devrez :
✔️Concevoir et développer des traitements/job de données complexes à l'aide de Talend pour l'ingestion, le nettoyage, la transformation et la distribution des données.
✔️Collaborer étroitement avec les équipes métier pour comprendre les besoins en matière de données et concevoir des solutions adaptées.
✔️Mettre en œuvre des bonnes pratiques de développement ETL, y compris la documentation, les tests unitaires et l'intégration continue.
✔️Assurer la surveillance et la maintenance des traitements/job de données en production, en résolvant les incidents et en effectuant des mises à jour si nécessaire.
📋 Qualifications et compétences :
👉Expérience avérée dans le développement de solutions de gestion et d'intégration de données, sur Talend.
👉Maîtrise des langages de requête SQL pour l'extraction et la manipulation des données.
👉Connaissance approfondie des bases de données relationnelles et des entrepôts de données.
👉Compétences en programmation avec Java, Python ou d'autres langages similaires.
👉Capacité à travailler de manière autonome tout en collaborant efficacement avec les membres de l'équipe.
👉Excellentes compétences en communication écrite et verbale.
👉Maitrise de l'outil ETL Talend.
👉Expérience avec d'autres outils d'intégration de données tels que Informatica, BODS, Altéryx.
👉Certification Talend serait un plus.
👉Expérience dans le domaine de l'assurance souhaitée
Cette offre vous intéresse ! Postulez !
🏆🙏🚀🎉 !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Full Stack Data Engineer,bsport,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/full-stack-data-engineer-at-bsport-3848363421?position=54&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=zVntVhMlT7QiaTnDEAUXjA%3D%3D&trk=public_jobs_jserp-result_search-card,"Do you know about bsport?
We are a Barcelona based company that offers a platform combining boutique fitness and advanced technology. Our all-in-one features cover bookings, payroll, marketing and more, helping our partners streamline operations and boost their commercial success.
We have more than 2’000 clients in 40+ countries and continue to expand rapidly.
We provide our partners with:
Our platform - the heart of the system (B2B)
A white label iOS and Android mobile application (B2C)
An integrated Video on Demand tool
Our self-built Smart Marketing Suite
A Webshop to up- and cross-sell different products
Our first successes
Since we launched in 2019, we have already achieved the following:
We’ve built a community of over 6 million users
Finalised a Series A Fundraising of $4+ million in December 2022
Grown our team to more than 150 employees
We’re continuing to grow our team to become the #1 tech partner for boutique studios in Europe and the rest of the world!
We are seeking a talented and experienced Full Stack Data Engineer to join our dynamic tech team based in Paris. As a Full Stack Data Engineer, you will play a crucial role in enhancing our data platform and driving innovation through data engineering solutions.
What your future position looks like:
The primary focus of this position is data engineering, encompassing tasks such as building, optimizing, and maintaining our data platform. You will be responsible for making continuous improvements to our data infrastructure to ensure its reliability, scalability, and performance. As an integral part of the data team, you will collaborate closely with team members to address various data-related challenges and opportunities. This may involve tasks ranging from designing and implementing data pipelines to conducting in-depth data analysis to extract actionable insights.
The role will focus on:
Build and maintain bsport’s data architecture
Ensure the sustainability and scalability of the diverse components by leveraging bsport's cloud provider services and adhering to all DevOps best practices
You will enjoy working within an internal team of 25 people, consisting of Tech, Product, and Data experts, directly surrounded by our Senior Leads and CTO.
Our stack is fully automated with push-to-deploy on both frontend and backend. We use Kubernetes and AWS, and our CI is self-hosted. Our methodology is based on agile principles, with weekly releases to production and staging to iterate, gather feedback, and drive progress.
You will be a good fit to join us if you:
Already built or maintained a data architecture at scale in a top cloud provider (AWS, GCP or Snowflake)
Already deployed data science models in production or built a data ingestion pipeline
Familiar with DevOps best practices
Proficiency in SQL, Python and Spark
Experience with dbt and airbyte
Qualifications
Bachelor’s degree in Data Science, Computer Science, Engineering, or related field; advanced degree preferred.
Relevant experience in data engineering
Strong analytical and problem-solving skills, with the ability to work independently and in a team environment.
We'd love to have you join us for many reasons, such as:
🌍 A multicultural and international team!
🚀 A stimulating SaaS environment within a supportive and a fast-growing company
🔋 Enjoy 25 days of paid leave to recharge
🏡 Embrace days of remote work
🏢 Work from our stunning office in the heart of Bastille
❤️‍🩹
Health insurance half covered
🛵 Public Transportation half covered
🏄🏽 Take part in bsport team building and sport initiatives
🛌🏽 Supported by bsport on sick days
Interview Process
First interview with one of our Talent Acquisition team members (30 min)
Technical Interview with our Lead Data (1h30)
Technical Interview with our CPO (1h)
Final Interview with our CTO (1h)
About our Company Culture:
At bsport, we collaborate with passionate individuals who value diverse ideas and backgrounds. We believe that diversity is our most valuable asset.
Our commitment is to foster a positive and inclusive culture. We achieve this through team-building initiatives, open communication, professional growth opportunities, and by celebrating diversity in all its forms.
We value and respect every individual who is eager to make a difference, empowering them to contribute their unique skills and perspectives. Join our dedicated team to help create a thriving and welcoming workplace.
Join our team of passionate and committed people who are dedicated to creating a thriving and welcoming environment. Let's make it happen together!
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': ['Remote', 'Full', 'Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer H/F,Ippon Technologies,Greater Nantes Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=55&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=mHG%2FOUZA7Y3wljivPZOIeQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Envie de rejoindre la communauté DATA la plus dynamique de France ?
Notre spécialité est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.
Membre de la Practice Data, le/la futur(e) Data Engineer sera intégré(e) à nos équipes de conseil et sera suivi(e) par un(e) mentor qui l’aidera à monter en compétences.
Votre champs d’expertise :
Intervenir sur les data platforms de nos clients pour développer de nouveaux pipelines de données (ingestion, traitement, exposition).
Travailler en collaboration avec les métiers et les data scientists pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship etc…)
Déployer des infrastructures cloud full
infra-as-code
(Terraform, CloudFormation).
Participer aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).
Capitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne.
Vos connaissances :
Un framework de calcul distribué tel que Spark, Storm, Flink.
Un ou plusieurs langages de programmation (Python, Scala, Java...)
Différents systèmes de stockage de données (SQL ou NoSQL) et bien sûr le langage SQL.
La connaissance de Snowflake est bienvenue ;-)
Un framework de streaming de données tel que Kafka ou Amazon Kinesis.
Une expérience sur les technologies Cloud : AWS, GCP, Azure
Le delivery et les projets en production faisant partie de notre ADN, vous êtes capable de livrer du code de qualité dans des environnements agiles.
De plus en plus de nos projets se font en remote avec des clients du monde entier, il devient nécessaire d’être à l’aise en Anglais.
Ippon technologies c’est aussi :
👍 Bénéficier d'un suivi de proximité réalisé par votre manager technique : points réguliers pour votre suivi en mission, votre formation et votre évolution de carrière
✌️ Rejoindre une entreprise où les valeurs du sport sont nos leitmotiv : dépassement de soi, travail en équipe, bienveillance.
🗒️ Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL
😁 Travailler en pair programming ou avec un.e mentor pour gravir les échelons !
💪 Pouvoir participer à une aventure humaine au sein de notre Fondation Ippon pour réduire la fracture numérique dans le monde !
🤝 Participer à nos apéros et divers évènements internes pour consolider la cohésion d’équipe
Et après ?
Et oui alors ? Que se passe-t-il une fois que vous êtes convaincu d’avoir lu l’offre d’emploi qui vous correspond bien ?
Nous vous proposons de prendre contact et de nous rencontrer !
Les Next Steps :
1 call RH
1 échange RH
1 échange Technique
Si le match est bon des deux côtés : Hadjimé ! Vous vous lancerez sur le tatami Ippon !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform', 'CloudFormation'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer,Equativ,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-equativ-3814251519?position=56&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=d9nf6mGNenymXJjW70Bn6g%3D%3D&trk=public_jobs_jserp-result_search-card,"👫 About the team
At Equativ, we’re on a mission to develop advertising technologies that empower our customers to reach their digital business goals. This means that we rely on massively scalable, widely distributed, highly available, and efficient software systems; the platform deals with over 100 billions requests per day and above 40 Gbps of network traffic.
Our innovation team based in Paris, Nantes, Limoges, Kraków and Berlin is composed of 90 straightforward and energetic engineers working in an Agile environment and ready to tackle the most complex technical challenges.
Our data engineering team is composed of 10 skilled engineers and is based in Paris. We are part of the R&D department which is composed of 140+ engineers spread across Paris, Nantes, Limoges, Kraków and Berlin all working in an Agile environment and ready to tackle the most complex technical challenges.
Our mission 👇
Data Engineering team is central to Equativ’s data centric business and is responsible to ingest, transform, model and redistribute all data coming from our adtech platform.
We aim at building scalable and robust Big Data platforms from ingestion to business actionable consumption. Our Big Data ecosystem must handle massive log ingestion (tens of billions per day), short & long term data storage, complex data modelling, real-time and batch ELT as well as providing external access through dedicated APIs.
Data Engineers serve Equativ data directly to our customers and throughout the company whether it is for BI analysis, data science algorithms (clustering and optimization), customer reporting, invoicing and more.
Equativ Data Engineering team is engaged in an ambitious migration of its main data stack (Hadoop on-premise) to GCP with the objective to increase reporting features, lower maintenance time, improve performances and simplify the access to our raw data.
What you'll do ✏️
As a Big Data Engineer, you’ll primarily focus on maintaining and enhancing the operationality of our on-premise and cloud data pipelines which feed our warehouses and APIs
-Design, develop, test, promote and industrialize all data components from data ingestion to datawarehouse delivery (ClickHouse, BigQuery):
Propose and develop innovative solutions to achieve the best levels of scalability and performance for our Big Data engines
Automate and streamline our real-time and batch data pipelines (on-premise and in the cloud) in order to simplify access to our data by other teams and lower amount of work spent by other teams on ETL processes
Perform end-to-end monitoring to ensure high availability of production data processing, data quality and reliability
Apply best in class Devops guidelines and secure deployments
-Brainstorm with other team members working on our data backend (datawarehouse modelling and data exposure through our reporting APIs) on optimizing our architecture and support them in the use of our pipelines
-Contribute to data roadmap definition in coordination with other R&D and product teams in order to build a best in class data infrastructure that will generate insights for Equativ’s analytics
-Take part in improving and deploy data engineering standards, procedures, processes and operational guidelines around target data components at Equativ
💪 About you
Master degree in Computer Science or similar technical field of study
3+ years of software development with open source technologies
Fluent in Java and/or in Scala. SQL mastery
Very good understanding of Devops principles (Gitlab, Docker, Kubernetes, Gradle, ci/cd)
Experience with large-scale data engineering technologies (ClickHouse, Flink, Kafka, Hadoop, Spark, Hbase)
Experience on building data pipelines on Google Cloud Platform (BigQuery, Dataflow, GCS, Cloud Run, Airflow …) would be a big plus
Experience in working with high QPS Rest APIs is a plus
Entrepreneurial spirit and know-how to identify opportunities of improvement
Working proficiency and communication skills in verbal and written English
Passion for playing with large volume of data
🚀 How you'll grow
Within 1 month:
You'll be just finishing your onboarding.
You'll probably have tackled a few small tasks in peer-coding
Within 4 months:
You'll have an overview of 50% of the stack, CI/CD and team’s main processes. You’ll be able to work on more complex developments
You'll now have enough knowledge to participate to deployments of chosen applications
Within 9 months:
You'll be autonomous on most of our stack and will have participated to major projects
You’ll be helping the team on production matters
👋 About us
Equativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus — four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.
Headquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.
The company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times’ FT 1000: Europe’s Fastest-Growing Companies.
Equativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.
Come and lead the charge with us in building a transparent ecosystem based on quality!
Show more
Show less","{'ProgLanguage': ['Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': ['HBase'], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Big Data', 'ML', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer Cloud (F/H),Apside,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-cloud-f-h-at-apside-3904088503?position=57&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=I%2BZTRr6Z9jhcj14PZMK92A%3D%3D&trk=public_jobs_jserp-result_search-card,"💥
Découvrez la Vie Apsidienne
📹
et vous aussi, devenez Apsidien
On aurait pu demander à Chat GPT de vous démontrer en quoi
Apside est l’ESN qu’il vous faut,
mais on préfère que vous le découvriez vous-mêmes 👇😏
🔥
Découvrez votre future mission
👉
Contexte
Rejoignez notre Practise Cloud/Data, afin d’intervenir sur des sujets à haute valeur ajoutée !
Notre
client migre actuellement toutes ses applications vers le cloud AWS.
De plus, dans le cadre du développement d'un produit de restitution automatisée de données, ils recherchent actuellement développeur data ayant déjà travaillé sur un projet similaire. La solution produit est techniquement conçue en lien avec le Tech Lead validant l'architecture logicielle à mettre en place sur le cloud AWS.
Secteur
: culture/média
Méthode de travail
: Agile Safe
😎 Mission
Capter les données (structurées et non structurées) produites dans les différentes applications
Intégrer les éléments
Structurer la donnée (sémantique, etc…)
Cartographier les éléments à disposition
Nettoyer la donnée (élimination des doublons, etc…)
Valider la donnée
Créer les référentiels de données
Environnement technique
:
Python
Lambda
Step Function
AWS / AWS RDS
PostegreSQL
Snowflake
Spark
📍
Localisation
La Défense
💰
Le package salarial que nous vous proposons
Contrat :
CDI
Avantages groupe :
carte ticket restaurant Swile, prime de mobilité, RTT, accord télétravail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle à 100% etc…
Avantages agence :
Communauté Cloud/Data, afterworks, communauté techlead
Formation :
certifications techniques, cours particuliers d’anglais en interne, accès à un catalogue de formations grâce à notre plateforme e-learning (
Academy by Apside
) ou via nos organismes partenaires.
🔮
Ô vous futur Apsidien, qui êtes-vous ?
Au moins 4 ans d'expérience en tant que Data Engineer
Maitrise de l’environnement cloud AWS
Force de proposition, bon relationnel et autonome
😏
Apside a suscité votre curiosité ?
Dans un environnement marqué par une accélération des évolutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients à créer de la valeur et à adresser leurs enjeux stratégiques en leur mettant à disposition des expertises technologiques (
Data / IA, Cloud, Cyber
) et une expérience sectorielle (
Industrie, Banque, Assurance, Service, Secteur Public
). Pour un accompagnement global, le groupe propose des offres transverses autour du
Handicap
(Apsid’EA), du
Digital Learning
, et du
Conseil
.
🤔
Et votre place dans tout ça ?
👉 Notre volonté
est de vous accompagner dans la construction et l’épanouissement de votre carrière
en nous appuyant notamment
sur 3 piliers :
Une
rémunération
à hauteur de vos investissements et de vos compétences
Une
trajectoire professionnelle
stimulante sur mesure
Un
engagement
autour des valeurs Apsidiennes : la qualité de vie et des conditions de travail au cœur de nos enjeux
Engagée pour
un monde plus inclusif et plus responsable
, Apside réinvente l’ESN et propose l’Engagement Sociétal et Numérique. Découvrez notre démarche RSE ainsi que notre vision de l’Entreprise Engagée.
Convaincu ? A vous de jouer, envoyez-nous votre CV !
Rejoignez l’aventure Apsidienne et découvrez notre vision d’une ESN singulière et résiliente
🚀
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data Engineer (F/H) - en alternance,Carrefour,"Massy, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-en-alternance-at-carrefour-3884390443?position=58&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=fpp9k2zsbFGYqI5rGtDPMQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Le saviez-vous ?
Nous rejoindre, c’est rejoindre l’un des leaders mondiaux de la distribution qui met l'accent au quotidien sur la diversité, la RSE et le digital, pour satisfaire nos clients et nos collaborateurs. En tant que partenaire premium des Jeux Olympiques et Paralympiques de Paris 2024, nous partageons les valeurs du sport en permettant à nos équipes de se dépasser et encourageons une alimentation saine au juste prix pour tous.
Vous cherchez à travailler dans une entreprise dynamique où votre travail rime avec impact social et environnemental ? Bienvenue chez nous !
Data Engineer (F/H) - en alternance
En tant qu' alternant, vous intégrerez la plateforme supply chain où vous serez amené à appuyer le pôle prévision et optimisation particulièrement sur des sujets data et d’analyse de données.
Au sein d'une équipe composée de data scientists et de data engineers organisée en mode Scrum Agile, vous travaillerez pour améliorer au quotidien un outil de calcul de prévision (prévision de la demande des entrepôts Carrefour). Vous participez à l'évolution fonctionnelle et technique de l'application.
🎯 Les missions
Dans ce cadre, vous serez amené à
Explorer et analyser les données du datalake carrefour
Participer au cadrage des nouvelles fonctionnalités
Développer les évolutions des traitements, des modèles statistiques et de machine learning de prévision et des reporting
Tester les fonctionnalités développées
Répondre aux demandes utilisateurs
👥 Profil
Vous êtes en école d’ingénieur, en master 2 ou équivalent avec une spécialisation data science, data engineering, statistique, informatique.
Vous avez une expérience en traitement et analyse de données.
Vous avez un esprit d’analyse et la capacité de travailler en équipe et à distance.
Vous êtes autonome et rigoureux, fluide dans votre communication orale et écrite et à l'écoute des besoins de vos interlocuteurs.
Vous êtes reconnu pour vos capacités d'anticipation, votre sens de l'initiative et votre réactivité.
Vous avez une bonne connaissance des langages suivants
SQL
Python
Une connaissance de GCP et de Big query serait un plus.
Une connaissance même théorique de la méthodologie agile serait un plus
Une connaissance de GIT serait un plus.
Encore plus de bonnes raisons de nous rejoindre
Intégrer une équipe conviviale à taille humaine au sein d‘un grand groupe
12 % de remise sur achat
📝 Informations complémentaires
Date de début  09 septembre 2024
Durée  1 an
Lieu  Lyon
Déplacements en magasin et en concurrence dans la région parisienne
Avantages 50 % du titre de transport pris en charge par Carrefour
Envie de rejoindre l’aventure ?
Chez Carrefour, nous avons à cœur de ne passer à côté d’aucun talent et sommes fiers de compter des équipes représentatives de la société dans son ensemble. Nous encourageons ainsi tous types de profils à postuler à cette offre et garantissons un processus de recrutement dénué de toutes formes de discriminations.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Big Query'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Statistiques'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer - Nantes,Capgemini,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=59&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=FOcexJ8yPYHe2I%2F%2FMuM8YA%3D%3D&trk=public_jobs_jserp-result_search-card,"Capgemini
Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.
Vos missions :
Intégré(e) au sein d'une équipe projets intervenant pour des clients dans des secteurs d'activités variées, vous serez notamment en charge des missions suivantes :
Concevoir et mettre en oeuvre des stratégies sécurisées d'acquisition et d'intégration de données,
Configurer des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),
Construire des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée.
Votre profil :
Diplôme d’ingénieur ou équivalent universitaire
Minimum 3 ans d'expérience
Anglais courant
Maîtrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.
Expertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks…
3 raisons de nous rejoindre :
Qualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité
professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel
vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités :
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
Capgemini
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Data Engineer (Snowflake),MindPal,"Marseille, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896997028?position=60&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=mVauwVbbjFSlaeW9klBsag%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for experienced
Data Engineers
with knowledge of
Snowflake
platform.
Responsibilities
Creating and managing data in the Snowflake environment
Designing and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms
Optimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately
Automating data processing workflows using tools such as Airflow or other workflow management tools
Deploying and configuring tools to monitor and report on the performance of the Snowflake system
Requirements
Minimum 1 year of experience as a Data Engineer
Ability to use Snowflake
Very good knowledge of SQL and programming in Python
Ability to work with databases, including the Snowflake platform
Knowledge of ETL tools and data integration
Ability to work in a team and good communication skills
Fluent English in speaking and writing
We Offer
B2B contract type
Full-time job
Remote and flexible working hours
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,DATA Engineer (H/F),Boulanger,"Lesquin, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-boulanger-3854554057?position=1&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=Av0nle7xMrq%2F2medaWCbng%3D%3D&trk=public_jobs_jserp-result_search-card,"Au sein de la direction informatique, le pôle DATA a pour missions de maximiser la mise en valeur des données de BOULANGER ,ELECTRO-DEPOT et KREFEL/HIFI afin d’aider nos décideurs à agir sur les leviers de leur performance par des processus décisionnels efficients.
Au sein de ce pôle, tu prendras en charge un large domaine métier qu'il te faudra maitriser de bout en bout : de la données brutes, sa transformation jusqu'à son exposition dans les reporting. Cela afin d'en assurer le bon fonctionnement, les évolutions constantes et sa pérennité
Tes tâches principales portent sur :
Le pilotage et la mise en œuvre de projets DATA.
La collecte, le stockage et l’exploitation fluides des données par le développement de solutions
Missions
Maitriser les règles fonctionnelles et les KPI de ton domaine afin de challenger les métiers dans les évolutions et les nouveaux projets
Accompagner des équipes métiers dans leurs travaux d’identification et expression des besoins sur la data
Participer aux ateliers de conception et développement des applications data
Modéliser la solution à mettre en œuvre
Concevoir et mettre (ou faire mettre) en œuvre des flux les pipelines d’intégration (en mode batch ou fil de l'eau) de données structurées/semi-structurées
Transformer les données : consolider, enrichir et optimiser les données, qui seront exploitées par le métier
Créer, faire évoluer et optimiser les restitutions
Suivre et animer les développeurs (ETL, restitution, self-BI internes ou externes)
Gérer le RUN
Maitrise le SQL et la base de données (Oracle, Snowflake)
Maîtrise d’outils de restitution (tel que Business Object (BO), PowerBI…)
Capacité relationnelle, rigueur et dynamisme
Maîtrise un ou plusieurs outils de préparation et traitement de la donnée (DataStage, Stambia, ...)
Capacité à s’adapter à tout type d’interlocuteurs (technique, métiers, Direction)
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['Oracle', 'Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer H/F,Neosoft,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-neosoft-3877878521?position=2&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=7ObkquXOFViiSrYhqZHL9w%3D%3D&trk=public_jobs_jserp-result_search-card,"Tous nos postes sont ouverts au télétravail
Groupe indépendant de conseil en transformation digitale de près de 1800 collaborateurs, Néosoft s’est construit, depuis 2005, sur un modèle qui place l’excellence, le dépassement de soi et la RSE au cœur de sa stratégie.
En nous rejoignant, vous intégrez des communautés d’experts et de talents qui vous permettent de développer vos compétences et d’offrir à nos clients le meilleur accompagnement possible.
Notre savoir-faire s’articule autour de nos 6 domaines d’expertise :
Conseil & Agilité
Cybersécurité
Data
DevOps
Infrastructures & Cloud
Software Engineering
Nous recherchons pour intégrer notre
agence lilloise
un(e)
Data Engineer confirmé(e)
.
Nous aimerions vous voir rayonner au sein de notre communauté DATA (+100 collaborateurs) animée par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients à consolider un patrimoine Data responsable.
🎯
Vos missions :
Après une période d’intégration, en tant que
Data Engineer
, voici à quoi ressembleront vos activités dans des contextes clients Retail ou Banque / Assurance / Finance :
Analyser et s'approprier les cas d'usages
Analyser et valoriser les données du patrimoine
Mettre en place des flux de transformation de données
Réaliser les tests permettant de s'assurer la qualité du delivery
Continuer la mise au point de frameworks data
Créer et développer des modules de déploiement des solutions
Assurer l'industrialisation de moteurs basés sur l'IA
Assurer le niveau de performance des pipelines
Implémenter les outils de monitoring du socles de données
📝
Votre profil :
Nous vous imaginons avec au moins 4 ans d’expériences sur des projets autour de la
Data
, une maîtrise des
bases de données (SQL)
, des outils de transformation de la donnée
(Talend, BigQuery, Airflow)
, et un socle de compétences solides autours des langages
Python, Spark, Scala, Hadoop, Java.
👉
Votre carrière chez Néosoft
Depuis sa création, Néosoft place ses collaborateurs au cœur de sa stratégie. Notre culture pourrait se résumer en un mot : le collectif.
Nos communautés d’experts vous donnent la possibilité d’apprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.
Nous veillons à ce que chacun bénéficie d’un accompagnement de proximité et d’un suivi de carrière personnalisé auprès de votre manager dédié :
1 bilan d’activité trimestriel pour suivre le développement de vos compétences
1 entretien d’évaluation qui a lieu chaque année pour évaluer votre performance et déterminer vos nouveaux objectifs
1 entretien annuel auprès de votre RH dans le but de cartographier vos nouvelles compétences pour échanger sur vos projets professionnels et souhaits de formations
👉
Vos avantages
Formations et développement de l’expertise :
Vous disposez de temps alloué et rémunéré en contribuant au développement de votre expertise technique et de celle du groupe (Participations à des Tech days, animation d’une conférence à l’interne ou à l’externe, rédaction d’articles, rencontres avec nos candidats en processus de recrutement…)
Un abonnement illimité LinkedIn Learning offert
Bien-être au travail :
Un accord de télétravail flexible jusqu’à 100% de télétravail et personnalisable
Un partenariat avec Gymlib qui favorise le sport en entreprise
Des initiatives locales (afterworks, défis sportifs, team buildings, …)
Et bien plus encore :
Parce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive rémunérée dès l’arrivée du collaborateur
En plus de votre salaire : participation, compte épargne temps, actionnariat...
👉
Votre parcours candidat
Notre processus de recrutement se compose de deux étapes clés :
Un entretien de recrutement RH avec un Talent Acquisition Spécialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carrière possibles au sein de notre groupe
Un entretien d’évaluation technique pour réaliser un diagnostic de vos compétences techniques et identifier les compétences sur lesquels poursuivre votre évolution
Vous aurez également la possibilité de rencontrer pour compléter votre processus un acteur de notre pôle Business ou un pair de votre métier pour échanger sur son expérience collaborateur.
Nous avons hâte de vous rencontrer !
A bientôt,
L’équipe Néosoft 🖐
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': ['Salaire'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data Engineer - Bordeaux,Capgemini,"Bordeaux, Nouvelle-Aquitaine, France",https://fr.linkedin.com/jobs/view/data-engineer-bordeaux-at-capgemini-3889788624?position=3&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=pSrQIJJFQcDAjttBdi%2F%2FHw%3D%3D&trk=public_jobs_jserp-result_search-card,"Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une
communauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la
valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.
Vos missions :
Vous êtes passionné par le domaine de la Data, vous souhaitez prendre part à des projets d'envergure, concevoir des solutions, les implémenter et les faire évoluer?
Alors rejoignez notre équipe Data Engineering Services au sein de Capgemini Cloud Infrastructure Services en tant que Data Engineer.
Vous avez acquis une expérience solide dans le développement, la mise en œuvre et l’optimisation de solutions pour le traitement d'un grand volume de données, vous êtes capable de créer des solutions qui répondent aux besoins métiers et IT, alors rejoignez notre équipe d’experts.
En qualité de Data engineer, vos missions sont les suivantes :
▪ Concevoir et développer des solutions Data/IA.
▪ Accompagner les Métier dans la compréhension et la mise en œuvre de solution orientées données.
▪ Collaborer avec les Dev, les Ops, les experts infrastructures dans la construction de solutions et d’infrastructures axées sur les données.
▪ Gérer un écosystème de partenaires data et assurer un haut niveau d'expertise
▪ Assurer un rôle de veille technologique sur tous les outils autours de la data, de l’IA et de la BI.
Votre profil :
Vous êtes issu d’une formation ingénieur ou équivalent bac+5 informatique spécialisée en DATA et vous justifiez d’une expérience de 3 à 5 ans dans un rôle similaire. Expert dans une technologie de base de données relationnelle (PostgreSQL, Oracle...)
Expert dans une technologie de base NoSQL (MongoDB, Cassandra...)
Vous maitrisez un framework de manipulation de données (Hadoop, Spark, Kafka...)
Vous maitrisez les concepts DevOps et avez de bonnes notions en scripting et développement
Vous avez une expérience des outils BI et de data visualisation (Kibana, PowerBI...)
La maitrise de l'anglais est nécessaire.
3 raisons de nous rejoindre :
Qualité de vie au travail :
accord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu :
certifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE :
plan actionnariat, tarif préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culture
Nos engagements et priorités :
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
À propos de Capgemini :
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'Cassandra'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['PostgreSQL', 'Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer,eXalt Value,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=4&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=aJYgYrY%2FQ2YWFPS1t2rvFA%3D%3D&trk=public_jobs_jserp-result_search-card,"eXalt
est un cabinet de conseil IT
Pure player Data
& IA basé à Paris.
Notre offre s’articule autour de 4 piliers réunis au sein d’une même communauté pour un accompagnement à 360° alliant une expertise technique et méthodologique à une approche conseil métier:
Data Gouvernance & Project
Data Engineering & Big Data
Data Performance & Analytics
Data Science & IA
Filiale du groupe eXalt, créé en 2018,
regroupant plus de
950 collaborateurs en France
(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)
et à l’international
(Colombie, Etats-Unis, Espagne, Belgique),
eXalt Value
apporte une
expertise approfondie
dans le domaine de la Data & IA et conseille les entreprises dans le déploiement de leurs stratégies data-driven.
Bénéficiant du support du groupe eXalt
(1er dans la catégorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value
est en pleine croissance et regroupe aujourd’hui une communauté d’expertise de plus de 60 collaborateurs en région parisienne.
Nos consultants interviennent sur d
es projets d’envergure
dans divers secteurs d’activité,
Banque & Assurance, Médias, Transports, Retail, Tourisme, etc.
Nous recherchons un
Data Engineer Confirmé H/F (minimum 4 ans d'expérience dans la fonction)
pour rejoindre notre communauté sur le
pilier Data Engineering & Big Data.
Vos missions:
Concevoir et développer des pipelines et des flux de données.
Intégrer et transformer des données provenant de différentes sources.
Développer et mettre en œuvre des algorithmes de traitement de données avancés.
Collaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.
Assurer la qualité et la fiabilité des solutions développées.
Conseiller les équipes clients sur les solutions à mettre en place.
Les Prérequis :
Titulaire d'un Bac+5, Ecole d'Ingénieur
Maîtrise d'un ou plusieurs langages de programmation (
Python, Scala, Spark, etc
.).
Expérience approfondie des technologies
Big Data (Hadoop, Spark, Kafka, Talend, etc.)
Expérience avérée
en
environnement Cloud (AWS, GCP, ou Azure)
.
Solides compétences en conception et en optimisation de pipelines de données.
Expérience de travail en
méthode Agile
Capacité à travailler de manière autonome et en équipe.
Excellentes compétences en communication et en résolution de problèmes.
Maîtrise de l’anglais (oral & écrit dans un contexte international professionnel).
Votre environnement eXalté:
Un environnement de travail Collaboratif
favorisant les initiatives et projets transverses à la Practice Data & IA (Lab IA, Data Hub, etc.).
Un collectif de consultants passionnés,
s’intéressant aux tendances innovantes du secteur.
Une Practice de proximité,
privilégiant la montée en compétence de ses collaborateurs (formations, coachings, mentorats, etc.)
Un suivi individualisé et de proximité
par un.e Data Sales Manager référent du compte client, un.e Chargé.e RH et un.e Practice Manager
Une équipe ouverte et dynamique,
qui privilégie les moments de partage et de convivialité (séminaires, eXaltemps, meet-up, déjeuners d’équipe, etc.)
Notre processus de recrutement :
Un entretien RH avec Estelle,
à la suite duquel vous saurez tout (ou presque) d’eXalt Value,
Un entretien technique avec un Manager assorti d’un test technique,
lors duquel vous aurez l’occasion de démontrer vos talents mais aussi d’apprendre avant même de dire oui,
Un entretien final avec la Directrice Associée ou le Directeur Opérationnel,
pour finir de vous convaincre de nous rejoindre 😊
Nous avons hâte de recevoir vos CV, et de faire votre connaissance!
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Résolution de problèmes'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data engineer - F / H,United Robotics Group,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=5&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=va3%2FvkwzPqFEg1ZOXUYHcw%3D%3D&trk=public_jobs_jserp-result_search-card,"Bienvenue chez
Aldebaran
, leader européen de la robotique
au sein du groupe
United Robotics Group
.
Nous concevons et industrialisons des robots innovants avec une vision sociétale ambitieuse pour façonner un monde plus humain. Depuis 2005, nous sommes à l'avant-garde de l'interaction homme-robot avec des produits emblématiques tels que NAO et Pepper.
Notre dernier-né,
Plato
,
incarne notre engagement envers la technologie de pointe et la sécurité,
fabriqué en France avec des composants européens.
Rejoignez nos équipes multiculturelles et dynamiques pour être au cœur de la révolution de la robotique.
Si vous êtes passionné.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer à façonner l'avenir, nous vous offrons une expérience enrichissante et stimulante.
En tant que membre de notre équipe, vous bénéficierez d'une culture d'entreprise axée sur le sens de ce que nous faisons et valorisant la responsabilité sociale et environnementale.
Chez Aldebaran, nous valorisons l'innovation, la diversité et l'égalité et encourageons chacun.e à être ouvert.e, authentique, courageux.se, responsable et engagé.e.
Finalité du poste
Au sein de l'équipe Cloud-Online Services, le Data engineer intégrera l'équipe Data, responsable du développement des produits destinés à la collecte, aux process et à l'exploitation des données de nos robots.
Il aura pour rôle de définir et d'implémenter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui gèrent les robots du groupe.
Missions principales
Le Data engineer aura pour responsabilités de :
évaluer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,
concevoir et développer des services Data en respectant la spécification fonctionnelle et la méthodologie agile,
agréger et stocker de grandes quantités de données,
mettre en place des solutions de data processing,
intégrer/développer des outils de visualisation de données et analyser les KPI,
développer, tester, sélectionner et mettre en production des algorithmes qui permettent de répondre aux besoins,
réaliser des analyses de données,
mettre en place des tests de charge et fonctionnels pour les solutions Data,
investiguer et corriger les bugs remontés par les utilisateurs,
contribuer à la mise en place de l'infrastructure et outil de déploiement (CI/CD)
Rejoignez-nous pour faire partie d'une aventure passionnante où Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !
Requirements
Pour la bonne exécution des missions confiées, vous témoignez d'au moins 6 ans d'expérience en tant que développeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.
Compétences demandées :
Bonne compréhension des technologies d'infrastructure et de déploiement,
Compétences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,
Bonne compréhension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,
Bonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)
Bonne connaissance et une expérience pratique de Scrum\Scrumban et des méthodes agiles,
Une certification AWS sera appréciée,
Un niveau de français et d'anglais courant est indispensable,
Des expériences dans des environnements fortement internationaux sont un plus
Benefits
Nos principaux avantages :
Une culture du bien-être en entreprise qui a fait ses preuves (budget célébration et moments de convivialité par équipes et directions, restauration collective de qualité, environnement de travail agréable)
Un engagement fort en matière de responsabilité sociale et environnementale (promotion de l'égalité professionnelle, performance de notre plan diversité et inclusion, référent handicap, fresque du numérique)
Une culture du télétravail encadrée de manière appropriée !
Tous nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
LinkedIn,Data Engineer,Coders Connect,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-coders-connect-3870419202?position=6&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=JR82LAk%2FERF273UoMx67Kg%3D%3D&trk=public_jobs_jserp-result_search-card,"Coders Connect and Sanofi are joining forces to bring an electrifying twist to the biopharmaceutical world!
Work with a rhythm that suits your style (2 days remote and 3 days onsite magic).
Language
: Proficiency in English is required for this role to ensure effective communication within our diverse, global team.
About Sanofi:
We're not just a company; we're a global movement, focused on human health and making a real difference. Our mission? To battle pain, ease suffering, and sprinkle a little bit of magic in the process by developing life-changing medicines and vaccines through breakthrough science and wizard-level technology.
Digital & Data: The Pulse of Our Mission
At the heart of our quest lies our digital and data powerhouse. Think of us as the digital healthcare platform of your dreams, where innovation meets speed, and technology shakes hands with medicine. With our scale, deep-rooted connections in health ecosystems worldwide, and a knack for pushing boundaries, we're here to revolutionise medicine, one digital solution at a time.
The Role: Data Engineering Virtuoso
As our Data Engineering Virtuoso, you're tasked with designing and orchestrating the data pipelines that power our ambitious data analytics initiatives. You'll ensure our data's integrity and accessibility, laying the groundwork for groundbreaking insights and innovations.
Requirements
Cloud Platforms: Proficient in AWS services, with Azure & GCP knowledge a plus. Your work involves leveraging cloud solutions for scalable data processing and storage.
Data Modeling & Query Performance: Expertise in crafting data models and optimizing SQL queries to enhance performance. Experience with Snowflake or similar data warehousing solutions is highly valued.
Integration Services: Skilled in utilizing Integration Services like IICS and Tibco, you facilitate seamless data flow and integration across various platforms.
Scripting & Development: Proficient in scripting languages such as Python and R, enabling you to automate tasks and manipulate data efficiently. Familiarity with GitHub for source code management underscores your commitment to collaborative development and version control.
Visualization & Reporting: Knowledgeable in creating insightful data visualizations using tools like PowerBI, Tableau, or similar, turning complex data into actionable insights.
Data Governance & Compliance: A keen understanding of data quality, security, and governance standards, especially in healthcare environments subject to regulations like GxP, SOX, and data privacy laws.
Real World Data & Standard Models: Experience with Real World Data (e.g., EHR, Claims) and familiarity with standard data models (e.g., OMOP, FHIR) enhance your ability to contribute to our healthcare objectives.
Pipeline Frameworks: Proficiency in using frameworks like Apache Airflow or Kedro for crafting efficient, reliable data pipelines that support our AI and ML initiatives.
The Reward:
A chance to play a crucial role in a collaboration that's redefining healthcare through digital transformation.
A seat at the round table of diversity and inclusion, where different backgrounds and experiences conjure the richness of our culture.
An endless horizon of professional growth, learning opportunities, and the chance to contribute to a future where better health is a global reality.
The Call to Adventure:
If you're ready to join a quest for better – better treatments, better outcomes, and better science – and believe in the magic of bringing diverse talents together to make miracles happen, we want you. Let's embark on this journey together and transform the future of healthcare.
Better is out there. Are you ready to find it with us?
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration', 'Initiative']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (H/F),Web Transition,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=7&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=84yz5eoQP3z73ZQRUtCzzQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Web Transition, c’est qui ?
Fondée en 2011,
Web transition
est une entreprise de services numériques opérant sur le marché de l’IT/Digital !
Constituant une part essentielle de
MoOngy Digital Lab
, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et également en Data !
Notre objectif : nous implanter comme un acteur principal sur le marché de la Transformation Digitale en accompagnant et valorisant les compétences de nos collaborateurs !
Nous sommes convaincus que le succès de MoOngy Digital Lab réside dans la somme des potentiels de nos équipes 🤝
Ton équipe : La tribu Data
Parce qu’il est indispensable que tu puisses partager tes connaissances mais aussi en acquérir de nouvelles, tu feras partie de l’une de nos tribus : celle de la Data. De plus, cela te permettra d’être acteur dans le développement et la stratégie de Web Transition. Ce système de co-réflexion et co-construction est un fondement essentiel chez nous !
Dans cette aventure, tu :
T’assures
de la maîtrise de la donnée et est garant de la qualité de son utilisation (référencement, normalisation, et qualification)
Travailles
à la compréhension et l'intégration des données en provenance des différents formats
des interfaces de flux
également à la définition de la politique de la donnée et à la structuration de son cycle de vie dans le respect des réglementations en vigueur
la supervision et l'intégration des données de diverse nature qui proviennent de ces sources multiples et vérifie la qualité des données qui entrent dans le Data Lake
Garantis
l'accès qualitatif aux sources de données
Facilites
l’accès aux données pour tes collègues (data scientists, data analysts…)
Assistes
les autres équipes dans l'accès et la compréhension des données des socles.
Rejoins-nous si tu as :
Expérience d’au-moins 4 ans dans la Data
Appétence à la qualité des données.
Connaissance familière des Datawarehouses.
Maitrise de Python, Oracle SQL, GCP/Power BI
Aisance avec les indicateurs, tu as une bonne capacité d'analyse et de rédaction.
Ton savoir-être :
Ouvert d’esprit
Rigoureux
Autonome
Respectueux des différences de chacun
Curieux
Proactif
Agile
Par où on commence ?
Un premier entretien RH d’1h pour comprendre ton parcours et tes aspirations
Un second entretien de 45 minutes avec l’un de nos Business Manager afin de valider tes compétences et qu’il se projette sur l’une des missions qu’il pourrait te proposer
Un troisième entretien de quelques minutes avec notre responsable d’agence pour te proposer d’intégrer notre superbe Team Web !
3 entretiens en peu de temps, si ton profil correspond tu intègreras très vite nos équipes 😉
Prêt pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant à cette offre ! Voici les avantages qui t’attendent en tant que Weber :
🤩 Des collègues incroyables
🏆 Certifiée Great Place to Work
🎮 Des bureaux sympas (où vous serez toujours les bienvenus)
🎉 Des teambuilding et évents tous les mois
💻 Des tributs métiers pour échanger entre Weber du même métier
Des missions chez le client qui sont accompagnées et coachées par ton manager
Un accompagnement dans ton plan de carrière et tes envies de re skilling
🤓 Un catalogue de formations certifiantes ouvert à tous les salariés
🍽️ Une carte tickets restaurant MyEdenred
❤️ Une mutuelle GrasSavoye
🚎 Une prise en charge des frais de transport à 100%
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data Engineer,Airswift,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-airswift-3909165766?position=8&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=6kRhAuUBrxZsA0vvv008UA%3D%3D&trk=public_jobs_jserp-result_search-card,"Data Engineer
Location
: Paris (Hybrid)
Contract type
: 12 months +
Years of Experience
: 4+
Recruitment Partner:
Airswift
Key Words:
Project Management | Jira | Digiboard | Banking | Stakeholder Management | Architecture | Cloud | Payments/Credit |ServiceNow | PPM |
Responsibilities
Design, develop, and implement data pipelines to collect, process, and store structured and unstructured data from various sources.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and translate them into technical solutions.
Optimize and tune data pipelines for performance, scalability, and reliability.
Ensure data quality and integrity throughout the data lifecycle, implementing data validation and monitoring processes.
Evaluate and implement new tools and technologies to enhance our data infrastructure and capabilities.
Requirements
:
Extensive experience in Python.
Strong experience with data processing frameworks and tools such as Apache Spark.
Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform.
Solid understanding of data modelling, database design, and SQL
French and English speaking
Freelancing opportunity
The next step
We have an exceptional team in place, and we are pleased to be able to appoint a further person to our growing business. We are aware that you may not ‘tick all the boxes’, but if you believe you can genuinely offer some valuable skills and experience to our business, please in the first instance contact our recruitment partner Airswift.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA'], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (H/F),Beelix,"Antibes, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-beelix-3838611420?position=9&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=tWinPOqjhPLKXwnj%2BfvA6w%3D%3D&trk=public_jobs_jserp-result_search-card,"Qui sommes-nous ?
Depuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants :
🚗Automobile
⚡Energie
📡Médias & Télécoms
👗Luxe & Retail
💶 Banque, Finance & Assurance
✈️Défense
Aujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.
Dans le cadre de notre développement, nous recherchons un Data Engineer (H/F) pour l'un de nos clients.
Quelles missions au quotidien ?
Être le leader de la brique Datalakehouse
Développer les scripts de transformations de données et les pipelines d’alimentation
Proposer des évolutions architecturales ou de fonctionnalités pour améliorer le socle technique
Être le back-up du leader technique sur la partie reporting (Power BI)
Orientation satisfaction client et résultat final forte mais également sensibilité au « comment »
Innovation et proposition de nouvelles pratiques pour améliorer l’environnement et les conditions de travail des équipes
A propos de vous ?
5 + années d'expérience en tant que Data Engineer
Maîtrise des technologies suivantes : Microsoft Azure, Microsoft Azure Synapse Analytics (Spark / Python / Pipeline / Serverless), fichiers parquet / delta, Microsoft Power BI, Microsoft SQL Server, langage SQL, Datawarehousing / Modélisation de données
Analyses et export de données
Connaissance de l’ensemble du processus depuis la collecte jusqu’à la mise à disposition des données en ayant comme point fort la maitrise de sa transformation et mise en forme
Vous avez un bon niveau d’anglais
Localisation : Biot et/ou Carros
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer (F/H),Aubay,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=10&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=ikgRu7W36JbbvXgo9R7FUA%3D%3D&trk=public_jobs_jserp-result_search-card,"Passionné par la Data, tu souhaites rejoindre une communauté d’experts dans le domaine afin de développer tes compétences en Data Engineering. Aubay renforce ses équipes Data et recherche des Data Engineers pour intégrer des dispositifs de projets pointus et variés.
Ton quotidien en tant que Data Engineer chez Aubay, :
Définition de la stratégie de stockage et mise en œuvre des technologie appropriées (base de données SQL, NoSQL, stockage distribué,…)
Ingestion des données (structurées, semi-structurées ou non-structurées) selon différentes fréquences : batch, micro-batch ou temps réel
Conception et mise en œuvre de pipelines de données afin de fournir des données prêtes à l’emploi aux consommateurs : uniformisation, mise en qualité, enrichissement, calcul d’indicateurs,…
Conception et développement d’API pour exposer les données auprès d’applications tierces
Appui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning
Préparation et animation d’ateliers de travail avec des interlocuteurs variés : recueil/approfondissement des besoins métiers, avancement/restitution des travaux, transfert de compétences,…
Ton profil :
Tu dispose d’une formation niveau BAC+5 (Master 2 ou école d’ingénieur) spécialisée en informatique
Tu as déjà une première expérience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data
Les technologies telles que Hadoop, Spark ou Kafka sont tes technologies de prédilection
La programmation n’a plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python
Tu maîtrises les tenants et aboutissants de la philosophie DevOps et des outils orientés CI/CD
Tu es soucieux de la qualité et de la performance de tes développements et tu t'intéresse à l’innovation frugale
Tu es un expert technique dans ton domaine sans pour autant oublier l’importance d’une communication orale et écrite de qualité et adaptée à chacun de tes interlocuteurs
Tu travaille au quotidien en mode agile et tu en maitrise les fondements
Ce qui nous caractérise :
Des missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs variés (Banque, Assurance, Telecom, Industrie,…) qui permettent à nos collaborateurs de monter en compétences et de devenir des experts Data reconnus
De l’apprentissage en continu avec des formations et des certifications sur les technologies Data d’aujourd’hui et de demain
Des experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets
Des communautés de savoir-faire Data proposant de manière régulière aux collaborateurs d’Aubay du contenu et des évènements de partage (webinar, meetup/afterwork, BBL,…) sur les thématiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,…
Aubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.
Ta carrière chez Aubay :
Tu auras la possibilité de développer et certifier tes compétences sur les dernières technologies Data avec un focus fort sur les plateformes Data Cloud telles qu’Azure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks
Tu pourras rejoindre la BU d’excellence Data et évoluer au sein d’un environnement humain et professionnel de haut niveau. Tu profiteras d’un management sur-mesure pour t'accompagner dans ta trajectoire de carrière
Au sein de la BU d’excellence, de multiples perspectives s’offriront à toi :
Rôle de « Lead » : Vous pourrez gagner en responsabilité sur le plan technologique et devenir un référent auprès de nos clients et des collaborateurs de la communauté Data Engineering
Rôle de « Champion » : Vous représenterez Aubay auprès d’un ou plusieurs de nos partenaires éditeurs stratégiques et vous participerez activement à l’animation de la relation sur le plan technologique
Rôle de « Head » : Vous pourrez prendre la responsabilité du savoir-faire Data Engineering et de ses offres et en assurer le développement au sens large (développement business, recrutement, management de collaborateurs, définition de la stratégie et animation de la communauté au sein du groupe Aubay,…)
Besoin d’en savoir plus sur le processus de recrutement ?
Un échange macro au niveau RH avec Doriane
Un entretien technique avec Marius ou Peter, deux de nos référents techniques
Un échange managérial avec le Directeur de la BU Modern BI & Data
A savoir que l’ordre des étapes peut varier selon tes envies (ex : échange managérial avec l’échange technique)
Aubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure', 'Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
LinkedIn,Data Solutions Engineer (Data & AI),LVMH,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=1&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=3ow1m%2BXt5JtMIV2JBi%2FJsw%3D%3D&trk=public_jobs_jserp-result_search-card,"LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.
We are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.
If you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,
If you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,
If you like building tech solutions having direct impacts on billion-dollar-valued businesses,
If you have good communication skills and like sharing your knowledge,
Apply now, and join us!
The mission
The Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.
Our team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.
You will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).
Main responsibilities
You will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).
You will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.
You will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.
Applying the quality and security standards. Making them evolve if necessary.
Producing realistic, understandable and documented solutions following the group guidelines.
Sharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.
Taking responsibility as member of the team on the product performances (delivery and long term usage)
Required expertise and knowledge
Ability to build technical solutions answering concrete usage (User Stories) and communicate them to the team.
Dimension and evaluate complexity for technical solution productions.
Extensive knowledge and experience with good learning and sharing abilities.
Evaluate quickly risks and opportunities about technical choices.
Solid oral, written, presentation and interpersonal communication and relationship skills.
Problem-solving skills on Data and AI, coding and software development
Tech lover
Feedback taker and giver
Team player
Key benefits to join our team
Attractive packages
Offices in the 8th arrondissement near the Champs Elysées
Flexibility on the working hours
Remote work possible (~40%)
7 weeks of holidays (congés payés + RTT)
LVMH brands exclusive private sales
Great employee committee and health insurance (CE, mutuelle)
Last generation MacBooks
Part of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!
You’re eligible if
You have a strong experience (3+ years) in cloud data architecting or consultancy.
You graduated from an engineering (or equivalent) with a master’s degree. Computer Science knowledge is mandatory.
Experience on data stacks and/or Google Cloud (built in components) is a huge plus.
French and English both written and oral (Maisons are all over the world)
You’re thrilled to support the #1 luxury group to get even better.
Hiring Process
Call with our HR partner dedicated to the Tech Team
Technical interview with the Solution Engineering Manager
Technical test
Interview with the Head of Engineering
Still here? Apply now!!
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Flexibility']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['40'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer GCP (F/H),Apside,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-gcp-f-h-at-apside-2902806697?position=2&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=ctZFklaEBa2A%2BElqpEb7zA%3D%3D&trk=public_jobs_jserp-result_search-card,"Envie de rejoindre une entreprise apprenante ? Engagée pour t’accompagner dans ton évolution professionnelle et dans tes projets personnels ?
Rejoins Apside pour travailler sur les projets de demain !
Le poste ?
Pour le compte de notre
client acteur mondial de la beauté et cosmétique,
tu interviendras dans la
transformation d’un projet worlwide,
où tu devras
développer la Data Platform et l'ensemble des services Data qui seront exposés aux différentes équipes du client. Aussi, tu seras amené à développer des use cases data.
Dans ce sens, tes missions seront les suivantes :
Designer l'architecture et développer la solution
Définir et développer les Data Model
Être garant de la qualité du code
Être DevOps (Utilisation/mise en place de chaine CI/CD et support niveau L3 des développements)
Environnement technique :
GCP (BigQuery, Cloud Run, Cloud Build)
SQL
Python
DevOps (Github)
API Development
Terraform
Méthodologie Agile
Toi ?
Tu as déjà travaillé sur
Google Cloud Platform (GCP)
?
Tu es
autonome
,
rigoureux
, et
bon communiquant
?
Tu souhaites participer à un
projet d’envergure associant cloud et Big Data
?
Et la suite ?
Tu rencontres d’abord l’équipe RH pour parler de tes attentes, ton projet, ton futur !
Puis les managers pour parler concret : missions, projets, parcours de carrière, et bien sûr salaire et avantages J
Et tu discutes avec un de nos Tech Leads, pour évaluer tes compétences/ te challenger.
Les infos en plus !
Télétravail ! 😊
Un salaire attractif en fonction de ton expérience + différents avantages
Un groupe en pleine croissance avec un management bienveillant
Et une évolution personnalisée avec la possibilité de se former via une plateforme interne
Tu souhaites donner un nouvel élan à ta carrière ? Rejoins la vie Apsidienne !
Pour en savoir plus à
www.apside.com
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['Salaire'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer / Developpeur Talend,Siderlog Conseil,"Niort, Nouvelle-Aquitaine, France",https://fr.linkedin.com/jobs/view/data-engineer-developpeur-talend-at-siderlog-conseil-3861714639?position=3&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=cWPbrcZyyB%2BcGzsdXBIyuw%3D%3D&trk=public_jobs_jserp-result_search-card,"Siderlog est un cabinet de conseil spécialisé implanté à Niort depuis 2004 qui accompagne les directions métiers et SI sur des projets de:
- Business et Data Analyse
- Management de projets
- Conduite du changement
Pour soutenir notre croissance, nous prévoyons à Niort le recrutement de 20 consultants d'ici 2025.
Nos consultants bénéficient d'un modèle qui favorise l'épanouissement professionnel et le bien être:
🍃Un processus d'intégration spécifique et un suivi régulier
🍃Une écoute active des attentes, notamment en terme de formations, certifications
🍃Des déjeuners et évènements mensuels
🍃Un management et un accompagnement de proximité
🍃Un package salarial attractif
🍃La possibilité de contribuer aux projets d'entreprise ( RSE, communautés métiers, pôle conseil et expertise)
🍃Entreprise labellisée Happy At Work, charte Télétravail...
🍃De nombreux autres avantages que nous vous invitons à venir découvrir
Siderlog recherche pour renforcer son équipe, à Niort un(e) consultant(e) Data Engineer / Developpeur Talend.
Dans ce cadre vous devrez :
✔️Concevoir et développer des traitements/job de données complexes à l'aide de Talend pour l'ingestion, le nettoyage, la transformation et la distribution des données.
✔️Collaborer étroitement avec les équipes métier pour comprendre les besoins en matière de données et concevoir des solutions adaptées.
✔️Mettre en œuvre des bonnes pratiques de développement ETL, y compris la documentation, les tests unitaires et l'intégration continue.
✔️Assurer la surveillance et la maintenance des traitements/job de données en production, en résolvant les incidents et en effectuant des mises à jour si nécessaire.
📋 Qualifications et compétences :
👉Expérience avérée dans le développement de solutions de gestion et d'intégration de données, sur Talend.
👉Maîtrise des langages de requête SQL pour l'extraction et la manipulation des données.
👉Connaissance approfondie des bases de données relationnelles et des entrepôts de données.
👉Compétences en programmation avec Java, Python ou d'autres langages similaires.
👉Capacité à travailler de manière autonome tout en collaborant efficacement avec les membres de l'équipe.
👉Excellentes compétences en communication écrite et verbale.
👉Maitrise de l'outil ETL Talend.
👉Expérience avec d'autres outils d'intégration de données tels que Informatica, BODS, Altéryx.
👉Certification Talend serait un plus.
👉Expérience dans le domaine de l'assurance souhaitée
Cette offre vous intéresse ! Postulez !
🏆🙏🚀🎉 !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Full Stack Data Engineer,bsport,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/full-stack-data-engineer-at-bsport-3848363421?position=4&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=TlXXliAVNig68JS53ybBYg%3D%3D&trk=public_jobs_jserp-result_search-card,"Do you know about bsport?
We are a Barcelona based company that offers a platform combining boutique fitness and advanced technology. Our all-in-one features cover bookings, payroll, marketing and more, helping our partners streamline operations and boost their commercial success.
We have more than 2’000 clients in 40+ countries and continue to expand rapidly.
We provide our partners with:
Our platform - the heart of the system (B2B)
A white label iOS and Android mobile application (B2C)
An integrated Video on Demand tool
Our self-built Smart Marketing Suite
A Webshop to up- and cross-sell different products
Our first successes
Since we launched in 2019, we have already achieved the following:
We’ve built a community of over 6 million users
Finalised a Series A Fundraising of $4+ million in December 2022
Grown our team to more than 150 employees
We’re continuing to grow our team to become the #1 tech partner for boutique studios in Europe and the rest of the world!
We are seeking a talented and experienced Full Stack Data Engineer to join our dynamic tech team based in Paris. As a Full Stack Data Engineer, you will play a crucial role in enhancing our data platform and driving innovation through data engineering solutions.
What your future position looks like:
The primary focus of this position is data engineering, encompassing tasks such as building, optimizing, and maintaining our data platform. You will be responsible for making continuous improvements to our data infrastructure to ensure its reliability, scalability, and performance. As an integral part of the data team, you will collaborate closely with team members to address various data-related challenges and opportunities. This may involve tasks ranging from designing and implementing data pipelines to conducting in-depth data analysis to extract actionable insights.
The role will focus on:
Build and maintain bsport’s data architecture
Ensure the sustainability and scalability of the diverse components by leveraging bsport's cloud provider services and adhering to all DevOps best practices
You will enjoy working within an internal team of 25 people, consisting of Tech, Product, and Data experts, directly surrounded by our Senior Leads and CTO.
Our stack is fully automated with push-to-deploy on both frontend and backend. We use Kubernetes and AWS, and our CI is self-hosted. Our methodology is based on agile principles, with weekly releases to production and staging to iterate, gather feedback, and drive progress.
You will be a good fit to join us if you:
Already built or maintained a data architecture at scale in a top cloud provider (AWS, GCP or Snowflake)
Already deployed data science models in production or built a data ingestion pipeline
Familiar with DevOps best practices
Proficiency in SQL, Python and Spark
Experience with dbt and airbyte
Qualifications
Bachelor’s degree in Data Science, Computer Science, Engineering, or related field; advanced degree preferred.
Relevant experience in data engineering
Strong analytical and problem-solving skills, with the ability to work independently and in a team environment.
We'd love to have you join us for many reasons, such as:
🌍 A multicultural and international team!
🚀 A stimulating SaaS environment within a supportive and a fast-growing company
🔋 Enjoy 25 days of paid leave to recharge
🏡 Embrace days of remote work
🏢 Work from our stunning office in the heart of Bastille
❤️‍🩹
Health insurance half covered
🛵 Public Transportation half covered
🏄🏽 Take part in bsport team building and sport initiatives
🛌🏽 Supported by bsport on sick days
Interview Process
First interview with one of our Talent Acquisition team members (30 min)
Technical Interview with our Lead Data (1h30)
Technical Interview with our CPO (1h)
Final Interview with our CTO (1h)
About our Company Culture:
At bsport, we collaborate with passionate individuals who value diverse ideas and backgrounds. We believe that diversity is our most valuable asset.
Our commitment is to foster a positive and inclusive culture. We achieve this through team-building initiatives, open communication, professional growth opportunities, and by celebrating diversity in all its forms.
We value and respect every individual who is eager to make a difference, empowering them to contribute their unique skills and perspectives. Join our dedicated team to help create a thriving and welcoming workplace.
Join our team of passionate and committed people who are dedicated to creating a thriving and welcoming environment. Let's make it happen together!
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'ML', 'Cloud'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': ['Remote', 'Full', 'Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer H/F,Ippon Technologies,Greater Nantes Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=5&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=CgC9wZD4Kt6bLAagR1%2FJQA%3D%3D&trk=public_jobs_jserp-result_search-card,"Envie de rejoindre la communauté DATA la plus dynamique de France ?
Notre spécialité est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.
Membre de la Practice Data, le/la futur(e) Data Engineer sera intégré(e) à nos équipes de conseil et sera suivi(e) par un(e) mentor qui l’aidera à monter en compétences.
Votre champs d’expertise :
Intervenir sur les data platforms de nos clients pour développer de nouveaux pipelines de données (ingestion, traitement, exposition).
Travailler en collaboration avec les métiers et les data scientists pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship etc…)
Déployer des infrastructures cloud full
infra-as-code
(Terraform, CloudFormation).
Participer aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).
Capitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne.
Vos connaissances :
Un framework de calcul distribué tel que Spark, Storm, Flink.
Un ou plusieurs langages de programmation (Python, Scala, Java...)
Différents systèmes de stockage de données (SQL ou NoSQL) et bien sûr le langage SQL.
La connaissance de Snowflake est bienvenue ;-)
Un framework de streaming de données tel que Kafka ou Amazon Kinesis.
Une expérience sur les technologies Cloud : AWS, GCP, Azure
Le delivery et les projets en production faisant partie de notre ADN, vous êtes capable de livrer du code de qualité dans des environnements agiles.
De plus en plus de nos projets se font en remote avec des clients du monde entier, il devient nécessaire d’être à l’aise en Anglais.
Ippon technologies c’est aussi :
👍 Bénéficier d'un suivi de proximité réalisé par votre manager technique : points réguliers pour votre suivi en mission, votre formation et votre évolution de carrière
✌️ Rejoindre une entreprise où les valeurs du sport sont nos leitmotiv : dépassement de soi, travail en équipe, bienveillance.
🗒️ Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL
😁 Travailler en pair programming ou avec un.e mentor pour gravir les échelons !
💪 Pouvoir participer à une aventure humaine au sein de notre Fondation Ippon pour réduire la fracture numérique dans le monde !
🤝 Participer à nos apéros et divers évènements internes pour consolider la cohésion d’équipe
Et après ?
Et oui alors ? Que se passe-t-il une fois que vous êtes convaincu d’avoir lu l’offre d’emploi qui vous correspond bien ?
Nous vous proposons de prendre contact et de nous rencontrer !
Les Next Steps :
1 call RH
1 échange RH
1 échange Technique
Si le match est bon des deux côtés : Hadjimé ! Vous vous lancerez sur le tatami Ippon !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform', 'CloudFormation'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer,Equativ,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-equativ-3814251519?position=6&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=ya1Qp5kjARTzCi3eNWECoA%3D%3D&trk=public_jobs_jserp-result_search-card,"👫 About the team
At Equativ, we’re on a mission to develop advertising technologies that empower our customers to reach their digital business goals. This means that we rely on massively scalable, widely distributed, highly available, and efficient software systems; the platform deals with over 100 billions requests per day and above 40 Gbps of network traffic.
Our innovation team based in Paris, Nantes, Limoges, Kraków and Berlin is composed of 90 straightforward and energetic engineers working in an Agile environment and ready to tackle the most complex technical challenges.
Our data engineering team is composed of 10 skilled engineers and is based in Paris. We are part of the R&D department which is composed of 140+ engineers spread across Paris, Nantes, Limoges, Kraków and Berlin all working in an Agile environment and ready to tackle the most complex technical challenges.
Our mission 👇
Data Engineering team is central to Equativ’s data centric business and is responsible to ingest, transform, model and redistribute all data coming from our adtech platform.
We aim at building scalable and robust Big Data platforms from ingestion to business actionable consumption. Our Big Data ecosystem must handle massive log ingestion (tens of billions per day), short & long term data storage, complex data modelling, real-time and batch ELT as well as providing external access through dedicated APIs.
Data Engineers serve Equativ data directly to our customers and throughout the company whether it is for BI analysis, data science algorithms (clustering and optimization), customer reporting, invoicing and more.
Equativ Data Engineering team is engaged in an ambitious migration of its main data stack (Hadoop on-premise) to GCP with the objective to increase reporting features, lower maintenance time, improve performances and simplify the access to our raw data.
What you'll do ✏️
As a Big Data Engineer, you’ll primarily focus on maintaining and enhancing the operationality of our on-premise and cloud data pipelines which feed our warehouses and APIs
-Design, develop, test, promote and industrialize all data components from data ingestion to datawarehouse delivery (ClickHouse, BigQuery):
Propose and develop innovative solutions to achieve the best levels of scalability and performance for our Big Data engines
Automate and streamline our real-time and batch data pipelines (on-premise and in the cloud) in order to simplify access to our data by other teams and lower amount of work spent by other teams on ETL processes
Perform end-to-end monitoring to ensure high availability of production data processing, data quality and reliability
Apply best in class Devops guidelines and secure deployments
-Brainstorm with other team members working on our data backend (datawarehouse modelling and data exposure through our reporting APIs) on optimizing our architecture and support them in the use of our pipelines
-Contribute to data roadmap definition in coordination with other R&D and product teams in order to build a best in class data infrastructure that will generate insights for Equativ’s analytics
-Take part in improving and deploy data engineering standards, procedures, processes and operational guidelines around target data components at Equativ
💪 About you
Master degree in Computer Science or similar technical field of study
3+ years of software development with open source technologies
Fluent in Java and/or in Scala. SQL mastery
Very good understanding of Devops principles (Gitlab, Docker, Kubernetes, Gradle, ci/cd)
Experience with large-scale data engineering technologies (ClickHouse, Flink, Kafka, Hadoop, Spark, Hbase)
Experience on building data pipelines on Google Cloud Platform (BigQuery, Dataflow, GCS, Cloud Run, Airflow …) would be a big plus
Experience in working with high QPS Rest APIs is a plus
Entrepreneurial spirit and know-how to identify opportunities of improvement
Working proficiency and communication skills in verbal and written English
Passion for playing with large volume of data
🚀 How you'll grow
Within 1 month:
You'll be just finishing your onboarding.
You'll probably have tackled a few small tasks in peer-coding
Within 4 months:
You'll have an overview of 50% of the stack, CI/CD and team’s main processes. You’ll be able to work on more complex developments
You'll now have enough knowledge to participate to deployments of chosen applications
Within 9 months:
You'll be autonomous on most of our stack and will have participated to major projects
You’ll be helping the team on production matters
👋 About us
Equativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus — four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.
Headquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.
The company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times’ FT 1000: Europe’s Fastest-Growing Companies.
Equativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.
Come and lead the charge with us in building a transparent ecosystem based on quality!
Show more
Show less","{'ProgLanguage': ['Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git', 'Docker'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': ['HBase'], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Big Data', 'ML', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer Cloud (F/H),Apside,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-cloud-f-h-at-apside-3904088503?position=7&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=Od0kUUGYPxYerGEI1x4MHA%3D%3D&trk=public_jobs_jserp-result_search-card,"💥
Découvrez la Vie Apsidienne
📹
et vous aussi, devenez Apsidien
On aurait pu demander à Chat GPT de vous démontrer en quoi
Apside est l’ESN qu’il vous faut,
mais on préfère que vous le découvriez vous-mêmes 👇😏
🔥
Découvrez votre future mission
👉
Contexte
Rejoignez notre Practise Cloud/Data, afin d’intervenir sur des sujets à haute valeur ajoutée !
Notre
client migre actuellement toutes ses applications vers le cloud AWS.
De plus, dans le cadre du développement d'un produit de restitution automatisée de données, ils recherchent actuellement développeur data ayant déjà travaillé sur un projet similaire. La solution produit est techniquement conçue en lien avec le Tech Lead validant l'architecture logicielle à mettre en place sur le cloud AWS.
Secteur
: culture/média
Méthode de travail
: Agile Safe
😎 Mission
Capter les données (structurées et non structurées) produites dans les différentes applications
Intégrer les éléments
Structurer la donnée (sémantique, etc…)
Cartographier les éléments à disposition
Nettoyer la donnée (élimination des doublons, etc…)
Valider la donnée
Créer les référentiels de données
Environnement technique
:
Python
Lambda
Step Function
AWS / AWS RDS
PostegreSQL
Snowflake
Spark
📍
Localisation
La Défense
💰
Le package salarial que nous vous proposons
Contrat :
CDI
Avantages groupe :
carte ticket restaurant Swile, prime de mobilité, RTT, accord télétravail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle à 100% etc…
Avantages agence :
Communauté Cloud/Data, afterworks, communauté techlead
Formation :
certifications techniques, cours particuliers d’anglais en interne, accès à un catalogue de formations grâce à notre plateforme e-learning (
Academy by Apside
) ou via nos organismes partenaires.
🔮
Ô vous futur Apsidien, qui êtes-vous ?
Au moins 4 ans d'expérience en tant que Data Engineer
Maitrise de l’environnement cloud AWS
Force de proposition, bon relationnel et autonome
😏
Apside a suscité votre curiosité ?
Dans un environnement marqué par une accélération des évolutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients à créer de la valeur et à adresser leurs enjeux stratégiques en leur mettant à disposition des expertises technologiques (
Data / IA, Cloud, Cyber
) et une expérience sectorielle (
Industrie, Banque, Assurance, Service, Secteur Public
). Pour un accompagnement global, le groupe propose des offres transverses autour du
Handicap
(Apsid’EA), du
Digital Learning
, et du
Conseil
.
🤔
Et votre place dans tout ça ?
👉 Notre volonté
est de vous accompagner dans la construction et l’épanouissement de votre carrière
en nous appuyant notamment
sur 3 piliers :
Une
rémunération
à hauteur de vos investissements et de vos compétences
Une
trajectoire professionnelle
stimulante sur mesure
Un
engagement
autour des valeurs Apsidiennes : la qualité de vie et des conditions de travail au cœur de nos enjeux
Engagée pour
un monde plus inclusif et plus responsable
, Apside réinvente l’ESN et propose l’Engagement Sociétal et Numérique. Découvrez notre démarche RSE ainsi que notre vision de l’Entreprise Engagée.
Convaincu ? A vous de jouer, envoyez-nous votre CV !
Rejoignez l’aventure Apsidienne et découvrez notre vision d’une ESN singulière et résiliente
🚀
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data Engineer (F/H) - en alternance,Carrefour,"Massy, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-en-alternance-at-carrefour-3884390443?position=8&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=x6YzTwVoIs89U8m4IejwAQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Le saviez-vous ?
Nous rejoindre, c’est rejoindre l’un des leaders mondiaux de la distribution qui met l'accent au quotidien sur la diversité, la RSE et le digital, pour satisfaire nos clients et nos collaborateurs. En tant que partenaire premium des Jeux Olympiques et Paralympiques de Paris 2024, nous partageons les valeurs du sport en permettant à nos équipes de se dépasser et encourageons une alimentation saine au juste prix pour tous.
Vous cherchez à travailler dans une entreprise dynamique où votre travail rime avec impact social et environnemental ? Bienvenue chez nous !
Data Engineer (F/H) - en alternance
En tant qu' alternant, vous intégrerez la plateforme supply chain où vous serez amené à appuyer le pôle prévision et optimisation particulièrement sur des sujets data et d’analyse de données.
Au sein d'une équipe composée de data scientists et de data engineers organisée en mode Scrum Agile, vous travaillerez pour améliorer au quotidien un outil de calcul de prévision (prévision de la demande des entrepôts Carrefour). Vous participez à l'évolution fonctionnelle et technique de l'application.
🎯 Les missions
Dans ce cadre, vous serez amené à
Explorer et analyser les données du datalake carrefour
Participer au cadrage des nouvelles fonctionnalités
Développer les évolutions des traitements, des modèles statistiques et de machine learning de prévision et des reporting
Tester les fonctionnalités développées
Répondre aux demandes utilisateurs
👥 Profil
Vous êtes en école d’ingénieur, en master 2 ou équivalent avec une spécialisation data science, data engineering, statistique, informatique.
Vous avez une expérience en traitement et analyse de données.
Vous avez un esprit d’analyse et la capacité de travailler en équipe et à distance.
Vous êtes autonome et rigoureux, fluide dans votre communication orale et écrite et à l'écoute des besoins de vos interlocuteurs.
Vous êtes reconnu pour vos capacités d'anticipation, votre sens de l'initiative et votre réactivité.
Vous avez une bonne connaissance des langages suivants
SQL
Python
Une connaissance de GCP et de Big query serait un plus.
Une connaissance même théorique de la méthodologie agile serait un plus
Une connaissance de GIT serait un plus.
Encore plus de bonnes raisons de nous rejoindre
Intégrer une équipe conviviale à taille humaine au sein d‘un grand groupe
12 % de remise sur achat
📝 Informations complémentaires
Date de début  09 septembre 2024
Durée  1 an
Lieu  Lyon
Déplacements en magasin et en concurrence dans la région parisienne
Avantages 50 % du titre de transport pris en charge par Carrefour
Envie de rejoindre l’aventure ?
Chez Carrefour, nous avons à cœur de ne passer à côté d’aucun talent et sommes fiers de compter des équipes représentatives de la société dans son ensemble. Nous encourageons ainsi tous types de profils à postuler à cette offre et garantissons un processus de recrutement dénué de toutes formes de discriminations.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Big Query'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Statistiques'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer - Nantes,Capgemini,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=9&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=DheaFnpiwdjTtO17fS5wAQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Capgemini
Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.
Vos missions :
Intégré(e) au sein d'une équipe projets intervenant pour des clients dans des secteurs d'activités variées, vous serez notamment en charge des missions suivantes :
Concevoir et mettre en oeuvre des stratégies sécurisées d'acquisition et d'intégration de données,
Configurer des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),
Construire des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée.
Votre profil :
Diplôme d’ingénieur ou équivalent universitaire
Minimum 3 ans d'expérience
Anglais courant
Maîtrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.
Expertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks…
3 raisons de nous rejoindre :
Qualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité
professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel
vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités :
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
Capgemini
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Data Engineer (Snowflake),MindPal,"Marseille, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896997028?position=10&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=WW8bMRocTD8OQx6MQxnAeA%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for experienced
Data Engineers
with knowledge of
Snowflake
platform.
Responsibilities
Creating and managing data in the Snowflake environment
Designing and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms
Optimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately
Automating data processing workflows using tools such as Airflow or other workflow management tools
Deploying and configuring tools to monitor and report on the performance of the Snowflake system
Requirements
Minimum 1 year of experience as a Data Engineer
Ability to use Snowflake
Very good knowledge of SQL and programming in Python
Ability to work with databases, including the Snowflake platform
Knowledge of ETL tools and data integration
Ability to work in a team and good communication skills
Fluent English in speaking and writing
We Offer
B2B contract type
Full-time job
Remote and flexible working hours
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Consultant·e Data Engineer,Ntico,"Villeneuve-d’Ascq, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/consultant%C2%B7e-data-engineer-at-ntico-3902424755?position=1&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=M32mBwSUc%2BeWFVPltZpppA%3D%3D&trk=public_jobs_jserp-result_search-card,"Sois acteur de ta réussite et rejoins notre équipe de 140 collaborateurs·trices qui ne font pas que des projets, mais qui vivent une vraie expérience humaine unique !
💡 Partage, Progrès, Plaisir : nos valeurs, ton avenir !
🌐 Présents à Lille, Orléans, Montpellier : des expert·e·s partout en France !
💼 + de 40 clients qui nous font confiance
🧑‍💻 Recrutement sur profil
🎯
TA MISSION :
* Tu intègres une communauté Data, en tant que Data Engineer.
* Tu conçois et modélises les données et identifies les sources et flux à réaliser.
* Tu es en lien permanent avec les équipes métiers et IT.
* Tu formes et transmets ton savoir.
* Tu es garant·e de la qualité des livraisons.
🧑‍💻
TES COMPÉTENCES :
Talend, ODI, Stambia, Kafka, API, Bases SQL, et NoSQL, GCP, AWS
🥇
TON PROFIL :
Tu es expert·e des flux de données.
La manipulation et le traitement des données est une seconde nature.
Tu as le sens du service et tu apportes des solutions innovantes.
Tu aimes transmettre et partager ton savoir.
Tu justifies impérativement d’au moins 3 ans d’expérience et tu as développé·e une autonomie sur ton domaine de compétence.
Tu souhaites diversifier tes compétences pour être toujours à la pointe des cas d’usages métiers et des nouvelles technologies Data.
🙌
NOS AVANTAGES :
✨ Pourquoi nous rejoindre ?
💪
Développement Continu
: Chez Ntico, tu montes en compétences grâce à nos communautés d’experts et nos formations !
🤝
Management de proximité
: On t'écoute, on te valorise et on t'accompagne dans ton projet pro, en toute transparence !
🎉
Moments conviviaux
: Sport, culture, DIY, insolite… Tu peux participer à nos événements tous les mois, et en proposer ! On n’est jamais à court d’idées pour des animations uniques !
Ntico, c'est un cadre de travail bienveillant, un environnement dynamique où l'épanouissement personnel est aussi important que le succès collectif !
Postule dès maintenant et prépare-toi à vivre une expérience humaine unique ! ✨
De notre côté, on te contacte dans les 72h suivant ta candidature et on te propose un processus de recrutement rapide. 🚀
Ntico s'engage activement en faveur de l'inclusion professionnelle des personnes en situation de handicap, tout en promouvant la mixité, la diversité et l'égalité au sein de son effectif.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Senior Data Engineer (H/F),relevanC,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-h-f-at-relevanc-3845776902?position=2&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=ggs4hDgN4d0J%2BA6DT8nBCg%3D%3D&trk=public_jobs_jserp-result_search-card,"relevanC est une filiale du Groupe Casino et a été fondée en 2017.
Nous avons des bureaux en France, au Brésil et en Colombie et opérons à l'échelle mondiale.
Nos solutions de Retail Media permettent à nos clients de générer de nouvelles sources de revenus publicitaires grâce à des annonces pertinentes et personnalisées.
En tant que Data Engineer tu auras accès aux données de nos clients internes (enseignes du groupe Casino) et externes à traiter au sein de notre data warehouse. Tes missions seront les suivantes :
travailler en étroite collaboration avec tous les autres membres de la squad
écrire / relire du code en respectant les bonnes pratiques de développement ainsi que les tests unitaires et participer
assurer la co-responsabilité du déroulement des déploiements, des mises en production et du bon fonctionnement des applications avec les autres membres de la squad
rédiger la documentation technique quand cela est nécessaire
mettre en œuvre les bonnes pratiques relatives au RGPD telles que définies par le tech lead
Ce CDI basé à Paris centre (1er arrondissement) débutera dès que possible.
Faire partie de relevanC, qu’est-ce que ça signifie ?
Travailler sur une stack technologique de pointe (Python, PySpark, Google BigQuery, Apache, Airflow…)
Être membre à part entière d’une équipe dynamique et passionnée aux profils très variés (chefs de projets, développeurs, designers, animations commerciales)
Travailler dans un environnement stimulant et relever des nouveaux défis chaque jour
Rejoindre une entreprise en pleine expansion avec des opportunités fortes de développements et d’innovation
Profil recherché
Diplômé(e) d’une grande école d’ingénieur ou profil universitaire spécialisé en Data / Informatique / Math / Stats.
5 ans (et plus) d’expérience en Data Engineering
Appétence forte pour le marketing digital et le retail, force de proposition, business oriented et moteur d’innovation
Une maitrise parfaites des bonnes pratiques de développement
Solides compétences en Python, Spark et SQL
Une expérience sur Google Cloud Platform est un plus
Lien vers notre politique de traitement des données : https://relevanc.com/fr/politique-de-protection-des-donn%C3%A9es-recrutemen
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Chef', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer (H/F) - Lille - CDI,METEOJOB by CleverConnect,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-lille-cdi-at-meteojob-by-cleverconnect-3858145866?position=3&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=tvqEla8Q4uUopai9kdIyDw%3D%3D&trk=public_jobs_jserp-result_search-card,"Entreprise
Chez LJE Solutions, nous plaçons l’humain au cœur de chaque projet. Au-delà des compétences, nous valorisons les
aspirations
et les
valeurs
de chaque individu.
Nous intervenons dans tous les secteurs d'activité en France et en Suisse.
Description Du Poste
LJE Solutions recherche pour un de ses clients basé à Lille, un/une Data Engineer.
Notre client est une
ESN dynamique basée à Lille, qui se distingue dans l'intégration et la restitution de données. Partenaire privilégié de technologies de pointe comme Power BI, Tableau et Qlik, il recherche des talents désireux de participer à notre aventure entrepreneuriale.
Nous recherchons un Data Engineer curieux et motivé pour jouer un rôle clé dans l'organisation et le développement de l'agence. Ce poste offre une opportunité unique de travailler directement avec les fondateurs, experts en technologies, et de contribuer significativement à la formation interne et à l'expertise chez nos clients.
Vos Responsabilités
Travailler en étroite collaboration avec les fondateurs sur des projets d'intégration et de restitution de données,
Participer activement à la croissance de l'entreprise en apportant des idées innovantes et en prenant part à des projets variés,
Monter en compétence techniquement, avec la possibilité d'évoluer vers des rôles de Team Lead ou Tech Lead selon vos aspirations.
Cette entreprise offre un environnement convivial et ambitieux, encourageant la prise d'initiative. Leur structure à taille humaine valorise chaque collaborateur, avec une approche personnalisée et une hiérarchie plate qui favorise l'expression et la participation active de tous.
Rémunération Et Avantages
Poste basé à Lille, avec possibilité de télétravail partiel,
Rémunération compétitive basée sur l'expérience, fourchette indicative de 44k à 48k € en fixe, + variables,
Tickets restaurant,
Mutuelle d'entreprise.
Description Du Profil
Passion pour les technologies de la data, avec une expertise ou un intérêt pour XDi et Talend, sans exclure d'autres ETL du marché,
Plus de 4 ans d'expérience dans le domaine de la data engineering,
Curiosité intellectuelle, agilité, excellent savoir-être, forte capacité de travail en équipe et de partage de connaissances,
Localisation à Lille ou disposition à déménager, avec une préférence pour les candidats de la région pour faciliter la collaboration et le partage au sein de notre agence physique.
Show more
Show less","{'ProgLanguage': ['R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration', 'Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Ingénieur Data ETL,Klanik,"Valbonne, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-etl-at-klanik-3918894069?position=4&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=Yal%2Ffuu%2F8kYrTIr2bQuwVw%3D%3D&trk=public_jobs_jserp-result_search-card,"Nous recherchons un profil Data ETL expérimenté orienté sur les API pour rejoindre notre équipe dynamique. Le candidat idéal doit avoir une expertise approfondie dans la conception, le développement et la gestion d'API, en particulier dans les environnements SOAP, UI et REST. Ce rôle nécessite la capacité de travailler avec des API existantes, de les évaluer, de les améliorer et de proposer des solutions innovantes pour répondre aux besoins de notre client.
Le profil recherché devra gérer la configuration de notre outil d'injection de données, automatiser les scripts de population de données, coordonner les activités de traitement des rejets et construire des vues de surveillance.
Responsabilités :
Concevoir, développer et implémenter des API performantes et évolutives.
Mettre en place de l'outil de transformation et d'injection interne existant en définissant le séquençage des appels API et le mappage des données avec les appels API en tenant compte des exigences du client.
Collaborer avec les équipes techniques pour intégrer efficacement les API dans nos applications et systèmes.
Analyser et améliorer les API existantes pour optimiser les performances et la sécurité.
Proposer des solutions innovantes pour résoudre les problèmes et améliorer l'expérience utilisateur.
Assurer la documentation complète des API développées ou modifiées.
Rédiger un plan de test principal et concevoir des cas de test pour valider la configuration de l'outil, développer des scripts de cas de test automatisés le cas échéant et enrichir les suites de tests de régression sur la base du plan de test défini.
Analyser les API des services web et la documentation des écrans d'interface utilisateur pour élaborer des documents de cartographie d'interface.
Maintenir des scripts pour la population et la migration des données, en utilisant Python et VBA.
Valider et vérifier les configurations livrées à nos clients.
Suivre les données pour les KPI afin de mesurer l'effort de l'équipe, et contribuer à la création de rapports.
Appliquer le modèle de gouvernance concernant la propriété des données, l'accès aux données et le cycle de vie des changements de données.
Compétences Requises :
Expérience pratique significative dans le développement d'API, y compris SOAP, UI et REST.
Maîtrise des langages de programmation courants pour le développement d'API (comme Python, Java, Node.js, etc.).
Compréhension approfondie des bonnes pratiques de conception d'API, de la sécurité et de la gestion du cycle de vie des API.
Capacité à travailler efficacement dans un environnement agile, en équipe multidisciplinaire.
Solides compétences en résolution de problèmes et capacité à travailler de manière autonome.
Qualifications Additionnelles :
Diplôme (bac+5 ou diplôme d'ingénieur) en informatique, génie logiciel, ou expérience équivalente.
Expérience préalable dans le développement de solutions de données ou d'intégration.
Informations Complémentaires :
Ce poste offre l'opportunité de travailler dans un environnement stimulant, où l'innovation et la collaboration sont encouragées. Si vous êtes passionné par le développement d'API et la gestion de données, et que vous souhaitez contribuer à des projets stimulants et agiles, alors vous êtes la personne que nous recherchons !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Résolution de problèmes', 'Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's']}"
LinkedIn,Tech Lead Data Engineer,AXA en France,"Hauts-de-Seine, Île-de-France, France",https://fr.linkedin.com/jobs/view/tech-lead-data-engineer-at-axa-en-france-3905641945?position=5&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=GT9b1dyWWtVSzrqUxtYdmg%3D%3D&trk=public_jobs_jserp-result_search-card,"Environnement
En tant que
Tech Lead Data Engineer F/H
, vous allez contribuer directement aux projets des directions métier (ex : fraude santé, multiéquipements, pricing IARD, optimisation du lead management, fragilité auto, …) d’AXA France et à la construction du socle technique Big Data.
Vous allez intégrer une équipe d'une dizaine de personne composée de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus métier de la Direction Transformation Digital Tech et DATA (DT2).
La Direction Transfo. & Tech. d'AXA France en quelques mots :
Une organisation agile en feature teams : tribus, guildes, squads
Des projets sur des applications innovantes à fort trafic (web, mobile…)
Des méthodologies craft (TDD, BDD, clean code, code review…) et DevOps
Une communauté de partage de bonnes pratiques (BBL, dojo, meetup, conf…)
Votre rôle et vos missions
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
D’accompagner techniquement les Data Engineer de l’équipe (coaching, code review, pair programming…)
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le data lake
Les exploiter pour atteindre la finalité business (exposition de business view, réintégration des résultats dans le SI, service de scoring, …)
De travailler à la création du socle technique Big Data et industrialiser le cycle de développement de l'équipe
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Votre profil
D'une formation supérieure en informatique ou scientifique (Master ou Diplôme d'ingénieur), vous justifiez de plusieurs expériences significatives (+ de 7 ans)
sur du développement big data, en particulier sur du PySpark.
Compétences techniques :
Connaissances avancées en développement en
PySpark (Spark avec le langage Python)
Maitrise de l'environnement
Microsoft Azure
Connaissances avancées d'outils de BI comme
PowerBI
Compétences transverses :
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Expérience en mode de delivery Agile (Scrum, Kanban, etc...)
Driver et accompagner des Data Engineer sur le plan opérationnel
Et Idéalement :
Avoir une expérience en tant que lead
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA
Maitrise des Traitements Big Data en mode Streaming avec Kafka
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
Mais pourquoi AXA France ?
Nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs ! Les avantages que nous proposons à nos salariés sont nombreux.
Nous choisir, c’est bénéficier par exemple :
D’un package de rémunération complet comprenant un salaire fixe, un complément de rémunération variable, des primes, de la participation et de l’intéressement, la possibilité d’acquérir des actions AXA, ou encore des solutions d’épargne avantageuses ;
Equilibre vie Pro / Perso. : D’un cadre de travail flexible jusqu’à 3 jours de télétravail possible par semaine, des tickets restaurant pour les jours télétravaillés ou encore une participation à l’achat d’un écran ou fauteuil ergonomique ;
D’une politique visant à concilier vie personnelle et vie professionnelle avec 28 jours de congés payés, entre 14 et 16 RTT selon les années, des formules de travail à temps partiel ou encore des jours d’absence rémunérées pour la rentrée scolaire ou un déménagement par exemple ;
De la possibilité de s’engager pour une cause qui vous tient à cœur grâce à nos associations telles que AXA Atout Cœur, AXA Compétences Solidaires ou encore AXA Prévention ;
Et bien plus encore ! Perspectives de développement des compétences et de carrières immenses, CE, conciergerie, offres privilèges, soutien en cas d’épreuve personnelle…On s’arrête là, la liste est longue
Qui sommes nous ?
AXA est un des leaders de l’assurance et de la gestion d’actifs dans le monde.
Nous aidons nos 108 millions de clients à traverser les petites et grandes difficultés de la vie.
Chaque jour, nous agissons ensemble pour inventer la meilleure manière de les protéger et voulons donner à chacun les moyens de vivre une vie meilleure.
Un challenge qui donne le sourire et envie de se lever le matin !
Chez AXA, nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C’est pour cette raison que nous menons une politique RH engagée qui favorise la diversité, qui préserve l’équilibre vie privée-vie professionnelle et accélère le développement des compétences et des carrières.
Ainsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une véritable culture d’expertise, accélérant le développement des compétences de chacun et proposant une rémunération attractive.
Pourquoi nous rejoindre ?
Vous êtes porteur d’idées et d’initiatives innovantes ? Vous proposez des solutions et êtes au service du client ? Faites partie de notre grande famille en rejoignant
Un leader mondial offrant des opportunités de carrières intéressantes
Une entreprise qui donne une place de choix à l’innovation, à l’initiative et aux actions solidaires (notamment via l’association AXA Atout Cœur)
Un environnement inclusif à tous les niveaux (mixité, handicap, initiatives pour favoriser l’insertion des jeunes, orientation sexuelle, etc.)
Un accès à de multiples avantages (congés, temps partiel, télétravail, etc.)
Un cadre stimulant, qui permet de rencontrer des collaborateurs performants et d’enrichir ses compétences
Victime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Teams'], 'Other': ['DevOps', 'Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['3', '3'], 'Level': [], 'Experience': ['a', 'n', 's', '7', '7', '7']}"
LinkedIn,Data engineer H/F,Akkodis,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-akkodis-3890779946?position=6&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=Oa7lMHaesBbGmnh3YiFQig%3D%3D&trk=public_jobs_jserp-result_search-card,"La ligne de service Consulting & Solutions d’Akkodis France renforce ses équipes en région Hauts-de-France et recrute un
Data engineer H/F
en
CDI
sur la
métropole lilloise
:
Description de la mission :
Concevoir, mettre en oeuvre et maintenir des pipelines de données efficaces et évolutifs dans un environnement cloud (comme AWS, Azure, Google Cloud Platform…)
Assurer la qualité des données et des modèles
Définir les bonnes pratiques de développement en implémentant des outils de CI/CD
Assurer une veille technologique sur les technologies Cloud
Capacité à interagir avec des parties prenantes diverses : business analyst, architecte, métier…
Veiller au bon fonctionnement des pipelines en production
Profil :
De formation
Bac +4/5 en informatique
ou issu d'une
école d'ingénieur
, vous possédez une expérience de
3 ans
minimum en tant que data engineer ainsi que les compétences suivantes :
Une bonne connaissance des écosystèmes liés à la data (Kafka, ETL, base de données…)
Une première expérience sur un cloud provider (AWS, Azure, GCP)
Une bonne maitrise de langages de programmation tels que SQL, Python, Scala
Akkodis accompagne ses clients dans la mondialisation de leurs projets, aussi un anglais courant est requis pour l’ensemble de nos collaborateurs.
Processus de recrutement :
Une chargée de recrutement vous contacte pour échanger sur votre projet professionnel
Vous échangez ensuite avec un.e manager sur les aspects techniques, les projets
Chez Akkodis nous sommes convaincus que de l’intelligence collective naît le succès. Il n’existe pas qu’un modèle, nous valorisons l’agilité et l’excellence, l’audace et la créativité.
Et si nous parlions ensemble de vos ambitions pour les prochaines années ?
Akkodis est une entreprise handi-engagée et inclusive. Tous nos postes sont ouverts aux handicaps et à la diversité. Tous différents, tous compétents !
Akkodis, est un acteur mondial de l’ingénierie et de l’IT et un leader dans la smart industrie. Nous accompagnons nos clients dans leurs projets de transformation digitale via 4 lignes de service : Consulting, Solutions, Talent et Academy. Akkodis est un partenaire technologique de confiance pour ses clients à l’échelle internationale. Nous co-créons et nous imaginons des solutions de pointe pour répondre aux défis majeurs de notre société, qu'il s'agisse d'accélérer la transition énergétique et de développer la mobilité verte, ou encore de construire des approches centrées sur les utilisateurs.
Dotés d’une forte culture de l’inclusion et de la diversité, nos 50 000 experts en IT et en ingénierie, présents dans 30 pays, allient les meilleures compétences technologiques à une connaissance transverse de toutes les industries pour façonner un futur plus durable. Nous sommes passionnés par l’idée d’inventer ensemble un avenir meilleur.
Akkodis en France, ce sont près de 9.000 experts en IT et en ingénierie répartis sur l'ensemble du territoire, des collaborateurs partageant des valeurs fortes d'honnêteté, de respect, d'équité et d'inclusion. Notre engagement : leur permettre au quotidien d'être eux-mêmes au travail, et acteurs de leur vie et de leur développement au sein d'Akkodis.
*Akkodis est une marque commerciale sous laquelle les entités AKKA et Modis opèrent
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud', 'CI/CD'], 'FrSoftSkills': ['Créativité'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Data Engineer,Mobiskill | WEFY Group,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-mobiskill-wefy-group-3907391938?position=7&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=eT3vGYULy5bSpvwf%2FX9gYw%3D%3D&trk=public_jobs_jserp-result_search-card,"La société ?
Cette startup a été créée en 2018 et vise à aider la prise de décision de ses clients qui sont principalement dans le secteur du retail ou de l'alimentaire.
Ils permettent d'enrichir la donnée afin d'améliorer la stratégie de vente et marketing d'une entreprise grâce à leur plateforme Saas basée sur des algorithmes d'IA.
Ils ont besoin de renforcer leur équipe en Data Engineering pour gérer au mieux leur volumétrie.
Les missions ?
- Editer le cahier des charges des données à collecter auprès de nos partenaires distributeurs
- Prendre en main la gestion de la donnée dans le cloud de la société pour optimiser les coûts et l’efficacité des analyses effectuées par l’équipe Analytics
- Anticiper les évolutions et participer aux choix structurants de la société liés à la gestion de la data
Le profil recherché ?
- Avoir 2/3 ans d'expérience en Data Engineering (hors stage et alternance)
- Avoir pu travaillé en Python comme langage de programmation
Avoir travaillé au moins deux ans et si possible sur des sujets d'optimisation avec Spark !
- La maîtrise des outils tels Airflow, Kafka et Snowflake seraient un plus apprécié
- Maîtriser un des cloud providers et si possible avoir une expérience sur Azure
Pourquoi les rejoindre ?
- Une société stable financièrement (fonds propres uniquement)
- Une startup en pleine croissance
- Une rémunération en fonction de votre séniorité
- Volumétrie de données incroyable, il y a de quoi s'amuser !
- Faire parti de l'unique retail-tech qui a un impact écologique positif (fin des prospectus, éviter le gâchis alimentaire)
Hâte de vous en dire plus rapidement !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Data Engineer / Data Ops,FRG Technology Consulting,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-data-ops-at-frg-technology-consulting-3913842168?position=8&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=cQ8SitVQDCHcJKbOT0F5og%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous êtes un expert passionné par la Data et à la recherche de défis excitants ? Mon client recherche actuellement un
Data Engineer
/ Data Ops
talentueux pour rejoindre une équipe dynamique et humaine.
Missions principales :
Participation active au déploiement de la nouvelle plateforme sur Azure & Snowflake
Forte autonomie et gestion complète des projets data
Analyse des besoins actuels et futurs
Création de spécifications fonctionnelles et techniques
Modélisation de données
Développement de packages SSIS
Intégration des données dans SnowFlake & Azure,
Création de rapports avec Power BI et Excel
Profil recherché :
3 à 4 ans d'expérience
minimum
dans la BI (SSIS, SQLServer, SSAS, SSRS) et/ou le cloud (Azure , Snowflake) ainsi qu'en SQL
Compétences en
architecture sur Snowflake
fortement appréciées
1 à 2 ans d'expérience en tant que DevOps ( CI/CD ; GitLab)
Autonome, rigoureux et anglais courant
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['3'], 'Level': [], 'Experience': ['a', 'n', 's', '4', '4', '4']}"
LinkedIn,Data engineer H/F,Extia,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-extia-3599188121?position=9&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=pq5NdvpIJcK61%2FJVu1Ybeg%3D%3D&trk=public_jobs_jserp-result_search-card,"Vous souhaitez rejoindre une entreprise qui place l’humain au cœur de ses préoccupations ? On vous attend chez
Extia
!
Société de conseil spécialisée dans les métiers de l’IT, de l’ingénierie et du digital, Extia privilégie depuis sa création en 2007 une approche qui allie performance et bien-être au travail. Une vision de l’entreprise partagée aujourd’hui par plus de 2 500 Extiens en France et à l'international et récompensée par le label Great Place to Work® depuis 13 ans, notamment en
2024 où les Extiens se hissent à la première place du palmarès Best Workplaces France
!
Chez Extia, c’est « D’abord qui, ensuite quoi » alors, allons-y !
D'abord qui
Vous êtes habitué à travailler aussi bien avec des méta-données qu’avec des données non-structurées. A cet effet vous maitrisez un ou plusieurs des concepts comme l’ETL, le Data mining le Machine learning, les Big data ou encore la Théorie des graphes par exemple,
Vous maitrisez les bases de l’analyse statistique,
Vous êtes apte à rédiger des scripts en Python et/ou R, et une connaissance d'autres langages de programmation comme Java, Scala ou SAS est un plus,
Vous maitrisez Spark et Hadoop
Vous êtes familiarisé avec l’environnement Linux,
Une expérience avec les outils de Stockage de fichiers volumineux (HDFS, Data Lake, S3, stockage Blob), la connaissance des infrastructures cloud AWS ou GCP et des bases en streaming temps réel seront aussi de réels atouts.
Ensuite quoi
Vous aurez le rôle de support technique aux équipes d’analyse : structurer les données, réaliser des analyses « statistiques » ou « techniques » sur les données, développer des outils d’analyse…
Vous mènerez des études afin d’évaluer les nouvelles technologies dans le domaine du Big Data, Data Mining ou Machine Learning afin d’identifier les solutions les plus pertinentes.
Vous serez en charge de :
Participer à la définition des besoins et à la rédaction des User Stories,
Collaborer avec les Data Scientists au développement des modules d’analyse de donnée,
Concevoir et construire des architectures de données,
Intégrer des sources de données,
Vous assurez que les données sont facilement accessibles et que leur exploitation fonctionne comme demandé, même dans des circonstances hautement évolutives,
Exécuter des processus ETL (extraire / transformer / charger) à partir d'ensembles de données complexes et / ou volumineux
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': ['Statistiques'], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': ['Linux'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning', 'Statistiques', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '13', '13', '13']}"
LinkedIn,Data Engineer (Snowflake),MindPal,"Lyon, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3910994899?position=10&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=JBJP3r8cDISpd7T04G1vsQ%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for experienced
Data Engineers
with knowledge of
Snowflake
platform.
Responsibilities
Creating and managing data in the Snowflake environment
Designing and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms
Optimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately
Automating data processing workflows using tools such as Airflow or other workflow management tools
Deploying and configuring tools to monitor and report on the performance of the Snowflake system
Requirements
Minimum 1 year of experience as a Data Engineer
Ability to use Snowflake
Very good knowledge of SQL and programming in Python
Ability to work with databases, including the Snowflake platform
Knowledge of ETL tools and data integration
Ability to work in a team and good communication skills
Fluent English in speaking and writing
We Offer
B2B contract type
Full-time job
Remote and flexible working hours
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer Talend F/H,Orange Business,Greater Lille Metropolitan Area,https://fr.linkedin.com/jobs/view/data-engineer-talend-f-h-at-orange-business-3916552363?position=1&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=B7xw7hTucHearR4LMUncOg%3D%3D&trk=public_jobs_jserp-result_search-card,"L’ambition d’Orange Business est de devenir l’intégrateur réseau et numérique de référence en Europe, en nous appuyant sur nos forces autour des solutions de connectivité nouvelle génération, du cloud et de la cybersécurité.
Nos 30 000 femmes et hommes présents dans 65 pays, dont chaque voix compte, sont tous animés par la même détermination et le même esprit d’équipe, pour construire les solutions digitales d’aujourd’hui et de demain et créer un impact positif pour nos clients, pour leurs salariés et pour la planète.
Nous offrons des opportunités passionnantes grâce à des projets innovants dans la data et le digital, le cloud, l’IA, la cybersécurité, l’IoT, ou encore le digital workspace et le big data.
Venez vivre cette aventure avec nous !
Afin de développer notre équipe lilloise, nous recherchons aujourd'hui, un Ingénieur DATA à même d’accompagner nos clients dans la structuration de leurs SI autour de la donnée.
Vos principales missions seront les suivantes
:
- Concevoir des solutions de traitement et collecter des volumes importants de données.
- Participer à des études de cadrage pour collecter le besoin métier et concevoir les solutions qui répondent au besoin du client.
- Apporter son expertise sur des problématiques précises rencontrées chez les clients.
- Participer à la veille technologique
- Réaliser les
développements TALEND
- Rester informé et former sur les nouvelles solutions DATA
- Contribuer aux phases d'avant-vente et au développement business.
- Participer à la conception, l'évolution et la présentation de nos offres DATA.
Vous
:
- Êtes issu(e) de formation bac+5 ?
- Vous justifiez d'au moins 3 ans d'expériences en qualité d'Ingénieur DATA sur la solution TALEND Enterprise (Data Integration) et avez idéalement une connaissance des solutions Cloud d'AWS et d'AZURE ?
- Vous êtes intervenu sur des projets intégrant des pratiques DevOps et AGILE ?
Alors postulez, ce poste est fait pour vous !
Vos compétences clés
:
- Expertise sur l'outil
ETL TALEND
Enterprise (administration et développement)
- Fortes connaissances des solutions de bases de données (SQL, NoSQL…)
- Connaissances en langages objets ou scripts (notamment Java mais aussi Javascript, Scala, Python…)
- Divers systèmes d'exploitation : UNIX, Windows
Autonomie, rigueur, curiosité, dynamisme et sens du service sont des qualités nécessaires pour ce poste.
Les compétences complémentaires qui seraient appréciées :
- Connaissances d'autres modules Talend (MDM, ESB, Data Quality, Cloud…)
- Maîtrise des technologies du Big Data (Hadoop, Spark, Kafka…)
- Expertise sur d'autres outils ETL (Informatica, SSIS, DataStage...)
- Notions en architecture des Systèmes d'Information
- Maîtrise de l'anglais (oral et écrit)
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'JavaScript'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': ['Orange'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': ['Windows'], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Data engineer AWS/Azure,Apside,"Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-aws-azure-at-apside-3825012802?position=2&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=tCEzMqlfv%2BbUvDPus74BWA%3D%3D&trk=public_jobs_jserp-result_search-card,"💥
Découvrez la Vie Apsidienne
📹
et vous aussi, devenez Apsidien
On aurait pu demander à Chat GPT de vous démontrer en quoi
Apside est l’ESN qu’il vous faut,
mais on préfère que vous le découvriez vous-mêmes 👇😏
🔥
Découvrez votre future mission
👉
Contexte
Rejoignez notre Practise Cloud/Data, afin d’intervenir sur des sujets à haute valeur ajoutée !
Secteur
: Banque/Finance
Méthode de travail
: Agile
L’équipe Data de l’un de nos clients grands compte du secteur bancaire vise à faciliter la construction de parcours transversaux par les DSI en proximité des Métiers en leur proposant des solutions technologiques sur étagère, qu'elles pourront assembler rapidement, de manière agile.
Des solutions informatiques basées sur les technologies BigData sont donc mises en place dont une qui est un framework de contrôles. Celle-ci est proposée aux applications du groupe afin de les aider dans l'implémentation des contrôles de Data Quality sur les plates formes Bigdata on premise, ainsi que cloud Azure et AWS.
Or ils souhaitent aujourd’hui mettre en oeuvre plusieurs évolutions de son socle technique.
😎 Mission
L'ajout de fonctionnalités sur le moteur de calcul
IHM de paramétrage
Compatibilité avec la plateforme Azure et AWS
Développements des évolutions sur le moteur des contrôles
Tests (en TDD) en mode Agile
Contribution à la validation de l'usage de solution en production pour les nouvelles applications utilisatrices du framework.
Environnement technique
:
Amazon Web Services
GitHub
Hadoop
Kubernetes
MS Azure
Python
Scala
Spark
💰
Le package salarial que nous vous proposons
Contrat :
CDI
Avantages groupe :
carte ticket restaurant Swile, prime de mobilité, RTT, accord télétravail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle à 100% etc…
Avantages agence :
intégration de la Practise Cloud/Data, afterworks, communauté techlead
Formation :
certifications techniques, cours particuliers d’anglais en interne, accès à un catalogue de formations grâce à notre plateforme e-learning (
Academy by Apside
) ou via nos organismes partenaires.
🔮
Ô vous futur Apsidien, qui êtes-vous ?
Au moins 5 ans d'expérience en tant que Data Engineer
Maitrise de l’environnement cloud AWS ou Azure
Force de proposition, bon relationnel et autonome
😏
Apside a suscité votre curiosité ?
Dans un environnement marqué par une accélération des évolutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients à créer de la valeur et à adresser leurs enjeux stratégiques en leur mettant à disposition des expertises technologiques (
Data / IA, Cloud, Cyber
) et une expérience sectorielle (
Industrie, Banque, Assurance, Service, Secteur Public
). Pour un accompagnement global, le groupe propose des offres transverses autour du
Handicap
(Apsid’EA), du
Digital Learning
, et du
Conseil
.
🤔
Et votre place dans tout ça ?
👉 Notre volonté
est de vous accompagner dans la construction et l’épanouissement de votre carrière
en nous appuyant notamment
sur 3 piliers :
Une
rémunération
à hauteur de vos investissements et de vos compétences
Une
trajectoire professionnelle
stimulante sur mesure
Un
engagement
autour des valeurs Apsidiennes : la qualité de vie et des conditions de travail au cœur de nos enjeux
Engagée pour
un monde plus inclusif et plus responsable
, Apside réinvente l’ESN et propose l’Engagement Sociétal et Numérique. Découvrez notre démarche RSE ainsi que notre vision de l’Entreprise Engagée.
Convaincu ? A vous de jouer, envoyez-nous votre CV !
Rejoignez l’aventure Apsidienne et découvrez notre vision d’une ESN singulière et résiliente
🚀
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer,HarfangLab,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-at-harfanglab-3849760187?position=3&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=skdkpLczYGSCzfAm1n2LAw%3D%3D&trk=public_jobs_jserp-result_search-card,"Who we are?
HarfangLab
is a
cybersecurity scale-up
, and we have developed an
Endpoint Detection and Response
(EDR) software to
detect and mitigate modern cyberattacks
on a company's workstations and servers. Our algorithms detect abnormal behaviors and generate security alerts or block program execution.
From 50 to 100 employees in 2023, HarfangLab is experiencing hypergrowth and has already achieved several significant milestones: winners of the Ministry of Defense's cyber challenge in 2019, recipients of the BPI's I-Nov competition in 2020, and software certified by ANSSI in 2021.
Our initial clients include CAC40 industrial companies and government entities. We completed our
first funding round of €5 million in 2021 and our second funding round of €25 millions in 2023
, which will enable us to strengthen our teams, and to expand internationally in Europe.
Our mission is to
protect businesses and government agencies from modern cybersecurity threats
(cybercrime, data theft, influence)
that endanger the economic health of companies and the security of the nation
.
What you will do with us?
You will work within the
Artificial Intelligence team
, consisting of 5 individuals, under the direct and daily supervision of the team lead.
This team designs, implements, and deploys supervised algorithms for detecting malicious behavior.
As a
Data Engineer
you will:
Gather requirements from stakeholders,
Manage data for the AI and CTI departments,
Design, develop, and maintain the existing data warehouse,
Implement a data lake if deemed appropriate,
Create data pipelines using ELT processes,
Design tools for data visualization.
About You
Hard Skills
Master’s degree in Computer Science, Engineering, or a related field,
Proven experience as a Data Engineer, 2 years minimum,
Proficient in Python,
SQL: Strong in SQL syntax and query optimization, NoSQL will be a big plus,
Competence in data warehousing and data lake architecture,
Proficiency in at least one ELT tool and strong understanding of related processes.
Soft Skills
Strong communication and teamwork skills,
Excellent problem-solving and attention to detail,
You enjoy learning and sharing your knowledge with others,
You demonstrate initiative - when an opportunity arises to improve existing processes, you seize it.
About Us
Our office and Team Life:
Offices located in the heart of Paris, near Bourse (75002),
High-quality equipment based on preferences and needs (PC, Mac, additional screens, etc.),
Thanks to our Office Manager, we regularly organize events such as seminars, happy hours, themed evenings, and more,
An onboarding process to welcome each new colleague with an explanation of the roles and a mentor to support you during your early days!""
A great team that always seeks to improve their skills
And more:
An attractive package: Base salary + profit sharing,
Flexible remote work options,
A mentor to guide you throughout your probationary period,
Health insurance: The best health insurance with Alan and Moka Care, a mental health at work app,
Meal vouchers: We use the Swile card and also have access to a discount platform through our works council,
7 to 11 additional days off (RTT) per year, in addition to the 25 days of paid vacation. GymLib subscription, covered 80% by HarfangLab,
Access to training and events of your choice and according to your professional needs.
The recruitment process
A 30-minutes call with our Talent Acquisition Manager,
A 30-minutes visio interview with the Hiring Manager,
A 1 hour on-site interview + 30 minutes with the team for a team fit assessment,
A psychometric test to assess your motivations and soft skills,
A final HR video appointment to review your soft skills and motivations.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['ML'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication', 'Teamwork', 'Initiative']}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['7', '7'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Lead Data Engineer,Ippon Technologies,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/lead-data-engineer-at-ippon-technologies-3851535052?position=4&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=SDvZzVRJfCa5AUR0lcL0eQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Cabinet de conseil et d'expertise en technologie, international et indépendant.
En quelques mots : 700 passionnés de tech, 12 agences dans le monde, 6 communautés d’expertise d’excellence, contributeur actif et sponsor de l'écosystème numérique, des publications soutenues et reconnues sur nos réseaux.
Rejoignez notre communauté de 70 experts en data, dont 30 à Paris, où la collaboration dynamique entre data engineers, data analysts et data architects est le moteur de notre succès. Avec une communication proactive sur des canaux internes, restez constamment informé des dernières tendances, participez à des discussions stimulantes et contribuez à l'organisation d'événements passionnants (dataday, datapéro, datalunch…).
Faites partie d'une équipe où l'innovation et l'engagement sont les clés de notre excellence collective !
Notre spécialité ? construire des data platforms dans le cloud public avec les meilleures technos du moment.
En tant que tech lead, tu interviendras sur la création d'un entrepôt de données pour les KPIs d’un grand groupe dans le secteur de l’énergie. Le but étant de leur permettre de superviser leurs activités afin de supporter leurs décisions stratégiques.
Ton rôle :
Intervenir sur l’architecture et le développement d’une pipeline d'alimentation de données
Travailler sur la modélisation et l’implémentation de l'entrepôt de données
Conseiller et accompagner les équipes dans la réalisation des dashboards de suivi des KPIs
DevOps: projet entièrement Terraformé (ressources + droits), CI/CD Gitlab, administration GCP
Faire une veille technologique active et partager tes connaissances en interne
Travailler en collaboration avec les métiers et les data analysts pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship etc…)
Si tu le souhaites, tu pourras également :
Participer aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups)
Capitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne.
Tes connaissances :
Tu maîtrises le développement en Python
Tu as de l’expérience dans la mise en place de pipeline de données jusqu’en production (CI/CD Gitlab, Terraform)
Tu as une expérience dans un environnement Cloud (GCP de préférence, AWS, Azure)
Tu as une bonne connaissance d’un outil de visualisation (Looker Studio, Power BI)
Tu accompagnes des data engineers dans la mise en place des bonnes pratiques
Tu es capable de proposer/challenger la stack technique
Ippon c’est aussi :
Travailler en équipe au sein d'une communauté data à la pointe des évolutions
Un suivi de proximité réalisé par ton manager (expert data)
Devenir ceinture noire en data grâce à notre programme d’accompagnement de carrière Blackbelt
Participer à nos apéros et divers évènements internes pour consolider la cohésion d’équipe
Notre process de recrutement :
Préqualification téléphonique - 20 min
Un entretien RH / Sales - 1H00
Un entretien technique avec 2 consultants data
Si le match est bon des deux côtés : Hadjimé ! Tu te lanceras sur le tatami Ippon !
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Communication', 'Collaboration', 'Organisation'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer – Alternance H/F,HUTTOPIA Jobs,"Saint-Genis-les-Ollières, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-alternance-h-f-at-huttopia-jobs-3902055574?position=5&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=eGiPNGddRzkmDAThGsqSWg%3D%3D&trk=public_jobs_jserp-result_search-card,"Huttopia, opérateur international reconnu du tourisme durable et acteur du développement territorial, poursuit un développement soutenu en France et à l’international (Pays-Bas, Espagne, Canada, USA, Chine…). Présent dans les domaines de l’Hospitality avec 127 camping-nature exploités, les activités industrielles avec la fabrication d’hébergements en bois et toile, et dans le numérique, le groupe Huttopia a généré en 2023 un chiffre d’affaires de 160 M€ avec plus de 700 collaborateurs permanents et plus de 1800 personnes l’été.
Pour accompagner son fort développement, Huttopia recrute ses talents de demain !
Nous recherchons notre futur
Data Engineer H/F
en alternance à compter de
septembre 2024.
VOS MISSIONS :
Sous la responsabilité hiérarchique du Responsable Data et sous la responsabilité technique de la Data Engineer, vous interviendrez comme :
Co-responsable de
l’exploitation des données
:
Produire et livrer des tableaux de bord (sous Tableau)
Participer à l’administration de l’outil de Data Viz
Accompagner et former des utilisateurs (lecteurs et développeurs)
Produire des demandes de chiffres ad-hoc en interrogeant la BDD centrale (SQL),
Participer aux développements des algorithmes d’exploitation des données (en Python)
Participant actif à
la récupération et la structuration des données
:
Développer des flux de récupération des données entre une source et la BDD centrale
Réaliser la maintenance et monitoring des flux déjà développés
VOTRE PROFIL :
Vous êtes le candidat idéal pour rejoindre notre équipe si …
Vous préparez une formation supérieure en M1 ou M2, spécialisée en DATA dans le cadre de votre alternance.
Vous avez des connaissances en langage de programmation : SQL, Python et Java. Vous souhaitez développer vos compétences sur les ETL type Talend et les outils de Data Viz (Tableau, Power BI, Qlik..) vous sont familiers.
Au-delà de vos compétences, nous nous intéressons à vous, à votre personnalité.
Doté d’un bon relationnel, vous êtes reconnu pour votre rigueur, votre sens du service et votre curiosité technique. Vous appréciez travailler en équipe et vous êtes adaptable.
Enfin, vous êtes attiré par le secteur de l’outdoor et notamment du tourisme !
Nous sommes l’entreprise qu’il vous faut si …
Vous êtes prêt à relever de nouveaux défis en rejoignant une entreprise aux collaborateurs engagés et autonomes.
Vous souhaitez évoluer dans des bureaux en bois où il fait bon travailler avec des espaces pour se détendre et des évènements festifs… Parce que chez Huttopia on aime travailler sérieusement sans se prendre au sérieux.
LES PLUS HUTTOPIA :
Ticket restaurant : 9.92€/jour travaillé
Chèques culture mensuels
Une bonne mutuelle santé
LES POINTS PRATIQUES :
Contrat apprentissage ou contrat de professionnalisation
Date de démarrage : Septembre 2024
Durée du contrat : 12 mois
Métropole Lyonnaise à Saint Genis les Ollières – à 5 minutes de Tassin-La-Demi-Lune
Prise en charge à hauteur de 50% de l’abonnement de transports en commun.
NOTRE PROCESS DE RECRUTEMENT :
Bérangère, notre chargée de recrutement vous contactera pour un premier échange en visio, vous passerez ensuite un entretien dans nos bureaux.
Vous serez informé à chaque étape de l’évolution de votre candidature.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer H/F,Lincoln France,"Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-lincoln-france-3829857168?position=6&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=SmxRFRPcnUVxFqCIJQZpiw%3D%3D&trk=public_jobs_jserp-result_search-card,"DATA ENGINEER H/F
CDI
3 ans minimum
Chez Lincoln
, nous formons une communauté d'innovateurs passionnés qui redéfinissent l'analyse de données depuis
plus de 30 ans
. En tant que
Pure Player Data
, notre expertise est reconnue dans les domaines
de la Modern BI, du Big Data et de la Science des données
.
Notre mission ?
Transformer les données en solutions concrètes pour nos clients grands comptes dans divers secteurs tels que la banque, le retail, les télécoms, l'industrie, la santé, etc.
Description de poste
Nous recherchons un
Data Engineer H/F
pour accompagner nos clients dans leurs projets stratégiques.
Vos missions :
Concevoir et développer des pipelines de données robustes et évolutifs.
Intégrer et transformer des données provenant de différentes sources.
Développer et mettre en œuvre des algorithmes de traitement de données avancés.
Collaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.
Assurer la qualité et la fiabilité des solutions développées.
Prérequis :
Maîtrise des langages de programmation (
Python, Scala, etc
.).
Connaissance approfondie des bases de données et des technologies
Cloud (GCP, AWS, Azure, Snowflake, etc.)
Expérience avec
MySQL, PostgreSQL, MongoDB.
Maitrise ETL/ELT (Talend, Stambia, etc.)
Solides compétences en conception et en optimisation de pipelines de données.
Expérience de travail en
méthode Agile
pour la gestion de projet et le développement de solutions.
Capacité à travailler de manière autonome et en équipe.
Excellentes compétences en communication et en résolution de problèmes.
Les plus du poste :
Environnement Collaboratif
: projets innovants favorisant le partage des connaissances.
Accompagnement individualisé et de proximité
: formations certifiantes, attribution d’un Career Manager pour vous orienter dans votre trajectoire professionnelle, opportunités d’évolution de carrière.
Flexibilité du Travail
: Télétravail et horaires flexibles pour votre équilibre vie professionnelle-personnelle.
Rémunération Compétitive
: Salaire compétitif avec des avantages sociaux attrayants.
Mobilité
: Possibilité de mobilité à Paris, Lyon ou Aix-en-Provence offrant des expériences diversifiées au sein de Lincoln.
Notre processus de recrutement :
un entretien RH (1h) et entretien technique (1h)
Cette annonce n’est pas faite pour vous si :
Vous êtes freelance et vous comptez le rester !
Toujours là ? Postulez et rejoignez nos
400 experts en Data
😉.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', ' MongoDB'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': [], 'Os': [], 'DBMS': ['MySQL', 'PostgreSQL', 'Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': ['Communication', 'Résolution de problèmes', 'Flexibilité'], 'EnSoftSkils': ['Communication']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['400'], 'Level': [], 'Experience': ['a', 'n', 's', '3', '3', '3']}"
LinkedIn,Alternance - Data Engineer H/F,Hermès,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/alternance-data-engineer-h-f-at-herm%C3%A8s-3889716412?position=7&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=wkNcO1RWrxC1NFezsJWZyA%3D%3D&trk=public_jobs_jserp-result_search-card,"Eléments de contexte
Hermès Digital Ventes et Services recherche pour sa direction Data & Performance :
Un Alternant Data Engineer (H/F)
Contrat d'alternance de 12 mois
A partir de Septembre 2024
Basé à Paris
Principales activités
Vous êtes rattaché au Data manager.
Vous avez pour principale mission d’accompagner l’équipe Data dans les tâches quotidiennes :
Reporting et statistiques de ventes et trafic (notamment via l’outil Google Analytics et Google BigQuery)
Analyse des leviers d’acquisition de traffic SEA/SEO/Referral
Création de Dashboard via l’outil Google Data Studio
Participation aux travaux de CRO (Conversion Rate Optimization) et d’AB testing
Mise en place d’étude prédictive sur les données des sites Ecommerce
Profil
Etudiant en école d’ingénieur possédant une forte culture Internet et une sensibilité aux problématiques digitales e-commerce, vous avez une première expérience en entreprise
Profil technique ou aisé avec la technique, une spécialisation en digital est en plus
Organisé, rigoureux, curieux, autonome, bonne expression écrite et aisance relationnelle
Maîtrise du Pack Office indispensable, ayant déjà utilisé Google Analytics
La connaissance d’outils de BI / Datavisualisation serait appréciée (Google Data Studio, Tableau Software, Bime ou QlikView/QlikSense, PowerBI), de Base de Données (SQL, MySQL, BigQuery)
Une appétence pour la Data, ses languages (Python, R) et ses technologies (Notebooks, modélisation statistique, Machine learning) est fortement appréciée.
Anglais courant souhaité
Sensible aux produits hauts de gamme, vous souhaitez vous investir dans un stage riche et formateur
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'PowerBI'], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['MySQL', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Statistiques'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer F/H,Valeuriad,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-valeuriad-3741223009?position=8&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=vksQPYG8IfOb6g1PzOjDwQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Rejoins la
Team Data
créée par
Nicolas Greffard,
Docteur en Intelligence Artificielle
, déjà composée de
20
Data Engineers
et
Datascientists
talentueux 😍
Nous recherchons de
nouvelles pépites
pour rejoindre notre équipe de choc et répondre aux
multiples problématiques Big Data
de nos
clients nantais
mais également
contribuer à nos projets de R&D
et travailler sur des
conférences incroyables
(DevFest, Salon de la Data)
🤩
Ta future mission si tu l'acceptes
😉
Nous te proposons d'intervenir au sein de nos
grandes DSI clientes
, sur des sujets de
collecte
, d
'alimentation
et de
transformation de données
sur un environnement
Big Data
Apache
(
Hadoop, Spark, Ambari, Hive
) sur les technologies suivantes :
Hadoop, Apache Ambari, RabbitMQ, Java, Scala, YarnApplication, Teradata, Squoop, Kudu, Hue, Hive, Impala, Dataiku, Flink, Kafka, Spark, Kibana, Oozie, Git, GitLabCI, Jenkins
,
AWS.
Le job en détail
🤩
Étude, conception et réalisation de traitements Big Data ;
Échange avec les architectes, les PO et PPO, les développeurs et la gouvernance de données ;
Exploration des données et des usages des utilisateurs avec Impala ;
Import de données (SFTP, Kafka, RabbitMQ) ;
Alimentation du cluster Hadoop via des composants développé en Java avec le Framework Spark sur IntelliJ ;
Utilisation d’Apache Ambari pour gérer et surveiller un cluster Hadoop, visualisation des jobs en cours via YarnApplication et des flux Oozie ;
Collecte des données depuis Teradata via l’outil Sqoop dans une base de données Hive ;
Transformation des données avec Spark (HDFS, Hive, Kafka, Hbase, Phoenix) ;
Utilisation de Apache Kudu afin d'optimiser les requêtes utilisateurs sur les données chaudes ;
Exposition de données sur Dataiku pour la création de modèle de DataScience ;
Réalisation en Java – Flink pour gérer les traitements complexe et volumineux ;
Gestion de configuration sous Git avec GitLab ;
Intégration continue avec Jenkins et Sonar ;
Lecture de fichier parquet depuis un répertoire S3 sous AWS ;
Requêtage de bases de donnée depuis l'outil Athena d'AWS ;
Transformation des données et calcul d'indicateurs sous Hive ;
Utilisation de Oozie pour l’ordonnancement de flux ;
Utilisation de Kibana pour visualiser et mesurer la volumétrie de traitements quotidien et en streaming.
Pourquoi choisir Valeuriad ?
😊
En plus d’être aujourd’hui un acteur nantais reconnu de l’expertise IT, nous nous inscrivons depuis notre création dans une démarche d'entreprise
Opale
et
Holacratique
, où l'ensemble de nos prises de décisions et projets sont réalisés par et avec l'ensemble de nos
119 coéquipiers
💪
Rejoindre Valeuriad, c'est
pouvoir s'investir dans la co-construction de l'entreprise
:
Par un rôle, avec une fiche de poste et un temps dédié (gestionnaire des Ci’s, porteur des partenariats écoles, organisateur d’événements, PO des projets internes, gestion de l'Académie Valeuriad…).
Par les projets stratégiques (200 jours mis à disposition pour les coéquipiers chaque année) pour créer et faire grandir des projets structurants (création de nouveaux avantages à l'ancienneté, création d'indicateurs mensuels pour être toujours plus transparents, mécénat de compétences pour des associations caritatives...).
Par les projets cagnottes (150€ par coéquipiers et par an) pour réaliser des projets collaboratifs qui te tiennent à cœur avec d'autres Valeurieux (découverte du cécifoot, challenge écologique, challenges sportifs pour des dons à des associations humanitaires, borne photo...).
Par les ateliers collaboratifs, chaque mois des brainstorming et ateliers de travail sont proposés par les différents porteurs de projets et sont ouverts à tous les volontaires.
Mais avant-tout nous sommes une
équipe soudée
, des collègues qui apprécient
passer du temps ensemble
lors de nos
soirées hebdomadaires
et se créer des
souvenirs inoubliables
🤩 C'est pour ça que chez Valeuriad, le plus important pour nous reste le
savoir-être
:
des passionnés, du dynamisme, des sourires, de l'écoute et le sens de la fête
😉
Show more
Show less","{'ProgLanguage': ['Java', 'Scala', 'R', 'Go'], 'DataBase': ['HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Flink'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,DATA ENGINEER,Apside,"Nantes, Pays de la Loire, France",https://fr.linkedin.com/jobs/view/data-engineer-at-apside-3909772916?position=9&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=aYYNuTDFLAFR6HvwcvR%2FAg%3D%3D&trk=public_jobs_jserp-result_search-card,"Offre d'Emploi : DATA ENGINEER H/F chez Apside
Description du poste :
Nous sommes à la recherche d'un Data Engineer passionné pour rejoindre notre équipe dynamique. Si vous avez une expertise dans le Big Data, la Data Science, l'analyse de données et l'architecture de données, cette opportunité est faite pour vous. Intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.
Vos missions :
Développement des jobs Spark pour la collecte et la transformation des données comptables disponibles dans les bucket S3.
Optimisation des jobs Spark.
Développement des batchs Java et écriture des données au formats comptables.
Écriture et ordonnancement des DAGs Airflow.
Support du développement Spark Scala.
Maintenance applicative.
Production des événements dédiés à la plateforme de données.
.
Votre rôle, vos compétences :
Vous maîtrisez au minimum un langage de programmation appliqué à l’analyse de données (SQL, Scala, Python, Java).
Vous êtes passionné par le Big Data et le Machine Learning.
Vous concevez et mettez en œuvre des stratégies sécurisées d'acquisition et d'intégration de données.
Vous configurez des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks).
Environnement technique :
SQL
Python/Spark
Cloud AWS: AWS Glue, AWS Lambda (possibilité de vous former sur AWS)
Stockage objet (AWS S3)
Orchestration et scheduling de tâches (Apache Airflow)
Bases analytiques et bases NoSQL (ElasticSearch, AWS Athena)
Votre profil :
Fort de 4 années d’expérience en Data Engineer/ DATA ANALYST
Titulaire d’une formation supérieure IT.
Capacité à s’intégrer dans un cadre technique client tout en étant à même de proposer des pistes d’améliorations pertinentes.
Autonome dans la gestion des projets.
Curieux et impliqué, vous êtes bon communicant avec les clients et les acteurs de culture technique différente.
De bonnes raisons de rejoindre Apside ?
Un esprit start-up avec la stabilité d’un grand groupe, qui favorise l’agilité, le travail d’équipe et la proximité. Alors qu’Apside ne cesse d’agrandir sa famille déjà forte de plus de 3000 consultants, nous sommes à la recherche de nos nouveaux talents !
CDI + package salarial avantageux (Mutuelle offerte, RTT, Tickets Restaurant, Intéressement ...)
Participez et animez nos soirées techniques (Project Lab, Test Lab…),
Devenez speaker (Devoxx, DevFest, NCraft…),
Formez vous avec l’Academy By Apside (e-learning, formation, certification).
Développez votre réseau (Soirées trimestrielles, Afterwork, Soirées d’intégration…),
Intégrez notre Communautés d’Experts et testez les dernières innovations techniques sur notre Bac à Sable !
Apside s’engage en faveur de l’emploi des personnes en situation de handicap avec sa filiale Apsid’EA : 1ère entreprise adaptée totalement intégrée à une ESN !
Pour aller plus loin avec APSIDE !
https://www.apside.com/fr/nos-offres-emploi/?_joboffer-agency=17833&_paged=2
Ce poste de DATA ENGINEER est fait pour vous !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark', 'Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure', 'Google Cloud Platform'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': ['17833'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer – Grenoble,Capgemini,"Grenoble, Auvergne-Rhône-Alpes, France",https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-grenoble-at-capgemini-3905836212?position=10&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=KDNA8Rx%2B%2FhO7ZsBno6DlMw%3D%3D&trk=public_jobs_jserp-result_search-card,"Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une communauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.
Vos missions :
En tant que Data Engineer au sein d'une équipe multidisciplinaire, vos responsabilités principales seront les suivantes :
Intervenir sur les différentes phases d'un projet dans un environnement Cloud et Agile.
Contribuer à la gestion de la qualité des données et extraction et analyse de celle-ci, ainsi qu’à la présentation des données dans leur forme raffinée.
Proposer des nouvelles lectures de données via un travail de fouille sur les gisements d’information, notamment client.
Adopter une posture de consultant : proposer de nouvelles solutions et accompagner le client dans ses choix.
Votre profil :
Titulaire d'un Bac+5 en école d’ingénieur ou en université.
Connaissances approfondies des ETL (Talend, Informatica ou SSIS), du traitement de données (Spark, Python, Scala) ainsi que des bases de données (Oracle, SQL Server, Postgres).
Faculté pour se montrer curieux, autonome et proactif dans la réalisation de ses tâches.
Capacité à faire preuve de rigueur et à travailler en équipe.
Bon niveau d’anglais (B2 minimum).
3 raisons de nous rejoindre :
Qualité de vie au travail
: accord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.
Apprentissage en continu
: certifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.
Avantages groupe & CSE
: plan actionnariat, tarif préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités
:
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
À propos de Capgemini :
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Oracle', 'SQL Server'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5'], 'Experience': ['a', 'n', 's', '55', '55', '55']}"
LinkedIn,Data Engineer  H/F,Groupe INGENA,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-groupe-ingena-3883689479?position=1&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=9Wc3V%2BMHM4IlRePP6%2BcZVA%3D%3D&trk=public_jobs_jserp-result_search-card,"Le groupe INGENA promeut la transition numérique en étant acteur d’un monde souhaitable.
Votre mission :
Concevoir, développer et tester des algorithmes de collecte et de traitement de gros volumes de données sous Scala, Python ou Java
Automatiser et optimiser les flux de données et leurs visualisations en dashboards
Industrialiser les traitements, la qualité et l’intégrité des données
Participer à la Modélisation et à la Gouvernance des données (process, normalisation, référentiel,…)
Contribuer à la scalabilité, la sécurité, la stabilité et la disponibilité des données de la plateforme
Analyser les données pour répondre aux questions métiers et participer à l’évolution de l’architecture Big Data
Concevoir, Développer et Industrialiser des modèles de Machine Learning, Deep Learning, en collaboration avec les Data Scientists
Appliquer une démarche CI/CD (Git, Jira, Jenkins)
Les compétences techniques nécessaires sont :
Expérience de 5 ans minimum en développements Scala, Python ou Java
Expérience de 2 ans minimum sur SPARK et sur le traitement des flux en streaming
Expertise sur Hadoop (Hive, HBase, HDFS) sous distributions MapR ou Hortonworks
Expérience souhaitée sur ELK, Terraform, NoSQL,…
Fort background en Modélisation de données ou ETL
Maîtrise des briques analytiques des clouds AWS, GCP ou Azure
Sensibilisation à la démarche CI/CD tools (Git, Jenkins)
La connaissance de Docker, Kubernetes et Ansible est un plus
Mise en œuvre des méthodes Agile (Scrum, Kanban,…)
Anglais souhaité
Groupe INGENA
:
Le Groupe INGENA est spécialisé en Conseil Métier et en Intégration pour les marchés de l’assurance, de la banque et de la Finance. INGENA intervient notamment sur les projets associés à la Data, aux Risques et à la Distribution.
Le groupe comprend également la société DRiMS spécialisée en Finance de Marché.
Nos valeurs : Engagement, Intégrité et Bienveillance.
La mise en pratique du monde souhaitable, c’est pour nous une entreprise éco-responsable, éthique, inclusive, sociale, soucieuse du bien-être, de l’évolution et de l’épanouissement de ses équipes. Ce sont aussi des offres pour un monde durable comme la maîtrise des risques ou l’ESG.
Dans un esprit convivial et engagé, nous faisons en sorte que chacun puisse être acteur de l’INGENA souhaitable.
Bureau à Paris 9ème (Métro Le Peletier). Clients à Paris ou très proche banlieue.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'HBase'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': ['Ansible', 'Kubernetes'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': ['JIRA'], 'Other': ['Big Data', 'Machine Learning', 'Cloud', 'CI/CD'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer - Stream Data Processing - Distributed Data Processing,Pathway,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-stream-data-processing-distributed-data-processing-at-pathway-3887662141?position=2&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=ikwLbQi4nhCOigonivq9UA%3D%3D&trk=public_jobs_jserp-result_search-card,"About Pathway
Deeptech start-up, founded in March 2020.
Our primary developer offering is an ultra-performant Data Processing Framework (unified streaming + batch) with a Python API, distributed Rust engine, and capabilities for data source integration & transformation at scale (Kafka, S3, databases/CDC,...)
The single-machine version is provided on a free-to-use license (`pip install pathway`)
Major data use cases are around event-stream data (including real-world data such as IoT), and graph data that changes over time
Our enterprise offering is currently used by leaders of the logistics industry, such as DB Schenker or La Poste, and tested across multiple industries. Pathway has been featured in Gartner's market guide for Event Stream Processing
Learn more at http://pathway.com/ and https://github.com/pathwaycom/
Pathway is VC-funded, with amazing BAs from the AI space and industry. We have operations across Europe and in the US. We are headquartered in Paris, with significant support from the French ecosystem (BPI, Agoranov, WILCO,...).
The Team
Pathway is built by and for overachievers. Its co-founders and employees have worked in the best AI labs in the world (Microsoft Research, Google Brain, ETH Zurich), worked at Google, and graduated from top universities (Polytechnique, ENSAE, Sciences Po, HEC Paris, PhD obtained at the age of 20, etc...). Pathway's CTO is a co-author with Goeff Hinton and Yoshua Bengio. The management team also includes the co-founder of Spoj.com (1M+ developer users) and NK.pl (13.5M+ users) and experienced growth leader who has scaled companies with multiple exits.
The opportunity
We are searching for a person with a Data Processing or Data Engineering profile, willing to work with live client datasets, and to test, benchmark, and showcase our brand-new stream data processing technology.
The end-user of our product are mostly developers and data engineers working in a corporate environment. Our development framework is one day expected to become for them a part of their preferred development stack for analytics projects at work - their daily bread & butter.
You Will
You will be working closely with our CTO, Head of Product, as well as key developers. You will be expected to:
Implement the flow of data from their location in client's warehouses up to Pathway's ingress
Set up CDC interfaces for change streams between client data stores and i/o data processed by Pathway; ensuring data persistence for Pathway outputs
Design ETL pipelines within Pathway
Contribute to benchmark framework design (throughput / latency / memory footprint; consistency), including in a distributed system setup.
Contribute to building open-source test frameworks for simulated streaming data scenarios on public datasets
Requirements
Inside-out understanding of at least one major distributed data processing framework (Spark, Dask, Ray,...)
6 months+ experience working with a streaming dataflow framework (e.g.: Flink, Kafka Streams or ksqldb, Spark in streaming mode, Beam/Dataflow)
Ability to set up distributed dataflows independently
Experience with data streams: message queues, message brokers (Kafka), CDC
Working familiarity with data schema and schema versioning concepts; Avro, Protobuf, or others
Familiarities with Kubernetes
Familiarity with deployments in both Azure and AWS clouds
Good working knowledge of Python
Good working knowledge of SQL
Experienced in working for an innovative tech company (SaaS, IT infrastructure or similar preferred), with a long-term vision
Warmly disposed towards open-source and open-core software, but pragmatic about licensing
Bonus Points
Know the ways of developers in a corporate environment
Passionate about trends in data
Proficiency in Rust
Experience with Machine Learning pipelines or MLOps
Familiarity with any modern data transformation workflow tooling (dbt, Airflow, Dagster, Prefect,...)
Familiarity with Databricks Data Lakehouse architecture
Familiarity with Snowflake's data product vision (2022+)
Experience in a startup environment
Benefits
Why You Should Apply
Intellectually stimulating work environment. Be a pioneer: you get to work with a new type of stream processing framework
Work in one of the hottest data startups in France, with exciting career prospects
Responsibilities and ability to make significant contribution to the company' success
Compensation: annual salary of €60K-€100K + Employee stock option plan.
Inclusive workplace culture
Further details
Type of contract: Permanent employment contract
Preferable joining date: early 2023
Compensation: annual salary of €60K-€100K + Employee stock option plan
Location: Remote work from home. Possibility to work or meet with other team members in one of our offices:
Paris - Agoranov (where Doctolib, Alan, and Criteo were born) near Saint-Placide Metro (75006)
Paris Area - Drahi X-Novation Center, Ecole Polytechnique, Palaiseau
Wroclaw - University area
Candidates based anywhere in the EU, United States, and Canada will be considered.
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks', 'Flink'], 'MachingLearning': [], 'DataSerialization': ['Avro'], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['ML', 'Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['60K'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer H/F,ALFI : Financial Markets Consultancy Services,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-alfi-financial-markets-consultancy-services-3916559634?position=3&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=gTjggiRztds529DCE7H89w%3D%3D&trk=public_jobs_jserp-result_search-card,"ALFI est une société de conseil et services spécialisée en systèmes d’information. Depuis plus de 20 ans, ALFI est un acteur unique qui mêle technologie et humain pour accompagner les transformations numériques sur les marchés de l’Asset Management, la banque d’Investissement et les Services aux Investisseurs.
Avec plus de 46 référencements de rang un, ALFI est reconnu comme incontournable sur le secteur BFA. Nous avons plus de 35 clients grands comptes actifs tels que HSBC, Société Générale, BNP Paribas, Crédit Agricole, Axa…
Depuis 2015, ALFI a intégré le groupe
MoOngy
, qui compte plus de 6000 salariés dans toute l’Europe
Missions :
Pour l'un de nos clients grands comptes, nous vous proposons d'intervenir sur une fonction de Consultant Data engineer.
Les principales missions sont :
Comprendre les besoins des utilisateurs et les traduire de manière analytique
Développement de solutions permettant de traiter des volumes importants de données
Conception, collection et fabrication des données brutes
Développer des algorithmes permettant de répondre aux problèmes posés et veiller à leur industrialisation
Sécurisation des Pipelines données pour les Data Scientists et les Data Analysts
Construire des bases de données robustes
Organisation de l’architecture du cloud
Profil recherché :
Vous êtes issu d'une formation Bac +5 Ecole scientifique ou informatique.
Vous disposez d'une première expérience en développement et dans la data.
Vous disposez d'un niveau d'anglais opérationnel.
Java, Python, C++
SQL
Devops (Jenkins, Kubernetes, Docker)
Conformément à la règlementation, et à notre politique d’égalité professionnelle, tous nos postes sont ouverts aux personnes en situation de handicap;
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'C++', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Docker', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': [], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '20', '20', '20']}"
LinkedIn,Data Engineer,MindPal,"Marseille, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-at-mindpal-3896991761?position=4&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=mK%2Fm3m7kwlYQFk2YmDT9sA%3D%3D&trk=public_jobs_jserp-result_search-card,"We are looking for
Data Engineer!
Responsibilities
Designing, creating, and maintaining data processing systems
Analyzing and optimizing data processing workflows
Collaborating with the team to ensure data quality and efficiency
Testing and implementing new solutions
Requirements
At least 2 years of experience in designing and creating data processing systems
Proficiency in tools and programming languages related to data engineering (e.g. Hadoop, Spark, Scala, Python)
Excellent knowledge of databases and SQL language
Ability to work in a team and communicate effectively with other departments
Communicative English skills
Experience with AWS/AWS Glue is a plus
We Offer
B2B contract
Full-time job
Remote work and flexible hours
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote', 'Full'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer  H/F,METEOJOB by CleverConnect,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-meteojob-by-cleverconnect-3895135654?position=5&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=G%2BraCwgpd5v46E1jI26QuA%3D%3D&trk=public_jobs_jserp-result_search-card,"Entreprise
Qui sommes-nous ?
Nous sommes passionnés par les nouvelles technologies, et vous ?
Rejoindre Amiltone, c'est intégrer des équipes dynamiques et soudées dans le cadre de projets novateurs et ambitieux. Nous relevons les challenges techniques de nos clients et les accompagnons dans leur transformation digitale.
Pourquoi choisir Amiltone ?
Amiltone, plus qu'une entreprise, un état d'esprit !
Notre objectif ? Votre épanouissement professionnel !
Nous Avons à Cœur De
Vous accompagner au mieux au travers d'un suivi personnalisé
Vous faire monter en compétences en vous proposant des formations tout au long de votre carrière
Comprendre vos besoins et respecter nos engagements
Vous proposer des missions de qualité avec des technologies innovantes
Cultiver votre potentiel grâce à notre programme de développement personnel Addvise
Votre bien-être passe aussi par des activités extraprofessionnelles, c'est pourquoi nous vous proposons des séances sportives animées par nos coachs, soirées pour se retrouver et animations (à l'agence ou en visio), Gaming nights...
Description Du Poste
Les missions d'un Amiltonien :
En tant que Data Engineer
(H/F)
, vous serez en charge des missions suivantes :
Concevoir et développer les futures fonctionnalités de la plateforme Big Data sous Google Cloud Platform.
Concevoir les flux d'alimentation et les tables (structure de donnée).
Automatiser et industrialiser les flux.
Assurer le run applicatif, le cas échéant.
La Stack Technique
Maîtrise des langages suivants : SQL, Talend, BigQuery
Connaissances de Google (GCP)
Notion de programmation fonctionnelle
Description Du Profil
Le profil d'un Amiltonien :
Diplômé Bac+4/5 (Ecole d'ingénieur/Master), vous disposez de 2 années d'expérience dans le développement de data.
Toujours sur le qui-vive des nouveautés technologiques, vous êtes force de proposition sur des technos, des outils ou des process qui permettent d'améliorer la qualité du code et la stabilité de nos applications.
Outre vos compétences techniques, nous nous intéressons également à votre potentiel et votre motivation.
Nos postes sont ouverts aux personnes en situation de handicap.
Show more
Show less","{'ProgLanguage': ['R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Senior Data Engineer - Valbonne,Capgemini,"Valbonne, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/senior-data-engineer-valbonne-at-capgemini-3888072146?position=6&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=MuhVomcbD2tDd3DYDYZk7w%3D%3D&trk=public_jobs_jserp-result_search-card,"Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une communauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.
Vos missions :
En tant que Senior Data Engineer au sein d'une équipe multidisciplinaire, vos responsabilités principales seront les suivantes :
Participer à des ateliers clients.
Acquérir des données et optimiser le stockage.
Créer de flux de données optimisés et élaborer des algorithmes de transformation.
Traiter et analyser pour la visualisation et le machine learning.
Encadrer des ingénieurs juniors et contribuer à la communauté Data.
Votre profil :
Vous possédez un diplôme d'ingénieur informatique et/ou Master avec une spécialité data.
Vous parlez couramment français et anglais.
Vous possédez au minimum 6 ans d'expérience sur un rôle similaire.
Vous maîtrisez les outils Spark, Python, Scala ainsi qu'une bonne compréhension des systèmes d'extraction, de transformation et de changement (ETL).
Vous avez un certain leadership et un esprit d'équipe, idéalement dans un cadre agile.
3 raisons de nous rejoindre :
Qualité de vie au travail :
accord de télétravail en France et à l'international, accord sur l'égalité professionnelle, la parentalité, l'équilibre des temps et la mobilité durable.
Apprentissage en continu
: certifications et formations en libre accès, accompagnement sur mesure avec votre carrer manager, parcours d'intégration sur 9 mois.
Avantages groupe & CSE :
plan actionnariat, activités à tarifs préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.
Nos engagements et priorités :
Le groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s'engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.
A Propos de Capgemini :
Capgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.
Get The Future You Want* | www.capgemini.com/fr-fr
*Capgemini, le futur que vous voulez
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': ['Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Cloud'], 'FrSoftSkills': ['Leadership', 'Organisation'], 'EnSoftSkils': ['Leadership', 'Initiative']}","{'JobDetail': ['Junior', 'Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '6', '6', '6']}"
LinkedIn,Alternance Data Engineer - Lille (F/H),METEOJOB by CleverConnect,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/alternance-data-engineer-lille-f-h-at-meteojob-by-cleverconnect-3901971331?position=7&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=5Xzo%2B%2FEkXS0PjKMcS6ocHQ%3D%3D&trk=public_jobs_jserp-result_search-card,"Entreprise
L'ISCOD, spécialiste de la formation en Digital Learning, recherche pour son entreprise partenaire, une ESN agile et un groupe international, son Data Engineerq, en contrat d'apprentissage, pour préparer l'une de nos formations diplômantes reconnues par l'Etat de niveau 5 à niveau 7 (Bac+2, Bachelor/Bac+3 ou Mastère/Bac+5).
Optez pour l'alternance nouvelle génération avec l'ISCOD !
Description Du Poste
Durant cette alternance, tu auras l'opportunité de :
Collaborer avec les équipes pour définir les besoins ;
Organiser et traiter le flux de données quotidien ;
Effectuer la visualisation des données ;
Organiser, synthétiser l'information
Livrer les résultats (rapports, présentations, tableaux de bord...) ;
Description Du Profil
Étudiant en dernière année, tu suis une formation axée sur le Data Engineering et tu recherches une alternance.
Tu as une grande appétence pour le domaine de la Data et tu maîtrises un des langages de programmation suivants: SQL, Python, Java …
Tu connais le fonctionnement des ETL et les outils de visualisation, notamment Power BI et des gestion de données, notamment de MS Excel.Poste basé à LilleRémunération selon niveau d'études + âge
Formation prise en charge à 100% par l'entreprise
Ce poste vous intéresse ? Envoyez vite votre candidature !
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau', 'Power BI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': ['Bac+5', 'Bac+3'], 'Experience': ['a', 'n', 's']}"
LinkedIn,ALTERNANT DATA ENGINEER (H/F),METEOJOB by CleverConnect,"Pont-à-Mousson, Grand Est, France",https://fr.linkedin.com/jobs/view/alternant-data-engineer-h-f-at-meteojob-by-cleverconnect-3868016363?position=8&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=hZRnMmL0q03FEFk%2BycezAg%3D%3D&trk=public_jobs_jserp-result_search-card,"Entreprise
Saint-Gobain
conçoit, produit et distribue des matériaux et des solutions pensés pour le bien-être de chacun et l'avenir de tous. Rejoignez une communauté innovante, passionnée et entreprenante pour améliorer le monde de demain.
Acteur mondial de référence et leader européen de solutions complètes de canalisation en fonte ductile,
Saint-Gobain PAM Canalisation
conçoit, produit et commercialise un éventail complet de solutions dédiées au
transport de l'eau
.
Saint-Gobain PAM Canalisation dispose dans la région Grand-Est, à Maidieres, d'un
Centre de Recherche
unique dont les compétences et le savoir-faire contribuent à apporter à nos clients des
solutions innovantes
et à valeur ajoutée dans le
contrôle et la gestion patrimoniale de l'eau.
Description Du Poste
L'entreprise Saint-Gobain PAM Canalisation, spécialisée dans le développement, la production et la vente d'équipements de canalisation (tuyaux, robinetteries et regards de voirie) recrute pour son Siège à PONT-A-MOUSSON :
UN ALTERNANT DATA ENGINEER (H/F)
Intégré.e Aux Équipes De La Direction Des Systèmes D'Information PAM DIGITAL & IT Et Rattaché.e à Son Domaine DIGITAL TECHNOLOGIES, Vous Serez Notamment Amené(e) à
Identifier les sources des données et mettre en place leur intégration dans SnowFlake via notre Cloud AZURE,
Concevoir et modéliser des datawarehouses et des data hubs en fonction des Use Cases sur lesquels vous travaillerez,
Restituer des données par visualisation (avec PowerBI) ou APIsation (avec Microsoft Data Factory),
Réaliser des projets sur des technologies innovantes,
Monitorer et maintenir des plateformes d'échanges hébergeant ces flux.
Contrat d'alternance
Le poste est basé à Pont-à-Mousson ( à 30km de Nancy et Metz )
Description Du Profil
Vous préparez un Master (Bac+ 5),
Vous êtes passionné par le Big data, la Business intelligence, la Valorisation de données,
Vous avez des bases concernant les langages : Python , SQL et Java (Spring Batch) ainsi que les bases de données SQL,
Vous connaissez Snowflake,
Vous avez envie de faire de la DataViz avec PowerBI.
Vous êtes reconnu·e par vos collègues pour :
Votre aptitude à aller vers les autres, communiquer et écouter
Votre capacité à travailler en équipe, dans un environnement international
Votre rigueur, et votre autonomie.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Analytics Engineer,Vestiaire Collective,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/analytics-engineer-at-vestiaire-collective-3875996826?position=9&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=aKEX6pSYETA3IWaU83cDlg%3D%3D&trk=public_jobs_jserp-result_search-card,"Vestiaire Collective is the leading global online marketplace for desirable pre-loved fashion. Our mission is to transform the fashion industry for a more sustainable future by empowering our community to promote the circular fashion movement. Vestiaire was founded in 2009 and is headquartered in Paris with offices in London, Berlin, New York, Singapore, Ho Chi Minh and Hong Kong and warehouses in Tourcoing (France), Crawley (UK), Hong Kong and New York.
We currently have a diverse global team of 700 employees representing more than 50 nationalities. Our values are Activism, Transparency, Dedication and Greatness and Collective. We are proud to be a BCorp.
About The Role
This role is central to our data strategy and requires a balance of technical expertise and business acumen. As a Junior Analytics Engineer, you will be at the heart of our data-driven initiatives, working closely with cross-functional teams to transform raw data into a single source of truth data mart. Your work will directly influence key decisions in finance, payment systems and business performance.
What You'll Do
Design, implement, and maintain efficient and reliable data pipelines using a modern data stack: Airflow, Snowflake, DBT
Develop advanced data models to support complex analytics, including financial reconciliations, cost effectiveness and profitability models. Collaborate with finance, payments and tech teams to understand their data requirements and translate these into sophisticated technical solutions
Ensure scalability and performance of our data infrastructure to handle large-scale, multi-faceted data sets from diverse sources
Implement and maintain data quality checks and monitoring systems for accuracy and consistency
Innovate and integrate new technologies and methodologies to enhance data capabilities across finance domains
Assist the finance team in building key dashboards in Tableau to enable data driven decision making
Who You Are
Required Qualifications:
Bachelor’s/Master’s in Computer Science, Engineering, Statistics, or related field
At least one previous experience in analytics engineering, with strong skills in ETL and data modeling, an awareness of data warehousing and dataOps practices
Proficient in SQL and programming languages like Python or R
Experience with cloud data technologies and big data tools
Desirable Skills:
Apache Airflow: an understanding of workflow management
Git: Solid knowledge in version control and CI/CD integration
Cloud Service: AWS, Snowflake or similar cloud experience
Data Visualization Tools: Proficiency in tools like Tableau, Looker, Snowsight
Previous experience in DBT for data modeling
What we offer
🎁
A meaningful job with an impact on the way people consume fashion and promote sustainability
Flexible work possibilities
The opportunity to do career-defining work in a fast-growing French-born scale up
The possibility to work as part of a globally diverse team with more than 50 nationalities
Two days to help Project - reinforcing your activist journey and volunteer for an association
Significant investment in your learning and growth
Competitive Compensation And Benefits Package
As full member of our entrepreneurial project, you will be eligible to free shares
Vestiaire Collective is an equal opportunities employer
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Apache Airflow'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': ['Statistics'], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['Snowflake', 'Snowflake'], 'SoftBigDataProcessing': [], 'Automation': ['Airflow'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['Teams'], 'Other': ['Big Data', 'Cloud', 'CI/CD'], 'FrSoftSkills': [], 'EnSoftSkils': ['Initiative']}","{'JobDetail': ['Full', 'Junior'], 'TypeContract': [], 'Salary': ['Package'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data engineer,Harnham,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-at-harnham-3860004924?position=10&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=8Jke7%2FnB8H6WLoCi6KALqw%3D%3D&trk=public_jobs_jserp-result_search-card,"Data engineer
Minimum 2 ans
Paris
2j tt
CDI
UP TO 55k€
Rejoignez une équipe dynamique au cœur de l'innovation dans le domaine du retail !
Stack technique : Python, Pandas, SQL, Tableau
Vous utiliserez votre connaissance du développement et de la testabilité pour améliorer la conception, promouvoir des décisions d'ingénierie précises et mettre en œuvre des stratégies de prévention des bogues à la fois évolutives et maintenables dans ce ensemble de responsabilités clés :
Coopérer avec divers départements tels que la Recherche et le Développement, l'Infrastructure, l'Ingénierie et le Backend pour garantir la qualité et la ponctualité des livraisons de produits.
Élaborer et mettre en œuvre des scénarios de test, à la fois manuels et automatisés, pour les applications logicielles afin d'assurer la fiabilité des pipelines de données, des processus ETL et des transformations de données.
Concevoir et automatiser des tableaux de bord de qualité pour surveiller en continu la qualité des données.
Effectuer des tests fonctionnels, d'intégration, de régression et de performance des systèmes de bases de données en utilisant des technologies standard de l'industrie telles que SQL, Python, etc.
Créer et maintenir la documentation détaillée des plans de test, des cas de test et des résultats des tests.
Contribuer activement au succès du déploiement européen en assurant la fiabilité et la qualité des produits livrés.
Votre profil :
Solide expérience dans le développement et les tests de logiciels, avec une compréhension approfondie des pipelines de données, des processus ETL et des transformations de données.
Maîtrise des technologies standard de l'industrie telles que SQL, Python, etc., avec une capacité avérée à élaborer et exécuter des cas de test manuels et automatisés.
Compétences avancées en matière de surveillance et de garantie de la qualité des données, y compris la création et l'automatisation de tableaux de bord de qualité.
Capacité à travailler efficacement en collaboration avec diverses équipes, y compris la Recherche et le Développement, l'Infrastructure et le Backend, pour garantir la qualité et la ponctualité des livraisons de produits.
Excellentes compétences en communication et en documentation, avec une capacité à maintenir des rapports détaillés des plans de test, des cas de test et des résultats des tests.
Intéressé(e) ? Postulez !
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['Pandas', 'R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Tableau'], 'Statistics': [], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': [], 'FrSoftSkills': ['Communication', 'Collaboration'], 'EnSoftSkils': ['Communication', 'Collaboration']}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
LinkedIn,Data Engineer | ML Engineer | Up to 75k | Boulogne,Talent-R,Greater Paris Metropolitan Region,https://fr.linkedin.com/jobs/view/data-engineer-ml-engineer-up-to-75k-boulogne-at-talent-r-3904965048?position=1&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=goHKTsw2R1FcHVw1qNEXpg%3D%3D&trk=public_jobs_jserp-result_search-card,"📍
Localisation
: Région Parisienne & Remote Flexible (60%) - CDI
🔍
Seniorité
: Mid/Senior (>4/5 years of Data)
💰
Salaire
: Up to 75K€ fixe + Prime de participation, prime vacances et bonus...
L'entreprise
💼
Le groupe entre dans une nouvelle ère grâce à la stratégie qui place
L’IA
au cœur du business. Acteur incontournable de ce nouveau cycle ils participent activement à relever les challenges des
nouvelles mobilités et de l’industrie 4.0.
Pôle Architecture et Data :
L'objectif est de mettre en place les bases de
la plateforme IA
afin de répondre aux nouveaux besoins métiers.
Les missions
⚙️
Dans ce rôle, vous travaillerez en étroite collaboration avec les équipes métiers et les autres membres du Pôle Architecture & Data (Data Analysts, Scientists, architectes, etc.), en exploitant des quantités massives de données (flux d'événements en continu, traitements par lots et en temps réel, ainsi que les appels aux APIs).
L'objectif est notamment d'alimenter des modèles d'apprentissage automatique pour des tâches telles que la segmentation des clients et la détection automatique des pannes des véhicules.
Les avantages
😍
Variable de 6% (Objectifs individuels / Performance du groupe)
Prime intéressement (à peu près un mois de salaire)
Prime vacances (1% de salaire annuel)
Tarif préférentiels achats de véhicules
Avantage CE (200 - 400€ de chèques cadeaux)
Télé travail : 3 jours / semaine
Matériel IT + participation frais d’internet
10 jours de RTT
Le stack technique
👉🏻
Google Cloud Platform (BigQuery, PubSub, Dataflow, Vertex AI)
Airflow
Terraform
Python
Looker
Dataiku
Kubernetes, SQL, Git
Postulez si et seulement si
Vous disposez d'au moins
5
ans
d’expérience en data
Vous disposez d’une solide expérience en développement
Python
et
framework ML
(Vertex, Tensorflow, Scikit, PyTorch…)
Vous possédez une expérience de développement et orchestration de chaines ETL complexes via
Airflow
ou équivalent
Vous savez utiliser des services cloud (préférablement
GCP
)
Vous êtes capable d’échanger en
anglais
technique écrit et oral
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': ['TensorFlow', 'PyTorch'], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['Google Cloud Platform'], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes', 'Airflow'], 'InfrastructureAsCode': ['Terraform'], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Collaboration'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Remote', 'Senior'], 'TypeContract': ['CDI'], 'Salary': ['75K'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer,StackEase,"Marseille, Provence-Alpes-Côte d'Azur, France",https://fr.linkedin.com/jobs/view/data-engineer-at-stackease-3906618983?position=2&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=CTOOVDi7B%2FDx%2BYw6tjYbRA%3D%3D&trk=public_jobs_jserp-result_search-card,"Context :
It is challenging in many ways to develop a net zero electricity grid. Adding renewable energies means more need for storage and for grid balancing. Grid-scale batteries handle these issues but their operation is not quite simple.
A battery has access to many different revenue streams, each being specific. No revenue alone is enough to make it economically viable. All revenues stacked and optimized together ensure profitability in the long run and a fast deployment.
StackEase’s ambition is to develop optimization and trading algorithms that merge the battery revenue streams and ensure their sustainable development. The final tool will be an autopilot that makes the best decisions in real time, controls the battery accordingly and sends the corresponding orders to  the market.
About StackEase:
StackEase is a deeptech spinoff from the INRIA (French Institute for Research in Computer Science). It was created in August 2023 and secured its first fundings. Members are located in Marseille and Paris.
Our values are innovation, customer satisfaction, merit and sustainability. The company's purpose is to leverage Machine Learning and Mathematical Optimization to accelerate the energy transition.
Missions :
Define and develop the backend architecture of StackEase
Set up databases and data pipelines collecting battery and market data
Deploy and maintain optimisation algorithms and forecasts
Develop a robust and scalable SaaS platform for 24/7 battery management with high cyber standards
Participate in the UI/UX product definition
Skills Wishlist :
Scientific BS/MS/PhD with 2+ years of experience in software engineering
Experience with the common backend tools: Python, Git, Kubernetes, SQL/NoSQL … Knowledge of the AWS environment is a plus
Enthusiastic, rigorous, autonomous and willing to be involved in major technical decisions
Knowledge/Interest in the energy sector and ancillary services
Compensation :
45k€ - 60k€ salary range (incl. healthcare, unemployment rate, vacations, …)
Flexible remote work policies
You do not need to meet 100% of the requirements to apply, we will study all applications: please send your resume to jobs@stackease.fr. References and a cover letter are also welcome but not mandatory.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': [], 'Other': ['Machine Learning'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': ['Remote'], 'TypeContract': [], 'Salary': ['100'], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer Intern,Kyriba,"St.-Cloud, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-intern-at-kyriba-3886667187?position=3&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=hDnXaxqe0%2FKcc1Uyp5ctjw%3D%3D&trk=public_jobs_jserp-result_search-card,"It's fun to work in a company where people truly BELIEVE in what they're doing!
We're committed to bringing passion and customer focus to the business.
Kyriba is the global leader in cloud-based Enterprise Liquidity Management management solutions, delivering Software-as-a-Service (SaaS) financial technology to corporate CFOs and Treasurers. More than 2,600 global organizations (including Spotify, Ripple, Adecco, Auchan, Adobe, EuropCar, Eurostar International, Expedia, Electronic Arts and Takeda) use Kyriba to enhance their global cash visibility, improve financial controls, and increase productivity across their cash and liquidity, payments, supply chain finance and risk management operations
.
Being an intern at Kyriba means more than simply getting involved in the day-to-day operations of the company. It is an opportunity to introduce you to your potential future work environment so that you can better decide if it meets your career goals
We are looking for an enthusiastic
Data Engineer Intern
to join our Machine Learning Platform team, which is part of our Engineering Department. The main goal of this internship is to participate in building and delivery of Kyriba ML Platform. For this internship, you are going to work with the engineering manager, senior developers of your team, and also QA team.
Keywords: Data, Python, Java, SQL, Git, Machine Learning (ML), Containers, Orchestration, Kubernetes.
Requirements
Solid understanding of basics in Python and Java programming languages
Knowledge of SQL
Intermediate (at least) level of English
Ability to work in team
Motivation to learn new technologies and tools
Knowing ML fundamentals will be a plus but is not strictly required
Understanding and basic knowledge of Cloud and Cloud Platforms is also a plus
Responsibilities
Work as a part of our development team to build and deliver the Kyriba ML Platform
Focus on different aspects of the software development and operation lifecycle (designing, coding, test coverage, etc)
Participate in all the development-related ceremonies (Pull Requests review, SCRUM activities, grooming sessions, etc)
Learning Opportunities
Hands-on experience in designing and building cloud-native solutions using microservices-based architecture
Discover the world of Machine Learning from the developer’s perspective and exposure to industry-leading tools and technologies in this domain
Possibility to deep dive in containers’ orchestration
Insight into the intricacies of the Treasury Management, specifically in a Working Capital business domain
Benefits
Mentorship from experienced professionals in the area
Practical experience within an international SaaS provider leveraging public cloud solutions
Gain practical experience in utilizing Jira for project management
Flexibility in work hours to accommodate academic commitments
Networking opportunities within the organization
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Kubernetes'], 'Collaboration': ['JIRA'], 'Other': ['ML', 'Machine Learning', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': ['Organization', 'Flexibility']}","{'JobDetail': ['Senior'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Data Engineer BI,Listen too,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-bi-at-listen-too-3903073048?position=4&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=Rx7XexD69MeYfSqLD3%2ByeA%3D%3D&trk=public_jobs_jserp-result_search-card,"📢 Imaginez un lieu où votre voix compte autant que vos compétences…
Chez Listen too, nous sommes convaincus que l’écoute est indissociable du progrès. Cultivée au quotidien, elle devient un catalyseur de croissance et de réussite, tant pour l’entreprise que pour ses collaborateur·rice·s.
Listen too est une agence conseil spécialisée depuis 2013 dans la digitalisation de la Relation Client & Collaborateur. Grâce à la justesse de l’accompagnement offert à nos clients comme à nos consultant·e·s, nous nous sommes construits au fil des années sur notre marché une réputation de véritables « horlogers du conseil ».
Nos domaines d’expertise ?
🎯 Le Product Management : pour porter la vision Produit et maximiser la création de valeur.
🎨 Le Product Design : pour une conception Produit centrée utilisateur.
🔧 Le Product TechOps : pour déployer les meilleures solutions digitales.
📢 Le Product Marketing : pour maximiser le succès d’un produit/service sur son marché.
📊 Le Product Data : pour mettre la donnée au cœur de la stratégie de nos clients.
🤝 La Gouvernance projet : pour optimiser le mode de management et le cadre organisationnel de nos clients.
Être listenien·ne, c’est être au cœur d’une entreprise qui valorise votre voix, votre parcours, et votre évolution. Ici, nous appliquons les principes du Design Thinking à votre carrière, co-construisant avec vous une trajectoire où vos expertises, vos aspirations, et vos forces sont au premier plan.
💪 Vos futures missions
Nous recherchons un Data Engineer qui aura la capacité de gérer à la fois les aspects MCO/TMA mais également la partie projets pour les différentes directions du groupe en collaboration avec le reste de l’équipe.
À la suite d’une passation avec notre centre de service, vous devrez également être en mesure d’innover et d’améliorer les processus existants pour perfectionner l’efficience de l’équipe au quotidien dans ses différentes tâches.
Il s'agit d'un poste pour de l'internalisation.
Concrètement, vous aurez l’opportunité de :
• Participer à l’analyse du besoin avec la supervision du PPO ou Manager et à la conception du modèle de données permettant de répondre aux enjeux.
• Utiliser l’analyse des données pour fournir des éléments significatifs et éclairer la conception/mise en œuvre des projets
• Concevoir, développer et maintenir les pipelines de données pour l’acquisition, la transformation, le stockage et la mise à disposition des données.
• Optimiser les performances des développements pour assurer une disponibilité et une fiabilité maximale
• Identifier et résoudre les goulots d’étranglement et les problèmes de performances dans les flux de données
• Mettre en place des processus et des contrôles pour garantir la qualité des données
• Concevoir et mettre en œuvre des systèmes d’alerte, de monitoring et de reprise automatique efficaces.
• Réaliser la recette technique et animer la recette fonctionnelle avec les utilisateurs.
• Rédiger des documents associés au projet (Spécification Fonctionnelles, Spécifications Techniques, Cahier de recette Technique, Document d’exploitation).
• D’être force de proposition sur l’amélioration de notre stack Data.
• Faire le reporting d’avancement des travaux.
• Support au déploiement.
• Assurer la maintenance corrective ou évolutive.
• S’assurer de la cohérence des livrables avec les règles et bonnes pratiques définies au sein de l’équipe.
• S’assurer L’environnement Data dans le cadre de cette mission est actuellement constitué principalement d’une base de données Teradata (Cloud) ainsi qu’un environnement BigQuery.
• Langages/Framework : SQL, BigQuery, Python, Java, Shell
• Outils : OpenText, Talend
• Base de données : Teradata, BigQuery, SQL Server, IBM DB2
😎 Votre vie de listenien·ne
Ce qu’on vous propose ?
🤜 Être membre d’une communauté : partager et enrichir vos compétences au sein d’une équipe soudée.
🤝 Être accompagné·e : dans votre progression professionnelle et votre quotidien par votre Consultant Manager, votre Business Manager et notre Responsable Expérience collaborateur·rice.
🎓 Être formé·e en continu : grâce à un plan de développement des compétences co-construit et nourri par nos solutions d’e-learning, les formations de la Listen too Academy et nos formations externes.
🚀 Être intrapreneur·e : encouragé à innover, échanger, entreprendre et ainsi contribuer à l’épanouissement du cabinet.
☘️ Être engagé·e : à nos côtés dans une démarche sociétale et environnementale responsable et durable.
🤗 Être « bien » ! : parce que le bien-être physique et mental de nos collaborateur·rice·s est au cœur de notre réussite.
Concrètement ?
🎉 De multiples occasions de passer de bons moments avec notamment notre fameux sémineige, des afterworks et des teambuildings.
🏋️‍♂️ Du sport et de la santé avec des évènements sportifs, nos partenaires
Gymlib
,
Zenride
,
Moka.care
et le programme
Vitality
.
😎 Du confort avec du télétravail indemnisé.
🤗 Toujours plus liens avec un onboarding aux petits oignons, notre programme de parrainage, notre appli interne Mylistentoo, nos newsletters, nos podcasts et nos webinaires.
👍 De nombreux avantages avec notre CSE, des tickets restaurant, une prime vacances, des primes de cooptation, de participation, de développement…
🚀 Profil recherché
Vous avez au minimum 5 ans d’expérience.
Vous disposez d’une expérience confirmée chez un Grand Compte.
Vous maitrisez l’anglais dans un contexte professionnel tant à l’écrit qu’à l’oral.
Vous avez envie de progresser et d’évoluer au sein d’une société où votre voix compte.
Vous rêvez de vous investir au sein d’une communauté soudée et passionnée !
Déroulement des entretiens ?
🙋🏼‍♀️ Un premier entretien téléphonique ou en visio avec l'une de nos chargées de recrutement (Ines, Aurélie ou Yasmine) pour voir si ça colle entre nous !
💁🏻‍♀️ Un deuxième en visio ou en présentiel avec Aurélien, notre Directeur de Région ou un de nos Business Manager (Lauranne, Hugo), pour valider notre première impression.
🙋🏻‍♂️ Un dernier échange sur mesure avec Florent, Co-fondateur de l'agence, pour confirmer ce que nous savions déjà ! 😉
Vous l’aurez compris, cette phase de recrutement est avant tout l’occasion de s’écouter et d’échanger. Chez Listen too, nous célébrons la diversité et l’inclusion, convaincus que chaque talent, quelle que soit son origine ou son parcours, est une richesse pour notre équipe et contribue à notre réussite commune.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['SQL Server', 'BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['ML', 'Cloud'], 'FrSoftSkills': ['Collaboration', 'Organisation'], 'EnSoftSkils': ['Collaboration']}","{'JobDetail': ['Confirmé'], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data engineer python,FINAXYS,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-python-at-finaxys-3887107285?position=5&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=f3RCu7zpUVWESqBI4Y3PBg%3D%3D&trk=public_jobs_jserp-result_search-card,"LE CONTEXTE
Leader en
IT
, dans les domaines Banque
,
Finance
et
Assurance
,
Finaxys
est un cabinet de
conseil
créé en 2008. Nous accompagnons au quotidien les plus grandes banques du pays sur leur
transformation digitale
(BNP Paribas, Société Générale, Crédit Agricole, Natixis, etc.)
Nos clients bancaires travaillent également dans des contextes Big Data sur des applications centrales rattachées aux Datalakes.
LES MISSIONS
Développement et traitements sur des applications Big Data (Python)
Être force de proposition sur les choix techniques les plus pertinents
Maintenir la qualité des solutions, mesure de cette qualité, alerte sur les non-conformités et validation des solutions définitives.
Analyser des risques liés aux solutions envisagées et proposition des actions de remédiation.
Apporter des solutions IT répondant au mieux aux besoins du business porté par la/le Product Owner (Métiers/Fonctions) en cherchant toujours la maximisation de la valeur générée
Accompagner les équipes dans les migrations Cloud
ENVIRONNEMENT TECHNIQUE
Python
Pandas
Scirpting Big Data
Culture DevOps (Jenkins, Maven, Ansible)
PROFIL
Compétences Techniques et Fonctionnelles requises
Maitrise obligatoire de l’anglais
Show more
Show less","{'ProgLanguage': ['Python', 'R'], 'DataBase': [], 'DataAnalytics': ['Pandas', 'R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': ['Ansible'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps', 'Big Data', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's']}"
LinkedIn,Alternant - Data Engineer H/F,ALLIANCE EMPLOI,"La Couture, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/alternant-data-engineer-h-f-at-alliance-emploi-3913991795?position=6&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=IUN5rmntBuBv0NURN0B9ZA%3D%3D&trk=public_jobs_jserp-result_search-card,"La campagne "" alternance2024 "" est lancée ! Êtes-vous prêt(e) à monter en compétences et acquérir de l'expérience ? Avec 25 ans d'expertise, 2000 salariés et notre réseau de 400 entreprises issues des secteurs industriel, agroalimentaire, automobile, métallurgie, pharmaceutique, sidérurgie ou encore logistique, nous sommes la destination idéale pour celles et ceux qui cherchent une alternance.
Lauréat 2021 des Pépites de l'alternance, notre mission est simple : apporter la bonne compétence au bon moment. Et c'est là que vous entrez en jeu !
Nous sommes à la recherche d'un Alternant - Data Engineer (H/F) en alternance pour une entreprise partenaire spécialisée dans la chimie pour une durée de 12 à 24 mois au sein du Utilités qui a en charge la fourniture d'utilités pour l'ensemble du site.
Le poste et les missions ?
Informatique
Modernisation des pratiques de collecte de données industrielles,
Création d'interface saisie d'encours de production
Refonte des rapports d'exploitation
Algorithme de numérisation des rapports PDF
Automatisation des archivages
Gestion des datas consolidées
Statisitques
Intégration des statistiques au coeur des systèmes de production
Cartes de contrôles
Prévisions statistiques
Etude d'implantation de machine learning
Algorithme de traitement et nettoyage des données (analyse de la dérive)
Organisationnel
Déploiement des solutions de power BI au sein du service
Outils d'affiche, de partage et d'analyse de données
Analyse fonctionnelle des flux de données
Rédaction des logigrammes de gestion de données
Rédaction des bonnes pratiques d'archivage et de traitement de donénes
Formation des utilisateurs et propriétaires
Ce que nous allons aimer chez vous ?.
Vous avez le sens de l'organisation et du service, une capacité à s'intégrer dans un milieu technique de terrain, vous avez le sens de l'organisation et des priorités, vous êtes rigoureux
Mais aussi :
Vous préparez un diplôme d'ingénieur Bac +4 / 5 en informatique et analyse de données
Vous maîtrisez le développement informatique (base de données, Python, Java)
Les avantages de rejoindre Alliance Emploi
Contrat : ALTERNANCE de 12 à 24 mois
Début : Septembre 2024
Lieu : LESTREM [ site non accessible en transport en commun]
Rémunération : Selon le barème de l'alternance
Et aussi : intéressement, mutuelle, prévoyance, CSE, formations qualifiantes
Et ce n'est pas tout ! En choisissant Alliance Emploi, vous vivrez une expérience basée sur la confiance, la solidarité et l'engagement. Vous développerez vos compétences à travers une diversité de missions et notre réseau d'entreprises, et nous nous engageons à vous proposer un accompagnement personnalisé pour booster votre carrière.
Alors convaincu(e) ?
N'attendez plus pour postuler et venez découvrir la différence Alliance Emploi !
La diversité est une force. Nous sommes engagés pour l'inclusion en offrant des opportunités de carrière à toutes les personnes, indépendamment de leur genre ou de leur situation de handicap.
Show more
Show less","{'ProgLanguage': ['Python', 'Java', 'R', 'Go'], 'DataBase': [], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['Power BI'], 'Statistics': ['Statistiques'], 'CloudComputing': [], 'DevTools': [], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Machine Learning', 'Statistiques'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '25', '25', '25']}"
LinkedIn,Consultant Data Engineer,WHIZE,"Neuilly-sur-Seine, Île-de-France, France",https://fr.linkedin.com/jobs/view/consultant-data-engineer-at-whize-3916769237?position=7&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=tBWGhRh06hBOkPIFde2ODg%3D%3D&trk=public_jobs_jserp-result_search-card,"Offre d’emploi pour un CDI : Consultant Data Engineer
WHIZE est spécialisée dans le développement de solutions sur mesure en architecture Serverless (Azure, Amazon WS, Google CP) et de solutions basées sur l'écosystème Microsoft 365 (SharePoint, Teams, Power Platform).
Vos missions :
Concevoir des solutions de traitement de volume très important de données.
Développement de flux de données et préparation de leur analyse.
Préparation des données pour l'analyse des données collectées.
Profil recherché :
2 ans minimum d’expérience.
Maîtrise du langage Python et Scala
Connaissance d'un ou plusieurs ETL du marché (Talent , SSIS, Azure Data Factory, ...)
Forte expertise en SQL
Être à l’aise avec un ou plusieurs outils Devops (Jenkins, git, GitHub, gitlab, docker, kubernetes, etc…)
Connaissances appréciées :
Hadoop, Spark, Kafka
Connaissance des systèmes NoSQL : Elasticsearch, HBase, Cassandra, Redshift
Connaissance de l'offre data d'un des providers Cloud (GCP, Azure, AWS)
Qu’attendez vous pour nous rejoindre ?
Vous ferez partie d’une société à taille humaine et qui bénéficie des avantages d'un grand groupe. Nous adressons une centaine de clients en direct dont la moitié sont des grands comptes.
Vous serez accompagné(e) et managé(e) par le CEO de WHIZE (THE WHIZE MAN).
Vous allez compléter notre équipe dynamique et travailler avec nous dans une ambiance Start-up et conviviale.
Vous occuperez des postes intéressants et évolutifs.
Vous bénéficierez des évènements internes organisés pour parler tech, business et projets.
Vous réaliserez des projets à forte valeur ajoutée.
📍 : Neuilly-Sur-Seine+ Télétravail
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL', 'Cassandra', 'HBase', 'Elasticsearch'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': ['AWS', 'Azure'], 'DevTools': ['Git', 'Docker', 'Jenkins'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': ['HBase'], 'Automation': ['Kubernetes'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': ['Docker', 'Kubernetes'], 'Collaboration': ['Teams'], 'Other': ['DevOps', 'Cloud'], 'FrSoftSkills': [], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': ['CDI'], 'Salary': [], 'Level': [], 'Experience': ['a', 'n', 's', '2', '2', '2']}"
LinkedIn,Tech Lead Data Engineer,AXA en France,"Lille, Hauts-de-France, France",https://fr.linkedin.com/jobs/view/tech-lead-data-engineer-at-axa-en-france-3837261944?position=8&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=ZUYIbU0zZpg2YW%2FH1I6apw%3D%3D&trk=public_jobs_jserp-result_search-card,"Environnement
En tant que
Tech Lead Data Engineer F/H
, vous allez contribuer directement aux projets des directions métier (ex : fraude santé, multiéquipements, pricing IARD, optimisation du lead management, fragilité auto, …) d’AXA France et à la construction du socle technique Big Data.
Vous allez intégrer une équipe d'une dizaine de personne composée de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus métier de la Direction Transformation Digital Tech et DATA (DT2).
La Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :
- Une organisation agile en feature teams : tribus, guildes, squads
- Des projets sur des applications innovantes à fort trafic (web, mobile…)
- Des méthodologies craft (TDD, BDD, clean code, code review…) et DevOps
- Une communauté de partage de bonnes pratiques (BBL, dojo, meetup, conf…)
Votre rôle et vos missions
Vous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :
D’accompagner techniquement les Data Engineer de l’équipe (coaching, code review, pair programming…)
Passer de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le datalake
Consolider ces données au fur et à mesure de leur alimentation récurrente dans le data lake
Les exploiter pour atteindre la finalité business (exposition de business view, réintégration des résultats dans le SI, service de scoring, …)
De travailler à la création du socle technique Big Data et industrialiser le cycle de développement de l'équipe
De mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)
Votre profil
D'une formation supérieure en informatique ou scientifique (Master ou Diplôme d'ingénieur), vous justifiez de plusieurs expériences significatives (+ de 7 ans)
sur du développement big data, en particulier sur du PySpark.
Compétences techniques :
Connaissances avancées en développement en
PySpark (Spark avec le langage Python)
Maitrise de l'environnement
Microsoft Azure
Connaissances avancées d'outils de BI comme
PowerBI
Compétences transverses :
Capacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier
Expérience en mode de delivery Agile (Scrum, Kanban, etc...)
Driver et accompagner des Data Engineer sur le plan opérationnel
Et Idéalement :
Avoir une expérience en tant que lead
Des Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA
Maitrise des Traitements Big Data en mode Streaming
Maitrise des Bases de données relationnelles et NoSQL
Une expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory
Mais pourquoi AXA France ?
Nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs ! Les avantages que nous proposons à nos salariés sont nombreux.
Nous choisir, c’est bénéficier par exemple :
D’un package de rémunération complet comprenant un salaire fixe, un complément de rémunération variable, des primes, de la participation et de l’intéressement, la possibilité d’acquérir des actions AXA, ou encore des solutions d’épargne avantageuses ;
Equilibre vie Pro / Perso. : D’un cadre de travail flexible jusqu’à 3 jours de télétravail possible par semaine, des tickets restaurant pour les jours télétravaillés ou encore une participation à l’achat d’un écran ou fauteuil ergonomique ;
D’une politique visant à concilier vie personnelle et vie professionnelle avec 28 jours de congés payés, entre 14 et 16 RTT selon les années, des formules de travail à temps partiel ou encore des jours d’absence rémunérées pour la rentrée scolaire ou un déménagement par exemple ;
De la possibilité de s’engager pour une cause qui vous tient à cœur grâce à nos associations telles que AXA Atout Cœur, AXA Compétences Solidaires ou encore AXA Prévention ;
Et bien plus encore ! Perspectives de développement des compétences et de carrières immenses, CE, conciergerie, offres privilèges, soutien en cas d’épreuve personnelle…On s’arrête là, la liste est longue
Qui sommes nous ?
AXA est un des leaders de l’assurance et de la gestion d’actifs dans le monde.
Nous aidons nos 108 millions de clients à traverser les petites et grandes difficultés de la vie.
Chaque jour, nous agissons ensemble pour inventer la meilleure manière de les protéger et voulons donner à chacun les moyens de vivre une vie meilleure.
Un challenge qui donne le sourire et envie de se lever le matin !
Chez AXA, nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C’est pour cette raison que nous menons une politique RH engagée qui favorise la diversité, qui préserve l’équilibre vie privée-vie professionnelle et accélère le développement des compétences et des carrières.
Ainsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une véritable culture d’expertise, accélérant le développement des compétences de chacun et proposant une rémunération attractive.
Pourquoi nous rejoindre ?
Vous êtes porteur d’idées et d’initiatives innovantes ? Vous proposez des solutions et êtes au service du client ? Faites partie de notre grande famille en rejoignant
Un leader mondial offrant des opportunités de carrières intéressantes
Une entreprise qui donne une place de choix à l’innovation, à l’initiative et aux actions solidaires (notamment via l’association AXA Atout Cœur)
Un environnement inclusif à tous les niveaux (mixité, handicap, initiatives pour favoriser l’insertion des jeunes, orientation sexuelle, etc.)
Un accès à de multiples avantages (congés, temps partiel, télétravail, etc.)
Un cadre stimulant, qui permet de rencontrer des collaborateurs performants et d’enrichir ses compétences
Victime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL', 'NoSQL'], 'DataAnalytics': ['R'], 'BigData': ['Spark', 'Databricks'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': ['Azure'], 'DevTools': ['Git'], 'Os': [], 'DBMS': [], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': ['JIRA', 'Teams'], 'Other': ['DevOps', 'Big Data'], 'FrSoftSkills': ['Organisation'], 'EnSoftSkils': ['Initiative']}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['3', '3'], 'Level': [], 'Experience': ['a', 'n', 's', '7', '7', '7']}"
LinkedIn,Data Engineer,Pictarine,"Toulouse, Occitanie, France",https://fr.linkedin.com/jobs/view/data-engineer-at-pictarine-3911913926?position=9&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=n3HEyd%2F98Jz4oy37fKFd1g%3D%3D&trk=public_jobs_jserp-result_search-card,"Mission and challenges 🎯
Si tu es enthousiaste à embarquer dans la nouvelle équipe data de Pictarine pour la faire rayonner avec tout ton savoir-faire, alors c’est l’aventure qu’il te faut! 🏔️
Avec plus de 1K tables, 2M de clients et 4M de commandes en 2022, les équipes de Pictarine ne sont jamais à court d’idées pour explorer de nouveaux horizons. 🚀
En tant que Data Engineer chez Pictarine tu vas pouvoir utiliser toutes tes compétences SQL pour garantir la qualité de la data sur GCP, accompagner et challenger les besoins data.
Tu évolueras au sein de l’équipe Engineering, composée des pôles dev & data.
Ton rôle comprendra les aspects suivants 👇🏻
Tu es garant de la qualité de la data !
En simplifiant la structure de la data et réduisant le nombre de tables
En transformant les données pour les rendre facilement utilisables
En orchestrant le flux des données de manière continue et automatique
Tu accompagnes et challenges les équipes de Pictarine !
En co-construisant des solutions data appropriées
En élevant le niveau de jeu des méthodes data existantes
En faisant rayonner la data autour de bonnes pratiques et d’outillages adéquates
Profil Recherché
About you 💎
Tu as au moins 5 ans d’expérience sur un poste similaire
Tu maîtrises le data warehouse BigQuery et son langage SQL
Tu es à l'aise avec les services GCP
Tu as de bonnes connaissances dans la conception de modèles de données et les stratégies d'optimisation des requêtes SQL
Tu as des compétences en DevOps pour le déploiement et la gestion efficace des pipelines de données
Tu as une bonne maîtrise de Python & Github
Tu es organisé, rigoureux et portes une grande attention aux détails
Tu es doté d’excellentes qualités relationnelles, de communication et de vulgarisation
Tu as une passion pour résoudre des problèmes business avec la programmation
Tu es curieux de tester des nouvelles technologies
Tu es un team player et toujours à l'affût de nouvelles idées
Work @ Pictarine✨
Un environnement de travail agile, collaboratif, international et multiculturel
Des perspectives d’évolution rapides
Des locaux tout beaux à Labège avec du matériel dernier cri (mais aussi des snacks à profusion et un frigo à boissons toujours bien rempli)
Un apprentissage permanent : conférence, meet-up, Pictarine Academy, cours d’anglais.
Des events tous les mois : massage, pilates, TGIF, team building .
Un environnement de travail flexible : horaires, politique de remote hybride.
Un package de rémunération attractif : salaire compétitif, RTT, mutuelle & prévoyance 100% prise en charge, intéressement.
Des petits + : Développement de photos gratuit, subvention sport, 3 jours “entraide familiale”, jours de congés en plus avec l'ancienneté... 🤫 on ne te dévoile pas tout !
Recruitment process ⚙️
Tu souhaites nous rejoindre ? Viens rencontrer les gens avec qui tu vas bosser :
1er échange pour apprendre à se connaître avec Marie - Engineering Manager Data (15’)
Entretien Manager avec Marie (60-90’)
Test pratique afin de nous montrer tes talents 🙂 (3 heures)
Entretien final avec 2 membres du Codir (90’)
Welcome aboard !
Show more
Show less","{'ProgLanguage': ['Python', 'R', 'Go'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': [], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': [], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['BigQuery'], 'SoftBigDataProcessing': [], 'Automation': [], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['DevOps'], 'FrSoftSkills': ['Communication'], 'EnSoftSkils': ['Communication']}","{'JobDetail': ['Hybride', 'Remote'], 'TypeContract': [], 'Salary': ['100', '100'], 'Level': [], 'Experience': ['a', 'n', 's', '5', '5', '5']}"
LinkedIn,Data Engineer (H/F),CITECH,"Paris, Île-de-France, France",https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-citech-3908612761?position=10&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=QDl21Yh58PhaCfqsKPTodg%3D%3D&trk=public_jobs_jserp-result_search-card,"Description de l'entreprise
CITECH recrute !
Si vous souhaitez apporter vos compétences dans la réalisation d’un projet important, nous avons LA mission pour vous ! Nous recherchons en effet un(e)
Data Engineer (H/F)
Votre mission est pour un client reconnu dans le secteur bancaire, implanté dans de nombreuses villes en France, il a pour objectif d'accélérer sa transformation digitale afin d'offrir toujours plus de solutions et de services innovants.
Description du poste
Vous aurez donc les missions principales suivantes :
Support de l'application (notamment lors des clôtures mensuelles).
Participer à la maintenance évolutive.
Participer à la conception et l'implémentation de nouvelles fonctionnalités.
Participer à la refonte technique.
Participer à la migration de Talend vers Spark/scala.
Qualifications
De formation supérieure en informatique, vous justifiez de 5 années d’expérience minimum sur un poste similaire.
⚙️ Les compétences attendues sont les suivantes :
✔️ Vous maîtrisez Spark, Talend (Data Intégration, Big Data) et Scala.
✔️Vous avez des compétences en développement (Shell unix, Perl, PHP, Python, git, github).
✔️Vous avez des compétences sur l’environnement technique suivant :
Hadoop (Big Data), Hive, Microsoft PowerBI, Microsoft SQLServer Analysis services (Olap), Integration services, Reporting services, Scripting (GuitHub, Ansible, AWX, shell, vba) et SQL Server Database.
Informations supplémentaires
Poste situé à
Paris
☀️
Salaire :
45-65 K€ brut/an
Freelance :
300-450 € brut/jour
Référence :
240424_OUTIL DE PILOTAGE FINANCIER - TALEND / SPARK / SCALA
Pourquoi rejoindre Citech ?
Une ambiance de travail conviviale avec des afterworks organisés régulièrement !
Des missions de longues durées
Des formations adaptées à vos envies et vos aspirations
Une mobilité que si vous le souhaitez
Un accompagnement personnalisé avec un suivi régulier (autour d’un café ou un thé, c’est vous qui choisissez )
Une mutuelle avantageuse pour vous mais aussi pour les membres de votre famille
Une flexibilité sur la gestion de vos repas
Un statut Cadre et une convention collective SYNTEC
Alors qu’attendez-vous pour nous rejoindre ?
Company Description
CITECH ce n’est pas une Entreprise de Services du Numérique comme les autres : c’est avant tout une aventure humaine. Nous cherchons des personnalités passionnées qui nous ressemblent ! Dans un environnement convivial et chaleureux, venez révéler vos talents !
Nos fidèles clients, reconnus à l’échelle internationale, offrent à nos collaborateurs de multiples possibilités de carrière. Nos domaines d’interventions sont variés : banque, assurance, automobile, santé, transport ou encore la robotique, vous trouverez forcément un secteur épanouissant, à votre image !
Vous avez un projet professionnel ? Nous sommes là pour vous aider à le développer. Pour nous, l’essentiel c’est vous. C’est pourquoi nous assurons un suivi régulier et portons une attention toute particulière à votre plan de carrière.
Show more
Show less","{'ProgLanguage': ['Python', 'Scala', 'R'], 'DataBase': ['SQL'], 'DataAnalytics': ['R'], 'BigData': ['Hadoop', 'Spark'], 'MachingLearning': [], 'DataSerialization': [], 'DataVisualization': ['PowerBI'], 'Statistics': [], 'CloudComputing': [], 'DevTools': ['Git'], 'Os': [], 'DBMS': ['SQL Server'], 'SoftBigDataProcessing': [], 'Automation': ['Ansible'], 'InfrastructureAsCode': [], 'NetworkSecurty': [], 'Virtualisation': [], 'Containers': [], 'Collaboration': [], 'Other': ['Big Data'], 'FrSoftSkills': ['Flexibilité'], 'EnSoftSkils': []}","{'JobDetail': [], 'TypeContract': [], 'Salary': ['45'], 'Level': [], 'Experience': ['a', 'n', 's']}"
