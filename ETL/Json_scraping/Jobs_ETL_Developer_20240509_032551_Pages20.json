[
    {
        "source": "LinkedIn",
        "title": "SQL Developer",
        "company": "MCI",
        "location": "New Caledonia",
        "link": "https://nc.linkedin.com/jobs/view/sql-developer-at-mci-3912049201?position=2&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=acYZPwwJr44VgyKwDppebA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Full-Time\nJob Title\nGlobal Sales Director\nJob Type\nFull - Time\nLocation\nAngeles City, Remote US\nMCI is a leading Business Process Outsourcing (BPO) company that specializes in delivering tailored solutions to meet the diverse needs of our clients. With a commitment to excellence and a focus on innovation, we have established ourselves as a trusted partner in the industry.\nWe are seeking an SQL Developer, who will be responsible for designing, developing, and maintaining databases and database applications. The role will involve working closely with software developers, database administrators (DBAs), and other stakeholders to design database solutions, write SQL queries, optimize database performance, and ensure data integrity and security.\nTo be considered for this role, you must complete a full application on our company careers page, including all screening questions and a brief pre-employment test.\nKey Responsibilities\nDesign and develop database schemas, tables, views, stored procedures, and functions to support application requirements and business needs.\nCollaborate with software developers and architects to integrate database functionality into application architecture and design.\nWrite and optimize complex SQL queries for data retrieval, manipulation, and analysis, ensuring efficient performance and minimal resource consumption.\nIdentify and resolve performance bottlenecks, query inefficiencies, and indexing issues to enhance database performance and scalability.\nPerform routine database maintenance tasks, such as backups, restores, and data migrations, to ensure data availability, integrity, and recoverability.\nMonitor database performance, storage utilization, and resource utilization, and implement proactive measures to optimize performance and prevent downtime.\nDesign and implement data integration processes, ETL (Extract, Transform, Load) workflows, and data migration scripts to facilitate seamless data flow between different systems and platforms.\nValidate and cleanse data during the ETL process to maintain data quality and consistency across the organization.\nImplement database security policies, access controls, and encryption mechanisms to protect sensitive data and comply with regulatory requirements (e.g., GDPR, HIPAA).\nConduct regular security audits and vulnerability assessments to identify and mitigate security risks and ensure compliance with data protection standards.\nDocument database designs, data models, and technical specifications to facilitate system maintenance, troubleshooting, and knowledge sharing.\nProvide technical support and guidance to development teams, DBAs, and other stakeholders on database-related issues, best practices, and performance optimization techniques.\nWONDER IF YOU ARE A GOOD FIT FOR THIS POSITION?\nAll positive, and driven applicants are encouraged to apply. The Ideal candidates for this position are highly motivated and dedicated and should possess the below qualities\nBachelor's degree in Computer Science, Information Technology, or related field.\nProven experience as an SQL Developer or Database Developer, with expertise in SQL query writing, database design, and performance tuning.\nProficiency in SQL programming languages (e.g., T-SQL, PL/SQL) and database management systems (e.g., Microsoft SQL Server, Oracle, MySQL).\nStrong understanding of database architecture, relational database principles, and data modeling concepts.\nMust be authorized to work in the country where the job is based.\nMust be willing to submit up to a LEVEL II background and/or security investigation with a fingerprint. Job offers are contingent on background/security investigation results.\nMust be willing to submit to drug screening. Job offers are contingent on drug screening results.\nWANT AN EMPLOYER THAT VALUES YOUR CONTRIBUTION?\nWe offer competitive compensation packages, professional development opportunities, and a collaborative work environment that values diversity and inclusion.\nThis job operates in a professional office environment. While performing the duties of this job, the employee will be largely sedentary and will be required to sit/stand for long periods while using a computer and telephone headset. The employee will be regularly required to operate a computer and other office equipment, including a phone, copier, and printer. The employee may occasionally be required to move about the office to accomplish tasks; reach in any direction; raise or lower objects, move objects from place to place, hold onto objects, and move or exert force up to forty (40) pounds.\nIt is the policy of MCI and affiliates to provide reasonable accommodation when requested by a qualified applicant or employee with a disability unless such accommodation would cause undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment. If reasonable accommodation is needed, of Human Resources.\nAt MCI and its subsidiaries, we embrace differences and believe diversity is a benefit to our employees, our company, our customers, and our community. All aspects of employment at MCI are based solely on a person's merit and qualifications. MCI maintains a work environment free from discrimination, one where employees are treated with dignity and respect. All employees share in the responsibility for fulfilling MCI's commitment to a diverse and equal opportunity work environment.\nMCI does not discriminate against any employee or applicant on the basis of age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations, and ordinances. MCI will consider for employment qualified applicants with criminal histories in a manner consistent with local and federal requirements.\nMCI will not tolerate discrimination or harassment based on any of these characteristics. We adhere to these principles in all aspects of employment, including recruitment, hiring, training, compensation, promotion, benefits, social and recreational programs, and discipline. In addition, it is the policy of MCI to provide reasonable accommodation to qualified employees who have protected disabilities to the extent required by applicable laws, regulations, and ordinances where an employee works.\nMCI (www.mci.world) helps customers take on their CX and DX challenges differently, creating industry-leading solutions that deliver exceptional experiences and drive optimal performance. MCI assists companies with business process outsourcing, staff augmentation, call center services, customer services, and IT Services needs by providing general and specialized hosting, software, staff, and services.\nIn 2019 Marlowe Companies Inc. (MCI) was named by Inc. Magazine as Iowa\u2019s Fastest Growing Company in the State of Iowa and was named the 452nd Fastest Growing Privately Company in the USA, making the coveted top 500 for the first time. MCI\u2019s subsidiaries had previously made Inc. Magazine's List of Fastest-Growing Companies 15 times respectively. MCI has fifteen business process outsourcing service delivery facilities in Iowa, Georgia, Florida, Texas, Massachusetts, New Hampshire, South Dakota, New Mexico, California, Kansas, and Nova Scotia.\nDriving modernization through digitalization, MCI ensures clients do more for less. MCI is the holding company for a diverse lineup of tech-enabled business services operating companies. MCI organically grows, acquires, and operates companies that have a synergistic products and services portfolios, including but not limited to Automated Contact Center Solutions (ACCS), customer contact management, IT Services (IT Schedule 70), and Temporary and Administrative Professional Staffing (TAPS Schedule 736), Business Process Management (BPM), Business Process Outsourcing (BPO), Claims Processing, Collections, Customer Experience Provider (CXP), Customer Service, Digital Experience Provider (DXP), Account Receivables Management (ARM), Application Software Development, Managed Services, and Technology Services, to mid-market, Federal & enterprise partners. MCI now employs 10,000+ talented individuals with 150+ diverse North American client partners across the following MCI brands GravisApps, Mass Markets, MCI Federal Services (MFS), The Sydney Call Center, OnBrand24, and Valor Intelligent Processing (VIP).\nThe purpose of the above job description is to provide potential candidates with a general overview of the role. It's not an all-inclusive list of the duties, responsibilities, skills, and qualifications required for the job. You may be asked by your supervisors or managers to perform other duties. You will be evaluated in part based upon your performance of the tasks listed in this job description.\nThe employer has the right to revise this job description at any time. This job description is not a contract for employment, and either you or the employer may terminate employment at any time, for any reason.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "MySQL",
                "Oracle",
                "SQL Server"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": [
                "Organization"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": [],
            "Salary": [
                "40"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "digiRocks recrute \u2705",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-digirocks-recrute-%E2%9C%85-3903481080?position=3&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=u1YGRyud%2BVE0ms1zi0jDYQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83d\ude0e Envie d'accompagner des organisations dans leurs strat\u00e9gies, Fan de data?\nRejoins un jeune cabinet de conseil en strat\u00e9gie sp\u00e9cialis\u00e9 en data. Le cabinet a \u00e9t\u00e9 cr\u00e9\u00e9 il y a 4 ans pas des anciens de grands cabinets de conseil en strat\u00e9gie qui ne se reconnaissaient plus dans ce qu'est devenu le \"consulting\". Cependant ils n'ont pas perdu espoir de pouvoir apporter du conseil \u00e0 haute valeur ajout\u00e9e dans une ambiance friendly, fa\u00e7on start-up, sans sacrifier l'excellence.\nJean-Patrick recrute un(e) Consultant Data Engineer \u00e0 Paris en CDI\n\u2705 MISSION :\nVous serez responsable de la mise en \u0153uvre de bout en bout de la pile de donn\u00e9es, de la collecte au reporting, avec un accent sur l'infrastructure et les processus techniques. Vous travaillerez avec des Consultants en Strat\u00e9gie & Data et les soutiendrez dans la r\u00e9solution des d\u00e9fis li\u00e9s aux donn\u00e9es de leurs clients. Vous contribuerez \u00e0 la d\u00e9finition des strat\u00e9gies de donn\u00e9es, \u00e0 la mise en \u0153uvre des syst\u00e8mes de donn\u00e9es et vous soutiendrez l'exploitation des donn\u00e9es dans des projets transformationnels. En g\u00e9n\u00e9ral, vous serez responsable de comprendre intimement les probl\u00e8mes, de concevoir une strat\u00e9gie technique pour les adresser et de faciliter une ex\u00e9cution technique de haute qualit\u00e9.\n\u2705 R\u00c9SULTATS ATTENDUS :\n\ud83d\ude80 R\u00e9sultat 1: Unificateur de Donn\u00e9es : Architecturer, assembler, assimiler, nettoyer et conformer de grands ensembles de donn\u00e9es complexes pour livrer des insights commerciaux et alimenter les exp\u00e9riences de produits de donn\u00e9es.\n\ud83d\ude80 R\u00e9sultat 2: Agent de S\u00e9curit\u00e9 des Donn\u00e9es : Concevoir et construire une infrastructure de donn\u00e9es fiable et \u00e9volutive avec les techniques de confidentialit\u00e9 et de s\u00e9curit\u00e9 de pointe pour prot\u00e9ger les donn\u00e9es.\n\ud83d\ude80 R\u00e9sultat 3: DataOps : Poss\u00e9der la pile de donn\u00e9es de bout en bout, y compris la collecte d'\u00e9v\u00e9nements, la gouvernance des donn\u00e9es, les int\u00e9grations de donn\u00e9es et la mod\u00e9lisation.\n\ud83d\ude80 R\u00e9sultat 4: Gardien des Donn\u00e9es : Assurer la coh\u00e9rence et la qualit\u00e9 de l'environnement technique et de la structure des donn\u00e9es \u00e0 travers des m\u00e9triques, de la documentation, des processus, des tests de donn\u00e9es et de la formation.\nRequirements\n\u2705 PROFIL RECHERCH\u00c9 :\nDipl\u00f4m\u00e9 d'une Grande Ecole de Commerce ou d'ing\u00e9nieur, avec une premi\u00e8re exp\u00e9rience r\u00e9ussie comme Data Engineer, id\u00e9alement dans un contexte similaire au Conseil,\nConnaissance des services de Data Warehouses Cloud. Exp\u00e9rience avec Google BigQuery, Snowflake, AWS Redshift/Athena, Looker, Azure SQL DWH, ou Azure Databricks est tr\u00e8s souhaitable.\nConnaissance des architectures de donn\u00e9es relationnelles et de grandes donn\u00e9es, de l'entreposage de donn\u00e9es, de l'int\u00e9gration de donn\u00e9es, de la mod\u00e9lisation de donn\u00e9es, de l'optimisation de donn\u00e9es et des techniques d'analyse de donn\u00e9es.\nExp\u00e9rience dans la construction de pipelines de donn\u00e9es de bout en bout en utilisant des plateformes de donn\u00e9es sur site ou bas\u00e9es sur le cloud.\nExp\u00e9rience pratique dans la livraison de solutions comprenant des bases de donn\u00e9es, SQL avanc\u00e9 et d\u00e9veloppement logiciel dans des langues telles que Python.\nInt\u00e9ress\u00e9 et connaissant les technologies Big Data et les technologies de l'\u00e9cosyst\u00e8me Apache telles que Beam, Spark, Kafka, Airflow, bases de donn\u00e9es, int\u00e9gration, gestion des donn\u00e9es de r\u00e9f\u00e9rence, assurance qualit\u00e9, manipulation de donn\u00e9es et technologies de gouvernance des donn\u00e9es.\nExp\u00e9rience avec les plateformes cloud publiques et l'infrastructure cloud qui est essentielle.\nExpos\u00e9 aux outils ETL/ELT et de gouvernance.\nInt\u00e9ress\u00e9 par les technologies et principes IA et ML.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake",
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer H/F",
        "company": "Thales",
        "location": "Lyon, Auvergne-Rh\u00f4ne-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-thales-3903089036?position=4&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=5BTSlqnHIPJ1%2Bp4q6XNZcA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83d\udce2 Nous recherchons un(e) Data Engineer, bas\u00e9(e) \u00e0 Lyon\n\ud83d\udc49Quelques mots sur les activit\u00e9s num\u00e9riques de Thales Lyon :\nLes activit\u00e9s num\u00e9riques repr\u00e9sentent une entit\u00e9 rattach\u00e9e au groupe Thales, sp\u00e9cialis\u00e9e dans l\u2019IT et pr\u00e9sente au national.\nL\u2019agence de Lyon adresse divers sujets d\u2019expertise : ing\u00e9nierie logiciels, cybers\u00e9curit\u00e9, infog\u00e9rance des infrastructures et transformation digitale.\n\ud83c\udfaf\nVotre r\u00f4le et missions\nEn nous rejoignant, vous int\u00e9grerez le centre de comp\u00e9tences\nAugmented data\n,\nsp\u00e9cialis\u00e9 dans la conception, le d\u00e9veloppement et l\u2019\u00e9volution d\u2019applications data centr\u00e9es. Vous y boosterez votre carri\u00e8re en travaillant sur des technologies telles que\nSpark, Elasticsearch, Kube ...\nle plus souvent dans un environnement\nAgile\n.\nDans le cadre des projets que nous op\u00e9rons aujourd\u2019hui :\n- Vous contribuerez \u00e0 la conception, au maintien, \u00e0 la scalabilit\u00e9 des plateformes d\u2019analyse de donn\u00e9es au travers de votre expertise sur les sujets data (base de donn\u00e9es, gestion de flux, ETL \u2026)\n- Vous contribuerez \u00e0 la conception et \u00e0 la mise en production des pipelines d\u2019analyses et de transformations de donn\u00e9es en veillant \u00e0 leur bonne adaptation aux besoins m\u00e9tiers et aux contraintes techniques du client\n- Vous pourrez intervenir sur des sujets de visualisations, dans le but de notamment accompagn\u00e9es nos clients sur la conception de Dashboard m\u00e9tier intelligent \u2026\n- Vous serez \u00e9galement amen\u00e9es \u00e0 \u00e9changer directement avec des DevOps/Datascientist pour la mise en place, l\u2019int\u00e9gration des pipelines et l\u2019\u00e9laboration des algorithmes de traitements de donn\u00e9es.\n- A l\u2019\u00e9chelle du d\u00e9partement, Vous serez un acteur majeur du d\u00e9veloppement de notre activit\u00e9 et du lancement de nouveaux projets de valorisation de donn\u00e9es.\n\ud83d\ude4b\u200d\u2640\ufe0f \ud83d\ude4b\u200d\u2642\ufe0f\nVotre profil\nDe formation Bac +5 en informatique (\u00e9cole d\u2019ing\u00e9nieur, Master ou \u00e9quivalent), vous justifiez d\u2019une premi\u00e8re exp\u00e9rience r\u00e9ussie sur un projet data ? Vous souhaitez participer \u00e0 la conception et intervenir sur des solutions de r\u00e9cup\u00e9ration et d\u2019exploitation de donn\u00e9es m\u00e9tiers dans des contextes critiques et hautement s\u00e9curis\u00e9s ?\nAutonome, dynamique, organis\u00e9(e) et proactif(ve), vous souhaitez \u00e9voluer au sein d\u2019\u00e9quipes passionn\u00e9es par l\u2019exploration et l\u2019int\u00e9gration des technologies nouvelles au service des m\u00e9tiers de nos clients ?\nVous avez des comp\u00e9tences qui couvrent les domaines suivants :\nMise en place et gestion de base de donn\u00e9es (SQL, Elasticsearch, Clickhouse ...)\nLangages de programmations (Java, Python)\nGestion de flux (Kafka, flink, logstash \u2026)\nEnvironnements big Data (Spark/hadoop )\nPrincipes et outils de type ETL\nVous \u00eates de plus int\u00e9ress\u00e9(e):\nPar les environnements containeris\u00e9s (docker, kubernetes, helm ...)\nLes concepts DevOps (Ansible, CI/CD...)\nLes sujets de Datavisualisation (Vega, Kibana, python librairies...)\nVous aimez travailler en \u00e9quipe ? Vous \u00eates reconnu(e) pour vos qualit\u00e9s relationnelles et vos capacit\u00e9s de vulgarisation ?\nAlors notre poste d\u2019Ing\u00e9nieur(e) Data(H/F) est fait pour vous !\n\ud83d\ude4c\nVotre carri\u00e8re chez Thales\nDiff\u00e9rentes opportunit\u00e9s vous permettront de d\u00e9couvrir d'autres domaines ou sites. Vous pourrez \u00e9voluer et d\u00e9velopper vos comp\u00e9tences dans diff\u00e9rents domaines.\nExplorez un espace attentif au d\u00e9veloppement personnel.\nD\u00e9veloppez vos talents dans un autre domaine du groupe Thales, en d\u00e9couvrant de nouveaux produits, de nouveaux clients, un nouveau pays ou en vous orientant vers une solution plus complexe.\nChoisissez entre une expertise technique ou un parcours de leadership.\nVous travaillerez dans une entreprise r\u00e9solument humaine avec des valeurs fortes comme la s\u00e9curit\u00e9 au travail, l\u2019\u00e9galit\u00e9 Homme/Femme et l\u2019\u00e9quilibre vie personnelle/professionnelle (Accord T\u00e9l\u00e9travail).\nRattach\u00e9(e) \u00e0 la Convention m\u00e9tallurgie, vous b\u00e9n\u00e9ficierez aussi de ses multiples avantages (\u2026)\nVous souhaitez en savoir plus ?\nN\u2019h\u00e9sitez pas \u00e0 contacter notre \u00e9quipe de recrutement ou nos \u00e9quipes directement.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "Elasticsearch"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark",
                "Flink"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Ansible",
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Big Data",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Leadership"
            ],
            "EnSoftSkils": [
                "Leadership"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer / D\u00e9veloppeur Big Data # H/F",
        "company": "Air France",
        "location": "Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-d%C3%A9veloppeur-big-data-%23-h-f-at-air-france-3900080172?position=5&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=NRt5Zl9%2FQscrskQoCBvcMA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Description du poste\nIntitul\u00e9 du poste\nData Engineer / D\u00e9veloppeur Big Data # H/F\nM\u00e9tier\nSyst\u00e8mes d'informations - D\u00e9veloppement\nCat\u00e9gorie socio-professionnelle\nCadre\nPr\u00e9sentation du contexte\nVous avez peut-\u00eatre d\u00e9j\u00e0 voyag\u00e9 avec nous, mais que connaissez-vous de nos m\u00e9tiers et de la richesse des donn\u00e9es qu\u2019ils g\u00e9n\u00e8rent au quotidien ? Comment le traitement et l\u2019exploitation de ces donn\u00e9es peut contribuer \u00e0 notre strat\u00e9gie de Revenue Management, ou encore aux multiples op\u00e9rations \u00e0 r\u00e9aliser pour permettre \u00e0 un vol de partir \u00e0 l\u2019heure ?\nAir France-KLM fait r\u00eaver 104 millions de passagers par an, en les emmenant vers plus de 250 destinations, gr\u00e2ce \u00e0 une flotte de plus de 500 appareils. Le Groupe emploie 80 000 collaborateurs partout dans le monde :les opportunit\u00e9s sont vastes pour mettre \u00e0 profit ses comp\u00e9tences, apprendre et se d\u00e9velopper !\nLe d\u00e9partement de d\u00e9veloppement DATA, OR & AI d\u2019Air France, au sein de la direction des Syst\u00e8mes d\u2019Information, intervient dans toute la cha\u00eene de captation et de traitement des donn\u00e9es du groupe pour d\u00e9livrer \u00e0 nos m\u00e9tiers des solutions applicatives cl\u00e9s en main.\nLe d\u00e9partement est \u00e9galement en charge de l\u2019ensemble des outils techniques (ETL, DataLakes, DataWarehouses, Data visualisation) et du d\u00e9veloppement des talents et comp\u00e9tences de Data Engineering.\nNotre mission ? Transformer la donn\u00e9e brute en d\u00e9cision intelligente, pour mieux optimiser les m\u00e9tiers d\u2019Air France \u2013 KLM !\nPour cela, nous avons chacun un r\u00f4le essentiel \u00e0 jouer, pourquoi le v\u00f4tre ne serait pas celui de Data Engineer et de d\u00e9veloppeur Big Data ?\nDescription de la mission\nAu sein de notre d\u00e9partement, vous travaillerez main dans la main avec d\u2019autres Data Engineers et d\u00e9veloppeurs Big Data ainsi qu\u2019avec des sp\u00e9cialistes des m\u00e9tiers.\nInt\u00e9gr\u00e9 au sein d\u2019une product team agile passionn\u00e9e et dynamique :\nVous participez \u00e0 l\u2019analyse des besoins m\u00e9tiers du commercial, des op\u00e9rations a\u00e9riennes, de l\u2019exploitation sol en a\u00e9roport, de la maintenance a\u00e9ronautique ou encore du Cargo.\nVous contribuez \u00e0 la d\u00e9finition, au d\u00e9veloppement, \u00e0 l\u2019industrialisation et \u00e0 la maintenance d\u2019applications Big Data ou en Business Intelligence\nVous pr\u00e9sentez la restitution de vos travaux et accompagnez les utilisateurs d\u2019un point de vue fonctionnel ou m\u00e9thodologique\nVous serez en contact avec les directions m\u00e9tier du groupe Air France KLM.\nNous attachons beaucoup d'importance au d\u00e9veloppement des comp\u00e9tences de nos collaborateurs ainsi qu\u2019\u00e0 leur offrir des conditions de travail favorables \u00e0 l\u2019autonomie et aux missions \u00e0 forte valeur ajout\u00e9e. L'ouverture, le respect, la bienveillance et le partage sont des valeurs humaines port\u00e9es par l'entreprise.\nProfil recherch\u00e9\nVous \u00eates dipl\u00f4m\u00e9 de niveau Master ou Ing\u00e9nieur dans les domaines informatiques, vous avez acquis une exp\u00e9rience professionnelle dans le d\u00e9veloppement d\u2019applications.\nVous disposez d\u2019une exp\u00e9rience du d\u00e9veloppement indispensable en Backend / Java\nVous ma\u00eetrisez les bases de donn\u00e9es relationnelles et le langage SQL\nEn Compl\u00e9ment, Vous Avez Une Connaissance Ou Une Exp\u00e9rience Dans Tout Ou Partie Des Concepts Ou Outils Suivants\nEnvironnement Big Data (Spark, Hadoop, Elasticsearch, Kafka, ...)\nBase de donn\u00e9es noSQL (MongoDB, HBase, REDIS) ou Data Warehouse Teradata\nOutil de Datavisualisation (Spotfire, PowerBI, Qlik ou Kibana)\nSolutions de Cloud (GCP) et hybride (GCP / AZURE)\n(Ces comp\u00e9tences compl\u00e9mentaires ou manquantes pouvant aussi s'acqu\u00e9rir \u00e0 travers un parcours de reskilling et de formations aux outils du data engineering dispens\u00e9 en interne).\nVous avez particip\u00e9 \u00e0 des projets organis\u00e9s en Scrum ou Kanban, et avez peut-\u00eatre m\u00eame \u0153uvr\u00e9 comme Scrum-Master, ce qui vous permettra de vous int\u00e9grer ais\u00e9ment au sein d\u2019une Product Team. Votre esprit de synth\u00e8se, votre force de conviction et votre ma\u00eetrise de la communication facilitent les d\u00e9cisions avec l\u2019ensemble des collaborateurs de l\u2019\u00e9quipe, \u00e9ventuellement en langue anglaise, \u00e0 l\u2019\u00e9crit comme \u00e0 l\u2019oral.\nVous \u00eates autonome, rigoureux(se), responsable et curieux(se), vous aimez travailler en \u00e9quipe. Vous poss\u00e9dez de bonnes capacit\u00e9s d'\u00e9coute, d'analyse, de synth\u00e8se et de communication.\nEt bien s\u00fbr, vous \u00eates passionn\u00e9(e), enthousiaste et ing\u00e9nieux(se)\nCe que nous vous offrons\nDe la cr\u00e9ation de valeur pour l\u2019ensemble des m\u00e9tiers d\u2019Air France KLM\nDes challenges et probl\u00e9matiques complexes \u00e0 r\u00e9soudre\nL\u2019opportunit\u00e9 de d\u00e9ployer des solutions Data industrielles \u00e0 l\u2019\u00e9chelle !\nUne grande part de responsabilit\u00e9 dans une structure hi\u00e9rarchique horizontale\nUn important degr\u00e9 de libert\u00e9 pour apprendre et d\u00e9velopper son expertise au sein de l\u2019\u00e9quipe\nOn vous attend le plus rapidement possible ! Et pour une dur\u00e9e ind\u00e9termin\u00e9e ;)\nType de contrat\nCDI\nTemps partiel possible\nNon\nType d'horaires\nAdministratif\nProfil candidat\nNiveau d'\u00e9tudes min. requis\nBac + 5 et plus\nLangue\nAnglais (4 - Confirm\u00e9 / C1)\nLocalisation du poste\nLocalisation du poste\nFrance, Provence-Alpes-C\u00f4te d'Azur, Alpes Maritimes (06)\nSite\nValbonne\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Java",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL",
                "HBase",
                "Elasticsearch"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [
                "Hybride",
                "Confirm\u00e9"
            ],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer \u2013 SQL & GCP - F/H",
        "company": "Orange Business",
        "location": "Greater Lille Metropolitan Area",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-sql-gcp-f-h-at-orange-business-3916557264?position=6&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=4BBfpaE2gtnp4tmuT%2BVqng%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l\u2019un des leaders europ\u00e9ens de la Data transformation ?\nNous l\u2019avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les \u00e9tapes du voyage de la donn\u00e9e.\nDepuis 30 ans, Business & Decision, entit\u00e9 de Digital Services s'est impos\u00e9e comme un partenaire strat\u00e9gique pour la transformation Data de nombreux clients, dans des secteurs tr\u00e8s vari\u00e9s. Digital Services est aujourd\u2019hui l\u2019ESN d\u2019Orange Business alliant les expertises historiques Cloud et Digital d\u2019Orange ainsi que le c\u0153ur de m\u00e9tier Data/IA de Business & Decision. Son but est d\u2019accompagner les entreprises et les acteurs publics dans leur transformation gr\u00e2ce aux 4000 experts pr\u00e9sents dans plusieurs grandes villes fran\u00e7aises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse \u2026\nDescription du poste\nOrange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.\nVotre quotidien ?\nEn int\u00e9grant Orange Business, vous pouvez participer \u00e0 une grande diversit\u00e9 d\u2019activit\u00e9s dans la Data. En voici un aper\u00e7u :\nAu d\u00e9marrage du projet :\nRecueillir et analyser les besoins du client\nR\u00e9diger les sp\u00e9cifications fonctionnelles et techniques\nEstimre les charges\nPendant la phase de r\u00e9alisation :\nMod\u00e9liser des datawarehouses et datamart (int\u00e9gration de flux et consolidation des donn\u00e9es)\nD\u00e9velopper les proc\u00e9dures d\u2019alimentation (ETL)\nD\u00e9velopper en SQL\n/ PLSQL / Shell\nGarantir la qualit\u00e9 des donn\u00e9es et leur disponibilit\u00e9\nConcevoir et d\u00e9velopper des solutions frontend BI \u00e0 des fins analytics & dashboarding\nR\u00e9aliser la recette et les tests\nSuivre et mettre en production\nEn fonction de votre \u00e9volution et de nos enjeux, vous pouvez aussi \u00e9voluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d\u2019initiative est toujours la bienvenue !\nQualifications\nVous poss\u00e9dez 3 ans d'exp\u00e9rience ou plus dans la mise en \u0153uvre de projets d\u00e9cisionnels et ing\u00e9nierie ou analyse data.\nVous avez de\nsolides comp\u00e9tences en d\u00e9veloppement SQL\n(job, scripting, d\u00e9ploiement), vous avez l\u2019habitude de travailler dans un\nenvironnement Google Cloud Plateform\nainsi qu\u2019avec\nPower BI\n.\nEnvie d\u2019apprendre de nouvelles technos ? Vous souhaitez partager vos comp\u00e9tences et b\u00e9n\u00e9ficier des expertises de la Team Orange Business ?\nOutre l\u2019aspect technique, c\u2019est une personnalit\u00e9 qui est recherch\u00e9e !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [
                "Orange"
            ],
            "DataSerialization": [],
            "DataVisualization": [
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "30",
                "30",
                "30"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer - Mod\u00e9lisation SQL - F/H",
        "company": "Orange Business",
        "location": "Greater Lille Metropolitan Area",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-mod%C3%A9lisation-sql-f-h-at-orange-business-3916551577?position=7&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=vVVscyPLDbH99Q%2BXCr5Ruw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l\u2019un des leaders europ\u00e9ens de la Data transformation ?\nNous l\u2019avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les \u00e9tapes du voyage de la donn\u00e9e.\nDepuis 30 ans, Business & Decision, entit\u00e9 de Digital Services s'est impos\u00e9e comme un partenaire strat\u00e9gique pour la transformation Data de nombreux clients, dans des secteurs tr\u00e8s vari\u00e9s. Digital Services est aujourd\u2019hui l\u2019ESN d\u2019Orange Business alliant les expertises historiques Cloud et Digital d\u2019Orange ainsi que le c\u0153ur de m\u00e9tier Data/IA de Business & Decision. Son but est d\u2019accompagner les entreprises et les acteurs publics dans leur transformation gr\u00e2ce aux 4000 experts pr\u00e9sents dans plusieurs grandes villes fran\u00e7aises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse \u2026\nDescription du poste\nOrange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.\nVotre quotidien ?\nEn int\u00e9grant Orange Business, vous pouvez participer \u00e0 une grande diversit\u00e9 d\u2019activit\u00e9s dans la Data. En voici un aper\u00e7u :\nAu d\u00e9marrage du projet :\nRecueillir et analyser les besoins du client\nR\u00e9diger les sp\u00e9cifications fonctionnelles et techniques\nEstimer les charges\nPendant la phase de r\u00e9alisation :\nMod\u00e9liser des datawarehouses et datamart (int\u00e9gration de flux et consolidation des donn\u00e9es)\nD\u00e9velopper les proc\u00e9dures d\u2019alimentation (ETL)\nD\u00e9velopper en SQL / PLSQL / Shell\nGarantir la qualit\u00e9 des donn\u00e9es et leur disponibilit\u00e9\nR\u00e9aliser la recette et les tests\nSuivre et mettre en production\nEn fonction de votre \u00e9volution et de nos enjeux, vous pouvez aussi \u00e9voluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d\u2019initiative est toujours la bienvenue !\nQualifications\nVous poss\u00e9dez 5 ans d'exp\u00e9rience ou plus dans la mise en \u0153uvre de projets d\u00e9cisionnels et en mod\u00e9lisation.\nVous avez de s\nolides comp\u00e9tences en d\u00e9veloppement SQL\n(job, scripting, d\u00e9ploiement) ainsi que sur Python.\nEnvie d\u2019apprendre de nouvelles technos ? Vous souhaitez partager vos comp\u00e9tences et b\u00e9n\u00e9ficier des expertises de la Team Orange Business ?\nOutre l\u2019aspect technique, c\u2019est une personnalit\u00e9 qui est recherch\u00e9e !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [
                "Orange"
            ],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "30",
                "30",
                "30"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer H/F",
        "company": "Inetum",
        "location": "St.-Ouen, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-inetum-3843966639?position=8&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=BlCfRho2hVXsr1gigd79gQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "D\u00e9tail de l'offre\nInformations g\u00e9n\u00e9rales\nEntit\u00e9 de rattachement\nInetum est un leader europ\u00e9en des services num\u00e9riques. Pour les entreprises, les acteurs publics et la soci\u00e9t\u00e9 dans son ensemble, les 28 000 consultants et sp\u00e9cialistes du groupe visent chaque jour l'impact digital des solutions qui contribuent \u00e0 la performance, \u00e0 l'innovation et au bien commun.\nPr\u00e9sent dans 19 pays au plus pr\u00e8s des territoires, et avec ses grands partenaires \u00e9diteurs de logiciels, Inetum r\u00e9pond aux enjeux de la transformation digitale avec proximit\u00e9 et flexibilit\u00e9.\nPort\u00e9 par son ambition de croissance et d'industrialisation, Inetum a g\u00e9n\u00e9r\u00e9 en 2023 un chiffre d'affaires de 2,5 milliards d'\u20ac.\nPour r\u00e9pondre \u00e0 un march\u00e9 en croissance continue depuis plus de 30ans, Inetum a fait le choix d\u00e9lib\u00e9r\u00e9 de se recentrer sur 4 m\u00e9tiers afin de gagner en puissance et proposer des solutions sur mesure, adapt\u00e9es aux besoins sp\u00e9cifiques de ses clients le conseil (Inetum Consulting), la gestion des infrastructures et applications \u00e0 fa\u00e7on (Inetum Technologies), l'impl\u00e9mentation de progiciels (Inetum Solutions) et sa propre activit\u00e9 d'\u00e9diteur de logiciels (Inetum Software). Inetum a conclu des partenariats strat\u00e9giques avec 4 grands \u00e9diteurs mondiaux - Salesforce, ServiceNow, Microsoft et SAP et poursuit une strat\u00e9gie d'acquisitions d\u00e9di\u00e9e afin d'entrer dans le top 5 europ\u00e9en sur ces technologies et proposer la meilleure expertise \u00e0 ses clients.\nTous nos postes sont ouverts aux personnes en situation de handicap.\nDescription du poste\nM\u00e9tier\nApplications Delivery - Software Development\nIntitul\u00e9 du poste\nData Engineer H/F\nContrat\nCDI\nDescription De La Mission\nLe p\u00f4le BFA de la branche Application Services du groupe INETUM, recherche plusieurs Data Engineers afin d'intervenir aupr\u00e8s de clients grands comptes au sein des march\u00e9s bancaires et de l'assurance.\nAu sein de l'\u00e9quipe Data, en tant que Data Engineer, vous participez \u00e0 la r\u00e9alisation de divers projets et vos missions sont\nApporter votre connaissance en Big Data permettant la manipulation des donn\u00e9es\nConcevoir les plateformes permettant de traiter des volumes de donn\u00e9es importants\nMettre en place des bases de donn\u00e9es\nPr\u00e9parer le pipeline de donn\u00e9es pour que les donn\u00e9es d\u00e9ploy\u00e9es soient s\u00e9curis\u00e9es et claires afin d'\u00eatre analys\u00e9es et transform\u00e9es.\nProfil\nDe formation ing\u00e9nieure en informatique Bac + 5 informatique ou scientifique\nBonne communication orale et \u00e9crite en fran\u00e7ais et niveau d\u2019anglais professionnel\nSavoir- \u00eatre Bon esprit d'analyse et de synth\u00e8se, sens de l'organisation et de la qualit\u00e9, force de proposition, rigueur, travail en \u00e9quipe, adaptabilit\u00e9.\nSi vous vous reconnaissez, n'h\u00e9sitez pas \u00e0 postuler !\nLocalisation du poste\nLocalisation du poste\nFrance\nVille\nSaint-Ouen\nCrit\u00e8res candidat\nNiveau d'\u00e9tudes min. requis\nBac+5\nNiveau d'exp\u00e9rience min. requis\nPlus de 2 ans\nComp\u00e9tences\nSQL\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data"
            ],
            "FrSoftSkills": [
                "Communication",
                "Adaptabilit\u00e9",
                "Organisation",
                "Flexibilit\u00e9"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "30",
                "30",
                "30"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F)",
        "company": "Scalian",
        "location": "Valbonne, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-scalian-3819563847?position=9&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=ls0HHJwQgfpq8HfBIbSMQA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Ing\u00e9nieur DATA / Data engineer (H/F)\nValbonne/Sophia-Antipolis\nType : CDI\nLieu : Locaux Scalian Sophia-Antipolis\nT\u00e9l\u00e9travail : En fonction des possibilit\u00e9s\nDate de prise de poste : imm\u00e9diatement ou en fonction de votre pr\u00e9avis\nSalaire : en fonction du profil - entre 40 et 48K Brut annuels (hors avantages Scalian)\nAvantages Scalian : Accord d'entreprise t\u00e9l\u00e9travail, Tickets restaurants, Mutuelle groupe, accord am\u00e9nagement temps de travail, compte \u00e9pargne temps, accord de participation et int\u00e9ressement groupe, programme cooptation et apports d'affaires, accompagnement parentalit\u00e9, avantages CSE\nVous \u00eates data engineer ou vous souhaitez le devenir !\nQuel sera votre r\u00f4le ?\nLa port\u00e9e de la mission comprend (sans toutefois s'y limiter) :\nScience des donn\u00e9es\nIng\u00e9nierie des donn\u00e9es\nAnalyse des donn\u00e9es\nG\u00e9nie logiciel\nCe que cette exp\u00e9rience va vous apporter\nVous \u00eates autonome, vous avez le sens du service et de l\u2019analyse, vous \u00eates impliqu\u00e9, nous vous offrons une ouverture sur des projets complexes et une rapide \u00e9volution de carri\u00e8re. Vous rejoignez notre business unit \u00e0 Sophia Antipolis compos\u00e9e d'environ 50 consultants, avec possibilit\u00e9 de t\u00e9l\u00e9travail en fonction des sujets.\nNous co-construisons votre trajectoire professionnelle et assurons votre mont\u00e9e en comp\u00e9tences.\nNous nous inscrivons ensemble dans la dur\u00e9e, nous assurons votre mont\u00e9e en comp\u00e9tences et disposons d'une vari\u00e9t\u00e9 de sujets passionnants.\nCe que nous recherchons chez vous\nDe formation sup\u00e9rieure (Bac+5, \u00e9cole ou universit\u00e9), vous poss\u00e9dez id\u00e9alement une premi\u00e8re exp\u00e9rience r\u00e9ussie dans ce domaine (d\u00e9butants accept\u00e9s), vous aimez le travail en \u00e9quipe.\nComp\u00e9tences requises\n:\nEtape d\u2019analyse : Comprendre l\u2019architecture technique, les sources de donn\u00e9es, les objectifs fonctionnels.\nEtape de conception : Solution de conception avec un fort centrage sur les pipelines de donn\u00e9es et les mod\u00e8les ML et l\u2019exposition des KPI via API\nMise en \u0153uvre : Apr\u00e8s les phases d\u2019analyse et de conception, proc\u00e9der \u00e0 a mise en \u0153uvre dans des technologies s\u00e9lectionn\u00e9es (Java,Scala,Python,Spark)\nCr\u00e9er un code test\u00e9 et document\u00e9\nTechno : Linux, Shell, Hadoop, Scrum, Python, Spark, Scala\nPourquoi feriez-vous le grand saut ?\nParce que Scalian vous accompagne dans le d\u00e9veloppement de votre carri\u00e8re :\nProgramme d'onboarding complet sur 1 an avec votre manager et votre RH\nProgramme de formation (Scalian Academy, e-learning, webinaires et formations externes)\nCommunaut\u00e9s techniques (Squads, Practices) afin de valoriser et d\u00e9velopper votre expertise\n\u00c9v\u00e9nements internes (Afterworks, Awards Dinner, Kick Off, Live Event du COMEX, Stand Up) et externes (participation \u00e0 des salons et forums sp\u00e9cialis\u00e9s dans nos domaines d\u2019activit\u00e9s\u2026)\nDispositif d\u2019acc\u00e9l\u00e9ration d\u2019acc\u00e8s \u00e0 la mobilit\u00e9 interne et \u00e0 des \u00e9changes internationaux type Erasmus\nParce que Scalian favorise la Qualit\u00e9 de Vie au Travail :\nCertifications Great Place to Work\u00ae et Best Workplaces for Women\u00ae\nPrime de cooptation, prime vacances, prise en charge par l\u2019employeur de 60% des titres-restaurant, Accord t\u00e9l\u00e9travail (jusqu\u2019\u00e0 2,5 jours par semaine indemnis\u00e9s), RTT (dont une partie mon\u00e9tisable), CSE (activit\u00e9s ludiques, ch\u00e8ques-cadeaux, ch\u00e8ques vacances)\nBerceaux en cr\u00e8ches inter-entreprises\nDon ou r\u00e9ception de jours de cong\u00e9s en cas de difficult\u00e9s personnelles\nParce que Scalian d\u00e9veloppe une politique RSE concr\u00e8te et ambitieuse :\nMobilit\u00e9 durable (indemnit\u00e9 kilom\u00e9trique v\u00e9lo, leasing de v\u00e9los \u00e0 assistance \u00e9lectrique)\nActions environnementales (Fresque du Climat, Reforest'Action, Clean Up Day, m\u00e9c\u00e9nat ONF)\nPostes ouverts aux personnes en situation de Handicap\nDiverses politiques de diversit\u00e9, d\u2019inclusion et d\u2019int\u00e9gration mises en place\nScalian c\u2019est aussi :\nUne entreprise en tr\u00e8s forte croissance qui, cr\u00e9\u00e9e en 1989, compte aujourd\u2019hui plus de 5500 personnes\nDes r\u00e9f\u00e9rences clients \u00e0 forte valeur ajout\u00e9e aupr\u00e8s de grands industriels fran\u00e7ais (du CAC40) et internationaux\nUn terrain de jeu o\u00f9 l\u2019expertise se conjugue avec audace, libert\u00e9 d\u2019entreprendre et convivialit\u00e9\nSi vous aspirez \u00e0 un environnement de travail qui valorise autant votre bien-\u00eatre que votre d\u00e9veloppement professionnel,\nrejoignez-nous et exprimez pleinement votre talent !\nEnvie d'\u00e9largir le cadre ?\nJe suis Liza Djehel, Talent Acquisition Officer.\nSi votre CV est retenu, je vous contacte pour un premier \u00e9change t\u00e9l\u00e9phonique de 15 \u00e0 20 minutes.\nNous d\u00e9terminons ensemble si ce poste est en ad\u00e9quation avec vos comp\u00e9tences et surtout, avec vos attentes.\nL'\u00e9change est positif ? Nous convenons d'un entretien de 1h (en pr\u00e9sentiel ou en visio) avec Lucas Daunar, Business Manager \u00e0 Sophia-Antipolis. Cet \u00e9change permet de revenir en d\u00e9tail sur vos comp\u00e9tences, vos attentes, de vous pr\u00e9senter le poste plus en d\u00e9tail, et d'\u00e9voquer d'autres opportunit\u00e9s.\nNous pr\u00e9voyons ensuite un rendez-vous technique de 1h (en pr\u00e9sentiel ou en visio) avec un de nos responsable technique.\nEnfin, nous vous pr\u00e9sentons notre proposition d'embauche.\nNotre processus de recrutement dure entre 15 et 30 jours\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [
                "Linux"
            ],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "ML"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [
                "40"
            ],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "DATA ENGINEER",
        "company": "Action for Market Transformation - A4MT",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-action-for-market-transformation-a4mt-3910049004?position=10&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=TK2pmr%2BnPCY9%2FL7EfnBCng%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "A4MT \u2013 Action pour la Transformation des March\u00e9s\nA4MT con\u00e7oit et impl\u00e9mente des programmes d\u2019engagement et de \u00ab Market Transformation \u00bb qui visent \u00e0 g\u00e9n\u00e9raliser des pratiques vertueuses \u2013 au sens environnemental et soci\u00e9tal \u2013 en modifiant la donne du march\u00e9, en reconfigurant le jeu d\u2019acteurs, g\u00e9n\u00e9ralement via des actions collectives.\nCes programmes agissent sur la demande en suscitant de nouvelles pratiques individuelles et collectives. A4MT assure le r\u00f4le de pilote, orchestrant les plans d\u2019action des parties prenantes gr\u00e2ce \u00e0 une \u00e9quipe de qualit\u00e9 \u00e0 caract\u00e8re international, un savoir-faire sur la mise en \u0153uvre des programmes, une connaissance technico-\u00e9conomique experte des sujets trait\u00e9s, et une capacit\u00e9 \u00e0 interpeller les d\u00e9cideurs \u00e0 bon niveau.\nChampionnat de France des \u00e9conomies d\u2019\u00e9nergie\nA4MT avec ses partenaires op\u00e8re l\u2019ensemble des concours CUBE en France (Championnat de France des Economies d\u2019Energies) et assure son d\u00e9veloppement international (Europe, Asie, etc.). CUBE est un concours original d\u2019\u00e9conomies d\u2019\u00e9nergie et de CO2 pour les b\u00e2timents tertiaires et r\u00e9sidentiels qui acc\u00e9l\u00e8re fortement l\u2019action de terrain gr\u00e2ce \u00e0 une intelligence collective sur le terrain.\nLe concours est aujourd\u2019hui pr\u00e9sent dans 8 pays et se d\u00e9veloppe encore. Au-del\u00e0 des \u00e9conomies les plus faciles, il s\u2019agit de mettre en \u0153uvre la trajectoire de gestion immobili\u00e8re et d\u2019investissement qui permettra, au-del\u00e0 des avanc\u00e9es dans ce programme \u00e0 faible investissement, de progresser sur la trajectoire de la neutralit\u00e9 carbone.\nhttps://championnatdefrancedeseconomiesdenergie.org/\nMISSION\nRendant compte au directeur d\u2019A4MT et en \u00e9troite collaboration avec le directeur technique A4MT, vous \u00eates Data Engineer, vous serez responsable de la conception, du d\u00e9veloppement et de la maintenance des bases de donn\u00e9es, et des outils de reporting. Vous travaillerez en \u00e9troite collaboration avec l'\u00e9quipe de d\u00e9veloppement (prestataire externe) et vous participez \u00e0 la structuration d\u2019une \u00e9quipe IT interne pour cr\u00e9er des solutions innovantes r\u00e9pondant aux besoins de l'entreprise.\nVotre mission s'articule autours des 3 axes ci-dessous:\n1/ Pilotage et et d\u00e9veloppement\nd\u00e9velopper et d\u00e9ployer des reporting robustes et \u00e9volutifs.\nle planning de d\u00e9veloppement et le budget allou\u00e9.\navec les \u00e9quipes d\u2019animation et le back office technique du programme et avec les prestataires externes pour comprendre les exigences et les sp\u00e9cifications du projet.\n\u00e0 la conception de l'architecture des bases de donn\u00e9es et \u00e0 la prise de d\u00e9cisions techniques.\nla qualit\u00e9 des donn\u00e9es en effectuant des contr\u00f4les qualit\u00e9.\nles performances des applications pour garantir une exp\u00e9rience utilisateur fluide.\nla maintenance et les mises \u00e0 jour r\u00e9guli\u00e8res des applications existantes.\n\u00e0 l'aff\u00fbt des tendances et des technologies \u00e9mergentes.\nVous serez responsable du process, de la ma\u00eetrise d\u2019ouvrage li\u00e9e \u00e0 la Data et garant(e) de la qualit\u00e9 de service.\n2/ Implication des \u00e9quipes et de la sous-traitance\nVous serez impliqu\u00e9 dans une \u00e9quipe informatique naissante et dans une \u00e9quipe projet avec les diff\u00e9rentes fonctions m\u00e9tiers. Vous devrez faire le suivi de votre implication avec le responsable de programme et le directeur technique d\u2019 A4MT :\n3/ Gestion de projet\nVous tiendrez le tableau de bord des outils : budgets, engagements, planning, r\u00e9sultats, d\u00e9veloppements.\nPROFIL\nVous avez une exp\u00e9rience significative d\u2019au moins 3 ann\u00e9es dans l\u2019\u00e9cosyst\u00e8me de big data, des serveurs et bases de donn\u00e9es dans des contextes de projets, d\u2019exploitation de migration.\nCOMPETENCES\nBac +5 dipl\u00f4m\u00e9(e) d\u2019une grande \u00e9cole d\u2019ing\u00e9nieur ou \u00e9quivalent, vous \u00eates :\n+5 dipl\u00f4m\u00e9 (e) d\u2019une \u00e9cole d\u2019ing\u00e9nieurs ou \u00e9quivalent, en Data science, Informatique, g\u00e9nie logiciel ou domaine connexe.\nprofessionnelle d\u00e9montr\u00e9e de 3 ans ou plus en tant que Data Engineer\ndes langages structur\u00e9s (JavaScript, Scala, Python\u2026),\navec les bases de donn\u00e9es relationnelles (MySQL, PostgreSQL) et non relationnelles (MongoDB, Firebase).\nau moins un outil de reporting (Power BI, Tableau \u2026)\ndes services de d\u00e9ploiement et d'h\u00e9bergement cloud comme AWS, Azure ou Google Cloud Platform.\ncomp\u00e9tences en d\u00e9veloppement back-end avec des technologies comme Node.js, Python, Ruby on Rails, ou Java. et notamment en PHP sont recommand\u00e9es\ndes langages de programmation front-end tels que HTML5, CSS3 et JavaScript (notamment frameworks: comme React, Angular ou Vue.js).\n\u00e0 travailler en \u00e9quipe, \u00e0 communiquer efficacement et \u00e0 r\u00e9soudre les probl\u00e8mes de mani\u00e8re autonome.\ndes principes de s\u00e9curit\u00e9 des applications web et des meilleures pratiques en mati\u00e8re de d\u00e9veloppement s\u00e9curis\u00e9 ainsi que le respect du RGPD.\nDate d\u2019entr\u00e9e et conditions\nLe poste est \u00e0 pourvoir imm\u00e9diatement; il est bas\u00e9 au 54, rue de Clichy, Paris (IX\u00e8me). Niveau de r\u00e9mun\u00e9ration selon exp\u00e9rience.\nContact\nMerci d\u2019adresser votre candidature compl\u00e8te (CV, lettre de motivation, pr\u00e9sentation du cursus en cours de conclusions et r\u00e9f\u00e9rences \u00e9ventuelles) \u00e0 l\u2019attention de M. Adrien Brunella sur le mail elisabeth.clement@a4mt.com\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go",
                "JavaScript",
                "HTML"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "MySQL",
                "PostgreSQL"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Database Developer",
        "company": "Selby Jennings",
        "location": "Greater Paris Metropolitan Region",
        "link": "https://fr.linkedin.com/jobs/view/database-developer-at-selby-jennings-3900070880?position=11&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=MmyZ4WJAhpEun%2BieFAUZTQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "I am working on a role with a global quantitative and systematic hedge fund who are looking for a Database Developer to join their team in Paris to work closely with trading and research functions.\nThe ideal candidate would need to be proficient in Python, have System Architecture Experience and have proven professional work experience as a database developer.\nKey Responsibilities\n\u2022 Your role involves providing daily trading insights to multiple activities and asset classes.\n\u2022 You will work with traders, quants and other quantitative developers to build robust reporting and analytical tools, thus providing constant feedback on investment strategy performance.\n\u2022 Given the dynamic nature of the business, you will continually seek solutions that are both flexible and robust and which integrate seamlessly into their technology landscape.\n\u2022 Design and maintain storage solutions to help automate daily processes to crunch trading data\n\u2022 Take ownership of production processes to ensure constant alignment with business objectives\n\u2022 Continually expand and upgrade the software infrastructure to accommodate the changing business needs\nYour present skillset\n\u2022 Expertise in\nrelational databases, NoSql databases\nor other storage solutions for large data sets\n\u2022 Proven professional work experience as a\ndatabase developer\n\u2022\nPython, C#\nexperience nice to have\n\u2022\nSystem Architecture Experience\n\u2022 Ability to multitask, set priorities and work in a team\n\u2022 Excellent communication skills and team player\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "C#",
                "R"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "ML"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Ramify",
        "location": "Greater Paris Metropolitan Region",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-ramify-3896146641?position=12&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=HexetsgIg2qZbxO1WiSWdQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ABOUT\nRamify\u2019s mission is to help people achieve financial freedom, no matter who they are and how much they have. We are revolutionizing the private wealth management industry by making smart and affordable financial products for everyone.\nNo more standardized solutions, hidden fees and complicated words, Ramify enables everyone to become a smart investor. The team combines elements of its research with technology to design customized investment portfolios composed of world-class financial products.\nThe team comprises around 15 talented individuals. Ramify is looking for talented people in all sectors, who want to have a huge impact, move fast and deliver.\nJOB DESCRIPTION\nThe Quantitative Investment Solutions (QIS) Team is dedicated to designing innovative investment portfolio models and developing cutting-edge investment features within our product suite. Moreover, the QIS Team is at the forefront of driving AI-based solutions for Ramify. This involves conceptualizing and implementing transformative AI solutions tailored to meet the diverse needs of various teams within Ramify.\nAs a Data Engineer on our QIS Team, you will play a pivotal role in shaping the future of investment strategies through data-driven insights and AI-based solutions. Collaborating closely with our talented team of quantitative researchers, and investment experts, your responsibilities will encompass architecting and implementing robust data pipelines. These pipelines will facilitate the seamless integration of diverse data sources, empowering Ramify teams to make informed decision-making.\nKey Responsibilities:\nDesign, build and launch data pipelines at scale to move data across Ramify platform with SQL technologies.\nDesign and implement processes and tools for data onboarding and quality, helping to deliver an industry best-practice solution for managing the data lifecycle.\nProduce stand-alone tools that can be used by other teams to automate data quality and discover faults.\nBuild analytical tools that provide insight into business metrics across Ramify.\nArchitect and lead the implementation of AI based solutions within Ramify\nPREFERRED EXPERIENCE\nWe're looking for people who:\nWant to make a difference. We are a small team effectively reshaping how people look at the industry. We need people who 'get it' and want to play an integral part in helping us accomplish this mission and are persistent in getting the job done.\nSkills we're looking for:\nMaster\u2019s or upper-year undergraduate-level coursework in either Computer Science, Management Information Systems, Business Information Systems, Mathematics or Finance related field.\n2+ years of professional experience in data engineering.\n2+ years of experience with one or more coding languages such as Python (is a must), Java.\nExperience with data modeling and ETL design, implementation and maintenance.\nDemonstrable mastery of industry best practices in the data lifecycle, including data quality automation and tooling.\nExcellent written and verbal communication skills with ability to communicate complex designs and solutions to non-technical and highly technical audiences alike.\nGood attention to detail.\nStrong analytics and strategic thinking skills\nNice-to-haves :\nUnderstanding of ML/ Generative AI technologies and their applications.\nPossess a passion, curiosity, and energy for finance + investing. You understand the ins and outs of the wealth management, trading, and more importantly - know how to explain these concepts simply\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (F/H)",
        "company": "Renault Digital",
        "location": "Boulogne-Billancourt, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-renault-digital-3911202728?position=13&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=qgGV8j9UkcjRkl8hja9q2A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Contexte :\nDans le cadre de son programme Industrie 4.0, Renault d\u00e9veloppe depuis 2017 sa propre plateforme pour connecter et agr\u00e9ger les donn\u00e9es industrielles des 22 sites du Groupe et de plus de 2500 machines.\nFort de partenariats strat\u00e9giques sign\u00e9s avec Google Cloud (stack data full GCP), Renault Digital est \u00e0 la recherche d\u2019un(e) Data Engineer au sein du P\u00f4le Architecture et Data pour mettre en place des cha\u00eenes de traitement de donn\u00e9es r\u00e9pondant \u00e0 de nouveaux besoins m\u00e9tiers.\nVous collaborerez au jour le jour avec les \u00e9quipes m\u00e9tiers ainsi qu\u2019avec les autres fonctions du P\u00f4le Architecture & Data (Data Analysts et Scientists, architectes, \u2026), exploitant des t\u00e9raoctets de donn\u00e9es (\u00e9v\u00e9nements en mode streaming, traitements en batch et temps r\u00e9els et les appels aux APIs) afin entre autres d\u2019alimenter des mod\u00e8les de machine learning (segmentation clients, d\u00e9tection automatiquement des pannes des v\u00e9hicules, \u2026).\nResponsabilit\u00e9s principales :\nVous participez aux phases de framing, MVP et release des produits, services et APIs orient\u00e9s data ;\nVous argumentez les choix d\u2019architecture des projets et de la plateforme datalake sur GCP ;\nVous contribuez \u00e0 la valeur m\u00e9tier des produits orient\u00e9s Data s\u2019appuyant sur le Datalake, en mettant en place des cha\u00eenes bout en bout de traitement de la data, de l\u2019ingestion \u00e0 l\u2019exposition d\u2019APIs et \u00e0 la visualisation des donn\u00e9es et des solutions ML/DS ;\nVous \u00eates garant de la qualit\u00e9 des donn\u00e9es transform\u00e9es dans le Datalake, du bon fonctionnement des cha\u00eenes de traitement et de l\u2019optimisation de l\u2019utilisation des ressources des ressources cloud ;\nVous proposez des standards d\u2019architecture et de d\u00e9veloppement ;\nVous \u00eates force de proposition, innovant(e) et bienveillant(e).\nEnvironement technique :\nSpark, Scala, Python, Java, Airflow, SQL, Google Cloud Platform (BigQuery, Cloud Storage, PubSub, Beam, Dataflow, Cloud ML, TensorFlow, Kubernetes), Git, Docker, JSON, Bash, Spotfire\nProfil recherch\u00e9 :\nVous avez minimum 5 ans d\u2019exp\u00e9rience en tant que Data Engineer ;\nVous disposez d\u2019une exp\u00e9rience en d\u00e9veloppement Spark, Scala, Python et requ\u00eatage SQL sur des gros volumes de donn\u00e9es ;\nVous avez une app\u00e9tence pour la data : validation, transformation, analyse, valorisation ;\nVous poss\u00e9dez une exp\u00e9rience de d\u00e9veloppement et orchestration de chaines ETL complexes via Airflow ou \u00e9quivalent ;\nVous pratiquez la m\u00e9thodologie agile (Agile Scrum et/ou Kanban) ;\nVous utilisez les services cloud (pr\u00e9f\u00e9rablement GCP) ;\nVous \u00eates capable d\u2019\u00e9changer en anglais technique \u00e9crit et oral.\nInformations compl\u00e9mentaires :\nVotre poste sera bas\u00e9 \u00e0 Boulogne-Billancourt (France) en CDI (temps plein)\nVous b\u00e9n\u00e9ficiez de 2 \u00e0 3 jours de t\u00e9l\u00e9travail par semaine\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go",
                "Bash"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "DataSerialization": [
                "Json"
            ],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes",
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "ML",
                "Machine Learning",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [
                "Temps plein",
                "Full"
            ],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer EMEA (F/M/D)",
        "company": "Flowdesk",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-emea-f-m-d-at-flowdesk-3860942388?position=14&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=%2FKDndimAKY6wICIB4Jcdkw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Flowdesk is seeking a highly motivated Data Engineer to join the team and managed by the Quentin, the Lead Data!\nThe data engineer will be an integral part of the growing data team, working closely with our trading and quantitative departments to provide a platform and tools to answer their analytical needs. The data platform revolves around Dagster deployed on Kubernetes for the orchestration, dbt on BigQuery for the transformations, and Looker for the visualization.\nAmong our current challenges, we are currently focusing on the real-time ingestion and processing of terabytes of market data.\nResponsibilities\nDesign, build, and maintain efficient, scalable, and reliable batch and streaming data pipelines to support Flowdesk's trading infrastructure.\nDevelop and maintain data warehouses, data lakes, and other data management systems with a strong focus on logical and physical modeling.\nContribute early on to the definition of the data team's data products to maximize ease of access, data quality, and related documentation.\nCollaborate with the software engineering team to integrate data-related functionality into Flowdesk's trading infrastructure and other applications.\nLeverage cutting-edge data engineering technologies to continually improve the speed, reliability, and scalability of Flowdesk's data processing capabilities.\nDevelop and maintain strong processes and procedures for data quality control, data validation, data documentation and, data integration.\nCollaborate with cross-functional teams including traders, developers, and compliance officers to ensure that all data used by Flowdesk is accurate, timely, and compliant with relevant regulations and requirements.\nRequirements\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n3+ years of experience in data engineering or related field.\nAwareness of software, data, and analytics engineering best practices (e.g. programming standards, data modelization, code idempotency....)\nGeneral understanding of systems architecture and concepts (distributed computing, lake-house architecture, ci/cd workflows...)\nExperience optimizing modern data warehousing platforms (BigQuery is a plus).\nStrong communication skills and ability to work collaboratively in a fast-paced international environment.\nKnowledge of the data engineering ecosystem (contribution to open source projects is a plus).\nStrong analytical and problem-solving skills with a keen attention to detail.\nBenefits\n\ud83c\udf0d International environment (English is the main language)\n\ud83d\ude83 50% of transportation costs & a sustainable mobility agreement\n\ud83c\udf54 Swile lunch voucher (\u20ac9.25 per day, 60% covered)\n\ud83c\udfe5 100% Alan Blue covered for you and your children\n\ud83d\udcbb Top of the range equipment{{:}} Macbook, keyboard, laptop stand, 4K monitor & headphones\n\ud83c\udf89 Team events and offsites\n\ud83d\udd1c Coming soon {{:}} gym memberships, international mobility & lot of other cool benefits !\nRecruitment process\n\ud83d\udc40 Are you interested in this job but feel you haven't ticked all the boxes? Don't hesitate to apply and tell us in the cover letter section why we should meet!\n\ud83d\udcdd Here's what you can expect if you apply{{:}}\nHR interview (30')\nTechnical test\nTechnical interview (60')\nChat with the Head of People (30') and the Head of Department (30')\nOn the agenda{{:}} discussions rather than trick questions! These moments of exchange will allow you to understand how Flowdesk works and its values. But they are also (and above all) an opportunity for you to present your career path and your expectations for your next job!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "DATA ENGINEER (H/F)",
        "company": "SFR",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-sfr-3879318123?position=15&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=SFaLXT0NP%2F1pgJi2pSe0Og%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "En tant que Data Ing\u00e9nieur exp\u00e9riment\u00e9, vous occuperez un r\u00f4le essentiel dans notre \u00e9quipe Data Science.\nVous serez responsable de la conception, du d\u00e9veloppement et de la maintenance des pipelines de donn\u00e9es ainsi que de l'int\u00e9gration de sources de donn\u00e9es multiples.\nVotre expertise sera cruciale pour garantir une gestion efficace des flux de donn\u00e9es, ainsi que pour faciliter l'analyse et la visualisation des donn\u00e9es en plus du support aux data scientists vos missions seront les suivantes :\nArchitecture projet des donn\u00e9es\n: Concevoir et d\u00e9velopper des architectures projet de donn\u00e9es robustes, \u00e9volutives et performantes pour int\u00e9grer et g\u00e9rer de grandes quantit\u00e9s de donn\u00e9es provenant de sources multiples. Assurer la fiabilit\u00e9, l'\u00e9volutivit\u00e9 et la s\u00e9curit\u00e9 des flux de donn\u00e9es entrant d\u2019un projet Data Science.\nInt\u00e9gration des donn\u00e9es\n: \u00c9laborer des pipelines de donn\u00e9es efficaces pour l'extraction, la transformation et le chargement des donn\u00e9es (via notre Framework ELT/ETL interne) provenant de diff\u00e9rentes sources. Mettre en place des processus d'int\u00e9gration automatis\u00e9s et veiller \u00e0 la qualit\u00e9 des donn\u00e9es.\nGestion des bases de donn\u00e9es\n: Concevoir et optimiser des bases de donn\u00e9es pour r\u00e9pondre aux besoins analytiques et de reporting. Assurer la performance, la disponibilit\u00e9 et la s\u00e9curit\u00e9 des bases de donn\u00e9es, ainsi que la gestion efficace des requ\u00eates.\nCollaboration interfonctionnelle\n: Support des Data Scientists, vous travaillerez avec les \u00e9quipes business pour comprendre leurs besoins et fournir des conseils et des recommandations bas\u00e9s sur les donn\u00e9es.\nOptimisation des performances\n: Surveiller et optimiser les performances des pipelines de donn\u00e9es, des bases de donn\u00e9es et des requ\u00eates. Identifier les goulots d'\u00e9tranglement et les points d'optimisation, et proposer des am\u00e9liorations pour garantir des performances optimales.\nS\u00e9curit\u00e9 et conformit\u00e9\n: Veiller \u00e0 ce que les donn\u00e9es soient trait\u00e9es et stock\u00e9es conform\u00e9ment aux normes de s\u00e9curit\u00e9 et de confidentialit\u00e9. Mettre en place des m\u00e9canismes de s\u00e9curit\u00e9 pour prot\u00e9ger les donn\u00e9es sensibles et garantir la conformit\u00e9 aux r\u00e9glementations en vigueur.\nVotre profil :\nVous avez un\nDipl\u00f4me universitaire en informatique, en g\u00e9nie logiciel, en science des donn\u00e9es ou dans un domaine connexe et vous avez \u00e0 minima 5 ans d'exp\u00e9rience en tant que Data Ing\u00e9nieur.\nVous poss\u00e9dez \u00e9galement une solide ma\u00eetrise des technologies et des outils suivants :\nHadoop, Spark, SQL, Kafka, GCP BigQuery,\nDe plus vous avez une bonne compr\u00e9hension des architectures, des mod\u00e8les et des concepts de base de donn\u00e9s avec une exp\u00e9rience avanc\u00e9e dans la mise en \u0153uvre de pipelines ETL et dans la gestion de bases de donn\u00e9es.\nVos connaissances en mati\u00e8re de s\u00e9curit\u00e9 des donn\u00e9es, de conformit\u00e9 aux r\u00e9glementations ainsi que vos comp\u00e9tences en programmation scripting et en d\u00e9veloppement logiciel seront un plus.\nVos excellentes comp\u00e9tences en communication seront des qualit\u00e9s appr\u00e9ci\u00e9es et\nun niveau d'anglais (appliqu\u00e9e au domaine technique) est un plus.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication",
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "RSight\u00ae",
        "location": "\u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-rsight%C2%AE-3856216625?position=16&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=qLSCutHr%2FcwFsKvRnygi3g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Nous recherchons pour notre client, un\nleader mondial des services et conseils en technologies\n, un\ning\u00e9nieur Databricks et Data Factory\nqui rejoindra une \u00e9quipe qui combine des comp\u00e9tences m\u00e9tiers avec une forte expertise data, analytique et d\u2019intelligence artificielle pour mettre en \u0153uvre des solutions qui visent \u00e0 am\u00e9liorer la gestion et la valorisation des donn\u00e9es.\nDescriptif des missions:\nVous \u00eates int\u00e9ress\u00e9 \u00e0 travailler sur une solution ayant un impact direct sur les ambitions de notre client en mati\u00e8re de data (datadriven, data d\u00e9mocratisation) ? Alors devenez membre de l\u2019\u00e9quipe Corporate Data Lake de notre client ! Comme tout autre membre de l'\u00e9quipe, vous :\nParticiper \u00e0 la d\u00e9finition des composants informatiques supportant la fourniture de services\nD\u00e9velopper, tester, industrialiser et d\u00e9ployer des composants en minimisant les impacts sur les utilisateurs (automatisation, 0 temps d'arr\u00eat,...)\nDocumenter la bonne utilisation des services\nD\u00e9ployer et supporter nos fonctionnalit\u00e9s sur la plateforme\nApporter assistance et conseils aux utilisateurs m\u00e9tiers\nOp\u00e9rer la solution en op\u00e9ration courante (incluant le suivi de la qualit\u00e9 des services) et intervenir dans la r\u00e9solution des incidents\nParticiper activement \u00e0 l'am\u00e9lioration continue des activit\u00e9s de l'\u00e9quipe\nExpliquer aux collaborateurs ce que le Corporate Data Lake peut faire pour eux\nConfigurer des espaces de travail pour eux\nFournir du coaching et de l'expertise lors de r\u00e9unions en face \u00e0 face ou sur les canaux communautaires\nParticiper \u00e0 l'effort de support de la plateforme dans une approche \"vous la construisez, vous l'ex\u00e9cutez\"\nContribuer aux premi\u00e8res phases de conception d\u00e9finissant l'avenir du Corporate Data Lake\nComp\u00e9tences:\n1er exp\u00e9rience Azure (PaaS et IaaS)\nConnaissance de Databricks et Data Factory\nMa\u00eetrise d'un ou plusieurs langages parmi : Python, Scala, Spark, PowerShell\nInt\u00e9gration et livraison continues (Jenkins, Azure Devops, GIT Lab CI, \u2026)\nPratique des fondamentaux du g\u00e9nie logiciel (Gestion de Configuration, Tests,...)\nAnglais : \u00e0 l'aise pour assister \u00e0 une r\u00e9union et r\u00e9diger de la documentation technique\nBonne capacit\u00e9 d'\u00e9coute, orientation client/utilisateur\nExpression orale et \u00e9crite adapt\u00e9e \u00e0 l'interlocuteur\nCuriosit\u00e9 et adaptation aux changements technologiques\nB\u00e9n\u00e9fices:\nUn processus de recrutement court, un accompagnement personnalis\u00e9, une \u00e9volution qui s'adapte \u00e0 votre trajectoire de carri\u00e8re.\nEn plus de votre quotidien li\u00e9 \u00e0 votre mission, vous pourrez entreprendre, \u00eatre form\u00e9, passer des certifications.\nPlan d'\u00e9pargne pour la retraite collectif, mutuelle, tickets restaurant, des cong\u00e9s d'anciennet\u00e9, un catalogue CE, des accords d\u2019entreprise relatifs au t\u00e9l\u00e9travail et \u00e0 la parentalit\u00e9 et autres avantages.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (F/H)",
        "company": "Thales",
        "location": "V\u00e9lizy-Villacoublay, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-thales-3908228180?position=17&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=NNrRMFzlfyvjpi8zEBQidg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs pr\u00e9sents sur tous les continents. Le Groupe investit dans les innovations du num\u00e9rique et de la \u00ab deep tech \u00bb \u2013 big data, intelligence artificielle, connectivit\u00e9, cybers\u00e9curit\u00e9 et quantique \u2013 pour construire un avenir de confiance, essentiel au d\u00e9veloppement de nos soci\u00e9t\u00e9s, en pla\u00e7ant l\u2019humain au c\u0153ur des d\u00e9cisions.\nThales propose des solutions, services et produits qui aident ses clients \u2013 entreprises, organisations, Etats \u2013 dans cinq grands march\u00e9s vitaux pour le fonctionnement de nos soci\u00e9t\u00e9s : identit\u00e9 et s\u00e9curit\u00e9 num\u00e9riques, d\u00e9fense, a\u00e9ronautique, espace, et transport.\nQUI ETES-VOUS ?\nDipl\u00f4m\u00e9 d\u2019un Bac+5 en \u00e9cole d\u2019ing\u00e9nieur ou \u00e9quivalent universitaire avec une sp\u00e9cialisation en informatique, vous avez au moins 3 ans d'exp\u00e9rience dans les technologies Big Data.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que Data Engineer, vous jouerez un r\u00f4le cl\u00e9 dans la conception, le d\u00e9veloppement et la maintenance de notre infrastructure de donn\u00e9es, ainsi que dans la transformation et la gestion des flux de donn\u00e9es.\nVOS MISSIONS :\n\u2022 Concevoir, d\u00e9velopper et d\u00e9ployer des solutions Big Data en utilisant les technologies Hadoop.\n\u2022 Mettre en place des pipelines de donn\u00e9es performants pour l'ingestion, le traitement et le stockage des donn\u00e9es massives.\n\u2022 Collaborer \u00e9troitement avec les \u00e9quipes m\u00e9tier pour comprendre leurs besoins en mati\u00e8re d'analyse de donn\u00e9es et proposer des solutions adapt\u00e9es.\n\u2022 Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des donn\u00e9es.\n\u2022 Assurer la qualit\u00e9 et la fiabilit\u00e9 des donn\u00e9es trait\u00e9es, en mettant en place des processus de validation et de nettoyage.\n\u2022 Identifier et r\u00e9soudre les probl\u00e8mes li\u00e9s \u00e0 l'infrastructure Big Data et proposer des am\u00e9liorations.\n\u2022 Travailler en \u00e9troite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents \u00e0 partir des donn\u00e9es.\nInnovation, passion, ambition : rejoignez Thales et cr\u00e9ez le monde de demain, d\u00e8s aujourd\u2019hui.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data"
            ],
            "FrSoftSkills": [
                "Collaboration",
                "Organisation"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer GCP (H/F)",
        "company": "SQLI",
        "location": "Greater Paris Metropolitan Region",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-gcp-h-f-at-sqli-3849296046?position=18&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=Lx91k5AJ27v1%2BCUIKqVSjw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Rejoignez\nSQLI\net faites partie de l'\u00e9quipe Data, au sein d\u2019une soci\u00e9t\u00e9 \u00e0 taille humaine, mais avec de grandes ambitions. Nous sommes plus de 2200 talents sur 13 pays et 3 continents.\u200b\nVotre futur \u00e9cosyst\u00e8me :\nAu sein du p\u00f4le Data de SQLI\n, pour rejoindre une \u00e9quipe dynamique de +40 passionn\u00e9s.\nDes projets digitaux pour des clients grands comptes\n: notamment dans les secteurs des services financiers, de l'industrie et du retail.\nUne organisation orient\u00e9e delivery\n: vous travaillerez au sein de nos locaux ou en \u00e9quipe int\u00e9gr\u00e9e chez nos clients en mode 100% agile, en mode projet.\nPossibilit\u00e9 de t\u00e9l\u00e9travail jusqu'\u00e0 3 jours par semaine\nDes communaut\u00e9s d\u2019Experts\n, pour vous aider \u00e0 progresser\u200b, avec des workshops et ateliers techniques favorisant le partage de connaissances.\u200b\nDes Managers de carri\u00e8re\n, pour \u00eatre suivi par l'un de vos pairs sur l'entit\u00e9 Data, avec un\naccompagnement dans l\u2019expression de vos talents\n(certifications et formations notamment via le partenariat Solutions de Microsoft, participation aux \u00e9v\u00e8nements/salons, publications dans la presse...).\nDescription du poste :\nUn poste de\nData Engineer GCP (H/F)\nest ouvert \u00e0\nPARIS ou ROUEN\n(selon votre localisation)\n, pour faire partie de l\u2019\u00e9quipe Data chez SQLI et vous investir dans un environnement technique innovant.\nVos missions seront :\nL'analyse et la compr\u00e9hension des besoins m\u00e9tiers.\nLa participation \u00e0 la d\u00e9finition et \u00e0 la conception de l\u2019architecture.\nLa r\u00e9alisation des pr\u00e9sentations, d\u00e9monstrations, POC ou Pilotes pour mettre en lumi\u00e8re les recommandations technologiques.\nLes d\u00e9veloppements de jobs d\u2019alimentation (pr\u00e9paration, ingestion, traitement et contr\u00f4le qualit\u00e9) et l'automatisation des flux d\u2019alimentation du Data Lake et du Datawarehouse\nLes tests de charge, tests unitaires\u2026\nLa maintenabilit\u00e9 de la solution Big Data/BI : optimisation et performance des traitements.\nQualifications :\nIng\u00e9nieur(e) de formation, avec minimum\n3 ans d\u2019exp\u00e9rience sur des projets Google Cloud Platform (BigQuery, Dataflow, ...)\nToujours en veille, \u00e0 l\u2019affut des nouveaut\u00e9s technologiques et vous aimez \u00e9changer (Events, conf\u00e9rences, meetups, etc\u2026).\nForce de proposition, vous vous sentez libre d\u2019oser et de vous surpassez en partageant vos id\u00e9es.\nComp\u00e9tences techniques requises :\nMa\u00eetrise d'un langage de programmation\n(Python, Java, R, Spark, Scala).\nMa\u00eetrise de\nSQL.\nUne exp\u00e9rience sur au moins un ETL/ELT\n(Talend, DBT).\nBonne connaissance des outils et framework d\u2019industrialisation\nCI/CD\net/ou gestion de version (Gitlab).\nSerait un plus : une exp\u00e9rience sur Power BI, TIBCO EBX et/ou BO DS + la gestion de Conteners et Kubernetes (GKE).\nVous pensez que ce poste est fait pour vous ? Transmettez-nous votre profil !\nTous nos postes sont ouverts aux personnes en situation de handicap.\nDes questions sur vos donn\u00e9es personnelles ? Retrouvez notre politique de confidentialit\u00e9 concernant les candidats :\nhttps://www.sqli.com/sites/default/files/2024-01/SQLI-PRIV-Politique-Confidentialite-Candidats-C0-29012024.pdf\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer - Profils exp\u00e9riment\u00e9s H/F",
        "company": "LCL",
        "location": "Villejuif, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-profils-exp%C3%A9riment%C3%A9s-h-f-at-lcl-3888403052?position=19&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=P1%2BGTBi8FCAuG8zwrP4fXA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83c\udfe6 LCL, c\u2019est LA banque urbaine du Groupe Cr\u00e9dit Agricole - avec nous, accompagnez la transformation, le d\u00e9veloppement et le maintien technologiques de nos outils avec une vision business et de satisfaction de nos 6 millions de clients.\nEn tant qu\u2019acteur majeur de la banque de d\u00e9tail, nous nous adaptons chaque jour aux nouveaux modes de consommation et les projets de nos de clients internes et externes tout en garantissant le besoin de s\u00e9curit\u00e9 et de d\u00e9veloppement technologique qu\u2019impliquent nos activit\u00e9s.\n\ud83d\udca1Organis\u00e9es en mode Agile, les 8 squads de la tribu DATA (6 squads M\u00e9tier et 2 squads transverses) \u0153uvrent au quotidien pour r\u00e9pondre \u00e0 un enjeu majeur pour la banque : la collecte, le stockage, la gestion et l\u2019usage de la donn\u00e9e. En interaction permanente avec les autres tribus IT et les m\u00e9tiers, elles \u00e9tudient et proposent les solutions et architectures \u00e0 d\u00e9ployer pour r\u00e9pondre au mieux aux strat\u00e9gies de d\u00e9veloppement et de pilotage de l\u2019ensemble des m\u00e9tiers de la banque.\nRejoignez-nous si vous souhaitez participer aux r\u00e9flexions et au d\u00e9veloppement de la trajectoire technique et DataCentric du SI LCL et plus largement du Groupe CA. Vous c\u00f4toierez et serez au c\u0153ur de l\u2019impl\u00e9mentation de technologies vari\u00e9es telles que les plateformes Teradata, les solutions d\u2019architecture applicative des technologies BigData ou IA, des environnements analytiques ou encore des solutions de datavisualisation. Vous assurerez le traitement de donn\u00e9es en temps r\u00e9el ou en batch et exposerez les donn\u00e9es sous diff\u00e9rentes formes.\nQue vous souhaitiez devenir expert sur les socles technologiques ou relever le challenge de la gestion de projets M\u00e9tier, nous vous aiderons \u00e0 atteindre vos propres objectifs.\nVous rejoindrez une \u00e9quipe pluridisciplinaire, clairement orient\u00e9e vers le d\u00e9veloppement de ses collaborateurs \u00e0 de nouvelles technologies !\n\ud83c\udfaf En tant que Data Engineer :\n\u00b7 Vous aimez analyser les besoins avec les m\u00e9tiers, challenger, identifier les sources de donn\u00e9es dans les diff\u00e9rents univers technologiques, industrialiser des algorithmes, concevoir et d\u00e9velopper des Datalab ou des Datamart sur les plateformes ? Vous saurez relever les challenges propos\u00e9s par les squads m\u00e9tier !\n\u00b7 Vous pr\u00e9f\u00e9rez travailler \u00e0 l\u2019architecture et au d\u00e9ploiement de nouvelles plateformes, \u00e0 la lev\u00e9e de la dette technologique ou encore r\u00e9aliser de la veille au service de notre trajectoire ? La squad Socles Data est faite pour vous !\n\u00b7 Au-del\u00e0 des projets que vous g\u00e9rerez, garant du bon fonctionnement de votre parc applicatif, vous attacherez une grande attention \u00e0 la mise en \u0153uvre de solutions optimis\u00e9es.\n\u00b7 La rigueur, la communication, l\u2019esprit d\u2019\u00e9quipe mais aussi la curiosit\u00e9 et la cr\u00e9ativit\u00e9 font partie de vos soft skills ! ils vous permettront de r\u00e9pondre aux enjeux de s\u00e9curit\u00e9, de qualit\u00e9, de transmission de la connaissance et contribueront \u00e0 l\u2019atteinte des objectifs de l\u2019IT et plus largement de LCL, au service de ses clients.\n\ud83d\udcbb Voici les principales technologies utilis\u00e9es au sein de la tribu, si certaines vous sont famili\u00e8res, nous vous aiderons \u00e0 monter en comp\u00e9tence sur d\u2019autres !\nLangages utilis\u00e9s : SQL, Python, Scala\nSGBD : Teradata et utilitaires (TPT, BTEQ, \u2026)\nStreaming : Kafka\nSearch : ElasticSearch, SolR\nEnvironnement : Unix\nSolutions Big Data : Hadoop Cloudera, DataIku, HDFS, Hive, Impala,\nDevops : GitLab, Jenkins, Nexus\nOutils de visualisation : MS BI (SSIS, SSAS, SSRS) Qlik Sens, BO\nMod\u00e9lisation : MEGA\nOutils collaboratifs : GIT, Jira, Confluence, Teams\n\u26a1Si les nouveaux enjeux bancaires vous int\u00e9ressent, que vous souhaitez int\u00e9grer une \u00e9quipe Agile au service des m\u00e9tiers dans laquelle vous serez force de proposition et que vous aimez travailler dans un environnement motivant et dynamique, rejoignez-nous, cette offre est faite pour vous !\n\ud83d\udd25 Les + de notre entreprise :\nAcc\u00e8s au Plan d\u2019\u00e9pargne Groupe, int\u00e9ressement et participation aux b\u00e9n\u00e9fices de l\u2019entreprise + abondement\nPrix pr\u00e9f\u00e9rentiels bancaires et avantages CSE\nParcours \u00e9volutif dans l\u2019entreprise et/ou dans le Groupe CA.S.A\nT\u00e9l\u00e9travail (jusqu'\u00e0 2 jours de t\u00e9l\u00e9travail par semaine)\nDe multiples commodit\u00e9s sur le campus (restaurants d'entreprise, salle de sport, cr\u00e8che, centre m\u00e9dical, m\u00e9diath\u00e8que...)\nForfait et avantages pratiques \u00ab mobilit\u00e9 durable \u00bb pour les velotafeurs\nDes \u00e9quipes aussi diversifi\u00e9es que structur\u00e9es dans une dynamique de transformation\nLCL s\u2019engage en faveur de la diversit\u00e9 et nous encourageons tout(e) candidat(e) ayant l\u2019exp\u00e9rience requise \u00e0 postuler \u00e0 nos offres. Tous nos postes sont ouverts aux personnes en situation de handicap.\nNous avons encore de nombreuses raisons \u00e0 vous pr\u00e9senter pour vous convaincre de nous rejoindre mais pour cela, il faudra postuler ici !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "Elasticsearch"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "JIRA",
                "Confluence",
                "Teams"
            ],
            "Other": [
                "DevOps",
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication",
                "Cr\u00e9ativit\u00e9"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F)",
        "company": "iPepperGroup",
        "location": "Valbonne, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ipeppergroup-3894091360?position=20&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=RFFhGqjJZXW%2B3KaXucElnA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "iPepper recrute pour l'un de ses clients une PME \u00e9diteur de logiciel dans le domaine du voyage, un\nData Engineer (H/F)\npassionn\u00e9(e) et exp\u00e9riment\u00e9(e) pour rejoindre une \u00e9quipe dynamique.\nEn tant qu'Ing\u00e9nieur(e) Data, vous serez en charge d'extraire et de transformer des donn\u00e9es, de construire et d'optimiser des pipelines de donn\u00e9es, ainsi que de concevoir des visualisations de donn\u00e9es intuitives et informatives.\nResponsabilit\u00e9s :\nConcevoir, construire et maintenir des pipelines de donn\u00e9es \u00e9volutifs et efficaces pour transf\u00e9rer des donn\u00e9es entre des bases de donn\u00e9es SQL et NoSQL.\nD\u00e9velopper et mettre en \u0153uvre des processus ETL pour extraire, transformer et charger des donn\u00e9es \u00e0 partir de diff\u00e9rentes sources dans notre entrep\u00f4t de donn\u00e9es.\nCollaborer avec des \u00e9quipes pluridisciplinaires pour comprendre les besoins en donn\u00e9es et garantir la fourniture r\u00e9ussie de solutions de donn\u00e9es.\nOptimiser et ajuster les pipelines de donn\u00e9es existants pour la performance et la fiabilit\u00e9.\nConcevoir et d\u00e9velopper des visualisations de donn\u00e9es et des tableaux de bord pour fournir des insights exploitables aux parties prenantes.\nSurveiller et r\u00e9soudre les probl\u00e8mes de pipelines de donn\u00e9es, en veillant \u00e0 la qualit\u00e9 et \u00e0 l'int\u00e9grit\u00e9 des donn\u00e9es.\nProfil recherch\u00e9 :\nDipl\u00f4me universitaire en informatique, en ing\u00e9nierie ou dans un domaine connexe.\nExp\u00e9rience av\u00e9r\u00e9e en tant que Data Engineer ou dans un r\u00f4le similaire, avec un accent particulier sur la construction de pipelines de donn\u00e9es et de processus ETL.\nCompr\u00e9hension solide des bases de donn\u00e9es\nSQL\net\nNoSQL\n, y compris la mod\u00e9lisation des donn\u00e9es et la conception de sch\u00e9mas.\nMa\u00eetrise des langages de programmation tels que\nPython, Java ou Scala.\nExp\u00e9rience avec des outils de visualisation de donn\u00e9es tels que\nTableau, Power BI.\nSolides comp\u00e9tences en analyse et en r\u00e9solution de probl\u00e8mes, avec la capacit\u00e9 de traduire des donn\u00e9es complexes en insights exploitables.\nExcellentes comp\u00e9tences en communication et en collaboration, avec la capacit\u00e9 de travailler efficacement dans un environnement d'\u00e9quipe pluridisciplinaire.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication",
                "R\u00e9solution de probl\u00e8mes",
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "PROXIAD",
        "location": "Greater Nice Metropolitan Area",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-proxiad-3901014428?position=21&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=JbkEw8N73f2WHgJMdBa5fg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Contexte\nEn tant que Data Engineer, votre r\u00f4le consistera \u00e0 r\u00e9aliser la conception, le d\u00e9veloppement, les tests unitaires, la qualification, l'int\u00e9gration continue et la mise en production d'\u00e9volutions sur les projets du p\u00f4le produits scoring.\nCes projets Big Data GCP ont pour objet de d\u00e9velopper des traitements de croisement de donn\u00e9es, exploration data en mode agile (scrum et Safe), industriel (respect de normes), sur l'environnement Google Cloud Platform.\n1 : Conception\nSp\u00e9cification et conception d'une solution se basant sur les d\u00e9veloppements existants.\nMettre en question les choix techniques dans le but de concevoir un logiciel r\u00e9pondant au mieux \u00e0 la demande au moindre co\u00fbt et avec la qualit\u00e9 demand\u00e9e.\nConception de l'expression de besoins, de la r\u00e9ponse \u00e0 l'expression de besoins \u00e0 l'aide des besoins m\u00e9tiers remont\u00e9s par le Product Owner.\n2 : R\u00e9alisation\nD\u00e9veloppement de nouvelles fonctionnalit\u00e9s sur les composants des applications du p\u00f4le produits scoring en environnement GCP (DataProc, GCS, BigQuery, Airflow...)\nTests des d\u00e9veloppements r\u00e9alis\u00e9s\nRevue de code des d\u00e9veloppements des autres d\u00e9veloppeurs\nMise en production via CICD des d\u00e9veloppements\n3 : Suivi du RUN applicatif\nPrendre en charge avec les autres membres de l'\u00e9quipe le RUN des applications du p\u00f4le produits scoring. Cela inclus les t\u00e2ches de rapport quotidien, la gestion des probl\u00e8mes applicatifs, le soutien aux utilisateurs.\nComp\u00e9tences attendues\nMa\u00eetrise op\u00e9rationnelle :\nConfluence\nImpl\u00e9mentation de l\u2019int\u00e9gration continue (Utilisation de la chaine CI/CD existante )\nConnaissance des principes DevOps\nJira\nAnglais (lu, \u00e9crit)\nMa\u00eetrise avanc\u00e9e :\nElaborer un cahier de recette\nBig Query\nSp\u00e9cifications technique et documentation\nD\u00e9veloppement :Python, SQL, Scala, Javascript, GitLab\nExpertise\nGCP : Exp\u00e9rience significative en tant que Data Engineer Cloud. Mise en pratique des produits GCP et en particulier Dataproc, Big Query, composer, workflow, PubSub\nD\u00e9veloppement : Java\nCompr\u00e9hension g\u00e9n\u00e9rale des travaux BigData et du profiling\nInformations compl\u00e9mentaires :\nT\u00e9l\u00e9travail 2 jours par semaines\nR\u00e9mun\u00e9ration aux alentours des 45K\u20ac\nExp\u00e9rience requise : 6 ans\nLocalisation : Mougins\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go",
                "JavaScript"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery",
                "Big Query"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "JIRA",
                "Confluence"
            ],
            "Other": [
                "DevOps",
                "Big Data",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "6",
                "6",
                "6"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer H/F",
        "company": "Chantelle",
        "location": "Cachan, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-chantelle-3909775663?position=22&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=6A0p8tASG3TahyMB1aX0EA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La Direction des Syst\u00e8mes d'Information et du Digital du groupe Chantelle recherche son/sa futur.e Data Engineer H/F, pour le lancement du grand chantier de r\u00e9novation de l'architecture Data : la bascule de l'int\u00e9gralit\u00e9 de son Data Warehouse vers Google Big Query.\nNous souhaitons recruter un Data Engineer H/F confirm\u00e9.e, charg\u00e9.e de contribuer \u00e0 la d\u00e9finition de la feuille de route de la Chantelle Data Plaform. En tant que Data Engineer vous travaillerez au sein de l'\u00e9quipe Data Int\u00e9gration en charge de la Chantelle Data Platform.\nVos Missions :\n- Mettre en \u0153uvre une infrastructure autour de Google Cloud Platform permettant de collecter (airbyte, API, ...) , transformer (dataform, Bigquery ...), exposer (dataviz, API, applications, ...) et historiser les donn\u00e9es g\u00e9n\u00e9r\u00e9es par l'entreprise.\n- Travailler en \u00e9troite proximit\u00e9 avec les responsables des diff\u00e9rents domaines fonctionnels (R\u00e9f\u00e9rentiels, Supply Chain, Manufacturing, B2B, Retail & e-commerce, Finance, ...), avec notre \u00e9quipe de Data Analysts ainsi qu'avec l'\u00e9quipe technique en charge des infrastructures transverses\n- \u00catre force de proposition sur tous les sujets d'architecture et de mod\u00e9lisation (choix de mise en place de pipeline temps r\u00e9els ou au contraire de flux de donn\u00e9es en mode batch, ou bien encore stockage sur Big Query / Big Table en fonction des cas d'usage).\n- D\u00e9finir les \u00e9l\u00e9ments structurants, en justifiant vos choix, et les mettre en \u0153uvre.\n- Rationaliser et moderniser notre architecture d'int\u00e9gration inter-applicative; se projeter sur la cr\u00e9ation d'un mod\u00e8le de donn\u00e9es de type Datamesh.\n- Faire la refonte de la BI de nombreux use cases tels que le pilotage de nos stocks, personnalisation de nos sites e-commerce en temps r\u00e9el en fonction de nos profils client, etc\u2026\nStack technique : Google Cloud Platform, BigQuery, DataForm, DataFlow, PubSub, Airbyte, Github ...\nBonne ma\u00eetrise des langages Python et SQL\nPourquoi travailler chez Chantelle ?\nUne flexibilit\u00e9 dans votre lieu de travail, selon la politique de t\u00e9l\u00e9travail de l'entreprise.\n11 jours de RTT/an ainsi qu'un 13\u00e8me mois.\nUne culture d'entreprise familiale bas\u00e9e sur des valeurs de respect, de cr\u00e9ativit\u00e9, de durabilit\u00e9 et de transparence\nUne aventure dans laquelle vous pourrez vous \u00e9panouir, apprendre et entreprendre, avec une grande vari\u00e9t\u00e9 de missions et beaucoup d'autonomie\nDes \u00e9quipes ressources humaines et des managers \u00e0 votre \u00e9coute pour vous accompagner dans votre parcours professionnel\nDes r\u00e9ductions sur nos produits et des ventes au personnel\nDes avantages dans votre qualit\u00e9 de vie au travail : une conciergerie compl\u00e8te proposant un large panel de services, des activit\u00e9s en interne, un CSE.\nVous souhaitez rejoindre un Groupe familial, innovant, engag\u00e9 et leader dans son secteur en France comme \u00e0 l'international et vous souhaitez apporter votre expertise et authenticit\u00e9 pour guider votre \u00e9quipe vers le succ\u00e8s : postulez et rejoignez le Groupe Chantelle !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery",
                "Big Query"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [
                "Cr\u00e9ativit\u00e9",
                "Flexibilit\u00e9"
            ],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [
                "Confirm\u00e9"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "EarthDaily Agro",
        "location": "Balma, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-earthdaily-agro-3883708013?position=23&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=Z1Bn3iRSRmq3lgA1wQOI9w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "About Us\nEarthDaily Agro provides space age data and analytics to the organizations and people who feed the planet!\nWith 35 years of industry experience, EarthDaily Agro provides customers with the data, analysis and knowledge they need to make more efficient and effective decisions. B2B services range from global risk management and monitoring of agricultural commodities to the marketing of inputs and precision agriculture consulting, using the latest research in agronomy, information technologies and remote sensing.\nEarthDaily Agro also develops highly customized business solutions for agricultural lenders, insurers, input suppliers and food companies, with easy-to-use analytics, that help reduce the daily risks of agriculture.\nEarthDaily Agro is headquartered in Minneapolis, MN, USA, with offices in France, Brazil, Australia and Switzerland and is a division of EarthDaily Analytics Corp.\nEarthDaily Analytics Corp., a vertically-integrated data processing and analytics company, is launching a new constellation of earth observation satellites. The EarthDaily satellite constellation will significantly enhance geospatial analytics capabilities in agriculture, forestry, environment, financial services, and intelligence, among many other verticals.\nMain Job Tasks And Responsibilities\nAs a EarthDailyAgro Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and infrastructure specialized for geospatial and remote sensing applications. You will work closely with data scientists, geospatial analysts, remote sensing experts, software engineers, and DevOps teams to ensure the successful deployment and scaling of data pipeline to feed geospatial data machine learning models. Your role will be crucial in optimizing the geospatial machine learning ecosystem and ensuring the seamless integration of AI-driven geospatial solutions into real-world applications.\nYour Responsibilities Include\nCloud-based data pipeline Conceptualization, Development and Scaling: Build up pipeline to ingest large volumes of geospatial data, pre-process them and meet data scientists\u2019 requirements, in terms of accessibility, speed, format, quality.\nAutomation and CI/CD: Industrialization of pipeline deployment, orchestration, workflows, and versioning.\nCost & Speed Optimization: Collaborate with infrastructure team to develop, optimize, and fine-tune pipeline.\nCloud and Containerization: Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization tools.\nInfrastructure Management: Utilize containerization technologies and cloud-based services to set up and manage infrastructure, enabling seamless deployment and scalability.\nMonitoring and Anomaly Detection: Implement monitoring systems to track pipeline performance and identify anomalies.\nVersion Control and Data Version Control: Proficient with version control systems like Git and DVC.\nSecurity and Compliance: Ensure the security and privacy of geospatial data, adhering to relevant data protection regulations and industry best practices.\nCollaboration and Communication: Collaborate with interdisciplinary teams to integrate data pipeline into existing applications or develop new geospatial products.\nIssue Resolution and Troubleshooting: Identify and resolve promptly technical issues related to geospatial data processing, performance, or infrastructure.\nEducation, Knowledge And Abilities\nRequirements\nEducation: Master's degree in Computer Science, specialisation in Geomatics and/or Remote sensing would be a plus.\nExperience: 3+ years experiences with data pipeline processes and deployment is a must-have. Proven hands-on experience in setting up pipelines and data processes with opensource tools (e.g., MLFlow, Argo, Kubeflow) is desirable.\nProgramming Skills: Proficiency in Python and with data manipulation frameworks (e.g., dataframe, numpy, pandas, xarray, rasterio) and librairies (e.g., Dask).\nProblem-Solving Skills: Autonomous, and strong analytical and problem-solving abilities to address complex geospatial data and analysis challenges.\nCommunication Skills: Excellent communication and interpersonal skills to collaborate effectively with cross-functional teams and stakeholders.\nFrench mandatory (job based in France). Fluent in English (oral and written):\u202fmeetings with internal are mostly in\u202fEnglish.\nPreferred Additional Skills\nExperience with Earth Observation (EO) data analysis and processing.\nExperience with geospatial data formats (e.g., GeoTIFF, Shapefile, NetCDF).\nSpatial Analysis Techniques: Understanding of spatial analysis techniques and algorithms commonly used in geospatial data manipulation.\nRemote Sensing Integration: Knowledge of remote sensing data sources (e.g., STAC catalog, satellite imagery, LiDAR, SAR) integration into data pipelines for accurate and up-to-date geospatial analysis.\nCONDITIONS\nFull time job based in Balma, near Toulouse, France.\nFixed + Bonuses\nTR / \"Family\" insurance / CSE\nPowered by JazzHR\nWrfSXQ5YJg\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "Pandas",
                "NumPy",
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "DevOps",
                "ML",
                "Machine Learning",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Communication",
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration",
                "Organization",
                "Interpersonal Skills"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Digital Waffle",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-digital-waffle-3913824888?position=24&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=sYvCEinWvVGScYf9pk8tqA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Digital Waffle is proud to have partnered with an innovative tech startup in Paris, who are looking for a talented Data Engineer to join their growing team!\nThey are made up of a powerhouse of experts, combining\ndata engineers, business process gurus, and Project managers\nwho leverage the most advanced solutions available; utilising process mining, automation tools, and smart execution systems.\nLooking for an experienced Data Engineer (3-5 years)\nWhat You'll Do:\nThis is a full-time,\nhybrid role (Paris-based)\nwhere you'll wear many hats: data exploration, system integration, data prep, data modeling, and implementing data solutions.\nExperience:\nExpertise in data engineering, data modeling, and ETL (Extract, Transform, Load) processes\nData warehousing and data analytics skills\nExperience handling large, complex datasets\nProficiency in SQL and programming languages like Python or Java\nStellar problem-solving and analytical skills\nTop-notch communication and collaboration abilities\nBachelor's or Master's in Computer Science, Information Systems, or a similar field (a plus for process mining or intelligent process automation experience)\nIf you are an experienced and driven Data Engineer, please apply here!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication",
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (Snowflake)",
        "company": "MindPal",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896992742?position=25&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=lPc66SjHIzg51G%2BwgpRsRw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "DATA Engineer (H/F)",
        "company": "Boulanger",
        "location": "Lesquin, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-boulanger-3854554057?position=26&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=M3pdzGobBgvn0n9%2B6lfQ%2Bw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Au sein de la direction informatique, le p\u00f4le DATA a pour missions de maximiser la mise en valeur des donn\u00e9es de BOULANGER ,ELECTRO-DEPOT et KREFEL/HIFI afin d\u2019aider nos d\u00e9cideurs \u00e0 agir sur les leviers de leur performance par des processus d\u00e9cisionnels efficients.\nAu sein de ce p\u00f4le, tu prendras en charge un large domaine m\u00e9tier qu'il te faudra maitriser de bout en bout : de la donn\u00e9es brutes, sa transformation jusqu'\u00e0 son exposition dans les reporting. Cela afin d'en assurer le bon fonctionnement, les \u00e9volutions constantes et sa p\u00e9rennit\u00e9\nTes t\u00e2ches principales portent sur :\nLe pilotage et la mise en \u0153uvre de projets DATA.\nLa collecte, le stockage et l\u2019exploitation fluides des donn\u00e9es par le d\u00e9veloppement de solutions\nMissions\nMaitriser les r\u00e8gles fonctionnelles et les KPI de ton domaine afin de challenger les m\u00e9tiers dans les \u00e9volutions et les nouveaux projets\nAccompagner des \u00e9quipes m\u00e9tiers dans leurs travaux d\u2019identification et expression des besoins sur la data\nParticiper aux ateliers de conception et d\u00e9veloppement des applications data\nMod\u00e9liser la solution \u00e0 mettre en \u0153uvre\nConcevoir et mettre (ou faire mettre) en \u0153uvre des flux les pipelines d\u2019int\u00e9gration (en mode batch ou fil de l'eau) de donn\u00e9es structur\u00e9es/semi-structur\u00e9es\nTransformer les donn\u00e9es : consolider, enrichir et optimiser les donn\u00e9es, qui seront exploit\u00e9es par le m\u00e9tier\nCr\u00e9er, faire \u00e9voluer et optimiser les restitutions\nSuivre et animer les d\u00e9veloppeurs (ETL, restitution, self-BI internes ou externes)\nG\u00e9rer le RUN\nMaitrise le SQL et la base de donn\u00e9es (Oracle, Snowflake)\nMa\u00eetrise d\u2019outils de restitution (tel que Business Object (BO), PowerBI\u2026)\nCapacit\u00e9 relationnelle, rigueur et dynamisme\nMa\u00eetrise un ou plusieurs outils de pr\u00e9paration et traitement de la donn\u00e9e (DataStage, Stambia, ...)\nCapacit\u00e9 \u00e0 s\u2019adapter \u00e0 tout type d\u2019interlocuteurs (technique, m\u00e9tiers, Direction)\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Oracle",
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer H/F",
        "company": "Neosoft",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-neosoft-3877878521?position=27&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=n7iKAdkp4FW8O5QNXJuUyg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Tous nos postes sont ouverts au t\u00e9l\u00e9travail\nGroupe ind\u00e9pendant de conseil en transformation digitale de pr\u00e8s de 1800 collaborateurs, N\u00e9osoft s\u2019est construit, depuis 2005, sur un mod\u00e8le qui place l\u2019excellence, le d\u00e9passement de soi et la RSE au c\u0153ur de sa strat\u00e9gie.\nEn nous rejoignant, vous int\u00e9grez des communaut\u00e9s d\u2019experts et de talents qui vous permettent de d\u00e9velopper vos comp\u00e9tences et d\u2019offrir \u00e0 nos clients le meilleur accompagnement possible.\nNotre savoir-faire s\u2019articule autour de nos 6 domaines d\u2019expertise :\nConseil & Agilit\u00e9\nCybers\u00e9curit\u00e9\nData\nDevOps\nInfrastructures & Cloud\nSoftware Engineering\nNous recherchons pour int\u00e9grer notre\nagence lilloise\nun(e)\nData Engineer confirm\u00e9(e)\n.\nNous aimerions vous voir rayonner au sein de notre communaut\u00e9 DATA (+100 collaborateurs) anim\u00e9e par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients \u00e0 consolider un patrimoine Data responsable.\n\ud83c\udfaf\nVos missions :\nApr\u00e8s une p\u00e9riode d\u2019int\u00e9gration, en tant que\nData Engineer\n, voici \u00e0 quoi ressembleront vos activit\u00e9s dans des contextes clients Retail ou Banque / Assurance / Finance :\nAnalyser et s'approprier les cas d'usages\nAnalyser et valoriser les donn\u00e9es du patrimoine\nMettre en place des flux de transformation de donn\u00e9es\nR\u00e9aliser les tests permettant de s'assurer la qualit\u00e9 du delivery\nContinuer la mise au point de frameworks data\nCr\u00e9er et d\u00e9velopper des modules de d\u00e9ploiement des solutions\nAssurer l'industrialisation de moteurs bas\u00e9s sur l'IA\nAssurer le niveau de performance des pipelines\nImpl\u00e9menter les outils de monitoring du socles de donn\u00e9es\n\ud83d\udcdd\nVotre profil :\nNous vous imaginons avec au moins 4 ans d\u2019exp\u00e9riences sur des projets autour de la\nData\n, une ma\u00eetrise des\nbases de donn\u00e9es (SQL)\n, des outils de transformation de la donn\u00e9e\n(Talend, BigQuery, Airflow)\n, et un socle de comp\u00e9tences solides autours des langages\nPython, Spark, Scala, Hadoop, Java.\n\ud83d\udc49\nVotre carri\u00e8re chez N\u00e9osoft\nDepuis sa cr\u00e9ation, N\u00e9osoft place ses collaborateurs au c\u0153ur de sa strat\u00e9gie. Notre culture pourrait se r\u00e9sumer en un mot : le collectif.\nNos communaut\u00e9s d\u2019experts vous donnent la possibilit\u00e9 d\u2019apprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.\nNous veillons \u00e0 ce que chacun b\u00e9n\u00e9ficie d\u2019un accompagnement de proximit\u00e9 et d\u2019un suivi de carri\u00e8re personnalis\u00e9 aupr\u00e8s de votre manager d\u00e9di\u00e9 :\n1 bilan d\u2019activit\u00e9 trimestriel pour suivre le d\u00e9veloppement de vos comp\u00e9tences\n1 entretien d\u2019\u00e9valuation qui a lieu chaque ann\u00e9e pour \u00e9valuer votre performance et d\u00e9terminer vos nouveaux objectifs\n1 entretien annuel aupr\u00e8s de votre RH dans le but de cartographier vos nouvelles comp\u00e9tences pour \u00e9changer sur vos projets professionnels et souhaits de formations\n\ud83d\udc49\nVos avantages\nFormations et d\u00e9veloppement de l\u2019expertise :\nVous disposez de temps allou\u00e9 et r\u00e9mun\u00e9r\u00e9 en contribuant au d\u00e9veloppement de votre expertise technique et de celle du groupe (Participations \u00e0 des Tech days, animation d\u2019une conf\u00e9rence \u00e0 l\u2019interne ou \u00e0 l\u2019externe, r\u00e9daction d\u2019articles, rencontres avec nos candidats en processus de recrutement\u2026)\nUn abonnement illimit\u00e9 LinkedIn Learning offert\nBien-\u00eatre au travail :\nUn accord de t\u00e9l\u00e9travail flexible jusqu\u2019\u00e0 100% de t\u00e9l\u00e9travail et personnalisable\nUn partenariat avec Gymlib qui favorise le sport en entreprise\nDes initiatives locales (afterworks, d\u00e9fis sportifs, team buildings, \u2026)\nEt bien plus encore :\nParce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive r\u00e9mun\u00e9r\u00e9e d\u00e8s l\u2019arriv\u00e9e du collaborateur\nEn plus de votre salaire : participation, compte \u00e9pargne temps, actionnariat...\n\ud83d\udc49\nVotre parcours candidat\nNotre processus de recrutement se compose de deux \u00e9tapes cl\u00e9s :\nUn entretien de recrutement RH avec un Talent Acquisition Sp\u00e9cialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carri\u00e8re possibles au sein de notre groupe\nUn entretien d\u2019\u00e9valuation technique pour r\u00e9aliser un diagnostic de vos comp\u00e9tences techniques et identifier les comp\u00e9tences sur lesquels poursuivre votre \u00e9volution\nVous aurez \u00e9galement la possibilit\u00e9 de rencontrer pour compl\u00e9ter votre processus un acteur de notre p\u00f4le Business ou un pair de votre m\u00e9tier pour \u00e9changer sur son exp\u00e9rience collaborateur.\nNous avons h\u00e2te de vous rencontrer !\nA bient\u00f4t,\nL\u2019\u00e9quipe N\u00e9osoft \ud83d\udd90\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Confirm\u00e9"
            ],
            "TypeContract": [],
            "Salary": [
                "Salaire"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer - Bordeaux",
        "company": "Capgemini",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bordeaux-at-capgemini-3889788624?position=28&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=nIragHs8rRPTeomiah0NXg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise o\u00f9 vous serez en mesure de fa\u00e7onner votre carri\u00e8re selon vos aspirations. Avec le soutien et l'inspiration d'une\ncommunaut\u00e9 d\u2019experts dans le monde entier, vous pourrez r\u00e9\u00e9crire votre futur. Rejoignez-nous pour red\u00e9finir les limites de ce qui est possible, contribuer \u00e0 lib\u00e9rer la\nvaleur de la technologie pour les plus grandes organisations et participer \u00e0 la construction d\u2019un monde plus durable et inclusif.\nVos missions :\nVous \u00eates passionn\u00e9 par le domaine de la Data, vous souhaitez prendre part \u00e0 des projets d'envergure, concevoir des solutions, les impl\u00e9menter et les faire \u00e9voluer?\nAlors rejoignez notre \u00e9quipe Data Engineering Services au sein de Capgemini Cloud Infrastructure Services en tant que Data Engineer.\nVous avez acquis une exp\u00e9rience solide dans le d\u00e9veloppement, la mise en \u0153uvre et l\u2019optimisation de solutions pour le traitement d'un grand volume de donn\u00e9es, vous \u00eates capable de cr\u00e9er des solutions qui r\u00e9pondent aux besoins m\u00e9tiers et IT, alors rejoignez notre \u00e9quipe d\u2019experts.\nEn qualit\u00e9 de Data engineer, vos missions sont les suivantes :\n\u25aa Concevoir et d\u00e9velopper des solutions Data/IA.\n\u25aa Accompagner les M\u00e9tier dans la compr\u00e9hension et la mise en \u0153uvre de solution orient\u00e9es donn\u00e9es.\n\u25aa Collaborer avec les Dev, les Ops, les experts infrastructures dans la construction de solutions et d\u2019infrastructures ax\u00e9es sur les donn\u00e9es.\n\u25aa G\u00e9rer un \u00e9cosyst\u00e8me de partenaires data et assurer un haut niveau d'expertise\n\u25aa Assurer un r\u00f4le de veille technologique sur tous les outils autours de la data, de l\u2019IA et de la BI.\nVotre profil :\nVous \u00eates issu d\u2019une formation ing\u00e9nieur ou \u00e9quivalent bac+5 informatique sp\u00e9cialis\u00e9e en DATA et vous justifiez d\u2019une exp\u00e9rience de 3 \u00e0 5 ans dans un r\u00f4le similaire. Expert dans une technologie de base de donn\u00e9es relationnelle (PostgreSQL, Oracle...)\nExpert dans une technologie de base NoSQL (MongoDB, Cassandra...)\nVous maitrisez un framework de manipulation de donn\u00e9es (Hadoop, Spark, Kafka...)\nVous maitrisez les concepts DevOps et avez de bonnes notions en scripting et d\u00e9veloppement\nVous avez une exp\u00e9rience des outils BI et de data visualisation (Kibana, PowerBI...)\nLa maitrise de l'anglais est n\u00e9cessaire.\n3 raisons de nous rejoindre :\nQualit\u00e9 de vie au travail :\naccord de t\u00e9l\u00e9travail en France et \u00e0 l\u2019international, accord sur l\u2019\u00e9galit\u00e9 professionnelle, la parentalit\u00e9, l\u2019\u00e9quilibre des temps et la mobilit\u00e9 durable.\nApprentissage en continu :\ncertifications et formations en libre acc\u00e8s, accompagnement sur mesure avec votre career manager, parcours d\u2019int\u00e9gration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, tarif pr\u00e9f\u00e9rentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culture\nNos engagements et priorit\u00e9s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int\u00e9grez un collectif qui valorise la diversit\u00e9, d\u00e9veloppe le potentiel de ses talents, s\u2019engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r\u00e9duire son impact environnemental sur tous ses sites et aupr\u00e8s de ses clients.\n\u00c0 propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr\u00e8s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d\u2019exp\u00e9rience, nous sommes un partenaire strat\u00e9gique des entreprises pour la transformation de leurs activit\u00e9s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp\u00e9tuelle \u00e9volution tels que le cloud, la data, l\u2019Intelligence Artificielle, la connectivit\u00e9, les logiciels, l\u2019ing\u00e9nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL",
                "Cassandra"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "PostgreSQL",
                "Oracle"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "eXalt Value",
        "location": "\u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=29&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=5UkNpHa5sTYINw6lkcL30g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "eXalt\nest un cabinet de conseil IT\nPure player Data\n& IA bas\u00e9 \u00e0 Paris.\nNotre offre s\u2019articule autour de 4 piliers r\u00e9unis au sein d\u2019une m\u00eame communaut\u00e9 pour un accompagnement \u00e0 360\u00b0 alliant une expertise technique et m\u00e9thodologique \u00e0 une approche conseil m\u00e9tier:\nData Gouvernance & Project\nData Engineering & Big Data\nData Performance & Analytics\nData Science & IA\nFiliale du groupe eXalt, cr\u00e9\u00e9 en 2018,\nregroupant plus de\n950 collaborateurs en France\n(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)\net \u00e0 l\u2019international\n(Colombie, Etats-Unis, Espagne, Belgique),\neXalt Value\napporte une\nexpertise approfondie\ndans le domaine de la Data & IA et conseille les entreprises dans le d\u00e9ploiement de leurs strat\u00e9gies data-driven.\nB\u00e9n\u00e9ficiant du support du groupe eXalt\n(1er dans la cat\u00e9gorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value\nest en pleine croissance et regroupe aujourd\u2019hui une communaut\u00e9 d\u2019expertise de plus de 60 collaborateurs en r\u00e9gion parisienne.\nNos consultants interviennent sur d\nes projets d\u2019envergure\ndans divers secteurs d\u2019activit\u00e9,\nBanque & Assurance, M\u00e9dias, Transports, Retail, Tourisme, etc.\nNous recherchons un\nData Engineer Confirm\u00e9 H/F (minimum 4 ans d'exp\u00e9rience dans la fonction)\npour rejoindre notre communaut\u00e9 sur le\npilier Data Engineering & Big Data.\nVos missions:\nConcevoir et d\u00e9velopper des pipelines et des flux de donn\u00e9es.\nInt\u00e9grer et transformer des donn\u00e9es provenant de diff\u00e9rentes sources.\nD\u00e9velopper et mettre en \u0153uvre des algorithmes de traitement de donn\u00e9es avanc\u00e9s.\nCollaborer \u00e9troitement avec les \u00e9quipes clients pour comprendre leurs besoins et fournir des solutions adapt\u00e9es.\nAssurer la qualit\u00e9 et la fiabilit\u00e9 des solutions d\u00e9velopp\u00e9es.\nConseiller les \u00e9quipes clients sur les solutions \u00e0 mettre en place.\nLes Pr\u00e9requis :\nTitulaire d'un Bac+5, Ecole d'Ing\u00e9nieur\nMa\u00eetrise d'un ou plusieurs langages de programmation (\nPython, Scala, Spark, etc\n.).\nExp\u00e9rience approfondie des technologies\nBig Data (Hadoop, Spark, Kafka, Talend, etc.)\nExp\u00e9rience av\u00e9r\u00e9e\nen\nenvironnement Cloud (AWS, GCP, ou Azure)\n.\nSolides comp\u00e9tences en conception et en optimisation de pipelines de donn\u00e9es.\nExp\u00e9rience de travail en\nm\u00e9thode Agile\nCapacit\u00e9 \u00e0 travailler de mani\u00e8re autonome et en \u00e9quipe.\nExcellentes comp\u00e9tences en communication et en r\u00e9solution de probl\u00e8mes.\nMa\u00eetrise de l\u2019anglais (oral & \u00e9crit dans un contexte international professionnel).\nVotre environnement eXalt\u00e9:\nUn environnement de travail Collaboratif\nfavorisant les initiatives et projets transverses \u00e0 la Practice Data & IA (Lab IA, Data Hub, etc.).\nUn collectif de consultants passionn\u00e9s,\ns\u2019int\u00e9ressant aux tendances innovantes du secteur.\nUne Practice de proximit\u00e9,\nprivil\u00e9giant la mont\u00e9e en comp\u00e9tence de ses collaborateurs (formations, coachings, mentorats, etc.)\nUn suivi individualis\u00e9 et de proximit\u00e9\npar un.e Data Sales Manager r\u00e9f\u00e9rent du compte client, un.e Charg\u00e9.e RH et un.e Practice Manager\nUne \u00e9quipe ouverte et dynamique,\nqui privil\u00e9gie les moments de partage et de convivialit\u00e9 (s\u00e9minaires, eXaltemps, meet-up, d\u00e9jeuners d\u2019\u00e9quipe, etc.)\nNotre processus de recrutement :\nUn entretien RH avec Estelle,\n\u00e0 la suite duquel vous saurez tout (ou presque) d\u2019eXalt Value,\nUn entretien technique avec un Manager assorti d\u2019un test technique,\nlors duquel vous aurez l\u2019occasion de d\u00e9montrer vos talents mais aussi d\u2019apprendre avant m\u00eame de dire oui,\nUn entretien final avec la Directrice Associ\u00e9e ou le Directeur Op\u00e9rationnel,\npour finir de vous convaincre de nous rejoindre \ud83d\ude0a\nNous avons h\u00e2te de recevoir vos CV, et de faire votre connaissance!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication",
                "R\u00e9solution de probl\u00e8mes"
            ],
            "EnSoftSkils": [
                "Communication",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Confirm\u00e9"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data engineer - F / H",
        "company": "United Robotics Group",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=30&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=zHBqRypzsrJd0gNx9KSZVA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Bienvenue chez\nAldebaran\n, leader europ\u00e9en de la robotique\nau sein du groupe\nUnited Robotics Group\n.\nNous concevons et industrialisons des robots innovants avec une vision soci\u00e9tale ambitieuse pour fa\u00e7onner un monde plus humain. Depuis 2005, nous sommes \u00e0 l'avant-garde de l'interaction homme-robot avec des produits embl\u00e9matiques tels que NAO et Pepper.\nNotre dernier-n\u00e9,\nPlato\n,\nincarne notre engagement envers la technologie de pointe et la s\u00e9curit\u00e9,\nfabriqu\u00e9 en France avec des composants europ\u00e9ens.\nRejoignez nos \u00e9quipes multiculturelles et dynamiques pour \u00eatre au c\u0153ur de la r\u00e9volution de la robotique.\nSi vous \u00eates passionn\u00e9.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer \u00e0 fa\u00e7onner l'avenir, nous vous offrons une exp\u00e9rience enrichissante et stimulante.\nEn tant que membre de notre \u00e9quipe, vous b\u00e9n\u00e9ficierez d'une culture d'entreprise ax\u00e9e sur le sens de ce que nous faisons et valorisant la responsabilit\u00e9 sociale et environnementale.\nChez Aldebaran, nous valorisons l'innovation, la diversit\u00e9 et l'\u00e9galit\u00e9 et encourageons chacun.e \u00e0 \u00eatre ouvert.e, authentique, courageux.se, responsable et engag\u00e9.e.\nFinalit\u00e9 du poste\nAu sein de l'\u00e9quipe Cloud-Online Services, le Data engineer int\u00e9grera l'\u00e9quipe Data, responsable du d\u00e9veloppement des produits destin\u00e9s \u00e0 la collecte, aux process et \u00e0 l'exploitation des donn\u00e9es de nos robots.\nIl aura pour r\u00f4le de d\u00e9finir et d'impl\u00e9menter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui g\u00e8rent les robots du groupe.\nMissions principales\nLe Data engineer aura pour responsabilit\u00e9s de :\n\u00e9valuer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,\nconcevoir et d\u00e9velopper des services Data en respectant la sp\u00e9cification fonctionnelle et la m\u00e9thodologie agile,\nagr\u00e9ger et stocker de grandes quantit\u00e9s de donn\u00e9es,\nmettre en place des solutions de data processing,\nint\u00e9grer/d\u00e9velopper des outils de visualisation de donn\u00e9es et analyser les KPI,\nd\u00e9velopper, tester, s\u00e9lectionner et mettre en production des algorithmes qui permettent de r\u00e9pondre aux besoins,\nr\u00e9aliser des analyses de donn\u00e9es,\nmettre en place des tests de charge et fonctionnels pour les solutions Data,\ninvestiguer et corriger les bugs remont\u00e9s par les utilisateurs,\ncontribuer \u00e0 la mise en place de l'infrastructure et outil de d\u00e9ploiement (CI/CD)\nRejoignez-nous pour faire partie d'une aventure passionnante o\u00f9 Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !\nRequirements\nPour la bonne ex\u00e9cution des missions confi\u00e9es, vous t\u00e9moignez d'au moins 6 ans d'exp\u00e9rience en tant que d\u00e9veloppeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.\nComp\u00e9tences demand\u00e9es :\nBonne compr\u00e9hension des technologies d'infrastructure et de d\u00e9ploiement,\nComp\u00e9tences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,\nBonne compr\u00e9hension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,\nBonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)\nBonne connaissance et une exp\u00e9rience pratique de Scrum\\Scrumban et des m\u00e9thodes agiles,\nUne certification AWS sera appr\u00e9ci\u00e9e,\nUn niveau de fran\u00e7ais et d'anglais courant est indispensable,\nDes exp\u00e9riences dans des environnements fortement internationaux sont un plus\nBenefits\nNos principaux avantages :\nUne culture du bien-\u00eatre en entreprise qui a fait ses preuves (budget c\u00e9l\u00e9bration et moments de convivialit\u00e9 par \u00e9quipes et directions, restauration collective de qualit\u00e9, environnement de travail agr\u00e9able)\nUn engagement fort en mati\u00e8re de responsabilit\u00e9 sociale et environnementale (promotion de l'\u00e9galit\u00e9 professionnelle, performance de notre plan diversit\u00e9 et inclusion, r\u00e9f\u00e9rent handicap, fresque du num\u00e9rique)\nUne culture du t\u00e9l\u00e9travail encadr\u00e9e de mani\u00e8re appropri\u00e9e !\nTous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "6",
                "6",
                "6"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Coders Connect",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-coders-connect-3870419202?position=31&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=beZujM6hc33RzbbSXAlpIw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Coders Connect and Sanofi are joining forces to bring an electrifying twist to the biopharmaceutical world!\nWork with a rhythm that suits your style (2 days remote and 3 days onsite magic).\nLanguage\n: Proficiency in English is required for this role to ensure effective communication within our diverse, global team.\nAbout Sanofi:\nWe're not just a company; we're a global movement, focused on human health and making a real difference. Our mission? To battle pain, ease suffering, and sprinkle a little bit of magic in the process by developing life-changing medicines and vaccines through breakthrough science and wizard-level technology.\nDigital & Data: The Pulse of Our Mission\nAt the heart of our quest lies our digital and data powerhouse. Think of us as the digital healthcare platform of your dreams, where innovation meets speed, and technology shakes hands with medicine. With our scale, deep-rooted connections in health ecosystems worldwide, and a knack for pushing boundaries, we're here to revolutionise medicine, one digital solution at a time.\nThe Role: Data Engineering Virtuoso\nAs our Data Engineering Virtuoso, you're tasked with designing and orchestrating the data pipelines that power our ambitious data analytics initiatives. You'll ensure our data's integrity and accessibility, laying the groundwork for groundbreaking insights and innovations.\nRequirements\nCloud Platforms: Proficient in AWS services, with Azure & GCP knowledge a plus. Your work involves leveraging cloud solutions for scalable data processing and storage.\nData Modeling & Query Performance: Expertise in crafting data models and optimizing SQL queries to enhance performance. Experience with Snowflake or similar data warehousing solutions is highly valued.\nIntegration Services: Skilled in utilizing Integration Services like IICS and Tibco, you facilitate seamless data flow and integration across various platforms.\nScripting & Development: Proficient in scripting languages such as Python and R, enabling you to automate tasks and manipulate data efficiently. Familiarity with GitHub for source code management underscores your commitment to collaborative development and version control.\nVisualization & Reporting: Knowledgeable in creating insightful data visualizations using tools like PowerBI, Tableau, or similar, turning complex data into actionable insights.\nData Governance & Compliance: A keen understanding of data quality, security, and governance standards, especially in healthcare environments subject to regulations like GxP, SOX, and data privacy laws.\nReal World Data & Standard Models: Experience with Real World Data (e.g., EHR, Claims) and familiarity with standard data models (e.g., OMOP, FHIR) enhance your ability to contribute to our healthcare objectives.\nPipeline Frameworks: Proficiency in using frameworks like Apache Airflow or Kedro for crafting efficient, reliable data pipelines that support our AI and ML initiatives.\nThe Reward:\nA chance to play a crucial role in a collaboration that's redefining healthcare through digital transformation.\nA seat at the round table of diversity and inclusion, where different backgrounds and experiences conjure the richness of our culture.\nAn endless horizon of professional growth, learning opportunities, and the chance to contribute to a future where better health is a global reality.\nThe Call to Adventure:\nIf you're ready to join a quest for better \u2013 better treatments, better outcomes, and better science \u2013 and believe in the magic of bringing diverse talents together to make miracles happen, we want you. Let's embark on this journey together and transform the future of healthcare.\nBetter is out there. Are you ready to find it with us?\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication",
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F)",
        "company": "Web Transition",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=32&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=vLfroncagdB3hXjfmeFKmg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Web Transition, c\u2019est qui ?\nFond\u00e9e en 2011,\nWeb transition\nest une entreprise de services num\u00e9riques op\u00e9rant sur le march\u00e9 de l\u2019IT/Digital !\nConstituant une part essentielle de\nMoOngy Digital Lab\n, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et \u00e9galement en Data !\nNotre objectif : nous implanter comme un acteur principal sur le march\u00e9 de la Transformation Digitale en accompagnant et valorisant les comp\u00e9tences de nos collaborateurs !\nNous sommes convaincus que le succ\u00e8s de MoOngy Digital Lab r\u00e9side dans la somme des potentiels de nos \u00e9quipes \ud83e\udd1d\nTon \u00e9quipe : La tribu Data\nParce qu\u2019il est indispensable que tu puisses partager tes connaissances mais aussi en acqu\u00e9rir de nouvelles, tu feras partie de l\u2019une de nos tribus : celle de la Data. De plus, cela te permettra d\u2019\u00eatre acteur dans le d\u00e9veloppement et la strat\u00e9gie de Web Transition. Ce syst\u00e8me de co-r\u00e9flexion et co-construction est un fondement essentiel chez nous !\nDans cette aventure, tu :\nT\u2019assures\nde la ma\u00eetrise de la donn\u00e9e et est garant de la qualit\u00e9 de son utilisation (r\u00e9f\u00e9rencement, normalisation, et qualification)\nTravailles\n\u00e0 la compr\u00e9hension et l'int\u00e9gration des donn\u00e9es en provenance des diff\u00e9rents formats\ndes interfaces de flux\n\u00e9galement \u00e0 la d\u00e9finition de la politique de la donn\u00e9e et \u00e0 la structuration de son cycle de vie dans le respect des r\u00e9glementations en vigueur\nla supervision et l'int\u00e9gration des donn\u00e9es de diverse nature qui proviennent de ces sources multiples et v\u00e9rifie la qualit\u00e9 des donn\u00e9es qui entrent dans le Data Lake\nGarantis\nl'acc\u00e8s qualitatif aux sources de donn\u00e9es\nFacilites\nl\u2019acc\u00e8s aux donn\u00e9es pour tes coll\u00e8gues (data scientists, data analysts\u2026)\nAssistes\nles autres \u00e9quipes dans l'acc\u00e8s et la compr\u00e9hension des donn\u00e9es des socles.\nRejoins-nous si tu as :\nExp\u00e9rience d\u2019au-moins 4 ans dans la Data\nApp\u00e9tence \u00e0 la qualit\u00e9 des donn\u00e9es.\nConnaissance famili\u00e8re des Datawarehouses.\nMaitrise de Python, Oracle SQL, GCP/Power BI\nAisance avec les indicateurs, tu as une bonne capacit\u00e9 d'analyse et de r\u00e9daction.\nTon savoir-\u00eatre :\nOuvert d\u2019esprit\nRigoureux\nAutonome\nRespectueux des diff\u00e9rences de chacun\nCurieux\nProactif\nAgile\nPar o\u00f9 on commence ?\nUn premier entretien RH d\u20191h pour comprendre ton parcours et tes aspirations\nUn second entretien de 45 minutes avec l\u2019un de nos Business Manager afin de valider tes comp\u00e9tences et qu\u2019il se projette sur l\u2019une des missions qu\u2019il pourrait te proposer\nUn troisi\u00e8me entretien de quelques minutes avec notre responsable d\u2019agence pour te proposer d\u2019int\u00e9grer notre superbe Team Web !\n3 entretiens en peu de temps, si ton profil correspond tu int\u00e8greras tr\u00e8s vite nos \u00e9quipes \ud83d\ude09\nPr\u00eat pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant \u00e0 cette offre ! Voici les avantages qui t\u2019attendent en tant que Weber :\n\ud83e\udd29 Des coll\u00e8gues incroyables\n\ud83c\udfc6 Certifi\u00e9e Great Place to Work\n\ud83c\udfae Des bureaux sympas (o\u00f9 vous serez toujours les bienvenus)\n\ud83c\udf89 Des teambuilding et \u00e9vents tous les mois\n\ud83d\udcbb Des tributs m\u00e9tiers pour \u00e9changer entre Weber du m\u00eame m\u00e9tier\nDes missions chez le client qui sont accompagn\u00e9es et coach\u00e9es par ton manager\nUn accompagnement dans ton plan de carri\u00e8re et tes envies de re skilling\n\ud83e\udd13 Un catalogue de formations certifiantes ouvert \u00e0 tous les salari\u00e9s\n\ud83c\udf7d\ufe0f Une carte tickets restaurant MyEdenred\n\u2764\ufe0f Une mutuelle GrasSavoye\n\ud83d\ude8e Une prise en charge des frais de transport \u00e0 100%\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Oracle"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Airswift",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-airswift-3909165766?position=33&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=H6EwNhVemE1u%2FM3i1U3DQg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data Engineer\nLocation\n: Paris (Hybrid)\nContract type\n: 12 months +\nYears of Experience\n: 4+\nRecruitment Partner:\nAirswift\nKey Words:\nProject Management | Jira | Digiboard | Banking | Stakeholder Management | Architecture | Cloud | Payments/Credit |ServiceNow | PPM |\nResponsibilities\nDesign, develop, and implement data pipelines to collect, process, and store structured and unstructured data from various sources.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and translate them into technical solutions.\nOptimize and tune data pipelines for performance, scalability, and reliability.\nEnsure data quality and integrity throughout the data lifecycle, implementing data validation and monitoring processes.\nEvaluate and implement new tools and technologies to enhance our data infrastructure and capabilities.\nRequirements\n:\nExtensive experience in Python.\nStrong experience with data processing frameworks and tools such as Apache Spark.\nExperience with cloud platforms such as AWS, Azure, or Google Cloud Platform.\nSolid understanding of data modelling, database design, and SQL\nFrench and English speaking\nFreelancing opportunity\nThe next step\nWe have an exceptional team in place, and we are pleased to be able to appoint a further person to our growing business. We are aware that you may not \u2018tick all the boxes\u2019, but if you believe you can genuinely offer some valuable skills and experience to our business, please in the first instance contact our recruitment partner Airswift.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure",
                "Google Cloud Platform"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F)",
        "company": "Beelix",
        "location": "Antibes, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-beelix-3838611420?position=34&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=SE7K53prpCanxUE8Nz3ejA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des probl\u00e9matiques de Product Management, Data et Design Thinking. Beelix contribue \u00e0 fa\u00e7onner le monde de demain en participant aux grandes avanc\u00e9es des secteurs suivants :\n\ud83d\ude97Automobile\n\u26a1Energie\n\ud83d\udce1M\u00e9dias & T\u00e9l\u00e9coms\n\ud83d\udc57Luxe & Retail\n\ud83d\udcb6 Banque, Finance & Assurance\n\u2708\ufe0fD\u00e9fense\nAujourd\u2019hui, Beelix compte plus de 200 collaborateurs motiv\u00e9s et dynamiques. Lab\u00e9lis\u00e9e Great Place To work en 2023, Beelix est aussi une entreprise engag\u00e9e o\u00f9 il fait bon vivre.\nDans le cadre de notre d\u00e9veloppement, nous recherchons un Data Engineer (H/F) pour l'un de nos clients.\nQuelles missions au quotidien ?\n\u00catre le leader de la brique Datalakehouse\nD\u00e9velopper les scripts de transformations de donn\u00e9es et les pipelines d\u2019alimentation\nProposer des \u00e9volutions architecturales ou de fonctionnalit\u00e9s pour am\u00e9liorer le socle technique\n\u00catre le back-up du leader technique sur la partie reporting (Power BI)\nOrientation satisfaction client et r\u00e9sultat final forte mais \u00e9galement sensibilit\u00e9 au \u00ab comment \u00bb\nInnovation et proposition de nouvelles pratiques pour am\u00e9liorer l\u2019environnement et les conditions de travail des \u00e9quipes\nA propos de vous ?\n5 + ann\u00e9es d'exp\u00e9rience en tant que Data Engineer\nMa\u00eetrise des technologies suivantes : Microsoft Azure, Microsoft Azure Synapse Analytics (Spark / Python / Pipeline / Serverless), fichiers parquet / delta, Microsoft Power BI, Microsoft SQL Server, langage SQL, Datawarehousing / Mod\u00e9lisation de donn\u00e9es\nAnalyses et export de donn\u00e9es\nConnaissance de l\u2019ensemble du processus depuis la collecte jusqu\u2019\u00e0 la mise \u00e0 disposition des donn\u00e9es en ayant comme point fort la maitrise de sa transformation et mise en forme\nVous avez un bon niveau d\u2019anglais\nLocalisation : Biot et/ou Carros\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "SQL Server"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (F/H)",
        "company": "Aubay",
        "location": "\u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=35&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=RBe%2FSfh1XFULSgxcv2128A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Passionn\u00e9 par la Data, tu souhaites rejoindre une communaut\u00e9 d\u2019experts dans le domaine afin de d\u00e9velopper tes comp\u00e9tences en Data Engineering. Aubay renforce ses \u00e9quipes Data et recherche des Data Engineers pour int\u00e9grer des dispositifs de projets pointus et vari\u00e9s.\nTon quotidien en tant que Data Engineer chez Aubay, :\nD\u00e9finition de la strat\u00e9gie de stockage et mise en \u0153uvre des technologie appropri\u00e9es (base de donn\u00e9es SQL, NoSQL, stockage distribu\u00e9,\u2026)\nIngestion des donn\u00e9es (structur\u00e9es, semi-structur\u00e9es ou non-structur\u00e9es) selon diff\u00e9rentes fr\u00e9quences : batch, micro-batch ou temps r\u00e9el\nConception et mise en \u0153uvre de pipelines de donn\u00e9es afin de fournir des donn\u00e9es pr\u00eates \u00e0 l\u2019emploi aux consommateurs : uniformisation, mise en qualit\u00e9, enrichissement, calcul d\u2019indicateurs,\u2026\nConception et d\u00e9veloppement d\u2019API pour exposer les donn\u00e9es aupr\u00e8s d\u2019applications tierces\nAppui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning\nPr\u00e9paration et animation d\u2019ateliers de travail avec des interlocuteurs vari\u00e9s : recueil/approfondissement des besoins m\u00e9tiers, avancement/restitution des travaux, transfert de comp\u00e9tences,\u2026\nTon profil :\nTu dispose d\u2019une formation niveau BAC+5 (Master 2 ou \u00e9cole d\u2019ing\u00e9nieur) sp\u00e9cialis\u00e9e en informatique\nTu as d\u00e9j\u00e0 une premi\u00e8re exp\u00e9rience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data\nLes technologies telles que Hadoop, Spark ou Kafka sont tes technologies de pr\u00e9dilection\nLa programmation n\u2019a plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python\nTu ma\u00eetrises les tenants et aboutissants de la philosophie DevOps et des outils orient\u00e9s CI/CD\nTu es soucieux de la qualit\u00e9 et de la performance de tes d\u00e9veloppements et tu t'int\u00e9resse \u00e0 l\u2019innovation frugale\nTu es un expert technique dans ton domaine sans pour autant oublier l\u2019importance d\u2019une communication orale et \u00e9crite de qualit\u00e9 et adapt\u00e9e \u00e0 chacun de tes interlocuteurs\nTu travaille au quotidien en mode agile et tu en maitrise les fondements\nCe qui nous caract\u00e9rise :\nDes missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs vari\u00e9s (Banque, Assurance, Telecom, Industrie,\u2026) qui permettent \u00e0 nos collaborateurs de monter en comp\u00e9tences et de devenir des experts Data reconnus\nDe l\u2019apprentissage en continu avec des formations et des certifications sur les technologies Data d\u2019aujourd\u2019hui et de demain\nDes experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets\nDes communaut\u00e9s de savoir-faire Data proposant de mani\u00e8re r\u00e9guli\u00e8re aux collaborateurs d\u2019Aubay du contenu et des \u00e9v\u00e8nements de partage (webinar, meetup/afterwork, BBL,\u2026) sur les th\u00e9matiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,\u2026\nAubay encourage la diversit\u00e9 sous toutes ses formes et garantit l'\u00e9galit\u00e9 des chances \u00e0 tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les am\u00e9nagements n\u00e9cessaires.\nTa carri\u00e8re chez Aubay :\nTu auras la possibilit\u00e9 de d\u00e9velopper et certifier tes comp\u00e9tences sur les derni\u00e8res technologies Data avec un focus fort sur les plateformes Data Cloud telles qu\u2019Azure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks\nTu pourras rejoindre la BU d\u2019excellence Data et \u00e9voluer au sein d\u2019un environnement humain et professionnel de haut niveau. Tu profiteras d\u2019un management sur-mesure pour t'accompagner dans ta trajectoire de carri\u00e8re\nAu sein de la BU d\u2019excellence, de multiples perspectives s\u2019offriront \u00e0 toi :\nR\u00f4le de \u00ab Lead \u00bb : Vous pourrez gagner en responsabilit\u00e9 sur le plan technologique et devenir un r\u00e9f\u00e9rent aupr\u00e8s de nos clients et des collaborateurs de la communaut\u00e9 Data Engineering\nR\u00f4le de \u00ab Champion \u00bb : Vous repr\u00e9senterez Aubay aupr\u00e8s d\u2019un ou plusieurs de nos partenaires \u00e9diteurs strat\u00e9giques et vous participerez activement \u00e0 l\u2019animation de la relation sur le plan technologique\nR\u00f4le de \u00ab Head \u00bb : Vous pourrez prendre la responsabilit\u00e9 du savoir-faire Data Engineering et de ses offres et en assurer le d\u00e9veloppement au sens large (d\u00e9veloppement business, recrutement, management de collaborateurs, d\u00e9finition de la strat\u00e9gie et animation de la communaut\u00e9 au sein du groupe Aubay,\u2026)\nBesoin d\u2019en savoir plus sur le processus de recrutement ?\nUn \u00e9change macro au niveau RH avec Doriane\nUn entretien technique avec Marius ou Peter, deux de nos r\u00e9f\u00e9rents techniques\nUn \u00e9change manag\u00e9rial avec le Directeur de la BU Modern BI & Data\nA savoir que l\u2019ordre des \u00e9tapes peut varier selon tes envies (ex : \u00e9change manag\u00e9rial avec l\u2019\u00e9change technique)\nAubay encourage la diversit\u00e9 sous toutes ses formes et garantit l'\u00e9galit\u00e9 des chances \u00e0 tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les am\u00e9nagements n\u00e9cessaires.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Azure",
                "Google Cloud Platform"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Big Data",
                "Machine Learning",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "2",
                "2",
                "2"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "ADVANCED Schema",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-advanced-schema-3886398270?position=36&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=0IzfiiIAtmjfmcIDT5XGsw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "En tant que Data Engineer, vous aurez les missions suivantes :\nConcevoir\ndes mode\u0301lisations physiques\nConstruire\ndes mappings techniques et r\u00e9daction de sp\u00e9cifications d\u2019alimentation.\nD\u00e9velopper\ndes flux des donn\u00e9es\nContribuer\nau pilotage de projets, de proof of concepts\nParticiper\na\u0300 des missions d\u2019expertise\nComp\u00e9tences professionnelles & niveau d'\u00e9tudes requis :\nVous \u00eates titulaire d'un dipl\u00f4me\nBac +3\nminimum dans le domaine de la\ndata\nVous poss\u00e9dez minimum\n1 an d'exp\u00e9rience\ndans le m\u00e9tier\n\u00catre\nenthousiaste\n\u00e0 l'id\u00e9e\nd'apprendre de nouvelles technologies\nExp\u00e9rience de la m\u00e9thodologie\nAgile / Scrum\nCapacit\u00e9 \u00e0\nplanifier et \u00e0 prioriser\nles\nt\u00e2ches\net les\nactivit\u00e9s confi\u00e9es\nen autonomie\nMa\u00eetrise\nde l\u2019anglais oral et technique obligatoire\nExpe\u0301rience\nav\u00e9r\u00e9e dans l'\u00e9criture de code propre avec 2 ou plusieurs des technologies suivantes :\nBASH, SQL, Java, Python, NoSQL\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R",
                "Bash"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "ADVANCED Schema",
        "location": "Greater Lille Metropolitan Area",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-advanced-schema-3539059697?position=37&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=vCi%2FAHpg6Emgt0PR%2BhHtxA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ADVANCED SCHEMA\nest une soci\u00e9t\u00e9 de services informatiques\nsp\u00e9cialis\u00e9e dans la donn\u00e9e.\nDepuis 20 ans, nous cr\u00e9ons des plateformes data sur mesure pour nos clients, orient\u00e9es usages et alliant qualit\u00e9, performance, s\u00e9curit\u00e9 et gouvernance.\nADVANCED SCHEMA\na d\u00e9velopp\u00e9 de nouvelles activit\u00e9s pour r\u00e9aliser l'ambition du groupe : devenir\nune entreprise end-to-end,\nen proposant une offre \u00e0 360\u00b0 \u00e0 nos clients pour les\naccompagner \u00e0 chaque \u00e9tape de leurs projets.\n\u00c0 ce jour, nous sommes pr\u00e8s de 220 passionn\u00e9s r\u00e9partis entre Paris, Lille, Nantes, Lyon mettant \u00e0 profit leur expertises aussi bien dans le domaine du retail, de la finance/assurance, du luxe, des m\u00e9dias, de la sant\u00e9 et de l'industrie.\nAujourd\u2019hui, nous souhaitons int\u00e9grer de nouveaux renforts dans nos \u00e9quipes Lilloises.\nEn tant que Data Engineer, vous aurez les missions suivantes :\nConcevoir des mode\u0301lisations physiques\nConstruire des mappings techniques et r\u00e9daction de sp\u00e9cifications d\u2019alimentation.\nD\u00e9velopper des flux des donn\u00e9es\nContribuer au pilotage de projets, de proof of concepts\nParticiper a\u0300 des missions d\u2019expertise\nComp\u00e9tences professionnelles & niveau d'\u00e9tudes requis :\nVous \u00eates titulaire d'un dipl\u00f4me Bac +3 minimum dans le domaine de la data\nVous poss\u00e9dez minimum 2 ans d'exp\u00e9rience dans le m\u00e9tier\nPositif(ve), curieux(se), rigoureux(se) et dot\u00e9(e) d'une bonne aisance relationnelle\n\u00catre enthousiaste \u00e0 l'id\u00e9e d'apprendre de nouvelles technologies\nExp\u00e9rience de la m\u00e9thodologie Agile / Scrum\nCapacit\u00e9 \u00e0 planifier et \u00e0 prioriser les t\u00e2ches et les activit\u00e9s confi\u00e9es en autonomie\nMa\u00eetrise de l\u2019anglais oral et technique obligatoire\nExp\u00e9rience av\u00e9r\u00e9e dans l'\u00e9criture de code propre avec 2 ou plusieurs des technologies suivantes : BASH, SQL, Java, Python, NoSQL\nNotre proposition :\nTemps plein en\nCDI\navec un\nsalaire attractif\n+ participation aux b\u00e9n\u00e9fices + prime(s) sur investissement personnel\nMode de\ntravail hybride\n(agence, site, t\u00e9l\u00e9travail selon projets/clients)\nTicket restaurant (Sodexo)\nMutuelle financ\u00e9e \u00e0 50%\nPr\u00e9voyance\nComit\u00e9 entreprise\n5 jours d\u2019onboarding plein temps via la\nADVANCED SCHEMA Academy\nNotre investissement :\nChez\nADVANCED SCHEMA\n, nous t\u2019offrons un environnement de travail stimulant et collaboratif ainsi que des possibilit\u00e9s de croissance et de d\u00e9veloppement professionnel. \u00c9galement un\naccompagnement/support au quotidien\npour te faire grandir et monter en comp\u00e9tences, sur des projets qui r\u00e9pondent \u00e0 de\nvrais enjeux pour nos clients\n. Si tu es passionn\u00e9(e) par les donn\u00e9es et pr\u00eat(e) \u00e0 relever de nouveaux d\u00e9fis, alors nous aussi nous aimerions te rencontrer\nProcess de recrutement :\nSi ta candidature retient notre attention, nous te proposons :\nUn premier \u00e9change t\u00e9l\u00e9phonique/visio\nUn entretien physique (+questionnaire d\u2019\u00e9valuation) avec un senior manager\nUn entretien final \u00e0 notre si\u00e8ge Parisien afin de rencontrer le DG\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R",
                "Go",
                "Bash"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [
                "Hybride",
                "Temps plein",
                "Senior"
            ],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [
                "50"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "20",
                "20",
                "20"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "AFD Technologies",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-afd-technologies-3899625641?position=38&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=bYpfT0MgYlzf9jo6i%2Fxwig%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "AFD.TECH part of Accenture\nest le sp\u00e9cialiste du conseil en transformation digitale des grandes entreprises \ud83d\ude80.\nA ce jour, le Groupe est compos\u00e9 de 2.000 talents r\u00e9partis dans 3 pays (France, Belgique & Maroc) \ud83c\udf0e pour un chiffre d\u2019affaires annuel de 125M\u20ac !\nNos Talents d\u2019abord \ud83d\ude0e:\nLes Talents d\u2019AFD.TECH part of Accenture sont au c\u0153ur de la strat\u00e9gie du Groupe et il est primordial pour nous que chaque collaborateur trouve du sens dans son travail.\nAu-del\u00e0 de proposer une carri\u00e8re ambitieuse et personnalis\u00e9e \u00e0 nos Talents, nous avons \u00e0 c\u0153ur de leur offrir un environnement de travail flexible (remote), inclusif et \u00e9panouissant dans tous nos bureaux (Paris, Bruxelles, Rabat, Lyon, Strasbourg, Lille, Nantes, Toulouse, Marseille, Bordeaux et Rennes)\ud83c\udf0d.\nAvec 20% de croissance par an et plus de 20 ans d\u2019existence, AFD.TECH part of Accenture est devenu l\u2019acteur incontournable du march\u00e9 des infrastructures informatiques, r\u00e9seaux et t\u00e9l\u00e9coms.\nNotre proposition de valeur ? Intervenir sur l\u2019ensemble du cycle de vie de projets complexes, pour des clients grands comptes, venant de secteurs tels que la banque, le ferroviaire, les m\u00e9dias, t\u00e9l\u00e9coms, etc (comme la Soci\u00e9t\u00e9 G\u00e9n\u00e9rale, Bouygues Telecom, Orange, Thales et bien d\u2019autres encore !)\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb.\nNous rejoindre est une formidable aventure humaine : nous vous proposons un poste un poste de\nData Engineer en CDI\n, au sein de notre agence Lilloise.\nVos missions \u2705:\nEn tant que Data Engineer pour l'un de nos clients grands comptes, votre r\u00f4le s\u2019articulera autour de diff\u00e9rents axes :\nAppr\u00e9hender le contexte et les enjeux m\u00e9tier du client.\nCollaborer avec les \u00e9quipes m\u00e9tier pour comprendre les exigences en mati\u00e8re de donn\u00e9es.\nD\u00e9finir des architectures data.\nConcevoir et mettre en place des pipelines de donn\u00e9es.\nConstruire des flux de donn\u00e9es complexes.\nVous travaillerez dans une mission \u00e0 forte valeur ajout\u00e9e et de longue dur\u00e9e (minimum 1 an et demi).\nVotre profil\u2705:\nVous ma\u00eetrisez le langage SQL, les ETL et les ELT.\nVous aimez automatiser, mettre en place vos data pipelines et ma\u00eetriser les technologies: CI/CD, Terraform, Github, Python, Kafka.\nVous poss\u00e9dez des comp\u00e9tences en data visualisation : Business Objects, Qlikview, Qlik Sense, PowerBI ou Data Studio.\nVous connaissez Google Cloud Platform (GCS, BigQuery).\nVous \u00eates dipl\u00f4m\u00e9(e) d\u2019une formation BAC + 5.\nVous avez une premi\u00e8re exp\u00e9rience significative dans la data engineering (\nminimum 3 ans\n).\nVous projetez votre carri\u00e8re dans un cabinet de conseil exigent et successful, qui vous permettra de d\u00e9velopper votre esprit entrepreneurial et de r\u00e9pondre \u00e0 vos ambitions.\nCe que nous offrons chez AFD.TECH part of Accenture \ud83e\udd17:\nUne politique de flexibilit\u00e9 dans votre organisation et un bon \u00e9quilibre de vie \ud83c\udfc3\u200d\u2642\ufe0f.\nDes avantages plus que comp\u00e9titifs \ud83d\udcb0.\nUn accompagnement et un suivi r\u00e9gulier durant tout votre parcours chez AFD.TECH (Launchpad, Linkers, rookies, etc\u2026).\nUn \u00e9tat d\u2019esprit familial et de la proximit\u00e9 entre tous \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66.\nDes moments de convivialit\u00e9 toute l\u2019ann\u00e9e \ud83c\udf7e (\u00e9vent en \u00e9quipe, s\u00e9minaire annuel, sports collectifs etc.).\nUn parcours d\u2019\u00e9volution sur mesure \ud83d\udd3c.\nA tr\u00e8s bient\u00f4t chez AFD.TECH part of Accenture!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [
                "Orange"
            ],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Organisation",
                "Flexibilit\u00e9"
            ],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "20",
                "20",
                "20"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Thales",
        "location": "Ollioules, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-thales-3902424527?position=39&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=gUaUTnXX4O5aMuZNjUpt4A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs pr\u00e9sents sur tous les continents. Le Groupe investit dans les innovations du num\u00e9rique et de la \u00ab deep tech \u00bb \u2013 big data, intelligence artificielle, connectivit\u00e9, cybers\u00e9curit\u00e9 et quantique \u2013 pour construire un avenir de confiance, essentiel au d\u00e9veloppement de nos soci\u00e9t\u00e9s, en pla\u00e7ant l\u2019humain au c\u0153ur des d\u00e9cisions.\nThales propose des solutions, services et produits qui aident ses clients \u2013 entreprises, organisations, Etats \u2013 dans cinq grands march\u00e9s vitaux pour le fonctionnement de nos soci\u00e9t\u00e9s : identit\u00e9 et s\u00e9curit\u00e9 num\u00e9riques, d\u00e9fense, a\u00e9ronautique, espace, et transport.\nQUI ETES-VOUS ?\nDipl\u00f4m\u00e9 d\u2019un Bac+5 en \u00e9cole d\u2019ing\u00e9nieur ou \u00e9quivalent universitaire avec une sp\u00e9cialisation en informatique, vous avez a\nu moins 3 ans d'exp\u00e9rience\ndans les technologies Big Data.\nPassionn\u00e9 par le\nsecteur de la D\u00e9fense et du Naval.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que\nData Engineer,\nvous jouerez un r\u00f4le cl\u00e9 dans la conception, le d\u00e9veloppement et la maintenance de notre infrastructure de donn\u00e9es, ainsi que dans la transformation et la gestion des flux de donn\u00e9es.\nVOS MISSIONS :\n\u2022 Concevoir, d\u00e9velopper et d\u00e9ployer des solutions Big Data en utilisant les technologies\nHadoop, Spark, Scala\n.\n\u2022 Mettre en place des pipelines de donn\u00e9es performants pour l'ingestion, le traitement et le stockage des donn\u00e9es massives.\n\u2022 Collaborer \u00e9troitement avec les \u00e9quipes m\u00e9tier pour comprendre leurs besoins en mati\u00e8re d'analyse de donn\u00e9es et proposer des solutions adapt\u00e9es.\n\u2022 Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des donn\u00e9es.\n\u2022 Assurer la qualit\u00e9 et la fiabilit\u00e9 des donn\u00e9es trait\u00e9es, en mettant en place des processus de validation et de nettoyage.\n\u2022 Identifier et r\u00e9soudre les probl\u00e8mes li\u00e9s \u00e0 l'infrastructure Big Data et proposer des am\u00e9liorations.\nInnovation, passion, ambition : rejoignez Thales et cr\u00e9ez le monde de demain, d\u00e8s aujourd\u2019hui.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer - Lille",
        "company": "Capgemini",
        "location": "Greater Lille Metropolitan Area",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-lille-at-capgemini-3914228495?position=40&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=p4ZL6Pvek5LSJf3xzYmxKw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini en quelques mots\nChoisir Capgemini, c'est choisir une entreprise o\u00f9 vous serez en mesure de fa\u00e7onner votre carri\u00e8re selon vos aspirations, o\u00f9 vous serez soutenu et inspir\u00e9 par une communaut\u00e9 d\u2019experts dans le monde entier, o\u00f9 vous pourrez r\u00e9\u00e9crire votre futur. Rejoignez-nous pour red\u00e9finir les limites de ce qui est possible, contribuer \u00e0 lib\u00e9rer la valeur de la technologie pour les plus grandes organisations et participez \u00e0 la construction d\u2019un monde plus durable et inclusif.\nVos missions\nVous maitrisez au minimum un langage de programmation appliqu\u00e9 \u00e0 l\u2019analyse de donn\u00e9es\n(Java, Python, Scala et les environnements Spark et / ou Hadoop).\nVous \u00eates passionn\u00e9 par le Big Data et le Machine Learning et l\u2019analyse de donn\u00e9es\nVous concevez et mettez en \u0153uvre des strat\u00e9gies s\u00e9curis\u00e9es d'acquisition et d'int\u00e9gration de donn\u00e9es\nVous configurez des r\u00e9f\u00e9rentiels de donn\u00e9es \u00e0 la pointe de la technologie dans des environnements distribu\u00e9s\nVous construisez des pipelines de donn\u00e9es pour collecter, transformer et traiter des donn\u00e9es en collaboration avec des scientifiques de donn\u00e9es afin de r\u00e9pondre aux exigences de la mod\u00e9lisation de donn\u00e9es d'analyse avanc\u00e9e\nVotre profil\nDipl\u00f4m\u00e9(e) de Bac+5 en informatique\n4 ans d\u2019exp\u00e9rience\n(au sein d\u2019une ESN ou chez un int\u00e9grateur) en conseil client\u00e8le\nUne solide culture technologique\nUn bon niveau d\u2019anglais\n3 raisons de nous rejoindre\nQualit\u00e9 de vie au travail :\naccord de t\u00e9l\u00e9travail en France et \u00e0 l\u2019international, accord sur l\u2019\u00e9galit\u00e9\nprofessionnelle, la parentalit\u00e9, l\u2019\u00e9quilibre des temps et la mobilit\u00e9 durable.\nApprentissage en continu :\ncertifications et formations en libre acc\u00e8s, accompagnement sur mesure avec\nvotre carreer manager, parcours d\u2019int\u00e9gration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, activit\u00e9s \u00e0 tarifs pr\u00e9f\u00e9rentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel\nNos engagements et priorit\u00e9s\nLe groupe Capgemini encourage une\nculture inclusive dans un cadre multiculturel et handi-accueillant.\nEn nous rejoignant, vous int\u00e9grez un collectif qui valorise la diversit\u00e9, d\u00e9veloppe le potentiel de ses talents, s\u2019engage dans des\ninitiatives solidaires avec ses partenaires, et se mobilise pour r\u00e9duire son impact environnemental sur tous ses sites et aupr\u00e8s de ses clients.\nCapgemini\nest un\nleader mondial\n, responsable et multiculturel, regroupant pr\u00e8s de 350 000 personnes dans plus de 50 pays. Fort de\n55 ans d\u2019exp\u00e9rience,\nnous sommes un partenaire strat\u00e9gique des entreprises pour la transformation de leurs activit\u00e9s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp\u00e9tuelle \u00e9volution tels que le\ncloud, la data, l\u2019Intelligence Artificielle, la connectivit\u00e9, les logiciels, l\u2019ing\u00e9nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Machine Learning",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration",
                "Organisation"
            ],
            "EnSoftSkils": [
                "Collaboration",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (F/H) \u00e0 Nantes",
        "company": "Siderlog Conseil",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-%C3%A0-nantes-at-siderlog-conseil-3858540683?position=41&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=HGGdfZPbjaOackab%2BATkIA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Rejoignez une Aventure Passionnante chez Nous !\n\ud83d\ude80\nSi vous recherchez une entreprise en pleine croissance o\u00f9 votre potentiel peut s'\u00e9panouir pleinement, vous \u00eates au bon endroit !\nChez nous, l'humain est au c\u0153ur de notre culture d'entreprise. Nous croyons en l'autonomie, la confiance et le partage comme des valeurs essentielles qui guident chacune de nos actions.\nNe perdez plus de temps, rencontrons-nous d\u00e8s maintenant !\nEn tant que membre de notre \u00e9quipe de consultants Siderlog, vous travaillerez en \u00e9troite collaboration avec nos clients. Voici un aper\u00e7u des missions qui vous attend :\nContribution \u00e0 la fabrication de produits dans un environnement Cloudera \ud83d\udee0\ufe0f\nAccompagnement sur la fabrication des mod\u00e8les de Machine Learning sur des donn\u00e9es \u00e9nerg\u00e9tiques et plus largement ESG. \ud83e\udd16\nAttendu :\nContribuer au sein d'une \u00e9quipe agile \u00e0 r\u00e9pondre aux besoins des Caisses r\u00e9gionales.. \ud83c\udf10\nD\u00e9finir les architectures des solutions avec le reste de l\u2019\u00e9quipe \ud83c\udfd7\ufe0f\nFabriquer et tester les solutions \ud83e\uddea\nD\u00e9ployer dans les diff\u00e9rents environnements \ud83d\ude80\nGarantir le bon fonctionnement en production \ud83d\udcbc\nAccompagner l\u2019\u00e9volution des pratiques de l\u2019\u00e9quipe dans une logique d\u2019am\u00e9lioration continue de la qualit\u00e9 du code \ud83d\udcc8\nEntrainer et tester des mod\u00e8les de Machine Learning \ud83e\udde0\nProfil :\nUne exp\u00e9rience entre 4 \u00e0 7 ans\nLieu : Nantes\nD\u00e9but : D\u00e8s que possible\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Machine Learning",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "7",
                "7",
                "7"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "ASTRELYA",
        "location": "Greater Paris Metropolitan Region",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-astrelya-3910760230?position=42&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=xS5BufbehfAwbLJ4GkTZBQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ASTRELYA est un groupe de conseil et d\u2019expertise IT fond\u00e9 en 2017, pr\u00e9sent en France (Paris et r\u00e9gions) et en Suisse (Gen\u00e8ve). Aujourd'hui plus de 280 collaborateurs accompagnent nos clients dans l\u2019acc\u00e9l\u00e9ration et la transformation de leurs organisations.\nDans le cadre de notre d\u00e9veloppement, nous recherchons un\nData Engineeer F/H\n.\nVos r\u00f4les et responsabilit\u00e9s :\nD\u00e9veloppements Java Spark\nOptimisation et gestion des \u00e9volutions de l&#39;architecture pour int\u00e9grer des calculs sur des volum\u00e9tries de plus en plus importantes\nSupport technique aupr\u00e8s des \u00e9quipes de d\u00e9veloppement et du responsable applicatif\nConception des solutions applicatives coh\u00e9rentes avec l&#39;ensemble du SI et avec les normes et standards\nD\u00e9velopper et garantir les pratiques de d\u00e9veloppement et de documentation associ\u00e9s (DevOps\nL\u2019environnement technique dans lequel vous \u00e9voluerez :\nJava, Scala, Spark, \u00e9cosyst\u00e8me Hadoop, environnement DevOps\nLes comp\u00e9tences recherch\u00e9es :\nFormation : \u00c9cole d\u2019ing\u00e9nieur ou \u00e9quivalent Bac+5\nExp\u00e9riences : Minimum 5 ans d\u2019exp\u00e9rience\nLangues : Anglais technique\nExcellent relationnel, force de proposition, autonome\nPourquoi rejoindre ASTRELYA ?\nUne gestion de carri\u00e8re personnalis\u00e9e et un management de proximit\u00e9\nUne politique active de formations / certifications (technique, m\u00e9tier, leadership)\nUne offre vari\u00e9e de missions d\u2019expertise\nUn engagement RSE fort : Ecovadis Gold, Signataire de la charte pour la diversit\u00e9, du Pacte des Nations Unies et mise en place du M\u00e9c\u00e9nat de comp\u00e9tences\nUn programme de cooptation attractif\nAfterworks, conf\u00e9rences techniques et activit\u00e9s sportives r\u00e9guliers\nCette annonce vous correspond ? Postulez !\n\ud83d\ude80\nTous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps"
            ],
            "FrSoftSkills": [
                "Leadership",
                "Organisation"
            ],
            "EnSoftSkils": [
                "Leadership"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Beelix",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-beelix-3865239426?position=43&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=X3Z%2FKDHugw6kJ06w9HTF%2Bg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des probl\u00e9matiques de Product Management, Data et Design Thinking. Beelix contribue \u00e0 fa\u00e7onner le monde de demain en participant aux grandes avanc\u00e9es des secteurs suivants:\n\ud83d\ude97Automobile\n\u26a1Energie\n\ud83d\udce1M\u00e9dias & T\u00e9l\u00e9coms\n\ud83d\udc57Luxe & Retail\n\ud83d\udcb6 Banque, Finance & Assurance\n\u2708\ufe0fD\u00e9fense\nAujourd\u2019hui, Beelix compte plus de 200 collaborateurs motiv\u00e9s et dynamiques. Lab\u00e9lis\u00e9e Great Place To work en 2023, Beelix est aussi une entreprise engag\u00e9e o\u00f9 il fait bon vivre.\nDans le cadre de notre d\u00e9veloppement, nous recherchons un Data Engineer en \u00cele-de-France.\nQuelles missions au quotidien ?\nVous aurez pour missions principales de d\u00e9velopper les projets Big Data demand\u00e9s par le m\u00e9tier, et notamment :\nPasser de la donn\u00e9e brute \u00e0 de la donn\u00e9e exploitable, expos\u00e9e sous forme de tables requ\u00eatables dans le Datalake\nConsolider ces donn\u00e9es au fur et \u00e0 mesure de leur alimentation r\u00e9currente dans le Datalake\nLes exploiter pour atteindre la finalit\u00e9 business (exposition de Business View, r\u00e9int\u00e9gration des r\u00e9sultats dans le SI, service de scoring, \u2026)\nDe mettre en place et de garantir le respect dans la dur\u00e9e d'un processus qualit\u00e9 sur l'ensemble du cycle de DEV (documents, tests unitaires / int\u00e9gration / fonctionnels, commentaires, versionning, etc.)\nAccompagner les Data Engineers sur son p\u00e9rim\u00e8tre pour garantir la qualit\u00e9 des livrables\nExpertise souhait\u00e9e\nExpertise en SPARK et PySpark\nExpertise sur Databricks\nUne exp\u00e9rience sur un cloud provider public comme Azure (id\u00e9alement), AWS, ou GCP\nConnaissances avanc\u00e9es d'outils de BI comme PowerBI (id\u00e9alement) ou Spotfire\nCapacit\u00e9 \u00e0 interagir avec des parties prenantes diverses : Business analyst, Architectes, M\u00e9tier\nEtre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de donn\u00e9es relationnelles et NoSQL\nUne exp\u00e9rience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nA propos de vous ?\nDipl\u00f4m\u00e9 d'une \u00e9cole d'ing\u00e9nieurs ou \u00e9quivalent\nAu moins 3 ans d'exp\u00e9rience en tant que Data Engineer\nExp\u00e9rience en mode de Delivery Agile (Scrum, Kanban, etc.\u2026)\nVous avez un bon niveau d\u2019anglais tant \u00e0 l\u2019\u00e9crit qu\u2019\u00e0 l\u2019oral\nPourquoi nous rejoindre ?\nUn suivi et un accompagnement au quotidien\nUn organisme de formation certifi\u00e9 Qualiopi, un abonnement linkedin learning pour chaque salari\u00e9 et des partenariats avec des sp\u00e9cialistes pour d\u2019autres expertises\nDe nombreux \u00e9v\u00e9nements : Afterworks, Communaut\u00e9s m\u00e9tiers, Happy talks\u2026\nune Exp\u00e9rience personnalis\u00e9e bas\u00e9e sur vos besoins gr\u00e2ce au Pr\u00e9dictive Index\nNotre package \u00ab unBeelievable \u00bb : 100% du titre de transport, Tickets restaurants, CSE, Prime de participation ...\nNombreux \u00e9v\u00e8nements (afterworks, sport, etc) et des communaut\u00e9s m\u00e9tiers dynamiques\nLe processus de recrutement ?\n\u00c9change t\u00e9l\u00e9phonique (15 min)\nEntretien 1 RH pour apprendre \u00e0 vous conna\u00eetre\nEntretien 2 avec votre futur N+1 pour appr\u00e9hender la relation manag\u00e9riale\nEntretien 3 avec un Responsable commercial pour avoir la vision strat\u00e9gique\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [
                "100"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F)",
        "company": "MP DATA",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-mp-data-3908719610?position=44&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=2MIiyRrB12%2BwsWUba%2B9GJQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "MP DATA est une soci\u00e9t\u00e9 sp\u00e9cialis\u00e9e dans l\u2019acquisition, le traitement, et la valorisation des donn\u00e9es.\nDepuis sa cr\u00e9ation en 2015, MP DATA accompagne ses clients, majoritairement industriels, dans le management de leur performance et l\u2019exploitation de leurs donn\u00e9es.\nLes collaborateurs, tous issus de grandes \u00e9coles, incarnent au quotidien les valeurs d\u2019Excellence, de Partage et d\u2019Engagement.\nIls associent savoir-faire technique, m\u00e9thodologie et passion et mettent leurs comp\u00e9tences au service de missions et projets au sein de grands groupes fran\u00e7ais.\nMP DATA accompagne ses clients sur toute la chaine au travers de 3 p\u00f4les d\u2019expertise : Conseil et Strat\u00e9gie, Infrastructure & CloudOPS, Data Science.\nChez MP DATA, les \u00e9quipes commerciales cherchent des missions en fonction des envies des collaborateurs et non pas l\u2019inverse. Les consultants sont accompagn\u00e9s dans tous leurs projets, de la mobilit\u00e9 g\u00e9ographique, au changement de secteur d\u2019activit\u00e9 en passant par le d\u00e9veloppement de nouvelles comp\u00e9tences.\nRejoindre MP DATA, c\u2019est la garantie de travailler sur des sujets passionnants avec un cadre technique fort.\nDescriptif du poste :\nNous recherchons un Data Engineer exp\u00e9riment\u00e9 pour rejoindre notre \u00e9quipe.\nEn tant que Data Engineer, vous serez responsable de la conception, du d\u00e9veloppement et de la mise en \u0153uvre de pipelines de traitement de donn\u00e9es en temps r\u00e9el \u00e0 grande \u00e9chelle.\nVous travaillerez avec des technologies telles que Kafka, Flink, Kinesis et vous utiliserez les services du cloud AWS pour stocker et traiter les donn\u00e9es.\nVos responsabilit\u00e9s :\nUtiliser Kafka pour le traitement de flux de donn\u00e9es en temps r\u00e9el \u00e0 grande \u00e9chelle, en travaillant avec les producteurs, les consommateurs et les topics.\nMettre en \u0153uvre des pipelines de traitement de donn\u00e9es en streaming avec Flink, en appliquant des transformations complexes et en g\u00e9rant les \u00e9tats.\n\u00c9crire du code efficace et maintenable en Java / Python pour manipuler et analyser les donn\u00e9es en temps r\u00e9el.\nUtiliser Kubernetes pour d\u00e9ployer et g\u00e9rer des applications conteneuris\u00e9es \u00e0 grande \u00e9chelle, en assurant la r\u00e9silience et l\u2019\u00e9volutivit\u00e9 des services.\nUtiliser les services AWS tels que Amazon S3, AWS Lambda, Elastic Kubernetes Service (EKS), Elastic Container Service (ECS) et Elastic Compute Cloud (EC2) pour le stockage, le traitement et le calcul des donn\u00e9es en temps r\u00e9el.\nSuivre les meilleures pratiques pour une utilisation efficace du cloud, en assurant la gestion des co\u00fbts, la s\u00e9curit\u00e9 des donn\u00e9es et la disponibilit\u00e9 des services.\nCollaborer avec l\u2019\u00e9quipe de d\u00e9veloppement logiciel et la gestion de projets pour assurer un flux de d\u00e9veloppement fluide et une livraison efficace des fonctionnalit\u00e9s.\nBon \u00e0 savoir :\nCDI / ASAP / Toulouse\nProfil recherch\u00e9:\nNous recherchons un candidat dipl\u00f4m\u00e9 d'une grande \u00e9cole d'Ing\u00e9nieur avec une premi\u00e8re exp\u00e9rience.\nComp\u00e9tences n\u00e9cessaires :\nExp\u00e9rience significative dans un environnement industriel en mode DevOps, avec des outils tels que CICD, gitlab, Jenkins, Sonar, Nexus, XLdeploy, Camunda, etc.\nMa\u00eetrise des langages de programmation tels que Python, Java et expertise dans l\u2019\u00e9criture et l\u2019optimisation du code SQL\nMa\u00eetrise du fran\u00e7ais et bonne maitrise de l\u2019anglais.\nCapacit\u00e9 \u00e0 travailler en \u00e9quipe et esprit d\u2019\u00e9quipe.\nLe processus de recrutement se d\u00e9roule en 3 entretiens :\nPrise de contact\n1er entretien : Pr\u00e9sentation et projet du candidat + pr\u00e9sentation MP DATA\n2\u00e8me entretien : Entretien de qualification technique\n3\u00e8me entretien : Rencontre avec les \u00e9quipes dans les locaux MP DATA + Proposition de collaboration\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Flink"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Shippeo",
        "location": "Greater Paris Metropolitan Region",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-shippeo-3908268236?position=45&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=DxKRCRuTeTU%2Bx7wYrAc6QQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Founded in 2014, Shippeo is a French based SaaS company providing supply chain visibility. Shippeo has grown from 70 to 220 employees during the last two years and is continuing to rapidly scale after an additional $40M fundraising round in October 2022.\nShippeo is an exceptionally diverse company with colleagues from 27 different nationalities and speaking 29 languages. With offices throughout Europe, North America and recently Asia, Shippeo provides global coverage to all of our clients.\nOur product is composed of a mission critical SaaS web platform (API everywhere), with high traffic inbound/outbound integrations.\nOur mission is to anticipate problems and proactively alert end-customers so they can efficiently manage exceptions. We achieve this by collecting and matching millions of theoretical and real data from different stakeholders.\nThe Data Intelligence Tribe is responsible for leveraging Shippeo\u2019s data from our large shipper and carrier base, to build data products that help our users (shippers and carriers alike) and ML models to provide predictive insights. This tribe\u2019s typical responsibilities are to:\nget accurately alerted in advance of any potential delays on their multimodal flows or anomalies so that they can proactively anticipate any resulting disruptions\nextract the data they need, get direct access to it or analyze it directly on the platform to gain actionable insights that can help them increase their operational performance and the quality and compliance of their tracking\nprovide best-in-class data quality by implementing advanced cleansing & enhancement rules\nAs a Data Engineer at Shippeo, your objective is to ensure that data is available and exploitable by our Data Scientists and Analysts on our different data platforms. You will contribute to the construction and maintenance of Shippeo\u2019s modern data stack that\u2019s composed of different technology blocks:\nData Acquisition (Kafka, KafkaConnect, RabbitMQ),\nBatch data transformation (Airflow, DBT),\nCloud Data Warehousing (Snowflake, BigQuery),\nStream/event data processing (Python, docker, Kubernetes) and all the underlying infrastructure that support these use cases.\nQualifications\nRequired:\nYou have a degree (MSc or equivalent) in Computer Science.\n3+ years of experience as a Data Engineer.\nExperience building, maintaining, testing and optimizing data pipelines and architectures\nProgramming skills in Python and experience with asynchronous event processing (asyncio).\nAdvanced working knowledge of SQL, experience working with relational databases and familiarity with a variety of databases.\nWorking knowledge of message queuing and stream processing.\nKnowledge of Docker and Kubernetes.\nKnowledge of a cloud platform (preferably GCP).\nExperience working with workflow management systems such as Airflow.\nDesired:\nExperience with cloud based data warehouse solutions (BigQuery, Snowflake).\nExperience with Kafka and KafkaConnect (Debezium).\nExperience with Infrastructure as code (Terraform/Terragrunt).\nExperience building and evolving CI/CD pipelines with Github Actions.\nMonitoring and alerting on Grafana / Prometheus.\nExperience working on Apache Nifi.\nInformations suppl\u00e9mentaires\nWe are looking for talents who share our values:\n\ud83d\ude80 Ambition\n\ud83d\udc99 Care\n\ud83c\udfaf Deliver\n\ud83e\udd1d Collaboration\nFind out more about our values in\nOur Culture Book\nIf you identify with our values and enjoy working in a fast-paced and international environment, Shippeo is just the place for you!\nWe are committed to fostering diversity and inclusion within our workplace as we value the unique perspectives and experiences that individuals from all backgrounds bring to our team. We are dedicated to providing equal employment opportunities to all candidates, regardless of their background or abilities, and our commitment to inclusion is reflected in our policies, practices, and workplace culture.\nWe understand that candidates may have unique needs or questions related to disability inclusion. To facilitate this, you can reach our dedicated Disability Advisor at\ninclusion@shippeo.com\nwith any inquiries or requests for accommodations during the application process.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake",
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes",
                "Airflow"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "ML",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Stage - Data Engineer - ML (H/F)",
        "company": "Withings",
        "location": "Issy-les-Moulineaux, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/stage-data-engineer-ml-h-f-at-withings-3613476264?position=46&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=%2F8DZLqwtmjFyUqn3vcgywQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Chez Withings, nous d\u00e9veloppons des appareils de sant\u00e9 connect\u00e9e : nos balances connect\u00e9es, montres hybrides, tensiom\u00e8tres, moniteurs de sommeil et tous les dispositifs de notre gamme sont aujourd'hui utilis\u00e9s par des millions d'utilisateurs. Notre objectif est de permettre la pr\u00e9vention, le d\u00e9pistage et l'accompagnement d'un certain nombre de maladies chroniques via des produits et des services innovants afin de r\u00e9volutionner la mani\u00e8re dont on prend soin de notre sant\u00e9.\nAu sein de l'\u00e9quipe Machine Learning, nous d\u00e9veloppons des algorithmes pour extraire des informations physiologiques et m\u00e9dicales pour nos utilisateurs tels que le SPO2, la fr\u00e9quence cardiaque, la d\u00e9tection de diverses pathologies comme la fibrillation atriale, l'apn\u00e9e du sommeil...\nInt\u00e9gr\u00e9.e au sein de l'\u00e9quipe Machine Learning, tu auras une ou plusieurs des responsabilit\u00e9s suivantes :\nD\u00e9velopper un outil de monitoring de la dette technique, des mauvaises pratiques de code, des failles de s\u00e9curit\u00e9 ;\nConstruire des dashboards de visualisation ;\nConstruire un syst\u00e8me d'alerte pour notifier les contributeurs d'\u00e9ventuels probl\u00e8mes ;\nD\u00e9velopper des outils permettant de corriger les \u00e9ventuels probl\u00e8mes de fa\u00e7on automatis\u00e9e ;\nRequirements\n\u00c0 la recherche d'un stage d'une dur\u00e9e de 3 \u00e0 6 mois ;\nPr\u00e9paration d'un Master en \u00e9cole d'ing\u00e9nieur ou \u00e9quivalent / ann\u00e9e de c\u00e9sure possible ;\nMa\u00eetrise de Python ;\nMa\u00eetrise de Debian ou de Ubuntu, de Shell et de l'environnement Linux ;\nPremi\u00e8re exp\u00e9rience sur du d\u00e9veloppement logiciel ;\nCulture DevOps (omnipr\u00e9sence du monitoring, automatisation des t\u00e2ches, ...)\nCompr\u00e9hension de la culture et des besoins des diff\u00e9rents membres de l'\u00e9quipe ;\nRigueur, autonomie, prise d'initiative, curiosit\u00e9\nBenefits\nRejoindre l'aventure Withings, c'est :\nInt\u00e9grer un des pionniers et leaders mondiaux de la sant\u00e9 connect\u00e9e, plusieurs fois prim\u00e9 au Consumer Electronic Show\nContribuer \u00e0 des projets innovants et ambitieux pour la sant\u00e9 de demain dans un environnement agile et en constante \u00e9volution\nInt\u00e9grer une entreprise internationale, membre de la FrenchTech 120, dont les \u00e9quipes sont bas\u00e9es \u00e0 Issy-les-Moulineaux, Boston, Hong-Kong et Shenzhen\nParticiper \u00e0 l'am\u00e9lioration continue de nos produits et services en les b\u00eata-testant avant leur sortie, notamment lors de nos nombreuses sessions sportives entre coll\u00e8gues\nParticiper \u00e0 la Withings Med Academy en assistant \u00e0 des conf\u00e9rences de professionnels de sant\u00e9 afin de renforcer ses connaissances dans le domaine m\u00e9dical\nCollaborer avec des coll\u00e8gues passionn\u00e9s et c\u00e9l\u00e9brer ensemble chacune de nos r\u00e9ussites !\nToutes les candidatures re\u00e7ues sont \u00e9tudi\u00e9es ind\u00e9pendamment de l'origine ethnique, des croyances, de la religion, du genre, de l'orientation sexuelle ou de la sant\u00e9 des candidats. Withings aspire \u00e0 offrir et garantir l'\u00e9galit\u00e9 des chances aux candidats et seules les personnes habilit\u00e9es (RH et Management) auront acc\u00e8s aux informations concernant votre candidature.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [
                "Linux"
            ],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Machine Learning"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Hybride"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer Senior",
        "company": "AXA en France",
        "location": "Hauts-de-Seine, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-senior-at-axa-en-france-3884386043?position=47&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=SDubjYBbL7PRNB7yLnRk%2Bg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Environnement\nEn tant que\nSenior Data Engineer F/H\n, vous allez contribuer directement aux projets des directions m\u00e9tier (ex : fraude sant\u00e9, multi\u00e9quipements, pricing IARD, optimisation du lead management, fragilit\u00e9 auto, \u2026) d\u2019AXA France et \u00e0 la construction du socle technique Big Data.\nVous allez int\u00e9grer une \u00e9quipe d'une dizaine de personne compos\u00e9e de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus m\u00e9tier de la Direction Transformation Digital Tech et DATA (DT2).\nLa Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :\n- Une organisation agile en feature teams : tribus, guildes, squads\n- Des projets sur des applications innovantes \u00e0 fort trafic (web, mobile\u2026)\n- Des m\u00e9thodologies craft (TDD, BDD, clean code, code review\u2026) et DevOps\n- Une communaut\u00e9 de partage de bonnes pratiques (BBL, dojo, meetup, conf\u2026)\nVotre r\u00f4le et vos missions\nVous aurez pour missions principales de d\u00e9velopper les projets Big Data demand\u00e9s par le m\u00e9tier, et notamment :\nPasser de la donn\u00e9e brute \u00e0 de la donn\u00e9e exploitable, expos\u00e9e sous forme de tables requ\u00eatables dans le datalake\nConsolider ces donn\u00e9es au fur et \u00e0 mesure de leur alimentation r\u00e9currente dans le data lake\nLes exploiter pour atteindre la finalit\u00e9 business (exposition de business view, r\u00e9int\u00e9gration des r\u00e9sultats dans le SI, service de scoring, \u2026)\nDe travailler \u00e0 la cr\u00e9ation du socle technique Big Data et industrialiser le cycle de d\u00e9veloppement de l'\u00e9quipe\nDe mettre en place et de garantir le respect dans la dur\u00e9e d'un processus qualit\u00e9 sur l'ensemble du cycle de DEV (documents, tests unitaires / int\u00e9gration / fonctionnels, commentaires, versionning, etc.)\nVotre profil\nVous justifiez de plusieurs exp\u00e9riences significatives (+ de 5 ans) sur du\nd\u00e9veloppement big data, en particulier sur du PySpark.\nComp\u00e9tences techniques :\nConnaissances avanc\u00e9es en d\u00e9veloppement en\nPySpark\n(Spark avec le langage Python)\nMaitrise de l'environnement\nMicrosoft Azure\nConnaissances avanc\u00e9es d'outils de BI comme\nPowerBI\nComp\u00e9tences transverses :\nCapacit\u00e9 \u00e0 interagir avec des parties prenantes diverses : Business analyst, Architectes, M\u00e9tier\nExp\u00e9rience en mode de delivery Agile (Scrum, Kanban, etc...)\nDriver et accompagner des Data Engineer junior sur les aspects technique\nEt Id\u00e9alement :\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de donn\u00e9es relationnelles et NoSQL\nUne exp\u00e9rience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nQui sommes nous ?\nAXA est un des leaders de l\u2019assurance et de la gestion d\u2019actifs dans le monde.\nNous aidons nos 108 millions de clients \u00e0 traverser les petites et grandes difficult\u00e9s de la vie.\nChaque jour, nous agissons ensemble pour inventer la meilleure mani\u00e8re de les prot\u00e9ger et voulons donner \u00e0 chacun les moyens de vivre une vie meilleure.\nUn challenge qui donne le sourire et envie de se lever le matin !\nChez AXA, nous sommes persuad\u00e9s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C\u2019est pour cette raison que nous menons une politique RH engag\u00e9e qui favorise la diversit\u00e9, qui pr\u00e9serve l\u2019\u00e9quilibre vie priv\u00e9e-vie professionnelle et acc\u00e9l\u00e8re le d\u00e9veloppement des comp\u00e9tences et des carri\u00e8res.\nAinsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une v\u00e9ritable culture d\u2019expertise, acc\u00e9l\u00e9rant le d\u00e9veloppement des comp\u00e9tences de chacun et proposant une r\u00e9mun\u00e9ration attractive.\nPourquoi nous rejoindre ?\nVous \u00eates porteur d\u2019id\u00e9es et d\u2019initiatives innovantes ? Vous proposez des solutions et \u00eates au service du client ? Faites partie de notre grande famille en rejoignant\nUn leader mondial offrant des opportunit\u00e9s de carri\u00e8res int\u00e9ressantes\nUne entreprise qui donne une place de choix \u00e0 l\u2019innovation, \u00e0 l\u2019initiative et aux actions solidaires (notamment via l\u2019association AXA Atout C\u0153ur)\nUn environnement inclusif \u00e0 tous les niveaux (mixit\u00e9, handicap, initiatives pour favoriser l\u2019insertion des jeunes, orientation sexuelle, etc.)\nUn acc\u00e8s \u00e0 de multiples avantages (cong\u00e9s, temps partiel, t\u00e9l\u00e9travail, etc.)\nUn cadre stimulant, qui permet de rencontrer des collaborateurs performants et d\u2019enrichir ses comp\u00e9tences\nVictime ou t\u00e9moin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination \u00e0 alerte.discrimination.harcelement@axa.fr\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "JIRA",
                "Teams"
            ],
            "Other": [
                "DevOps",
                "Big Data"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Junior",
                "Senior"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Analytics Data Engineer (Internship)",
        "company": "Equativ",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/analytics-data-engineer-internship-at-equativ-3901954759?position=48&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=YVUggz38DDZrNjhBOH0OuQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Your mission\nHelping the company be fully data-driven is not just a buzzword, it is our mission at Equativ. We, the Data Analytics team at Equativ, maximize efficiency by enabling easy and permanent access to quality data, valuable insights & rigorous thinking. Our responsibility is to ensure our local and central teams, clients and partners, make informed business decisions. To do so, we leverage huge volumes of data (Equativ handles 150Bn Auctions per Day) in a state-of-the-art tech stack (AirFlow, Snowflake, Tableau\u2026) in order to provide actionable insights to all teams at Equativ!\nReporting to the manager of the Analytics team, your mission will be to maintain and upgrade our data pipeline.\nWhat you'll do\nDay-to-day maintenance of our data pipeline:\nEnsure data pipeline ingestion accuracy in due time\nFollow-up on data quality issues raised by internal customers\nImprovement of sourcing processes:\nMigrate from Talend data flows to Python scripts for our sourcing jobs\nDevelop resources to make Data Analysts autonomous in sourcing data in our Snowflake database (through ready-to-use scripts or small interfaces)\nDeveloping new projects on our main platforms (Tableau & Snowflake)\nLeverage new resources to make the most out of Snowflake (Streamlit, Snowpark\u2026)\nIdentify new ways to structure our data sources in Tableau while reducing the loading time for the user\nParticipate in the restructuring of our data marts (schemas, stages & permissions)\nCommunication:\nSync with Data Analysts to make sure that their requests are properly prioritized\nSynchronize with other technical teams (Core-Data, Infra) to gather requirements of the migration and ensure a smooth transition\nUnderstand business needs to suggest the most efficient technical solution\nAbout You\nPragmatic & hands-on mindset is required: you\u2019ll have latitude to explore different options, but you need to go for the most effective solution\nTechnical knowledge of Python & SQL is a must\nKnowledge of collaboration platforms (Gitlab) & Agile processes is a plus\nYou can demonstrate your ability to solve problems end to end\nYou are fluent in English\n\ud83d\udc4b About us\nEquativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus \u2014 four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.\nHeadquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.\nThe company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times\u2019 FT 1000: Europe\u2019s Fastest-Growing Companies.\nEquativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.\nCome and lead the charge with us in building a transparent ecosystem based on quality!\nEquativ is an equal opportunity employer. Equal access to employment, services, and programs are available to everyone, regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you require reasonable accommodation throughout the application and/or interview process, please contact the recruitment team at ta-team@equativ.com\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML"
            ],
            "FrSoftSkills": [
                "Communication",
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F)",
        "company": "Technology & Strategy",
        "location": "Lyon, Auvergne-Rh\u00f4ne-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-technology-strategy-3881556102?position=49&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=V5yopRJgKm1yv0Jrdf2Uag%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "D\u00e9couvrez Novencia\n:\nExpert en Data et Intelligence Artificielle, nous aidons nos clients \u00e0 exploiter et \u00e0 valoriser leurs donn\u00e9es sous toutes ses formes en les accompagnant sur des projets de Data Analyse, Data Gourvernance, Data Architecture, Data Science, et Data Engineering\u2026\nVous avez une solide exp\u00e9rience de minimum 2 ans dans l'ing\u00e9nierie des donn\u00e9es et vous \u00eates \u00e0 la recherche de nouveaux d\u00e9fis ? Bouclez votre ceinture, la suite est pour vous !\nType de contrat : CDI\nLieu : Lyon\nEn qualit\u00e9 de Data Engineer (H/F), votre r\u00f4le sera :\nConcevoir et proposer les solutions de d\u00e9veloppement r\u00e9pondant aux besoins fonctionnels et techniques des projets big data.\nTu participes \u00e0 la conception de solutions permettant le traitement de volumes importants de pipelines donn\u00e9es.\nR\u00e9aliser ces solutions par l\u2019\u00e9criture de code, en respectant les m\u00e9thodes et proc\u00e9dures qualit\u00e9s d\u00e9finies au sein du d\u00e9partement Technique.\nMise \u00e0 disposition s\u00e9curis\u00e9 et lisible de la data.\nS\u2019assurer de la conformit\u00e9 fonctionnelle et technique de ces r\u00e9alisations en effectuant les tests automatis\u00e9s n\u00e9cessaire et la mise en place de monitoring (syst\u00e8me et qualit\u00e9).\nAssurer la maintenance des applicatifs / plateforme data science\nAssurer une veille technologique\nVous disposez des comp\u00e9tences suivantes :\nMaitrise des plateformes Cloud (AWS, GCP ou Azure), de Scala et de SQL.\nUn.e touche \u00e0 tout : poss\u00e9dant des comp\u00e9tences en langage Python/Spark, de bonnes capacit\u00e9s de mod\u00e9lisation, une forte app\u00e9tence pour le Big Data\nFin.e connaisseur.euse : Data Engineer convaincu, tr\u00e8s peu de secrets pour les clusters et pour les calculs parall\u00e8les\nExplorateur.trice : d\u00e9couvre de nouvelles technos gr\u00e2ce \u00e0 une veille r\u00e9guli\u00e8re\nD\u00e9brouillard.e : rel\u00e8ve de nouveaux d\u00e9fis\nNotre objectif commun est de co-construire votre carri\u00e8re en fonction de vos aspirations et de vos comp\u00e9tences.\nContactez-moi en message priv\u00e9 ou par mail \u00e0 s.ziki@technologyandstrategy.com !\nLet's make it possible #together\n*Nos postes sont ouverts aux personnes b\u00e9n\u00e9ficiant d\u2019une Reconnaissance de la Qualit\u00e9 de Travailleur Handicap\u00e9 (RQTH). T&S Groupe encourage la diversit\u00e9 et l\u2019\u00e9galit\u00e9 sur le lieu de travail. Tous les candidats qualifi\u00e9s H/F/* sont pris en consid\u00e9ration pour un emploi sur un m\u00eame pied d'\u00e9galit\u00e9.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "2",
                "2",
                "2"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Stage - Data Engineer",
        "company": "Exotec",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/stage-data-engineer-at-exotec-3918170659?position=50&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=4yuUcRj%2F0x7kYt4sveJZSg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Chez Exotec, nous mettons l'excellence technologique au service de la red\u00e9finition des relations entre humains et robots. A travers le monde, nos solutions r\u00e9volutionnent la fa\u00e7on dont nos clients d\u00e9livrent leurs produits aux consommateurs finaux. Nous contribuons au succ\u00e8s des plus grandes marques du commerce et de l'industrie, tout en am\u00e9liorant les conditions de travail de leurs salari\u00e9s.\nPar l'alliance de l'intelligence artificielle et d'un hardware performant, nos robots sont d\u00e9sormais d\u00e9ploy\u00e9s dans le monde entier et leur succ\u00e8s a fait de nous la premi\u00e8re licorne industrielle fran\u00e7aise.\nRejoindre Exotec, c'est l'opportunit\u00e9 de donner du sens \u00e0 vos comp\u00e9tences. Grandissez avec plus de 800 ExoPeople dans le monde entier pour faire de vos id\u00e9es des r\u00e9alit\u00e9s.\nLa r\u00e9volution robotique port\u00e9e par Exotec ne fait que commencer, vous en \u00eates ?\nAu sein du p\u00f4le Data, de la DSI d'Exotec, votre r\u00f4le sera de participer au d\u00e9veloppement de l'environnement et de l'infrastructure Data d'Exotec.\nPour cela :\nVous participez \u00e0 la mise en \u0153uvre des composants techniques de la plateforme de donn\u00e9es d'Exotec\nVous travaillez sur la collecte dans la plateforme de donn\u00e9es provenant de sources multiples : Salesforce, ERP, logiciels d\u00e9velopp\u00e9s en interne\nVous nettoyez, mettez en qualit\u00e9 et pr\u00e9parez les donn\u00e9es afin de les rendre disponibles pour les diff\u00e9rents cas d'usage qui en ont besoin\nVous migrez des reportings existants vers la plateforme de donn\u00e9es et mettez en \u0153uvre de nouveaux cas d'usage pour r\u00e9pondre aux besoins de l'entreprise\nVous travaillerez au sein de l'\u00e9quipe data et en \u00e9troite collaboration avec la software factory, ainsi qu'avec les utilisateurs des m\u00e9tiers qui ont besoin de rendre intelligibles les donn\u00e9es disponibles\nRequirements\nVous \u00eates \u00e9tudiant(e) d'une \u00e9cole d'Ing\u00e9nieur g\u00e9n\u00e9raliste avec une sp\u00e9cialisation programmation ou informatique\nVous recherchez un stage de fin d'\u00e9tudes d'une dur\u00e9e de 4 \u00e0 6 mois\nVous avez id\u00e9alement une premi\u00e8re exp\u00e9rience en Data Engineering et le d\u00e9veloppement de pipeline de donn\u00e9es\nVous maitrisez Python, l'ETL et SQL,\nCurieux(se) et rigoureux(se), vous souhaitez rejoindre une \u00e9quipe jeune et dynamique ainsi que vous investir dans des projets complexes et excitants\nVous avez un niveau d'anglais courant\nChez Exotec, nous garantissons l'\u00e9galit\u00e9 des chances dans notre processus de recrutement. L'ensemble des candidatures re\u00e7ues sont \u00e9tudi\u00e9es ind\u00e9pendamment de l'\u00e2ge, du genre, de l'origine, de la religion, de la couleur de peau, de la nationalit\u00e9, du sexe, du handicap, de l'orientation sexuelle ou de toute autre distinction prot\u00e9g\u00e9e par la loi. Nous mettons en place un environnement de travail inclusif et respectueux de toutes les diff\u00e9rences. En rejoignant le Pacte Parit\u00e9, Exotec s'engage pour un \u00e9cosyst\u00e8me French Tech plus paritaire.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Solutions Engineer (Data & AI)",
        "company": "LVMH",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=51&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=k9JSETDfWfypotKWZEAbAw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.\nWe are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.\nIf you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,\nIf you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,\nIf you like building tech solutions having direct impacts on billion-dollar-valued businesses,\nIf you have good communication skills and like sharing your knowledge,\nApply now, and join us!\nThe mission\nThe Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.\nOur team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.\nYou will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).\nMain responsibilities\nYou will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).\nYou will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.\nYou will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.\nApplying the quality and security standards. Making them evolve if necessary.\nProducing realistic, understandable and documented solutions following the group guidelines.\nSharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.\nTaking responsibility as member of the team on the product performances (delivery and long term usage)\nRequired expertise and knowledge\nAbility to build technical solutions answering concrete usage (User Stories) and communicate them to the team.\nDimension and evaluate complexity for technical solution productions.\nExtensive knowledge and experience with good learning and sharing abilities.\nEvaluate quickly risks and opportunities about technical choices.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProblem-solving skills on Data and AI, coding and software development\nTech lover\nFeedback taker and giver\nTeam player\nKey benefits to join our team\nAttractive packages\nOffices in the 8th arrondissement near the Champs Elys\u00e9es\nFlexibility on the working hours\nRemote work possible (~40%)\n7 weeks of holidays (cong\u00e9s pay\u00e9s + RTT)\nLVMH brands exclusive private sales\nGreat employee committee and health insurance (CE, mutuelle)\nLast generation MacBooks\nPart of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!\nYou\u2019re eligible if\nYou have a strong experience (3+ years) in cloud data architecting or consultancy.\nYou graduated from an engineering (or equivalent) with a master\u2019s degree. Computer Science knowledge is mandatory.\nExperience on data stacks and/or Google Cloud (built in components) is a huge plus.\nFrench and English both written and oral (Maisons are all over the world)\nYou\u2019re thrilled to support the #1 luxury group to get even better.\nHiring Process\nCall with our HR partner dedicated to the Tech Team\nTechnical interview with the Solution Engineering Manager\nTechnical test\nInterview with the Head of Engineering\nStill here? Apply now!!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication",
                "Flexibility"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": [],
            "Salary": [
                "40"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer GCP (F/H)",
        "company": "Apside",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-gcp-f-h-at-apside-2902806697?position=52&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=CwTgaf7bJVvvYa%2B8by%2FfjQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre une entreprise apprenante ? Engag\u00e9e pour t\u2019accompagner dans ton \u00e9volution professionnelle et dans tes projets personnels ?\nRejoins Apside pour travailler sur les projets de demain !\nLe poste ?\nPour le compte de notre\nclient acteur mondial de la beaut\u00e9 et cosm\u00e9tique,\ntu interviendras dans la\ntransformation d\u2019un projet worlwide,\no\u00f9 tu devras\nd\u00e9velopper la Data Platform et l'ensemble des services Data qui seront expos\u00e9s aux diff\u00e9rentes \u00e9quipes du client. Aussi, tu seras amen\u00e9 \u00e0 d\u00e9velopper des use cases data.\nDans ce sens, tes missions seront les suivantes :\nDesigner l'architecture et d\u00e9velopper la solution\nD\u00e9finir et d\u00e9velopper les Data Model\n\u00catre garant de la qualit\u00e9 du code\n\u00catre DevOps (Utilisation/mise en place de chaine CI/CD et support niveau L3 des d\u00e9veloppements)\nEnvironnement technique :\nGCP (BigQuery, Cloud Run, Cloud Build)\nSQL\nPython\nDevOps (Github)\nAPI Development\nTerraform\nM\u00e9thodologie Agile\nToi ?\nTu as d\u00e9j\u00e0 travaill\u00e9 sur\nGoogle Cloud Platform (GCP)\n?\nTu es\nautonome\n,\nrigoureux\n, et\nbon communiquant\n?\nTu souhaites participer \u00e0 un\nprojet d\u2019envergure associant cloud et Big Data\n?\nEt la suite ?\nTu rencontres d\u2019abord l\u2019\u00e9quipe RH pour parler de tes attentes, ton projet, ton futur !\nPuis les managers pour parler concret : missions, projets, parcours de carri\u00e8re, et bien s\u00fbr salaire et avantages J\nEt tu discutes avec un de nos Tech Leads, pour \u00e9valuer tes comp\u00e9tences/ te challenger.\nLes infos en plus !\nT\u00e9l\u00e9travail ! \ud83d\ude0a\nUn salaire attractif en fonction de ton exp\u00e9rience + diff\u00e9rents avantages\nUn groupe en pleine croissance avec un management bienveillant\nEt une \u00e9volution personnalis\u00e9e avec la possibilit\u00e9 de se former via une plateforme interne\nTu souhaites donner un nouvel \u00e9lan \u00e0 ta carri\u00e8re ? Rejoins la vie Apsidienne !\nPour en savoir plus \u00e0\nwww.apside.com\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Big Data",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [
                "Salaire"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer / Developpeur Talend",
        "company": "Siderlog Conseil",
        "location": "Niort, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-developpeur-talend-at-siderlog-conseil-3861714639?position=53&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=IHXQMJ2Ij4wKcXuVkn7anQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Siderlog est un cabinet de conseil sp\u00e9cialis\u00e9 implant\u00e9 \u00e0 Niort depuis 2004 qui accompagne les directions m\u00e9tiers et SI sur des projets de:\n- Business et Data Analyse\n- Management de projets\n- Conduite du changement\nPour soutenir notre croissance, nous pr\u00e9voyons \u00e0 Niort le recrutement de 20 consultants d'ici 2025.\nNos consultants b\u00e9n\u00e9ficient d'un mod\u00e8le qui favorise l'\u00e9panouissement professionnel et le bien \u00eatre:\n\ud83c\udf43Un processus d'int\u00e9gration sp\u00e9cifique et un suivi r\u00e9gulier\n\ud83c\udf43Une \u00e9coute active des attentes, notamment en terme de formations, certifications\n\ud83c\udf43Des d\u00e9jeuners et \u00e9v\u00e8nements mensuels\n\ud83c\udf43Un management et un accompagnement de proximit\u00e9\n\ud83c\udf43Un package salarial attractif\n\ud83c\udf43La possibilit\u00e9 de contribuer aux projets d'entreprise ( RSE, communaut\u00e9s m\u00e9tiers, p\u00f4le conseil et expertise)\n\ud83c\udf43Entreprise labellis\u00e9e Happy At Work, charte T\u00e9l\u00e9travail...\n\ud83c\udf43De nombreux autres avantages que nous vous invitons \u00e0 venir d\u00e9couvrir\nSiderlog recherche pour renforcer son \u00e9quipe, \u00e0 Niort un(e) consultant(e) Data Engineer / Developpeur Talend.\nDans ce cadre vous devrez :\n\u2714\ufe0fConcevoir et d\u00e9velopper des traitements/job de donn\u00e9es complexes \u00e0 l'aide de Talend pour l'ingestion, le nettoyage, la transformation et la distribution des donn\u00e9es.\n\u2714\ufe0fCollaborer \u00e9troitement avec les \u00e9quipes m\u00e9tier pour comprendre les besoins en mati\u00e8re de donn\u00e9es et concevoir des solutions adapt\u00e9es.\n\u2714\ufe0fMettre en \u0153uvre des bonnes pratiques de d\u00e9veloppement ETL, y compris la documentation, les tests unitaires et l'int\u00e9gration continue.\n\u2714\ufe0fAssurer la surveillance et la maintenance des traitements/job de donn\u00e9es en production, en r\u00e9solvant les incidents et en effectuant des mises \u00e0 jour si n\u00e9cessaire.\n\ud83d\udccb Qualifications et comp\u00e9tences :\n\ud83d\udc49Exp\u00e9rience av\u00e9r\u00e9e dans le d\u00e9veloppement de solutions de gestion et d'int\u00e9gration de donn\u00e9es, sur Talend.\n\ud83d\udc49Ma\u00eetrise des langages de requ\u00eate SQL pour l'extraction et la manipulation des donn\u00e9es.\n\ud83d\udc49Connaissance approfondie des bases de donn\u00e9es relationnelles et des entrep\u00f4ts de donn\u00e9es.\n\ud83d\udc49Comp\u00e9tences en programmation avec Java, Python ou d'autres langages similaires.\n\ud83d\udc49Capacit\u00e9 \u00e0 travailler de mani\u00e8re autonome tout en collaborant efficacement avec les membres de l'\u00e9quipe.\n\ud83d\udc49Excellentes comp\u00e9tences en communication \u00e9crite et verbale.\n\ud83d\udc49Maitrise de l'outil ETL Talend.\n\ud83d\udc49Exp\u00e9rience avec d'autres outils d'int\u00e9gration de donn\u00e9es tels que Informatica, BODS, Alt\u00e9ryx.\n\ud83d\udc49Certification Talend serait un plus.\n\ud83d\udc49Exp\u00e9rience dans le domaine de l'assurance souhait\u00e9e\nCette offre vous int\u00e9resse ! Postulez !\n\ud83c\udfc6\ud83d\ude4f\ud83d\ude80\ud83c\udf89 !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [
                "Package"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Full Stack Data Engineer",
        "company": "bsport",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/full-stack-data-engineer-at-bsport-3848363421?position=54&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=zVntVhMlT7QiaTnDEAUXjA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Do you know about bsport?\nWe are a Barcelona based company that offers a platform combining boutique fitness and advanced technology. Our all-in-one features cover bookings, payroll, marketing and more, helping our partners streamline operations and boost their commercial success.\nWe have more than 2\u2019000 clients in 40+ countries and continue to expand rapidly.\nWe provide our partners with:\nOur platform - the heart of the system (B2B)\nA white label iOS and Android mobile application (B2C)\nAn integrated Video on Demand tool\nOur self-built Smart Marketing Suite\nA Webshop to up- and cross-sell different products\nOur first successes\nSince we launched in 2019, we have already achieved the following:\nWe\u2019ve built a community of over 6 million users\nFinalised a Series A Fundraising of $4+ million in December 2022\nGrown our team to more than 150 employees\nWe\u2019re continuing to grow our team to become the #1 tech partner for boutique studios in Europe and the rest of the world!\nWe are seeking a talented and experienced Full Stack Data Engineer to join our dynamic tech team based in Paris. As a Full Stack Data Engineer, you will play a crucial role in enhancing our data platform and driving innovation through data engineering solutions.\nWhat your future position looks like:\nThe primary focus of this position is data engineering, encompassing tasks such as building, optimizing, and maintaining our data platform. You will be responsible for making continuous improvements to our data infrastructure to ensure its reliability, scalability, and performance. As an integral part of the data team, you will collaborate closely with team members to address various data-related challenges and opportunities. This may involve tasks ranging from designing and implementing data pipelines to conducting in-depth data analysis to extract actionable insights.\nThe role will focus on:\nBuild and maintain bsport\u2019s data architecture\nEnsure the sustainability and scalability of the diverse components by leveraging bsport's cloud provider services and adhering to all DevOps best practices\nYou will enjoy working within an internal team of 25 people, consisting of Tech, Product, and Data experts, directly surrounded by our Senior Leads and CTO.\nOur stack is fully automated with push-to-deploy on both frontend and backend. We use Kubernetes and AWS, and our CI is self-hosted. Our methodology is based on agile principles, with weekly releases to production and staging to iterate, gather feedback, and drive progress.\nYou will be a good fit to join us if you:\nAlready built or maintained a data architecture at scale in a top cloud provider (AWS, GCP or Snowflake)\nAlready deployed data science models in production or built a data ingestion pipeline\nFamiliar with DevOps best practices\nProficiency in SQL, Python and Spark\nExperience with dbt and airbyte\nQualifications\nBachelor\u2019s degree in Data Science, Computer Science, Engineering, or related field; advanced degree preferred.\nRelevant experience in data engineering\nStrong analytical and problem-solving skills, with the ability to work independently and in a team environment.\nWe'd love to have you join us for many reasons, such as:\n\ud83c\udf0d A multicultural and international team!\n\ud83d\ude80 A stimulating SaaS environment within a supportive and a fast-growing company\n\ud83d\udd0b Enjoy 25 days of paid leave to recharge\n\ud83c\udfe1 Embrace days of remote work\n\ud83c\udfe2 Work from our stunning office in the heart of Bastille\n\u2764\ufe0f\u200d\ud83e\ude79\nHealth insurance half covered\n\ud83d\udef5 Public Transportation half covered\n\ud83c\udfc4\ud83c\udffd Take part in bsport team building and sport initiatives\n\ud83d\udecc\ud83c\udffd Supported by bsport on sick days\nInterview Process\nFirst interview with one of our Talent Acquisition team members (30 min)\nTechnical Interview with our Lead Data (1h30)\nTechnical Interview with our CPO (1h)\nFinal Interview with our CTO (1h)\nAbout our Company Culture:\nAt bsport, we collaborate with passionate individuals who value diverse ideas and backgrounds. We believe that diversity is our most valuable asset.\nOur commitment is to foster a positive and inclusive culture. We achieve this through team-building initiatives, open communication, professional growth opportunities, and by celebrating diversity in all its forms.\nWe value and respect every individual who is eager to make a difference, empowering them to contribute their unique skills and perspectives. Join our dedicated team to help create a thriving and welcoming workplace.\nJoin our team of passionate and committed people who are dedicated to creating a thriving and welcoming environment. Let's make it happen together!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full",
                "Senior"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer H/F",
        "company": "Ippon Technologies",
        "location": "Greater Nantes Metropolitan Area",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=55&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=mHG%2FOUZA7Y3wljivPZOIeQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre la communaut\u00e9 DATA la plus dynamique de France ?\nNotre sp\u00e9cialit\u00e9 est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.\nMembre de la Practice Data, le/la futur(e) Data Engineer sera int\u00e9gr\u00e9(e) \u00e0 nos \u00e9quipes de conseil et sera suivi(e) par un(e) mentor qui l\u2019aidera \u00e0 monter en comp\u00e9tences.\nVotre champs d\u2019expertise :\nIntervenir sur les data platforms de nos clients pour d\u00e9velopper de nouveaux pipelines de donn\u00e9es (ingestion, traitement, exposition).\nTravailler en collaboration avec les m\u00e9tiers et les data scientists pour leur fournir un support \u00e0 l\u2019industrialisation de leurs travaux (tests, int\u00e9gration continue, scalabilit\u00e9 des mod\u00e8les, craftsmanship etc\u2026)\nD\u00e9ployer des infrastructures cloud full\ninfra-as-code\n(Terraform, CloudFormation).\nParticiper aux \u00e9v\u00e8nements internes \u00e0 la communaut\u00e9 data (BBL, webinar, datap\u00e9ro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).\nCapitaliser sur les missions et les diff\u00e9rents \u00e9v\u00e8nements de la communaut\u00e9 au travers d\u2019articles de blogs, REX, BBL interne.\nVos connaissances :\nUn framework de calcul distribu\u00e9 tel que Spark, Storm, Flink.\nUn ou plusieurs langages de programmation (Python, Scala, Java...)\nDiff\u00e9rents syst\u00e8mes de stockage de donn\u00e9es (SQL ou NoSQL) et bien s\u00fbr le langage SQL.\nLa connaissance de Snowflake est bienvenue ;-)\nUn framework de streaming de donn\u00e9es tel que Kafka ou Amazon Kinesis.\nUne exp\u00e9rience sur les technologies Cloud : AWS, GCP, Azure\nLe delivery et les projets en production faisant partie de notre ADN, vous \u00eates capable de livrer du code de qualit\u00e9 dans des environnements agiles.\nDe plus en plus de nos projets se font en remote avec des clients du monde entier, il devient n\u00e9cessaire d\u2019\u00eatre \u00e0 l\u2019aise en Anglais.\nIppon technologies c\u2019est aussi :\n\ud83d\udc4d B\u00e9n\u00e9ficier d'un suivi de proximit\u00e9 r\u00e9alis\u00e9 par votre manager technique : points r\u00e9guliers pour votre suivi en mission, votre formation et votre \u00e9volution de carri\u00e8re\n\u270c\ufe0f Rejoindre une entreprise o\u00f9 les valeurs du sport sont nos leitmotiv : d\u00e9passement de soi, travail en \u00e9quipe, bienveillance.\n\ud83d\uddd2\ufe0f Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL\n\ud83d\ude01 Travailler en pair programming ou avec un.e mentor pour gravir les \u00e9chelons !\n\ud83d\udcaa Pouvoir participer \u00e0 une aventure humaine au sein de notre Fondation Ippon pour r\u00e9duire la fracture num\u00e9rique dans le monde !\n\ud83e\udd1d Participer \u00e0 nos ap\u00e9ros et divers \u00e9v\u00e8nements internes pour consolider la coh\u00e9sion d\u2019\u00e9quipe\nEt apr\u00e8s ?\nEt oui alors ? Que se passe-t-il une fois que vous \u00eates convaincu d\u2019avoir lu l\u2019offre d\u2019emploi qui vous correspond bien ?\nNous vous proposons de prendre contact et de nous rencontrer !\nLes Next Steps :\n1 call RH\n1 \u00e9change RH\n1 \u00e9change Technique\nSi le match est bon des deux c\u00f4t\u00e9s : Hadjim\u00e9 ! Vous vous lancerez sur le tatami Ippon !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark",
                "Databricks",
                "Flink"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [
                "Terraform",
                "CloudFormation"
            ],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Equativ",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-equativ-3814251519?position=56&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=d9nf6mGNenymXJjW70Bn6g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83d\udc6b About the team\nAt Equativ, we\u2019re on a mission to develop advertising technologies that empower our customers to reach their digital business goals. This means that we rely on massively scalable, widely distributed, highly available, and efficient software systems; the platform deals with over 100 billions requests per day and above 40 Gbps of network traffic.\nOur innovation team based in Paris, Nantes, Limoges, Krak\u00f3w and Berlin is composed of 90 straightforward and energetic engineers working in an Agile environment and ready to tackle the most complex technical challenges.\nOur data engineering team is composed of 10 skilled engineers and is based in Paris. We are part of the R&D department which is composed of 140+ engineers spread across Paris, Nantes, Limoges, Krak\u00f3w and Berlin all working in an Agile environment and ready to tackle the most complex technical challenges.\nOur mission \ud83d\udc47\nData Engineering team is central to Equativ\u2019s data centric business and is responsible to ingest, transform, model and redistribute all data coming from our adtech platform.\nWe aim at building scalable and robust Big Data platforms from ingestion to business actionable consumption. Our Big Data ecosystem must handle massive log ingestion (tens of billions per day), short & long term data storage, complex data modelling, real-time and batch ELT as well as providing external access through dedicated APIs.\nData Engineers serve Equativ data directly to our customers and throughout the company whether it is for BI analysis, data science algorithms (clustering and optimization), customer reporting, invoicing and more.\nEquativ Data Engineering team is engaged in an ambitious migration of its main data stack (Hadoop on-premise) to GCP with the objective to increase reporting features, lower maintenance time, improve performances and simplify the access to our raw data.\nWhat you'll do \u270f\ufe0f\nAs a Big Data Engineer, you\u2019ll primarily focus on maintaining and enhancing the operationality of our on-premise and cloud data pipelines which feed our warehouses and APIs\n-Design, develop, test, promote and industrialize all data components from data ingestion to datawarehouse delivery (ClickHouse, BigQuery):\nPropose and develop innovative solutions to achieve the best levels of scalability and performance for our Big Data engines\nAutomate and streamline our real-time and batch data pipelines (on-premise and in the cloud) in order to simplify access to our data by other teams and lower amount of work spent by other teams on ETL processes\nPerform end-to-end monitoring to ensure high availability of production data processing, data quality and reliability\nApply best in class Devops guidelines and secure deployments\n-Brainstorm with other team members working on our data backend (datawarehouse modelling and data exposure through our reporting APIs) on optimizing our architecture and support them in the use of our pipelines\n-Contribute to data roadmap definition in coordination with other R&D and product teams in order to build a best in class data infrastructure that will generate insights for Equativ\u2019s analytics\n-Take part in improving and deploy data engineering standards, procedures, processes and operational guidelines around target data components at Equativ\n\ud83d\udcaa About you\nMaster degree in Computer Science or similar technical field of study\n3+ years of software development with open source technologies\nFluent in Java and/or in Scala. SQL mastery\nVery good understanding of Devops principles (Gitlab, Docker, Kubernetes, Gradle, ci/cd)\nExperience with large-scale data engineering technologies (ClickHouse, Flink, Kafka, Hadoop, Spark, Hbase)\nExperience on building data pipelines on Google Cloud Platform (BigQuery, Dataflow, GCS, Cloud Run, Airflow \u2026) would be a big plus\nExperience in working with high QPS Rest APIs is a plus\nEntrepreneurial spirit and know-how to identify opportunities of improvement\nWorking proficiency and communication skills in verbal and written English\nPassion for playing with large volume of data\n\ud83d\ude80 How you'll grow\nWithin 1 month:\nYou'll be just finishing your onboarding.\nYou'll probably have tackled a few small tasks in peer-coding\nWithin 4 months:\nYou'll have an overview of 50% of the stack, CI/CD and team\u2019s main processes. You\u2019ll be able to work on more complex developments\nYou'll now have enough knowledge to participate to deployments of chosen applications\nWithin 9 months:\nYou'll be autonomous on most of our stack and will have participated to major projects\nYou\u2019ll be helping the team on production matters\n\ud83d\udc4b About us\nEquativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus \u2014 four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.\nHeadquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.\nThe company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times\u2019 FT 1000: Europe\u2019s Fastest-Growing Companies.\nEquativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.\nCome and lead the charge with us in building a transparent ecosystem based on quality!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark",
                "Flink"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Kubernetes",
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "DevOps",
                "Big Data",
                "ML",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer Cloud (F/H)",
        "company": "Apside",
        "location": "\u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-cloud-f-h-at-apside-3904088503?position=57&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=I%2BZTRr6Z9jhcj14PZMK92A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83d\udca5\nD\u00e9couvrez la Vie Apsidienne\n\ud83d\udcf9\net vous aussi, devenez Apsidien\nOn aurait pu demander \u00e0 Chat GPT de vous d\u00e9montrer en quoi\nApside est l\u2019ESN qu\u2019il vous faut,\nmais on pr\u00e9f\u00e8re que vous le d\u00e9couvriez vous-m\u00eames \ud83d\udc47\ud83d\ude0f\n\ud83d\udd25\nD\u00e9couvrez votre future mission\n\ud83d\udc49\nContexte\nRejoignez notre Practise Cloud/Data, afin d\u2019intervenir sur des sujets \u00e0 haute valeur ajout\u00e9e !\nNotre\nclient migre actuellement toutes ses applications vers le cloud AWS.\nDe plus, dans le cadre du d\u00e9veloppement d'un produit de restitution automatis\u00e9e de donn\u00e9es, ils recherchent actuellement d\u00e9veloppeur data ayant d\u00e9j\u00e0 travaill\u00e9 sur un projet similaire. La solution produit est techniquement con\u00e7ue en lien avec le Tech Lead validant l'architecture logicielle \u00e0 mettre en place sur le cloud AWS.\nSecteur\n: culture/m\u00e9dia\nM\u00e9thode de travail\n: Agile Safe\n\ud83d\ude0e Mission\nCapter les donn\u00e9es (structur\u00e9es et non structur\u00e9es) produites dans les diff\u00e9rentes applications\nInt\u00e9grer les \u00e9l\u00e9ments\nStructurer la donn\u00e9e (s\u00e9mantique, etc\u2026)\nCartographier les \u00e9l\u00e9ments \u00e0 disposition\nNettoyer la donn\u00e9e (\u00e9limination des doublons, etc\u2026)\nValider la donn\u00e9e\nCr\u00e9er les r\u00e9f\u00e9rentiels de donn\u00e9es\nEnvironnement technique\n:\nPython\nLambda\nStep Function\nAWS / AWS RDS\nPostegreSQL\nSnowflake\nSpark\n\ud83d\udccd\nLocalisation\nLa D\u00e9fense\n\ud83d\udcb0\nLe package salarial que nous vous proposons\nContrat :\nCDI\nAvantages groupe :\ncarte ticket restaurant Swile, prime de mobilit\u00e9, RTT, accord t\u00e9l\u00e9travail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle \u00e0 100% etc\u2026\nAvantages agence :\nCommunaut\u00e9 Cloud/Data, afterworks, communaut\u00e9 techlead\nFormation :\ncertifications techniques, cours particuliers d\u2019anglais en interne, acc\u00e8s \u00e0 un catalogue de formations gr\u00e2ce \u00e0 notre plateforme e-learning (\nAcademy by Apside\n) ou via nos organismes partenaires.\n\ud83d\udd2e\n\u00d4 vous futur Apsidien, qui \u00eates-vous ?\nAu moins 4 ans d'exp\u00e9rience en tant que Data Engineer\nMaitrise de l\u2019environnement cloud AWS\nForce de proposition, bon relationnel et autonome\n\ud83d\ude0f\nApside a suscit\u00e9 votre curiosit\u00e9 ?\nDans un environnement marqu\u00e9 par une acc\u00e9l\u00e9ration des \u00e9volutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients \u00e0 cr\u00e9er de la valeur et \u00e0 adresser leurs enjeux strat\u00e9giques en leur mettant \u00e0 disposition des expertises technologiques (\nData / IA, Cloud, Cyber\n) et une exp\u00e9rience sectorielle (\nIndustrie, Banque, Assurance, Service, Secteur Public\n). Pour un accompagnement global, le groupe propose des offres transverses autour du\nHandicap\n(Apsid\u2019EA), du\nDigital Learning\n, et du\nConseil\n.\n\ud83e\udd14\nEt votre place dans tout \u00e7a ?\n\ud83d\udc49 Notre volont\u00e9\nest de vous accompagner dans la construction et l\u2019\u00e9panouissement de votre carri\u00e8re\nen nous appuyant notamment\nsur 3 piliers :\nUne\nr\u00e9mun\u00e9ration\n\u00e0 hauteur de vos investissements et de vos comp\u00e9tences\nUne\ntrajectoire professionnelle\nstimulante sur mesure\nUn\nengagement\nautour des valeurs Apsidiennes : la qualit\u00e9 de vie et des conditions de travail au c\u0153ur de nos enjeux\nEngag\u00e9e pour\nun monde plus inclusif et plus responsable\n, Apside r\u00e9invente l\u2019ESN et propose l\u2019Engagement Soci\u00e9tal et Num\u00e9rique. D\u00e9couvrez notre d\u00e9marche RSE ainsi que notre vision de l\u2019Entreprise Engag\u00e9e.\nConvaincu ? A vous de jouer, envoyez-nous votre CV !\nRejoignez l\u2019aventure Apsidienne et d\u00e9couvrez notre vision d\u2019une ESN singuli\u00e8re et r\u00e9siliente\n\ud83d\ude80\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [
                "100"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (F/H) - en alternance",
        "company": "Carrefour",
        "location": "Massy, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-en-alternance-at-carrefour-3884390443?position=58&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=fpp9k2zsbFGYqI5rGtDPMQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Le saviez-vous ?\nNous rejoindre, c\u2019est rejoindre l\u2019un des leaders mondiaux de la distribution qui met l'accent au quotidien sur la diversit\u00e9, la RSE et le digital, pour satisfaire nos clients et nos collaborateurs. En tant que partenaire premium des Jeux Olympiques et Paralympiques de Paris 2024, nous partageons les valeurs du sport en permettant \u00e0 nos \u00e9quipes de se d\u00e9passer et encourageons une alimentation saine au juste prix pour tous.\nVous cherchez \u00e0 travailler dans une entreprise dynamique o\u00f9 votre travail rime avec impact social et environnemental ? Bienvenue chez nous !\nData Engineer (F/H) - en alternance\nEn tant qu' alternant, vous int\u00e9grerez la plateforme supply chain o\u00f9 vous serez amen\u00e9 \u00e0 appuyer le p\u00f4le pr\u00e9vision et optimisation particuli\u00e8rement sur des sujets data et d\u2019analyse de donn\u00e9es.\nAu sein d'une \u00e9quipe compos\u00e9e de data scientists et de data engineers organis\u00e9e en mode Scrum Agile, vous travaillerez pour am\u00e9liorer au quotidien un outil de calcul de pr\u00e9vision (pr\u00e9vision de la demande des entrep\u00f4ts Carrefour). Vous participez \u00e0 l'\u00e9volution fonctionnelle et technique de l'application.\n\ud83c\udfaf Les missions\nDans ce cadre, vous serez amen\u00e9 \u00e0\nExplorer et analyser les donn\u00e9es du datalake carrefour\nParticiper au cadrage des nouvelles fonctionnalit\u00e9s\nD\u00e9velopper les \u00e9volutions des traitements, des mod\u00e8les statistiques et de machine learning de pr\u00e9vision et des reporting\nTester les fonctionnalit\u00e9s d\u00e9velopp\u00e9es\nR\u00e9pondre aux demandes utilisateurs\n\ud83d\udc65 Profil\nVous \u00eates en \u00e9cole d\u2019ing\u00e9nieur, en master 2 ou \u00e9quivalent avec une sp\u00e9cialisation data science, data engineering, statistique, informatique.\nVous avez une exp\u00e9rience en traitement et analyse de donn\u00e9es.\nVous avez un esprit d\u2019analyse et la capacit\u00e9 de travailler en \u00e9quipe et \u00e0 distance.\nVous \u00eates autonome et rigoureux, fluide dans votre communication orale et \u00e9crite et \u00e0 l'\u00e9coute des besoins de vos interlocuteurs.\nVous \u00eates reconnu pour vos capacit\u00e9s d'anticipation, votre sens de l'initiative et votre r\u00e9activit\u00e9.\nVous avez une bonne connaissance des langages suivants\nSQL\nPython\nUne connaissance de GCP et de Big query serait un plus.\nUne connaissance m\u00eame th\u00e9orique de la m\u00e9thodologie agile serait un plus\nUne connaissance de GIT serait un plus.\nEncore plus de bonnes raisons de nous rejoindre\nInt\u00e9grer une \u00e9quipe conviviale \u00e0 taille humaine au sein d\u2018un grand groupe\n12 % de remise sur achat\n\ud83d\udcdd Informations compl\u00e9mentaires\nDate de d\u00e9but  09 septembre 2024\nDur\u00e9e  1 an\nLieu  Lyon\nD\u00e9placements en magasin et en concurrence dans la r\u00e9gion parisienne\nAvantages 50 % du titre de transport pris en charge par Carrefour\nEnvie de rejoindre l\u2019aventure ?\nChez Carrefour, nous avons \u00e0 c\u0153ur de ne passer \u00e0 c\u00f4t\u00e9 d\u2019aucun talent et sommes fiers de compter des \u00e9quipes repr\u00e9sentatives de la soci\u00e9t\u00e9 dans son ensemble. Nous encourageons ainsi tous types de profils \u00e0 postuler \u00e0 cette offre et garantissons un processus de recrutement d\u00e9nu\u00e9 de toutes formes de discriminations.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [
                "Statistiques"
            ],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Big Query"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Machine Learning",
                "Statistiques"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer - Nantes",
        "company": "Capgemini",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=59&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=FOcexJ8yPYHe2I%2F%2FMuM8YA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini\nChoisir Capgemini, c'est choisir une entreprise o\u00f9 vous serez en mesure de fa\u00e7onner votre carri\u00e8re selon vos aspirations, o\u00f9 vous serez soutenu et inspir\u00e9 par une communaut\u00e9 d\u2019experts dans le monde entier, o\u00f9 vous pourrez r\u00e9\u00e9crire votre futur. Rejoignez-nous pour red\u00e9finir les limites de ce qui est possible, contribuer \u00e0 lib\u00e9rer la valeur de la technologie pour les plus grandes organisations et participez \u00e0 la construction d\u2019un monde plus durable et inclusif.\nVos missions :\nInt\u00e9gr\u00e9(e) au sein d'une \u00e9quipe projets intervenant pour des clients dans des secteurs d'activit\u00e9s vari\u00e9es, vous serez notamment en charge des missions suivantes :\nConcevoir et mettre en oeuvre des strat\u00e9gies s\u00e9curis\u00e9es d'acquisition et d'int\u00e9gration de donn\u00e9es,\nConfigurer des r\u00e9f\u00e9rentiels de donn\u00e9es \u00e0 la pointe de la technologie dans des environnements distribu\u00e9s, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),\nConstruire des pipelines de donn\u00e9es pour collecter, transformer et traiter des donn\u00e9es en collaboration avec des scientifiques de donn\u00e9es afin de r\u00e9pondre aux exigences de la mod\u00e9lisation de donn\u00e9es d'analyse avanc\u00e9e.\nVotre profil :\nDipl\u00f4me d\u2019ing\u00e9nieur ou \u00e9quivalent universitaire\nMinimum 3 ans d'exp\u00e9rience\nAnglais courant\nMa\u00eetrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.\nExpertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks\u2026\n3 raisons de nous rejoindre :\nQualit\u00e9 de vie au travail : accord de t\u00e9l\u00e9travail en France et \u00e0 l\u2019international, accord sur l\u2019\u00e9galit\u00e9\nprofessionnelle, la parentalit\u00e9, l\u2019\u00e9quilibre des temps et la mobilit\u00e9 durable.\nApprentissage en continu : certifications et formations en libre acc\u00e8s, accompagnement sur mesure avec votre carreer manager, parcours d\u2019int\u00e9gration sur 9 mois.\nAvantages groupe & CSE : plan actionnariat, activit\u00e9s \u00e0 tarifs pr\u00e9f\u00e9rentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorit\u00e9s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int\u00e9grez un collectif qui valorise la diversit\u00e9, d\u00e9veloppe le potentiel de ses talents, s\u2019engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r\u00e9duire son impact environnemental sur tous ses sites et aupr\u00e8s de ses clients.\nCapgemini\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr\u00e8s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d\u2019exp\u00e9rience, nous sommes un partenaire strat\u00e9gique des entreprises pour la transformation de leurs activit\u00e9s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp\u00e9tuelle \u00e9volution tels que le cloud, la data, l\u2019Intelligence Artificielle, la connectivit\u00e9, les logiciels, l\u2019ing\u00e9nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration",
                "Organisation"
            ],
            "EnSoftSkils": [
                "Collaboration",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (Snowflake)",
        "company": "MindPal",
        "location": "Marseille, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896997028?position=60&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=mVauwVbbjFSlaeW9klBsag%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "DATA Engineer (H/F)",
        "company": "Boulanger",
        "location": "Lesquin, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-boulanger-3854554057?position=1&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=Av0nle7xMrq%2F2medaWCbng%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Au sein de la direction informatique, le p\u00f4le DATA a pour missions de maximiser la mise en valeur des donn\u00e9es de BOULANGER ,ELECTRO-DEPOT et KREFEL/HIFI afin d\u2019aider nos d\u00e9cideurs \u00e0 agir sur les leviers de leur performance par des processus d\u00e9cisionnels efficients.\nAu sein de ce p\u00f4le, tu prendras en charge un large domaine m\u00e9tier qu'il te faudra maitriser de bout en bout : de la donn\u00e9es brutes, sa transformation jusqu'\u00e0 son exposition dans les reporting. Cela afin d'en assurer le bon fonctionnement, les \u00e9volutions constantes et sa p\u00e9rennit\u00e9\nTes t\u00e2ches principales portent sur :\nLe pilotage et la mise en \u0153uvre de projets DATA.\nLa collecte, le stockage et l\u2019exploitation fluides des donn\u00e9es par le d\u00e9veloppement de solutions\nMissions\nMaitriser les r\u00e8gles fonctionnelles et les KPI de ton domaine afin de challenger les m\u00e9tiers dans les \u00e9volutions et les nouveaux projets\nAccompagner des \u00e9quipes m\u00e9tiers dans leurs travaux d\u2019identification et expression des besoins sur la data\nParticiper aux ateliers de conception et d\u00e9veloppement des applications data\nMod\u00e9liser la solution \u00e0 mettre en \u0153uvre\nConcevoir et mettre (ou faire mettre) en \u0153uvre des flux les pipelines d\u2019int\u00e9gration (en mode batch ou fil de l'eau) de donn\u00e9es structur\u00e9es/semi-structur\u00e9es\nTransformer les donn\u00e9es : consolider, enrichir et optimiser les donn\u00e9es, qui seront exploit\u00e9es par le m\u00e9tier\nCr\u00e9er, faire \u00e9voluer et optimiser les restitutions\nSuivre et animer les d\u00e9veloppeurs (ETL, restitution, self-BI internes ou externes)\nG\u00e9rer le RUN\nMaitrise le SQL et la base de donn\u00e9es (Oracle, Snowflake)\nMa\u00eetrise d\u2019outils de restitution (tel que Business Object (BO), PowerBI\u2026)\nCapacit\u00e9 relationnelle, rigueur et dynamisme\nMa\u00eetrise un ou plusieurs outils de pr\u00e9paration et traitement de la donn\u00e9e (DataStage, Stambia, ...)\nCapacit\u00e9 \u00e0 s\u2019adapter \u00e0 tout type d\u2019interlocuteurs (technique, m\u00e9tiers, Direction)\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Oracle",
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer H/F",
        "company": "Neosoft",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-neosoft-3877878521?position=2&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=7ObkquXOFViiSrYhqZHL9w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Tous nos postes sont ouverts au t\u00e9l\u00e9travail\nGroupe ind\u00e9pendant de conseil en transformation digitale de pr\u00e8s de 1800 collaborateurs, N\u00e9osoft s\u2019est construit, depuis 2005, sur un mod\u00e8le qui place l\u2019excellence, le d\u00e9passement de soi et la RSE au c\u0153ur de sa strat\u00e9gie.\nEn nous rejoignant, vous int\u00e9grez des communaut\u00e9s d\u2019experts et de talents qui vous permettent de d\u00e9velopper vos comp\u00e9tences et d\u2019offrir \u00e0 nos clients le meilleur accompagnement possible.\nNotre savoir-faire s\u2019articule autour de nos 6 domaines d\u2019expertise :\nConseil & Agilit\u00e9\nCybers\u00e9curit\u00e9\nData\nDevOps\nInfrastructures & Cloud\nSoftware Engineering\nNous recherchons pour int\u00e9grer notre\nagence lilloise\nun(e)\nData Engineer confirm\u00e9(e)\n.\nNous aimerions vous voir rayonner au sein de notre communaut\u00e9 DATA (+100 collaborateurs) anim\u00e9e par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients \u00e0 consolider un patrimoine Data responsable.\n\ud83c\udfaf\nVos missions :\nApr\u00e8s une p\u00e9riode d\u2019int\u00e9gration, en tant que\nData Engineer\n, voici \u00e0 quoi ressembleront vos activit\u00e9s dans des contextes clients Retail ou Banque / Assurance / Finance :\nAnalyser et s'approprier les cas d'usages\nAnalyser et valoriser les donn\u00e9es du patrimoine\nMettre en place des flux de transformation de donn\u00e9es\nR\u00e9aliser les tests permettant de s'assurer la qualit\u00e9 du delivery\nContinuer la mise au point de frameworks data\nCr\u00e9er et d\u00e9velopper des modules de d\u00e9ploiement des solutions\nAssurer l'industrialisation de moteurs bas\u00e9s sur l'IA\nAssurer le niveau de performance des pipelines\nImpl\u00e9menter les outils de monitoring du socles de donn\u00e9es\n\ud83d\udcdd\nVotre profil :\nNous vous imaginons avec au moins 4 ans d\u2019exp\u00e9riences sur des projets autour de la\nData\n, une ma\u00eetrise des\nbases de donn\u00e9es (SQL)\n, des outils de transformation de la donn\u00e9e\n(Talend, BigQuery, Airflow)\n, et un socle de comp\u00e9tences solides autours des langages\nPython, Spark, Scala, Hadoop, Java.\n\ud83d\udc49\nVotre carri\u00e8re chez N\u00e9osoft\nDepuis sa cr\u00e9ation, N\u00e9osoft place ses collaborateurs au c\u0153ur de sa strat\u00e9gie. Notre culture pourrait se r\u00e9sumer en un mot : le collectif.\nNos communaut\u00e9s d\u2019experts vous donnent la possibilit\u00e9 d\u2019apprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.\nNous veillons \u00e0 ce que chacun b\u00e9n\u00e9ficie d\u2019un accompagnement de proximit\u00e9 et d\u2019un suivi de carri\u00e8re personnalis\u00e9 aupr\u00e8s de votre manager d\u00e9di\u00e9 :\n1 bilan d\u2019activit\u00e9 trimestriel pour suivre le d\u00e9veloppement de vos comp\u00e9tences\n1 entretien d\u2019\u00e9valuation qui a lieu chaque ann\u00e9e pour \u00e9valuer votre performance et d\u00e9terminer vos nouveaux objectifs\n1 entretien annuel aupr\u00e8s de votre RH dans le but de cartographier vos nouvelles comp\u00e9tences pour \u00e9changer sur vos projets professionnels et souhaits de formations\n\ud83d\udc49\nVos avantages\nFormations et d\u00e9veloppement de l\u2019expertise :\nVous disposez de temps allou\u00e9 et r\u00e9mun\u00e9r\u00e9 en contribuant au d\u00e9veloppement de votre expertise technique et de celle du groupe (Participations \u00e0 des Tech days, animation d\u2019une conf\u00e9rence \u00e0 l\u2019interne ou \u00e0 l\u2019externe, r\u00e9daction d\u2019articles, rencontres avec nos candidats en processus de recrutement\u2026)\nUn abonnement illimit\u00e9 LinkedIn Learning offert\nBien-\u00eatre au travail :\nUn accord de t\u00e9l\u00e9travail flexible jusqu\u2019\u00e0 100% de t\u00e9l\u00e9travail et personnalisable\nUn partenariat avec Gymlib qui favorise le sport en entreprise\nDes initiatives locales (afterworks, d\u00e9fis sportifs, team buildings, \u2026)\nEt bien plus encore :\nParce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive r\u00e9mun\u00e9r\u00e9e d\u00e8s l\u2019arriv\u00e9e du collaborateur\nEn plus de votre salaire : participation, compte \u00e9pargne temps, actionnariat...\n\ud83d\udc49\nVotre parcours candidat\nNotre processus de recrutement se compose de deux \u00e9tapes cl\u00e9s :\nUn entretien de recrutement RH avec un Talent Acquisition Sp\u00e9cialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carri\u00e8re possibles au sein de notre groupe\nUn entretien d\u2019\u00e9valuation technique pour r\u00e9aliser un diagnostic de vos comp\u00e9tences techniques et identifier les comp\u00e9tences sur lesquels poursuivre votre \u00e9volution\nVous aurez \u00e9galement la possibilit\u00e9 de rencontrer pour compl\u00e9ter votre processus un acteur de notre p\u00f4le Business ou un pair de votre m\u00e9tier pour \u00e9changer sur son exp\u00e9rience collaborateur.\nNous avons h\u00e2te de vous rencontrer !\nA bient\u00f4t,\nL\u2019\u00e9quipe N\u00e9osoft \ud83d\udd90\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Confirm\u00e9"
            ],
            "TypeContract": [],
            "Salary": [
                "Salaire"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer - Bordeaux",
        "company": "Capgemini",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bordeaux-at-capgemini-3889788624?position=3&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=pSrQIJJFQcDAjttBdi%2F%2FHw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise o\u00f9 vous serez en mesure de fa\u00e7onner votre carri\u00e8re selon vos aspirations. Avec le soutien et l'inspiration d'une\ncommunaut\u00e9 d\u2019experts dans le monde entier, vous pourrez r\u00e9\u00e9crire votre futur. Rejoignez-nous pour red\u00e9finir les limites de ce qui est possible, contribuer \u00e0 lib\u00e9rer la\nvaleur de la technologie pour les plus grandes organisations et participer \u00e0 la construction d\u2019un monde plus durable et inclusif.\nVos missions :\nVous \u00eates passionn\u00e9 par le domaine de la Data, vous souhaitez prendre part \u00e0 des projets d'envergure, concevoir des solutions, les impl\u00e9menter et les faire \u00e9voluer?\nAlors rejoignez notre \u00e9quipe Data Engineering Services au sein de Capgemini Cloud Infrastructure Services en tant que Data Engineer.\nVous avez acquis une exp\u00e9rience solide dans le d\u00e9veloppement, la mise en \u0153uvre et l\u2019optimisation de solutions pour le traitement d'un grand volume de donn\u00e9es, vous \u00eates capable de cr\u00e9er des solutions qui r\u00e9pondent aux besoins m\u00e9tiers et IT, alors rejoignez notre \u00e9quipe d\u2019experts.\nEn qualit\u00e9 de Data engineer, vos missions sont les suivantes :\n\u25aa Concevoir et d\u00e9velopper des solutions Data/IA.\n\u25aa Accompagner les M\u00e9tier dans la compr\u00e9hension et la mise en \u0153uvre de solution orient\u00e9es donn\u00e9es.\n\u25aa Collaborer avec les Dev, les Ops, les experts infrastructures dans la construction de solutions et d\u2019infrastructures ax\u00e9es sur les donn\u00e9es.\n\u25aa G\u00e9rer un \u00e9cosyst\u00e8me de partenaires data et assurer un haut niveau d'expertise\n\u25aa Assurer un r\u00f4le de veille technologique sur tous les outils autours de la data, de l\u2019IA et de la BI.\nVotre profil :\nVous \u00eates issu d\u2019une formation ing\u00e9nieur ou \u00e9quivalent bac+5 informatique sp\u00e9cialis\u00e9e en DATA et vous justifiez d\u2019une exp\u00e9rience de 3 \u00e0 5 ans dans un r\u00f4le similaire. Expert dans une technologie de base de donn\u00e9es relationnelle (PostgreSQL, Oracle...)\nExpert dans une technologie de base NoSQL (MongoDB, Cassandra...)\nVous maitrisez un framework de manipulation de donn\u00e9es (Hadoop, Spark, Kafka...)\nVous maitrisez les concepts DevOps et avez de bonnes notions en scripting et d\u00e9veloppement\nVous avez une exp\u00e9rience des outils BI et de data visualisation (Kibana, PowerBI...)\nLa maitrise de l'anglais est n\u00e9cessaire.\n3 raisons de nous rejoindre :\nQualit\u00e9 de vie au travail :\naccord de t\u00e9l\u00e9travail en France et \u00e0 l\u2019international, accord sur l\u2019\u00e9galit\u00e9 professionnelle, la parentalit\u00e9, l\u2019\u00e9quilibre des temps et la mobilit\u00e9 durable.\nApprentissage en continu :\ncertifications et formations en libre acc\u00e8s, accompagnement sur mesure avec votre career manager, parcours d\u2019int\u00e9gration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, tarif pr\u00e9f\u00e9rentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culture\nNos engagements et priorit\u00e9s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int\u00e9grez un collectif qui valorise la diversit\u00e9, d\u00e9veloppe le potentiel de ses talents, s\u2019engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r\u00e9duire son impact environnemental sur tous ses sites et aupr\u00e8s de ses clients.\n\u00c0 propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr\u00e8s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d\u2019exp\u00e9rience, nous sommes un partenaire strat\u00e9gique des entreprises pour la transformation de leurs activit\u00e9s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp\u00e9tuelle \u00e9volution tels que le cloud, la data, l\u2019Intelligence Artificielle, la connectivit\u00e9, les logiciels, l\u2019ing\u00e9nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL",
                "Cassandra"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "PostgreSQL",
                "Oracle"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "eXalt Value",
        "location": "\u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=4&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=aJYgYrY%2FQ2YWFPS1t2rvFA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "eXalt\nest un cabinet de conseil IT\nPure player Data\n& IA bas\u00e9 \u00e0 Paris.\nNotre offre s\u2019articule autour de 4 piliers r\u00e9unis au sein d\u2019une m\u00eame communaut\u00e9 pour un accompagnement \u00e0 360\u00b0 alliant une expertise technique et m\u00e9thodologique \u00e0 une approche conseil m\u00e9tier:\nData Gouvernance & Project\nData Engineering & Big Data\nData Performance & Analytics\nData Science & IA\nFiliale du groupe eXalt, cr\u00e9\u00e9 en 2018,\nregroupant plus de\n950 collaborateurs en France\n(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)\net \u00e0 l\u2019international\n(Colombie, Etats-Unis, Espagne, Belgique),\neXalt Value\napporte une\nexpertise approfondie\ndans le domaine de la Data & IA et conseille les entreprises dans le d\u00e9ploiement de leurs strat\u00e9gies data-driven.\nB\u00e9n\u00e9ficiant du support du groupe eXalt\n(1er dans la cat\u00e9gorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value\nest en pleine croissance et regroupe aujourd\u2019hui une communaut\u00e9 d\u2019expertise de plus de 60 collaborateurs en r\u00e9gion parisienne.\nNos consultants interviennent sur d\nes projets d\u2019envergure\ndans divers secteurs d\u2019activit\u00e9,\nBanque & Assurance, M\u00e9dias, Transports, Retail, Tourisme, etc.\nNous recherchons un\nData Engineer Confirm\u00e9 H/F (minimum 4 ans d'exp\u00e9rience dans la fonction)\npour rejoindre notre communaut\u00e9 sur le\npilier Data Engineering & Big Data.\nVos missions:\nConcevoir et d\u00e9velopper des pipelines et des flux de donn\u00e9es.\nInt\u00e9grer et transformer des donn\u00e9es provenant de diff\u00e9rentes sources.\nD\u00e9velopper et mettre en \u0153uvre des algorithmes de traitement de donn\u00e9es avanc\u00e9s.\nCollaborer \u00e9troitement avec les \u00e9quipes clients pour comprendre leurs besoins et fournir des solutions adapt\u00e9es.\nAssurer la qualit\u00e9 et la fiabilit\u00e9 des solutions d\u00e9velopp\u00e9es.\nConseiller les \u00e9quipes clients sur les solutions \u00e0 mettre en place.\nLes Pr\u00e9requis :\nTitulaire d'un Bac+5, Ecole d'Ing\u00e9nieur\nMa\u00eetrise d'un ou plusieurs langages de programmation (\nPython, Scala, Spark, etc\n.).\nExp\u00e9rience approfondie des technologies\nBig Data (Hadoop, Spark, Kafka, Talend, etc.)\nExp\u00e9rience av\u00e9r\u00e9e\nen\nenvironnement Cloud (AWS, GCP, ou Azure)\n.\nSolides comp\u00e9tences en conception et en optimisation de pipelines de donn\u00e9es.\nExp\u00e9rience de travail en\nm\u00e9thode Agile\nCapacit\u00e9 \u00e0 travailler de mani\u00e8re autonome et en \u00e9quipe.\nExcellentes comp\u00e9tences en communication et en r\u00e9solution de probl\u00e8mes.\nMa\u00eetrise de l\u2019anglais (oral & \u00e9crit dans un contexte international professionnel).\nVotre environnement eXalt\u00e9:\nUn environnement de travail Collaboratif\nfavorisant les initiatives et projets transverses \u00e0 la Practice Data & IA (Lab IA, Data Hub, etc.).\nUn collectif de consultants passionn\u00e9s,\ns\u2019int\u00e9ressant aux tendances innovantes du secteur.\nUne Practice de proximit\u00e9,\nprivil\u00e9giant la mont\u00e9e en comp\u00e9tence de ses collaborateurs (formations, coachings, mentorats, etc.)\nUn suivi individualis\u00e9 et de proximit\u00e9\npar un.e Data Sales Manager r\u00e9f\u00e9rent du compte client, un.e Charg\u00e9.e RH et un.e Practice Manager\nUne \u00e9quipe ouverte et dynamique,\nqui privil\u00e9gie les moments de partage et de convivialit\u00e9 (s\u00e9minaires, eXaltemps, meet-up, d\u00e9jeuners d\u2019\u00e9quipe, etc.)\nNotre processus de recrutement :\nUn entretien RH avec Estelle,\n\u00e0 la suite duquel vous saurez tout (ou presque) d\u2019eXalt Value,\nUn entretien technique avec un Manager assorti d\u2019un test technique,\nlors duquel vous aurez l\u2019occasion de d\u00e9montrer vos talents mais aussi d\u2019apprendre avant m\u00eame de dire oui,\nUn entretien final avec la Directrice Associ\u00e9e ou le Directeur Op\u00e9rationnel,\npour finir de vous convaincre de nous rejoindre \ud83d\ude0a\nNous avons h\u00e2te de recevoir vos CV, et de faire votre connaissance!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication",
                "R\u00e9solution de probl\u00e8mes"
            ],
            "EnSoftSkils": [
                "Communication",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Confirm\u00e9"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data engineer - F / H",
        "company": "United Robotics Group",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=5&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=va3%2FvkwzPqFEg1ZOXUYHcw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Bienvenue chez\nAldebaran\n, leader europ\u00e9en de la robotique\nau sein du groupe\nUnited Robotics Group\n.\nNous concevons et industrialisons des robots innovants avec une vision soci\u00e9tale ambitieuse pour fa\u00e7onner un monde plus humain. Depuis 2005, nous sommes \u00e0 l'avant-garde de l'interaction homme-robot avec des produits embl\u00e9matiques tels que NAO et Pepper.\nNotre dernier-n\u00e9,\nPlato\n,\nincarne notre engagement envers la technologie de pointe et la s\u00e9curit\u00e9,\nfabriqu\u00e9 en France avec des composants europ\u00e9ens.\nRejoignez nos \u00e9quipes multiculturelles et dynamiques pour \u00eatre au c\u0153ur de la r\u00e9volution de la robotique.\nSi vous \u00eates passionn\u00e9.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer \u00e0 fa\u00e7onner l'avenir, nous vous offrons une exp\u00e9rience enrichissante et stimulante.\nEn tant que membre de notre \u00e9quipe, vous b\u00e9n\u00e9ficierez d'une culture d'entreprise ax\u00e9e sur le sens de ce que nous faisons et valorisant la responsabilit\u00e9 sociale et environnementale.\nChez Aldebaran, nous valorisons l'innovation, la diversit\u00e9 et l'\u00e9galit\u00e9 et encourageons chacun.e \u00e0 \u00eatre ouvert.e, authentique, courageux.se, responsable et engag\u00e9.e.\nFinalit\u00e9 du poste\nAu sein de l'\u00e9quipe Cloud-Online Services, le Data engineer int\u00e9grera l'\u00e9quipe Data, responsable du d\u00e9veloppement des produits destin\u00e9s \u00e0 la collecte, aux process et \u00e0 l'exploitation des donn\u00e9es de nos robots.\nIl aura pour r\u00f4le de d\u00e9finir et d'impl\u00e9menter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui g\u00e8rent les robots du groupe.\nMissions principales\nLe Data engineer aura pour responsabilit\u00e9s de :\n\u00e9valuer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,\nconcevoir et d\u00e9velopper des services Data en respectant la sp\u00e9cification fonctionnelle et la m\u00e9thodologie agile,\nagr\u00e9ger et stocker de grandes quantit\u00e9s de donn\u00e9es,\nmettre en place des solutions de data processing,\nint\u00e9grer/d\u00e9velopper des outils de visualisation de donn\u00e9es et analyser les KPI,\nd\u00e9velopper, tester, s\u00e9lectionner et mettre en production des algorithmes qui permettent de r\u00e9pondre aux besoins,\nr\u00e9aliser des analyses de donn\u00e9es,\nmettre en place des tests de charge et fonctionnels pour les solutions Data,\ninvestiguer et corriger les bugs remont\u00e9s par les utilisateurs,\ncontribuer \u00e0 la mise en place de l'infrastructure et outil de d\u00e9ploiement (CI/CD)\nRejoignez-nous pour faire partie d'une aventure passionnante o\u00f9 Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !\nRequirements\nPour la bonne ex\u00e9cution des missions confi\u00e9es, vous t\u00e9moignez d'au moins 6 ans d'exp\u00e9rience en tant que d\u00e9veloppeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.\nComp\u00e9tences demand\u00e9es :\nBonne compr\u00e9hension des technologies d'infrastructure et de d\u00e9ploiement,\nComp\u00e9tences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,\nBonne compr\u00e9hension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,\nBonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)\nBonne connaissance et une exp\u00e9rience pratique de Scrum\\Scrumban et des m\u00e9thodes agiles,\nUne certification AWS sera appr\u00e9ci\u00e9e,\nUn niveau de fran\u00e7ais et d'anglais courant est indispensable,\nDes exp\u00e9riences dans des environnements fortement internationaux sont un plus\nBenefits\nNos principaux avantages :\nUne culture du bien-\u00eatre en entreprise qui a fait ses preuves (budget c\u00e9l\u00e9bration et moments de convivialit\u00e9 par \u00e9quipes et directions, restauration collective de qualit\u00e9, environnement de travail agr\u00e9able)\nUn engagement fort en mati\u00e8re de responsabilit\u00e9 sociale et environnementale (promotion de l'\u00e9galit\u00e9 professionnelle, performance de notre plan diversit\u00e9 et inclusion, r\u00e9f\u00e9rent handicap, fresque du num\u00e9rique)\nUne culture du t\u00e9l\u00e9travail encadr\u00e9e de mani\u00e8re appropri\u00e9e !\nTous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "6",
                "6",
                "6"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Coders Connect",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-coders-connect-3870419202?position=6&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=JR82LAk%2FERF273UoMx67Kg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Coders Connect and Sanofi are joining forces to bring an electrifying twist to the biopharmaceutical world!\nWork with a rhythm that suits your style (2 days remote and 3 days onsite magic).\nLanguage\n: Proficiency in English is required for this role to ensure effective communication within our diverse, global team.\nAbout Sanofi:\nWe're not just a company; we're a global movement, focused on human health and making a real difference. Our mission? To battle pain, ease suffering, and sprinkle a little bit of magic in the process by developing life-changing medicines and vaccines through breakthrough science and wizard-level technology.\nDigital & Data: The Pulse of Our Mission\nAt the heart of our quest lies our digital and data powerhouse. Think of us as the digital healthcare platform of your dreams, where innovation meets speed, and technology shakes hands with medicine. With our scale, deep-rooted connections in health ecosystems worldwide, and a knack for pushing boundaries, we're here to revolutionise medicine, one digital solution at a time.\nThe Role: Data Engineering Virtuoso\nAs our Data Engineering Virtuoso, you're tasked with designing and orchestrating the data pipelines that power our ambitious data analytics initiatives. You'll ensure our data's integrity and accessibility, laying the groundwork for groundbreaking insights and innovations.\nRequirements\nCloud Platforms: Proficient in AWS services, with Azure & GCP knowledge a plus. Your work involves leveraging cloud solutions for scalable data processing and storage.\nData Modeling & Query Performance: Expertise in crafting data models and optimizing SQL queries to enhance performance. Experience with Snowflake or similar data warehousing solutions is highly valued.\nIntegration Services: Skilled in utilizing Integration Services like IICS and Tibco, you facilitate seamless data flow and integration across various platforms.\nScripting & Development: Proficient in scripting languages such as Python and R, enabling you to automate tasks and manipulate data efficiently. Familiarity with GitHub for source code management underscores your commitment to collaborative development and version control.\nVisualization & Reporting: Knowledgeable in creating insightful data visualizations using tools like PowerBI, Tableau, or similar, turning complex data into actionable insights.\nData Governance & Compliance: A keen understanding of data quality, security, and governance standards, especially in healthcare environments subject to regulations like GxP, SOX, and data privacy laws.\nReal World Data & Standard Models: Experience with Real World Data (e.g., EHR, Claims) and familiarity with standard data models (e.g., OMOP, FHIR) enhance your ability to contribute to our healthcare objectives.\nPipeline Frameworks: Proficiency in using frameworks like Apache Airflow or Kedro for crafting efficient, reliable data pipelines that support our AI and ML initiatives.\nThe Reward:\nA chance to play a crucial role in a collaboration that's redefining healthcare through digital transformation.\nA seat at the round table of diversity and inclusion, where different backgrounds and experiences conjure the richness of our culture.\nAn endless horizon of professional growth, learning opportunities, and the chance to contribute to a future where better health is a global reality.\nThe Call to Adventure:\nIf you're ready to join a quest for better \u2013 better treatments, better outcomes, and better science \u2013 and believe in the magic of bringing diverse talents together to make miracles happen, we want you. Let's embark on this journey together and transform the future of healthcare.\nBetter is out there. Are you ready to find it with us?\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication",
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F)",
        "company": "Web Transition",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=7&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=84yz5eoQP3z73ZQRUtCzzQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Web Transition, c\u2019est qui ?\nFond\u00e9e en 2011,\nWeb transition\nest une entreprise de services num\u00e9riques op\u00e9rant sur le march\u00e9 de l\u2019IT/Digital !\nConstituant une part essentielle de\nMoOngy Digital Lab\n, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et \u00e9galement en Data !\nNotre objectif : nous implanter comme un acteur principal sur le march\u00e9 de la Transformation Digitale en accompagnant et valorisant les comp\u00e9tences de nos collaborateurs !\nNous sommes convaincus que le succ\u00e8s de MoOngy Digital Lab r\u00e9side dans la somme des potentiels de nos \u00e9quipes \ud83e\udd1d\nTon \u00e9quipe : La tribu Data\nParce qu\u2019il est indispensable que tu puisses partager tes connaissances mais aussi en acqu\u00e9rir de nouvelles, tu feras partie de l\u2019une de nos tribus : celle de la Data. De plus, cela te permettra d\u2019\u00eatre acteur dans le d\u00e9veloppement et la strat\u00e9gie de Web Transition. Ce syst\u00e8me de co-r\u00e9flexion et co-construction est un fondement essentiel chez nous !\nDans cette aventure, tu :\nT\u2019assures\nde la ma\u00eetrise de la donn\u00e9e et est garant de la qualit\u00e9 de son utilisation (r\u00e9f\u00e9rencement, normalisation, et qualification)\nTravailles\n\u00e0 la compr\u00e9hension et l'int\u00e9gration des donn\u00e9es en provenance des diff\u00e9rents formats\ndes interfaces de flux\n\u00e9galement \u00e0 la d\u00e9finition de la politique de la donn\u00e9e et \u00e0 la structuration de son cycle de vie dans le respect des r\u00e9glementations en vigueur\nla supervision et l'int\u00e9gration des donn\u00e9es de diverse nature qui proviennent de ces sources multiples et v\u00e9rifie la qualit\u00e9 des donn\u00e9es qui entrent dans le Data Lake\nGarantis\nl'acc\u00e8s qualitatif aux sources de donn\u00e9es\nFacilites\nl\u2019acc\u00e8s aux donn\u00e9es pour tes coll\u00e8gues (data scientists, data analysts\u2026)\nAssistes\nles autres \u00e9quipes dans l'acc\u00e8s et la compr\u00e9hension des donn\u00e9es des socles.\nRejoins-nous si tu as :\nExp\u00e9rience d\u2019au-moins 4 ans dans la Data\nApp\u00e9tence \u00e0 la qualit\u00e9 des donn\u00e9es.\nConnaissance famili\u00e8re des Datawarehouses.\nMaitrise de Python, Oracle SQL, GCP/Power BI\nAisance avec les indicateurs, tu as une bonne capacit\u00e9 d'analyse et de r\u00e9daction.\nTon savoir-\u00eatre :\nOuvert d\u2019esprit\nRigoureux\nAutonome\nRespectueux des diff\u00e9rences de chacun\nCurieux\nProactif\nAgile\nPar o\u00f9 on commence ?\nUn premier entretien RH d\u20191h pour comprendre ton parcours et tes aspirations\nUn second entretien de 45 minutes avec l\u2019un de nos Business Manager afin de valider tes comp\u00e9tences et qu\u2019il se projette sur l\u2019une des missions qu\u2019il pourrait te proposer\nUn troisi\u00e8me entretien de quelques minutes avec notre responsable d\u2019agence pour te proposer d\u2019int\u00e9grer notre superbe Team Web !\n3 entretiens en peu de temps, si ton profil correspond tu int\u00e8greras tr\u00e8s vite nos \u00e9quipes \ud83d\ude09\nPr\u00eat pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant \u00e0 cette offre ! Voici les avantages qui t\u2019attendent en tant que Weber :\n\ud83e\udd29 Des coll\u00e8gues incroyables\n\ud83c\udfc6 Certifi\u00e9e Great Place to Work\n\ud83c\udfae Des bureaux sympas (o\u00f9 vous serez toujours les bienvenus)\n\ud83c\udf89 Des teambuilding et \u00e9vents tous les mois\n\ud83d\udcbb Des tributs m\u00e9tiers pour \u00e9changer entre Weber du m\u00eame m\u00e9tier\nDes missions chez le client qui sont accompagn\u00e9es et coach\u00e9es par ton manager\nUn accompagnement dans ton plan de carri\u00e8re et tes envies de re skilling\n\ud83e\udd13 Un catalogue de formations certifiantes ouvert \u00e0 tous les salari\u00e9s\n\ud83c\udf7d\ufe0f Une carte tickets restaurant MyEdenred\n\u2764\ufe0f Une mutuelle GrasSavoye\n\ud83d\ude8e Une prise en charge des frais de transport \u00e0 100%\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Oracle"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Airswift",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-airswift-3909165766?position=8&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=6kRhAuUBrxZsA0vvv008UA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data Engineer\nLocation\n: Paris (Hybrid)\nContract type\n: 12 months +\nYears of Experience\n: 4+\nRecruitment Partner:\nAirswift\nKey Words:\nProject Management | Jira | Digiboard | Banking | Stakeholder Management | Architecture | Cloud | Payments/Credit |ServiceNow | PPM |\nResponsibilities\nDesign, develop, and implement data pipelines to collect, process, and store structured and unstructured data from various sources.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and translate them into technical solutions.\nOptimize and tune data pipelines for performance, scalability, and reliability.\nEnsure data quality and integrity throughout the data lifecycle, implementing data validation and monitoring processes.\nEvaluate and implement new tools and technologies to enhance our data infrastructure and capabilities.\nRequirements\n:\nExtensive experience in Python.\nStrong experience with data processing frameworks and tools such as Apache Spark.\nExperience with cloud platforms such as AWS, Azure, or Google Cloud Platform.\nSolid understanding of data modelling, database design, and SQL\nFrench and English speaking\nFreelancing opportunity\nThe next step\nWe have an exceptional team in place, and we are pleased to be able to appoint a further person to our growing business. We are aware that you may not \u2018tick all the boxes\u2019, but if you believe you can genuinely offer some valuable skills and experience to our business, please in the first instance contact our recruitment partner Airswift.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure",
                "Google Cloud Platform"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F)",
        "company": "Beelix",
        "location": "Antibes, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-beelix-3838611420?position=9&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=tWinPOqjhPLKXwnj%2BfvA6w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des probl\u00e9matiques de Product Management, Data et Design Thinking. Beelix contribue \u00e0 fa\u00e7onner le monde de demain en participant aux grandes avanc\u00e9es des secteurs suivants :\n\ud83d\ude97Automobile\n\u26a1Energie\n\ud83d\udce1M\u00e9dias & T\u00e9l\u00e9coms\n\ud83d\udc57Luxe & Retail\n\ud83d\udcb6 Banque, Finance & Assurance\n\u2708\ufe0fD\u00e9fense\nAujourd\u2019hui, Beelix compte plus de 200 collaborateurs motiv\u00e9s et dynamiques. Lab\u00e9lis\u00e9e Great Place To work en 2023, Beelix est aussi une entreprise engag\u00e9e o\u00f9 il fait bon vivre.\nDans le cadre de notre d\u00e9veloppement, nous recherchons un Data Engineer (H/F) pour l'un de nos clients.\nQuelles missions au quotidien ?\n\u00catre le leader de la brique Datalakehouse\nD\u00e9velopper les scripts de transformations de donn\u00e9es et les pipelines d\u2019alimentation\nProposer des \u00e9volutions architecturales ou de fonctionnalit\u00e9s pour am\u00e9liorer le socle technique\n\u00catre le back-up du leader technique sur la partie reporting (Power BI)\nOrientation satisfaction client et r\u00e9sultat final forte mais \u00e9galement sensibilit\u00e9 au \u00ab comment \u00bb\nInnovation et proposition de nouvelles pratiques pour am\u00e9liorer l\u2019environnement et les conditions de travail des \u00e9quipes\nA propos de vous ?\n5 + ann\u00e9es d'exp\u00e9rience en tant que Data Engineer\nMa\u00eetrise des technologies suivantes : Microsoft Azure, Microsoft Azure Synapse Analytics (Spark / Python / Pipeline / Serverless), fichiers parquet / delta, Microsoft Power BI, Microsoft SQL Server, langage SQL, Datawarehousing / Mod\u00e9lisation de donn\u00e9es\nAnalyses et export de donn\u00e9es\nConnaissance de l\u2019ensemble du processus depuis la collecte jusqu\u2019\u00e0 la mise \u00e0 disposition des donn\u00e9es en ayant comme point fort la maitrise de sa transformation et mise en forme\nVous avez un bon niveau d\u2019anglais\nLocalisation : Biot et/ou Carros\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "SQL Server"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (F/H)",
        "company": "Aubay",
        "location": "\u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=10&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=ikgRu7W36JbbvXgo9R7FUA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Passionn\u00e9 par la Data, tu souhaites rejoindre une communaut\u00e9 d\u2019experts dans le domaine afin de d\u00e9velopper tes comp\u00e9tences en Data Engineering. Aubay renforce ses \u00e9quipes Data et recherche des Data Engineers pour int\u00e9grer des dispositifs de projets pointus et vari\u00e9s.\nTon quotidien en tant que Data Engineer chez Aubay, :\nD\u00e9finition de la strat\u00e9gie de stockage et mise en \u0153uvre des technologie appropri\u00e9es (base de donn\u00e9es SQL, NoSQL, stockage distribu\u00e9,\u2026)\nIngestion des donn\u00e9es (structur\u00e9es, semi-structur\u00e9es ou non-structur\u00e9es) selon diff\u00e9rentes fr\u00e9quences : batch, micro-batch ou temps r\u00e9el\nConception et mise en \u0153uvre de pipelines de donn\u00e9es afin de fournir des donn\u00e9es pr\u00eates \u00e0 l\u2019emploi aux consommateurs : uniformisation, mise en qualit\u00e9, enrichissement, calcul d\u2019indicateurs,\u2026\nConception et d\u00e9veloppement d\u2019API pour exposer les donn\u00e9es aupr\u00e8s d\u2019applications tierces\nAppui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning\nPr\u00e9paration et animation d\u2019ateliers de travail avec des interlocuteurs vari\u00e9s : recueil/approfondissement des besoins m\u00e9tiers, avancement/restitution des travaux, transfert de comp\u00e9tences,\u2026\nTon profil :\nTu dispose d\u2019une formation niveau BAC+5 (Master 2 ou \u00e9cole d\u2019ing\u00e9nieur) sp\u00e9cialis\u00e9e en informatique\nTu as d\u00e9j\u00e0 une premi\u00e8re exp\u00e9rience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data\nLes technologies telles que Hadoop, Spark ou Kafka sont tes technologies de pr\u00e9dilection\nLa programmation n\u2019a plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python\nTu ma\u00eetrises les tenants et aboutissants de la philosophie DevOps et des outils orient\u00e9s CI/CD\nTu es soucieux de la qualit\u00e9 et de la performance de tes d\u00e9veloppements et tu t'int\u00e9resse \u00e0 l\u2019innovation frugale\nTu es un expert technique dans ton domaine sans pour autant oublier l\u2019importance d\u2019une communication orale et \u00e9crite de qualit\u00e9 et adapt\u00e9e \u00e0 chacun de tes interlocuteurs\nTu travaille au quotidien en mode agile et tu en maitrise les fondements\nCe qui nous caract\u00e9rise :\nDes missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs vari\u00e9s (Banque, Assurance, Telecom, Industrie,\u2026) qui permettent \u00e0 nos collaborateurs de monter en comp\u00e9tences et de devenir des experts Data reconnus\nDe l\u2019apprentissage en continu avec des formations et des certifications sur les technologies Data d\u2019aujourd\u2019hui et de demain\nDes experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets\nDes communaut\u00e9s de savoir-faire Data proposant de mani\u00e8re r\u00e9guli\u00e8re aux collaborateurs d\u2019Aubay du contenu et des \u00e9v\u00e8nements de partage (webinar, meetup/afterwork, BBL,\u2026) sur les th\u00e9matiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,\u2026\nAubay encourage la diversit\u00e9 sous toutes ses formes et garantit l'\u00e9galit\u00e9 des chances \u00e0 tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les am\u00e9nagements n\u00e9cessaires.\nTa carri\u00e8re chez Aubay :\nTu auras la possibilit\u00e9 de d\u00e9velopper et certifier tes comp\u00e9tences sur les derni\u00e8res technologies Data avec un focus fort sur les plateformes Data Cloud telles qu\u2019Azure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks\nTu pourras rejoindre la BU d\u2019excellence Data et \u00e9voluer au sein d\u2019un environnement humain et professionnel de haut niveau. Tu profiteras d\u2019un management sur-mesure pour t'accompagner dans ta trajectoire de carri\u00e8re\nAu sein de la BU d\u2019excellence, de multiples perspectives s\u2019offriront \u00e0 toi :\nR\u00f4le de \u00ab Lead \u00bb : Vous pourrez gagner en responsabilit\u00e9 sur le plan technologique et devenir un r\u00e9f\u00e9rent aupr\u00e8s de nos clients et des collaborateurs de la communaut\u00e9 Data Engineering\nR\u00f4le de \u00ab Champion \u00bb : Vous repr\u00e9senterez Aubay aupr\u00e8s d\u2019un ou plusieurs de nos partenaires \u00e9diteurs strat\u00e9giques et vous participerez activement \u00e0 l\u2019animation de la relation sur le plan technologique\nR\u00f4le de \u00ab Head \u00bb : Vous pourrez prendre la responsabilit\u00e9 du savoir-faire Data Engineering et de ses offres et en assurer le d\u00e9veloppement au sens large (d\u00e9veloppement business, recrutement, management de collaborateurs, d\u00e9finition de la strat\u00e9gie et animation de la communaut\u00e9 au sein du groupe Aubay,\u2026)\nBesoin d\u2019en savoir plus sur le processus de recrutement ?\nUn \u00e9change macro au niveau RH avec Doriane\nUn entretien technique avec Marius ou Peter, deux de nos r\u00e9f\u00e9rents techniques\nUn \u00e9change manag\u00e9rial avec le Directeur de la BU Modern BI & Data\nA savoir que l\u2019ordre des \u00e9tapes peut varier selon tes envies (ex : \u00e9change manag\u00e9rial avec l\u2019\u00e9change technique)\nAubay encourage la diversit\u00e9 sous toutes ses formes et garantit l'\u00e9galit\u00e9 des chances \u00e0 tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les am\u00e9nagements n\u00e9cessaires.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Azure",
                "Google Cloud Platform"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Big Data",
                "Machine Learning",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "2",
                "2",
                "2"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Solutions Engineer (Data & AI)",
        "company": "LVMH",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=1&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=3ow1m%2BXt5JtMIV2JBi%2FJsw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.\nWe are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.\nIf you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,\nIf you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,\nIf you like building tech solutions having direct impacts on billion-dollar-valued businesses,\nIf you have good communication skills and like sharing your knowledge,\nApply now, and join us!\nThe mission\nThe Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.\nOur team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.\nYou will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).\nMain responsibilities\nYou will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).\nYou will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.\nYou will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.\nApplying the quality and security standards. Making them evolve if necessary.\nProducing realistic, understandable and documented solutions following the group guidelines.\nSharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.\nTaking responsibility as member of the team on the product performances (delivery and long term usage)\nRequired expertise and knowledge\nAbility to build technical solutions answering concrete usage (User Stories) and communicate them to the team.\nDimension and evaluate complexity for technical solution productions.\nExtensive knowledge and experience with good learning and sharing abilities.\nEvaluate quickly risks and opportunities about technical choices.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProblem-solving skills on Data and AI, coding and software development\nTech lover\nFeedback taker and giver\nTeam player\nKey benefits to join our team\nAttractive packages\nOffices in the 8th arrondissement near the Champs Elys\u00e9es\nFlexibility on the working hours\nRemote work possible (~40%)\n7 weeks of holidays (cong\u00e9s pay\u00e9s + RTT)\nLVMH brands exclusive private sales\nGreat employee committee and health insurance (CE, mutuelle)\nLast generation MacBooks\nPart of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!\nYou\u2019re eligible if\nYou have a strong experience (3+ years) in cloud data architecting or consultancy.\nYou graduated from an engineering (or equivalent) with a master\u2019s degree. Computer Science knowledge is mandatory.\nExperience on data stacks and/or Google Cloud (built in components) is a huge plus.\nFrench and English both written and oral (Maisons are all over the world)\nYou\u2019re thrilled to support the #1 luxury group to get even better.\nHiring Process\nCall with our HR partner dedicated to the Tech Team\nTechnical interview with the Solution Engineering Manager\nTechnical test\nInterview with the Head of Engineering\nStill here? Apply now!!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication",
                "Flexibility"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": [],
            "Salary": [
                "40"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer GCP (F/H)",
        "company": "Apside",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-gcp-f-h-at-apside-2902806697?position=2&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=ctZFklaEBa2A%2BElqpEb7zA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre une entreprise apprenante ? Engag\u00e9e pour t\u2019accompagner dans ton \u00e9volution professionnelle et dans tes projets personnels ?\nRejoins Apside pour travailler sur les projets de demain !\nLe poste ?\nPour le compte de notre\nclient acteur mondial de la beaut\u00e9 et cosm\u00e9tique,\ntu interviendras dans la\ntransformation d\u2019un projet worlwide,\no\u00f9 tu devras\nd\u00e9velopper la Data Platform et l'ensemble des services Data qui seront expos\u00e9s aux diff\u00e9rentes \u00e9quipes du client. Aussi, tu seras amen\u00e9 \u00e0 d\u00e9velopper des use cases data.\nDans ce sens, tes missions seront les suivantes :\nDesigner l'architecture et d\u00e9velopper la solution\nD\u00e9finir et d\u00e9velopper les Data Model\n\u00catre garant de la qualit\u00e9 du code\n\u00catre DevOps (Utilisation/mise en place de chaine CI/CD et support niveau L3 des d\u00e9veloppements)\nEnvironnement technique :\nGCP (BigQuery, Cloud Run, Cloud Build)\nSQL\nPython\nDevOps (Github)\nAPI Development\nTerraform\nM\u00e9thodologie Agile\nToi ?\nTu as d\u00e9j\u00e0 travaill\u00e9 sur\nGoogle Cloud Platform (GCP)\n?\nTu es\nautonome\n,\nrigoureux\n, et\nbon communiquant\n?\nTu souhaites participer \u00e0 un\nprojet d\u2019envergure associant cloud et Big Data\n?\nEt la suite ?\nTu rencontres d\u2019abord l\u2019\u00e9quipe RH pour parler de tes attentes, ton projet, ton futur !\nPuis les managers pour parler concret : missions, projets, parcours de carri\u00e8re, et bien s\u00fbr salaire et avantages J\nEt tu discutes avec un de nos Tech Leads, pour \u00e9valuer tes comp\u00e9tences/ te challenger.\nLes infos en plus !\nT\u00e9l\u00e9travail ! \ud83d\ude0a\nUn salaire attractif en fonction de ton exp\u00e9rience + diff\u00e9rents avantages\nUn groupe en pleine croissance avec un management bienveillant\nEt une \u00e9volution personnalis\u00e9e avec la possibilit\u00e9 de se former via une plateforme interne\nTu souhaites donner un nouvel \u00e9lan \u00e0 ta carri\u00e8re ? Rejoins la vie Apsidienne !\nPour en savoir plus \u00e0\nwww.apside.com\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Big Data",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [
                "Salaire"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer / Developpeur Talend",
        "company": "Siderlog Conseil",
        "location": "Niort, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-developpeur-talend-at-siderlog-conseil-3861714639?position=3&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=cWPbrcZyyB%2BcGzsdXBIyuw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Siderlog est un cabinet de conseil sp\u00e9cialis\u00e9 implant\u00e9 \u00e0 Niort depuis 2004 qui accompagne les directions m\u00e9tiers et SI sur des projets de:\n- Business et Data Analyse\n- Management de projets\n- Conduite du changement\nPour soutenir notre croissance, nous pr\u00e9voyons \u00e0 Niort le recrutement de 20 consultants d'ici 2025.\nNos consultants b\u00e9n\u00e9ficient d'un mod\u00e8le qui favorise l'\u00e9panouissement professionnel et le bien \u00eatre:\n\ud83c\udf43Un processus d'int\u00e9gration sp\u00e9cifique et un suivi r\u00e9gulier\n\ud83c\udf43Une \u00e9coute active des attentes, notamment en terme de formations, certifications\n\ud83c\udf43Des d\u00e9jeuners et \u00e9v\u00e8nements mensuels\n\ud83c\udf43Un management et un accompagnement de proximit\u00e9\n\ud83c\udf43Un package salarial attractif\n\ud83c\udf43La possibilit\u00e9 de contribuer aux projets d'entreprise ( RSE, communaut\u00e9s m\u00e9tiers, p\u00f4le conseil et expertise)\n\ud83c\udf43Entreprise labellis\u00e9e Happy At Work, charte T\u00e9l\u00e9travail...\n\ud83c\udf43De nombreux autres avantages que nous vous invitons \u00e0 venir d\u00e9couvrir\nSiderlog recherche pour renforcer son \u00e9quipe, \u00e0 Niort un(e) consultant(e) Data Engineer / Developpeur Talend.\nDans ce cadre vous devrez :\n\u2714\ufe0fConcevoir et d\u00e9velopper des traitements/job de donn\u00e9es complexes \u00e0 l'aide de Talend pour l'ingestion, le nettoyage, la transformation et la distribution des donn\u00e9es.\n\u2714\ufe0fCollaborer \u00e9troitement avec les \u00e9quipes m\u00e9tier pour comprendre les besoins en mati\u00e8re de donn\u00e9es et concevoir des solutions adapt\u00e9es.\n\u2714\ufe0fMettre en \u0153uvre des bonnes pratiques de d\u00e9veloppement ETL, y compris la documentation, les tests unitaires et l'int\u00e9gration continue.\n\u2714\ufe0fAssurer la surveillance et la maintenance des traitements/job de donn\u00e9es en production, en r\u00e9solvant les incidents et en effectuant des mises \u00e0 jour si n\u00e9cessaire.\n\ud83d\udccb Qualifications et comp\u00e9tences :\n\ud83d\udc49Exp\u00e9rience av\u00e9r\u00e9e dans le d\u00e9veloppement de solutions de gestion et d'int\u00e9gration de donn\u00e9es, sur Talend.\n\ud83d\udc49Ma\u00eetrise des langages de requ\u00eate SQL pour l'extraction et la manipulation des donn\u00e9es.\n\ud83d\udc49Connaissance approfondie des bases de donn\u00e9es relationnelles et des entrep\u00f4ts de donn\u00e9es.\n\ud83d\udc49Comp\u00e9tences en programmation avec Java, Python ou d'autres langages similaires.\n\ud83d\udc49Capacit\u00e9 \u00e0 travailler de mani\u00e8re autonome tout en collaborant efficacement avec les membres de l'\u00e9quipe.\n\ud83d\udc49Excellentes comp\u00e9tences en communication \u00e9crite et verbale.\n\ud83d\udc49Maitrise de l'outil ETL Talend.\n\ud83d\udc49Exp\u00e9rience avec d'autres outils d'int\u00e9gration de donn\u00e9es tels que Informatica, BODS, Alt\u00e9ryx.\n\ud83d\udc49Certification Talend serait un plus.\n\ud83d\udc49Exp\u00e9rience dans le domaine de l'assurance souhait\u00e9e\nCette offre vous int\u00e9resse ! Postulez !\n\ud83c\udfc6\ud83d\ude4f\ud83d\ude80\ud83c\udf89 !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [
                "Package"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Full Stack Data Engineer",
        "company": "bsport",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/full-stack-data-engineer-at-bsport-3848363421?position=4&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=TlXXliAVNig68JS53ybBYg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Do you know about bsport?\nWe are a Barcelona based company that offers a platform combining boutique fitness and advanced technology. Our all-in-one features cover bookings, payroll, marketing and more, helping our partners streamline operations and boost their commercial success.\nWe have more than 2\u2019000 clients in 40+ countries and continue to expand rapidly.\nWe provide our partners with:\nOur platform - the heart of the system (B2B)\nA white label iOS and Android mobile application (B2C)\nAn integrated Video on Demand tool\nOur self-built Smart Marketing Suite\nA Webshop to up- and cross-sell different products\nOur first successes\nSince we launched in 2019, we have already achieved the following:\nWe\u2019ve built a community of over 6 million users\nFinalised a Series A Fundraising of $4+ million in December 2022\nGrown our team to more than 150 employees\nWe\u2019re continuing to grow our team to become the #1 tech partner for boutique studios in Europe and the rest of the world!\nWe are seeking a talented and experienced Full Stack Data Engineer to join our dynamic tech team based in Paris. As a Full Stack Data Engineer, you will play a crucial role in enhancing our data platform and driving innovation through data engineering solutions.\nWhat your future position looks like:\nThe primary focus of this position is data engineering, encompassing tasks such as building, optimizing, and maintaining our data platform. You will be responsible for making continuous improvements to our data infrastructure to ensure its reliability, scalability, and performance. As an integral part of the data team, you will collaborate closely with team members to address various data-related challenges and opportunities. This may involve tasks ranging from designing and implementing data pipelines to conducting in-depth data analysis to extract actionable insights.\nThe role will focus on:\nBuild and maintain bsport\u2019s data architecture\nEnsure the sustainability and scalability of the diverse components by leveraging bsport's cloud provider services and adhering to all DevOps best practices\nYou will enjoy working within an internal team of 25 people, consisting of Tech, Product, and Data experts, directly surrounded by our Senior Leads and CTO.\nOur stack is fully automated with push-to-deploy on both frontend and backend. We use Kubernetes and AWS, and our CI is self-hosted. Our methodology is based on agile principles, with weekly releases to production and staging to iterate, gather feedback, and drive progress.\nYou will be a good fit to join us if you:\nAlready built or maintained a data architecture at scale in a top cloud provider (AWS, GCP or Snowflake)\nAlready deployed data science models in production or built a data ingestion pipeline\nFamiliar with DevOps best practices\nProficiency in SQL, Python and Spark\nExperience with dbt and airbyte\nQualifications\nBachelor\u2019s degree in Data Science, Computer Science, Engineering, or related field; advanced degree preferred.\nRelevant experience in data engineering\nStrong analytical and problem-solving skills, with the ability to work independently and in a team environment.\nWe'd love to have you join us for many reasons, such as:\n\ud83c\udf0d A multicultural and international team!\n\ud83d\ude80 A stimulating SaaS environment within a supportive and a fast-growing company\n\ud83d\udd0b Enjoy 25 days of paid leave to recharge\n\ud83c\udfe1 Embrace days of remote work\n\ud83c\udfe2 Work from our stunning office in the heart of Bastille\n\u2764\ufe0f\u200d\ud83e\ude79\nHealth insurance half covered\n\ud83d\udef5 Public Transportation half covered\n\ud83c\udfc4\ud83c\udffd Take part in bsport team building and sport initiatives\n\ud83d\udecc\ud83c\udffd Supported by bsport on sick days\nInterview Process\nFirst interview with one of our Talent Acquisition team members (30 min)\nTechnical Interview with our Lead Data (1h30)\nTechnical Interview with our CPO (1h)\nFinal Interview with our CTO (1h)\nAbout our Company Culture:\nAt bsport, we collaborate with passionate individuals who value diverse ideas and backgrounds. We believe that diversity is our most valuable asset.\nOur commitment is to foster a positive and inclusive culture. We achieve this through team-building initiatives, open communication, professional growth opportunities, and by celebrating diversity in all its forms.\nWe value and respect every individual who is eager to make a difference, empowering them to contribute their unique skills and perspectives. Join our dedicated team to help create a thriving and welcoming workplace.\nJoin our team of passionate and committed people who are dedicated to creating a thriving and welcoming environment. Let's make it happen together!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full",
                "Senior"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer H/F",
        "company": "Ippon Technologies",
        "location": "Greater Nantes Metropolitan Area",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=5&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=CgC9wZD4Kt6bLAagR1%2FJQA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre la communaut\u00e9 DATA la plus dynamique de France ?\nNotre sp\u00e9cialit\u00e9 est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.\nMembre de la Practice Data, le/la futur(e) Data Engineer sera int\u00e9gr\u00e9(e) \u00e0 nos \u00e9quipes de conseil et sera suivi(e) par un(e) mentor qui l\u2019aidera \u00e0 monter en comp\u00e9tences.\nVotre champs d\u2019expertise :\nIntervenir sur les data platforms de nos clients pour d\u00e9velopper de nouveaux pipelines de donn\u00e9es (ingestion, traitement, exposition).\nTravailler en collaboration avec les m\u00e9tiers et les data scientists pour leur fournir un support \u00e0 l\u2019industrialisation de leurs travaux (tests, int\u00e9gration continue, scalabilit\u00e9 des mod\u00e8les, craftsmanship etc\u2026)\nD\u00e9ployer des infrastructures cloud full\ninfra-as-code\n(Terraform, CloudFormation).\nParticiper aux \u00e9v\u00e8nements internes \u00e0 la communaut\u00e9 data (BBL, webinar, datap\u00e9ro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).\nCapitaliser sur les missions et les diff\u00e9rents \u00e9v\u00e8nements de la communaut\u00e9 au travers d\u2019articles de blogs, REX, BBL interne.\nVos connaissances :\nUn framework de calcul distribu\u00e9 tel que Spark, Storm, Flink.\nUn ou plusieurs langages de programmation (Python, Scala, Java...)\nDiff\u00e9rents syst\u00e8mes de stockage de donn\u00e9es (SQL ou NoSQL) et bien s\u00fbr le langage SQL.\nLa connaissance de Snowflake est bienvenue ;-)\nUn framework de streaming de donn\u00e9es tel que Kafka ou Amazon Kinesis.\nUne exp\u00e9rience sur les technologies Cloud : AWS, GCP, Azure\nLe delivery et les projets en production faisant partie de notre ADN, vous \u00eates capable de livrer du code de qualit\u00e9 dans des environnements agiles.\nDe plus en plus de nos projets se font en remote avec des clients du monde entier, il devient n\u00e9cessaire d\u2019\u00eatre \u00e0 l\u2019aise en Anglais.\nIppon technologies c\u2019est aussi :\n\ud83d\udc4d B\u00e9n\u00e9ficier d'un suivi de proximit\u00e9 r\u00e9alis\u00e9 par votre manager technique : points r\u00e9guliers pour votre suivi en mission, votre formation et votre \u00e9volution de carri\u00e8re\n\u270c\ufe0f Rejoindre une entreprise o\u00f9 les valeurs du sport sont nos leitmotiv : d\u00e9passement de soi, travail en \u00e9quipe, bienveillance.\n\ud83d\uddd2\ufe0f Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL\n\ud83d\ude01 Travailler en pair programming ou avec un.e mentor pour gravir les \u00e9chelons !\n\ud83d\udcaa Pouvoir participer \u00e0 une aventure humaine au sein de notre Fondation Ippon pour r\u00e9duire la fracture num\u00e9rique dans le monde !\n\ud83e\udd1d Participer \u00e0 nos ap\u00e9ros et divers \u00e9v\u00e8nements internes pour consolider la coh\u00e9sion d\u2019\u00e9quipe\nEt apr\u00e8s ?\nEt oui alors ? Que se passe-t-il une fois que vous \u00eates convaincu d\u2019avoir lu l\u2019offre d\u2019emploi qui vous correspond bien ?\nNous vous proposons de prendre contact et de nous rencontrer !\nLes Next Steps :\n1 call RH\n1 \u00e9change RH\n1 \u00e9change Technique\nSi le match est bon des deux c\u00f4t\u00e9s : Hadjim\u00e9 ! Vous vous lancerez sur le tatami Ippon !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark",
                "Databricks",
                "Flink"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [
                "Terraform",
                "CloudFormation"
            ],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Equativ",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-equativ-3814251519?position=6&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=ya1Qp5kjARTzCi3eNWECoA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83d\udc6b About the team\nAt Equativ, we\u2019re on a mission to develop advertising technologies that empower our customers to reach their digital business goals. This means that we rely on massively scalable, widely distributed, highly available, and efficient software systems; the platform deals with over 100 billions requests per day and above 40 Gbps of network traffic.\nOur innovation team based in Paris, Nantes, Limoges, Krak\u00f3w and Berlin is composed of 90 straightforward and energetic engineers working in an Agile environment and ready to tackle the most complex technical challenges.\nOur data engineering team is composed of 10 skilled engineers and is based in Paris. We are part of the R&D department which is composed of 140+ engineers spread across Paris, Nantes, Limoges, Krak\u00f3w and Berlin all working in an Agile environment and ready to tackle the most complex technical challenges.\nOur mission \ud83d\udc47\nData Engineering team is central to Equativ\u2019s data centric business and is responsible to ingest, transform, model and redistribute all data coming from our adtech platform.\nWe aim at building scalable and robust Big Data platforms from ingestion to business actionable consumption. Our Big Data ecosystem must handle massive log ingestion (tens of billions per day), short & long term data storage, complex data modelling, real-time and batch ELT as well as providing external access through dedicated APIs.\nData Engineers serve Equativ data directly to our customers and throughout the company whether it is for BI analysis, data science algorithms (clustering and optimization), customer reporting, invoicing and more.\nEquativ Data Engineering team is engaged in an ambitious migration of its main data stack (Hadoop on-premise) to GCP with the objective to increase reporting features, lower maintenance time, improve performances and simplify the access to our raw data.\nWhat you'll do \u270f\ufe0f\nAs a Big Data Engineer, you\u2019ll primarily focus on maintaining and enhancing the operationality of our on-premise and cloud data pipelines which feed our warehouses and APIs\n-Design, develop, test, promote and industrialize all data components from data ingestion to datawarehouse delivery (ClickHouse, BigQuery):\nPropose and develop innovative solutions to achieve the best levels of scalability and performance for our Big Data engines\nAutomate and streamline our real-time and batch data pipelines (on-premise and in the cloud) in order to simplify access to our data by other teams and lower amount of work spent by other teams on ETL processes\nPerform end-to-end monitoring to ensure high availability of production data processing, data quality and reliability\nApply best in class Devops guidelines and secure deployments\n-Brainstorm with other team members working on our data backend (datawarehouse modelling and data exposure through our reporting APIs) on optimizing our architecture and support them in the use of our pipelines\n-Contribute to data roadmap definition in coordination with other R&D and product teams in order to build a best in class data infrastructure that will generate insights for Equativ\u2019s analytics\n-Take part in improving and deploy data engineering standards, procedures, processes and operational guidelines around target data components at Equativ\n\ud83d\udcaa About you\nMaster degree in Computer Science or similar technical field of study\n3+ years of software development with open source technologies\nFluent in Java and/or in Scala. SQL mastery\nVery good understanding of Devops principles (Gitlab, Docker, Kubernetes, Gradle, ci/cd)\nExperience with large-scale data engineering technologies (ClickHouse, Flink, Kafka, Hadoop, Spark, Hbase)\nExperience on building data pipelines on Google Cloud Platform (BigQuery, Dataflow, GCS, Cloud Run, Airflow \u2026) would be a big plus\nExperience in working with high QPS Rest APIs is a plus\nEntrepreneurial spirit and know-how to identify opportunities of improvement\nWorking proficiency and communication skills in verbal and written English\nPassion for playing with large volume of data\n\ud83d\ude80 How you'll grow\nWithin 1 month:\nYou'll be just finishing your onboarding.\nYou'll probably have tackled a few small tasks in peer-coding\nWithin 4 months:\nYou'll have an overview of 50% of the stack, CI/CD and team\u2019s main processes. You\u2019ll be able to work on more complex developments\nYou'll now have enough knowledge to participate to deployments of chosen applications\nWithin 9 months:\nYou'll be autonomous on most of our stack and will have participated to major projects\nYou\u2019ll be helping the team on production matters\n\ud83d\udc4b About us\nEquativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus \u2014 four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.\nHeadquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.\nThe company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times\u2019 FT 1000: Europe\u2019s Fastest-Growing Companies.\nEquativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.\nCome and lead the charge with us in building a transparent ecosystem based on quality!\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark",
                "Flink"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Kubernetes",
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "DevOps",
                "Big Data",
                "ML",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer Cloud (F/H)",
        "company": "Apside",
        "location": "\u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-cloud-f-h-at-apside-3904088503?position=7&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=Od0kUUGYPxYerGEI1x4MHA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83d\udca5\nD\u00e9couvrez la Vie Apsidienne\n\ud83d\udcf9\net vous aussi, devenez Apsidien\nOn aurait pu demander \u00e0 Chat GPT de vous d\u00e9montrer en quoi\nApside est l\u2019ESN qu\u2019il vous faut,\nmais on pr\u00e9f\u00e8re que vous le d\u00e9couvriez vous-m\u00eames \ud83d\udc47\ud83d\ude0f\n\ud83d\udd25\nD\u00e9couvrez votre future mission\n\ud83d\udc49\nContexte\nRejoignez notre Practise Cloud/Data, afin d\u2019intervenir sur des sujets \u00e0 haute valeur ajout\u00e9e !\nNotre\nclient migre actuellement toutes ses applications vers le cloud AWS.\nDe plus, dans le cadre du d\u00e9veloppement d'un produit de restitution automatis\u00e9e de donn\u00e9es, ils recherchent actuellement d\u00e9veloppeur data ayant d\u00e9j\u00e0 travaill\u00e9 sur un projet similaire. La solution produit est techniquement con\u00e7ue en lien avec le Tech Lead validant l'architecture logicielle \u00e0 mettre en place sur le cloud AWS.\nSecteur\n: culture/m\u00e9dia\nM\u00e9thode de travail\n: Agile Safe\n\ud83d\ude0e Mission\nCapter les donn\u00e9es (structur\u00e9es et non structur\u00e9es) produites dans les diff\u00e9rentes applications\nInt\u00e9grer les \u00e9l\u00e9ments\nStructurer la donn\u00e9e (s\u00e9mantique, etc\u2026)\nCartographier les \u00e9l\u00e9ments \u00e0 disposition\nNettoyer la donn\u00e9e (\u00e9limination des doublons, etc\u2026)\nValider la donn\u00e9e\nCr\u00e9er les r\u00e9f\u00e9rentiels de donn\u00e9es\nEnvironnement technique\n:\nPython\nLambda\nStep Function\nAWS / AWS RDS\nPostegreSQL\nSnowflake\nSpark\n\ud83d\udccd\nLocalisation\nLa D\u00e9fense\n\ud83d\udcb0\nLe package salarial que nous vous proposons\nContrat :\nCDI\nAvantages groupe :\ncarte ticket restaurant Swile, prime de mobilit\u00e9, RTT, accord t\u00e9l\u00e9travail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle \u00e0 100% etc\u2026\nAvantages agence :\nCommunaut\u00e9 Cloud/Data, afterworks, communaut\u00e9 techlead\nFormation :\ncertifications techniques, cours particuliers d\u2019anglais en interne, acc\u00e8s \u00e0 un catalogue de formations gr\u00e2ce \u00e0 notre plateforme e-learning (\nAcademy by Apside\n) ou via nos organismes partenaires.\n\ud83d\udd2e\n\u00d4 vous futur Apsidien, qui \u00eates-vous ?\nAu moins 4 ans d'exp\u00e9rience en tant que Data Engineer\nMaitrise de l\u2019environnement cloud AWS\nForce de proposition, bon relationnel et autonome\n\ud83d\ude0f\nApside a suscit\u00e9 votre curiosit\u00e9 ?\nDans un environnement marqu\u00e9 par une acc\u00e9l\u00e9ration des \u00e9volutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients \u00e0 cr\u00e9er de la valeur et \u00e0 adresser leurs enjeux strat\u00e9giques en leur mettant \u00e0 disposition des expertises technologiques (\nData / IA, Cloud, Cyber\n) et une exp\u00e9rience sectorielle (\nIndustrie, Banque, Assurance, Service, Secteur Public\n). Pour un accompagnement global, le groupe propose des offres transverses autour du\nHandicap\n(Apsid\u2019EA), du\nDigital Learning\n, et du\nConseil\n.\n\ud83e\udd14\nEt votre place dans tout \u00e7a ?\n\ud83d\udc49 Notre volont\u00e9\nest de vous accompagner dans la construction et l\u2019\u00e9panouissement de votre carri\u00e8re\nen nous appuyant notamment\nsur 3 piliers :\nUne\nr\u00e9mun\u00e9ration\n\u00e0 hauteur de vos investissements et de vos comp\u00e9tences\nUne\ntrajectoire professionnelle\nstimulante sur mesure\nUn\nengagement\nautour des valeurs Apsidiennes : la qualit\u00e9 de vie et des conditions de travail au c\u0153ur de nos enjeux\nEngag\u00e9e pour\nun monde plus inclusif et plus responsable\n, Apside r\u00e9invente l\u2019ESN et propose l\u2019Engagement Soci\u00e9tal et Num\u00e9rique. D\u00e9couvrez notre d\u00e9marche RSE ainsi que notre vision de l\u2019Entreprise Engag\u00e9e.\nConvaincu ? A vous de jouer, envoyez-nous votre CV !\nRejoignez l\u2019aventure Apsidienne et d\u00e9couvrez notre vision d\u2019une ESN singuli\u00e8re et r\u00e9siliente\n\ud83d\ude80\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [
                "100"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (F/H) - en alternance",
        "company": "Carrefour",
        "location": "Massy, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-en-alternance-at-carrefour-3884390443?position=8&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=x6YzTwVoIs89U8m4IejwAQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Le saviez-vous ?\nNous rejoindre, c\u2019est rejoindre l\u2019un des leaders mondiaux de la distribution qui met l'accent au quotidien sur la diversit\u00e9, la RSE et le digital, pour satisfaire nos clients et nos collaborateurs. En tant que partenaire premium des Jeux Olympiques et Paralympiques de Paris 2024, nous partageons les valeurs du sport en permettant \u00e0 nos \u00e9quipes de se d\u00e9passer et encourageons une alimentation saine au juste prix pour tous.\nVous cherchez \u00e0 travailler dans une entreprise dynamique o\u00f9 votre travail rime avec impact social et environnemental ? Bienvenue chez nous !\nData Engineer (F/H) - en alternance\nEn tant qu' alternant, vous int\u00e9grerez la plateforme supply chain o\u00f9 vous serez amen\u00e9 \u00e0 appuyer le p\u00f4le pr\u00e9vision et optimisation particuli\u00e8rement sur des sujets data et d\u2019analyse de donn\u00e9es.\nAu sein d'une \u00e9quipe compos\u00e9e de data scientists et de data engineers organis\u00e9e en mode Scrum Agile, vous travaillerez pour am\u00e9liorer au quotidien un outil de calcul de pr\u00e9vision (pr\u00e9vision de la demande des entrep\u00f4ts Carrefour). Vous participez \u00e0 l'\u00e9volution fonctionnelle et technique de l'application.\n\ud83c\udfaf Les missions\nDans ce cadre, vous serez amen\u00e9 \u00e0\nExplorer et analyser les donn\u00e9es du datalake carrefour\nParticiper au cadrage des nouvelles fonctionnalit\u00e9s\nD\u00e9velopper les \u00e9volutions des traitements, des mod\u00e8les statistiques et de machine learning de pr\u00e9vision et des reporting\nTester les fonctionnalit\u00e9s d\u00e9velopp\u00e9es\nR\u00e9pondre aux demandes utilisateurs\n\ud83d\udc65 Profil\nVous \u00eates en \u00e9cole d\u2019ing\u00e9nieur, en master 2 ou \u00e9quivalent avec une sp\u00e9cialisation data science, data engineering, statistique, informatique.\nVous avez une exp\u00e9rience en traitement et analyse de donn\u00e9es.\nVous avez un esprit d\u2019analyse et la capacit\u00e9 de travailler en \u00e9quipe et \u00e0 distance.\nVous \u00eates autonome et rigoureux, fluide dans votre communication orale et \u00e9crite et \u00e0 l'\u00e9coute des besoins de vos interlocuteurs.\nVous \u00eates reconnu pour vos capacit\u00e9s d'anticipation, votre sens de l'initiative et votre r\u00e9activit\u00e9.\nVous avez une bonne connaissance des langages suivants\nSQL\nPython\nUne connaissance de GCP et de Big query serait un plus.\nUne connaissance m\u00eame th\u00e9orique de la m\u00e9thodologie agile serait un plus\nUne connaissance de GIT serait un plus.\nEncore plus de bonnes raisons de nous rejoindre\nInt\u00e9grer une \u00e9quipe conviviale \u00e0 taille humaine au sein d\u2018un grand groupe\n12 % de remise sur achat\n\ud83d\udcdd Informations compl\u00e9mentaires\nDate de d\u00e9but  09 septembre 2024\nDur\u00e9e  1 an\nLieu  Lyon\nD\u00e9placements en magasin et en concurrence dans la r\u00e9gion parisienne\nAvantages 50 % du titre de transport pris en charge par Carrefour\nEnvie de rejoindre l\u2019aventure ?\nChez Carrefour, nous avons \u00e0 c\u0153ur de ne passer \u00e0 c\u00f4t\u00e9 d\u2019aucun talent et sommes fiers de compter des \u00e9quipes repr\u00e9sentatives de la soci\u00e9t\u00e9 dans son ensemble. Nous encourageons ainsi tous types de profils \u00e0 postuler \u00e0 cette offre et garantissons un processus de recrutement d\u00e9nu\u00e9 de toutes formes de discriminations.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [
                "Statistiques"
            ],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Big Query"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Machine Learning",
                "Statistiques"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer - Nantes",
        "company": "Capgemini",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=9&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=DheaFnpiwdjTtO17fS5wAQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini\nChoisir Capgemini, c'est choisir une entreprise o\u00f9 vous serez en mesure de fa\u00e7onner votre carri\u00e8re selon vos aspirations, o\u00f9 vous serez soutenu et inspir\u00e9 par une communaut\u00e9 d\u2019experts dans le monde entier, o\u00f9 vous pourrez r\u00e9\u00e9crire votre futur. Rejoignez-nous pour red\u00e9finir les limites de ce qui est possible, contribuer \u00e0 lib\u00e9rer la valeur de la technologie pour les plus grandes organisations et participez \u00e0 la construction d\u2019un monde plus durable et inclusif.\nVos missions :\nInt\u00e9gr\u00e9(e) au sein d'une \u00e9quipe projets intervenant pour des clients dans des secteurs d'activit\u00e9s vari\u00e9es, vous serez notamment en charge des missions suivantes :\nConcevoir et mettre en oeuvre des strat\u00e9gies s\u00e9curis\u00e9es d'acquisition et d'int\u00e9gration de donn\u00e9es,\nConfigurer des r\u00e9f\u00e9rentiels de donn\u00e9es \u00e0 la pointe de la technologie dans des environnements distribu\u00e9s, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),\nConstruire des pipelines de donn\u00e9es pour collecter, transformer et traiter des donn\u00e9es en collaboration avec des scientifiques de donn\u00e9es afin de r\u00e9pondre aux exigences de la mod\u00e9lisation de donn\u00e9es d'analyse avanc\u00e9e.\nVotre profil :\nDipl\u00f4me d\u2019ing\u00e9nieur ou \u00e9quivalent universitaire\nMinimum 3 ans d'exp\u00e9rience\nAnglais courant\nMa\u00eetrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.\nExpertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks\u2026\n3 raisons de nous rejoindre :\nQualit\u00e9 de vie au travail : accord de t\u00e9l\u00e9travail en France et \u00e0 l\u2019international, accord sur l\u2019\u00e9galit\u00e9\nprofessionnelle, la parentalit\u00e9, l\u2019\u00e9quilibre des temps et la mobilit\u00e9 durable.\nApprentissage en continu : certifications et formations en libre acc\u00e8s, accompagnement sur mesure avec votre carreer manager, parcours d\u2019int\u00e9gration sur 9 mois.\nAvantages groupe & CSE : plan actionnariat, activit\u00e9s \u00e0 tarifs pr\u00e9f\u00e9rentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorit\u00e9s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int\u00e9grez un collectif qui valorise la diversit\u00e9, d\u00e9veloppe le potentiel de ses talents, s\u2019engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r\u00e9duire son impact environnemental sur tous ses sites et aupr\u00e8s de ses clients.\nCapgemini\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr\u00e8s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d\u2019exp\u00e9rience, nous sommes un partenaire strat\u00e9gique des entreprises pour la transformation de leurs activit\u00e9s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp\u00e9tuelle \u00e9volution tels que le cloud, la data, l\u2019Intelligence Artificielle, la connectivit\u00e9, les logiciels, l\u2019ing\u00e9nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration",
                "Organisation"
            ],
            "EnSoftSkils": [
                "Collaboration",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (Snowflake)",
        "company": "MindPal",
        "location": "Marseille, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896997028?position=10&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=WW8bMRocTD8OQx6MQxnAeA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Consultant\u00b7e Data Engineer",
        "company": "Ntico",
        "location": "Villeneuve-d\u2019Ascq, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/consultant%C2%B7e-data-engineer-at-ntico-3902424755?position=1&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=M32mBwSUc%2BeWFVPltZpppA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Sois acteur de ta r\u00e9ussite et rejoins notre \u00e9quipe de 140 collaborateurs\u00b7trices qui ne font pas que des projets, mais qui vivent une vraie exp\u00e9rience humaine unique !\n\ud83d\udca1 Partage, Progr\u00e8s, Plaisir : nos valeurs, ton avenir !\n\ud83c\udf10 Pr\u00e9sents \u00e0 Lille, Orl\u00e9ans, Montpellier : des expert\u00b7e\u00b7s partout en France !\n\ud83d\udcbc + de 40 clients qui nous font confiance\n\ud83e\uddd1\u200d\ud83d\udcbb Recrutement sur profil\n\ud83c\udfaf\nTA MISSION :\n* Tu int\u00e8gres une communaut\u00e9 Data, en tant que Data Engineer.\n* Tu con\u00e7ois et mod\u00e9lises les donn\u00e9es et identifies les sources et flux \u00e0 r\u00e9aliser.\n* Tu es en lien permanent avec les \u00e9quipes m\u00e9tiers et IT.\n* Tu formes et transmets ton savoir.\n* Tu es garant\u00b7e de la qualit\u00e9 des livraisons.\n\ud83e\uddd1\u200d\ud83d\udcbb\nTES COMP\u00c9TENCES :\nTalend, ODI, Stambia, Kafka, API, Bases SQL, et NoSQL, GCP, AWS\n\ud83e\udd47\nTON PROFIL :\nTu es expert\u00b7e des flux de donn\u00e9es.\nLa manipulation et le traitement des donn\u00e9es est une seconde nature.\nTu as le sens du service et tu apportes des solutions innovantes.\nTu aimes transmettre et partager ton savoir.\nTu justifies imp\u00e9rativement d\u2019au moins 3 ans d\u2019exp\u00e9rience et tu as d\u00e9velopp\u00e9\u00b7e une autonomie sur ton domaine de comp\u00e9tence.\nTu souhaites diversifier tes comp\u00e9tences pour \u00eatre toujours \u00e0 la pointe des cas d\u2019usages m\u00e9tiers et des nouvelles technologies Data.\n\ud83d\ude4c\nNOS AVANTAGES :\n\u2728 Pourquoi nous rejoindre ?\n\ud83d\udcaa\nD\u00e9veloppement Continu\n: Chez Ntico, tu montes en comp\u00e9tences gr\u00e2ce \u00e0 nos communaut\u00e9s d\u2019experts et nos formations !\n\ud83e\udd1d\nManagement de proximit\u00e9\n: On t'\u00e9coute, on te valorise et on t'accompagne dans ton projet pro, en toute transparence !\n\ud83c\udf89\nMoments conviviaux\n: Sport, culture, DIY, insolite\u2026 Tu peux participer \u00e0 nos \u00e9v\u00e9nements tous les mois, et en proposer ! On n\u2019est jamais \u00e0 court d\u2019id\u00e9es pour des animations uniques !\nNtico, c'est un cadre de travail bienveillant, un environnement dynamique o\u00f9 l'\u00e9panouissement personnel est aussi important que le succ\u00e8s collectif !\nPostule d\u00e8s maintenant et pr\u00e9pare-toi \u00e0 vivre une exp\u00e9rience humaine unique ! \u2728\nDe notre c\u00f4t\u00e9, on te contacte dans les 72h suivant ta candidature et on te propose un processus de recrutement rapide. \ud83d\ude80\nNtico s'engage activement en faveur de l'inclusion professionnelle des personnes en situation de handicap, tout en promouvant la mixit\u00e9, la diversit\u00e9 et l'\u00e9galit\u00e9 au sein de son effectif.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Senior Data Engineer (H/F)",
        "company": "relevanC",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-engineer-h-f-at-relevanc-3845776902?position=2&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=ggs4hDgN4d0J%2BA6DT8nBCg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "relevanC est une filiale du Groupe Casino et a \u00e9t\u00e9 fond\u00e9e en 2017.\nNous avons des bureaux en France, au Br\u00e9sil et en Colombie et op\u00e9rons \u00e0 l'\u00e9chelle mondiale.\nNos solutions de Retail Media permettent \u00e0 nos clients de g\u00e9n\u00e9rer de nouvelles sources de revenus publicitaires gr\u00e2ce \u00e0 des annonces pertinentes et personnalis\u00e9es.\nEn tant que Data Engineer tu auras acc\u00e8s aux donn\u00e9es de nos clients internes (enseignes du groupe Casino) et externes \u00e0 traiter au sein de notre data warehouse. Tes missions seront les suivantes :\ntravailler en \u00e9troite collaboration avec tous les autres membres de la squad\n\u00e9crire / relire du code en respectant les bonnes pratiques de d\u00e9veloppement ainsi que les tests unitaires et participer\nassurer la co-responsabilit\u00e9 du d\u00e9roulement des d\u00e9ploiements, des mises en production et du bon fonctionnement des applications avec les autres membres de la squad\nr\u00e9diger la documentation technique quand cela est n\u00e9cessaire\nmettre en \u0153uvre les bonnes pratiques relatives au RGPD telles que d\u00e9finies par le tech lead\nCe CDI bas\u00e9 \u00e0 Paris centre (1er arrondissement) d\u00e9butera d\u00e8s que possible.\nFaire partie de relevanC, qu\u2019est-ce que \u00e7a signifie ?\nTravailler sur une stack technologique de pointe (Python, PySpark, Google BigQuery, Apache, Airflow\u2026)\n\u00catre membre \u00e0 part enti\u00e8re d\u2019une \u00e9quipe dynamique et passionn\u00e9e aux profils tr\u00e8s vari\u00e9s (chefs de projets, d\u00e9veloppeurs, designers, animations commerciales)\nTravailler dans un environnement stimulant et relever des nouveaux d\u00e9fis chaque jour\nRejoindre une entreprise en pleine expansion avec des opportunit\u00e9s fortes de d\u00e9veloppements et d\u2019innovation\nProfil recherch\u00e9\nDipl\u00f4m\u00e9(e) d\u2019une grande \u00e9cole d\u2019ing\u00e9nieur ou profil universitaire sp\u00e9cialis\u00e9 en Data / Informatique / Math / Stats.\n5 ans (et plus) d\u2019exp\u00e9rience en Data Engineering\nApp\u00e9tence forte pour le marketing digital et le retail, force de proposition, business oriented et moteur d\u2019innovation\nUne maitrise parfaites des bonnes pratiques de d\u00e9veloppement\nSolides comp\u00e9tences en Python, Spark et SQL\nUne exp\u00e9rience sur Google Cloud Platform est un plus\nLien vers notre politique de traitement des donn\u00e9es : https://relevanc.com/fr/politique-de-protection-des-donn%C3%A9es-recrutemen\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Chef",
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F) - Lille - CDI",
        "company": "METEOJOB by CleverConnect",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-lille-cdi-at-meteojob-by-cleverconnect-3858145866?position=3&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=tvqEla8Q4uUopai9kdIyDw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Entreprise\nChez LJE Solutions, nous pla\u00e7ons l\u2019humain au c\u0153ur de chaque projet. Au-del\u00e0 des comp\u00e9tences, nous valorisons les\naspirations\net les\nvaleurs\nde chaque individu.\nNous intervenons dans tous les secteurs d'activit\u00e9 en France et en Suisse.\nDescription Du Poste\nLJE Solutions recherche pour un de ses clients bas\u00e9 \u00e0 Lille, un/une Data Engineer.\nNotre client est une\nESN dynamique bas\u00e9e \u00e0 Lille, qui se distingue dans l'int\u00e9gration et la restitution de donn\u00e9es. Partenaire privil\u00e9gi\u00e9 de technologies de pointe comme Power BI, Tableau et Qlik, il recherche des talents d\u00e9sireux de participer \u00e0 notre aventure entrepreneuriale.\nNous recherchons un Data Engineer curieux et motiv\u00e9 pour jouer un r\u00f4le cl\u00e9 dans l'organisation et le d\u00e9veloppement de l'agence. Ce poste offre une opportunit\u00e9 unique de travailler directement avec les fondateurs, experts en technologies, et de contribuer significativement \u00e0 la formation interne et \u00e0 l'expertise chez nos clients.\nVos Responsabilit\u00e9s\nTravailler en \u00e9troite collaboration avec les fondateurs sur des projets d'int\u00e9gration et de restitution de donn\u00e9es,\nParticiper activement \u00e0 la croissance de l'entreprise en apportant des id\u00e9es innovantes et en prenant part \u00e0 des projets vari\u00e9s,\nMonter en comp\u00e9tence techniquement, avec la possibilit\u00e9 d'\u00e9voluer vers des r\u00f4les de Team Lead ou Tech Lead selon vos aspirations.\nCette entreprise offre un environnement convivial et ambitieux, encourageant la prise d'initiative. Leur structure \u00e0 taille humaine valorise chaque collaborateur, avec une approche personnalis\u00e9e et une hi\u00e9rarchie plate qui favorise l'expression et la participation active de tous.\nR\u00e9mun\u00e9ration Et Avantages\nPoste bas\u00e9 \u00e0 Lille, avec possibilit\u00e9 de t\u00e9l\u00e9travail partiel,\nR\u00e9mun\u00e9ration comp\u00e9titive bas\u00e9e sur l'exp\u00e9rience, fourchette indicative de 44k \u00e0 48k \u20ac en fixe, + variables,\nTickets restaurant,\nMutuelle d'entreprise.\nDescription Du Profil\nPassion pour les technologies de la data, avec une expertise ou un int\u00e9r\u00eat pour XDi et Talend, sans exclure d'autres ETL du march\u00e9,\nPlus de 4 ans d'exp\u00e9rience dans le domaine de la data engineering,\nCuriosit\u00e9 intellectuelle, agilit\u00e9, excellent savoir-\u00eatre, forte capacit\u00e9 de travail en \u00e9quipe et de partage de connaissances,\nLocalisation \u00e0 Lille ou disposition \u00e0 d\u00e9m\u00e9nager, avec une pr\u00e9f\u00e9rence pour les candidats de la r\u00e9gion pour faciliter la collaboration et le partage au sein de notre agence physique.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Collaboration",
                "Organisation"
            ],
            "EnSoftSkils": [
                "Collaboration",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Ing\u00e9nieur Data ETL",
        "company": "Klanik",
        "location": "Valbonne, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-etl-at-klanik-3918894069?position=4&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=Yal%2Ffuu%2F8kYrTIr2bQuwVw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Nous recherchons un profil Data ETL exp\u00e9riment\u00e9 orient\u00e9 sur les API pour rejoindre notre \u00e9quipe dynamique. Le candidat id\u00e9al doit avoir une expertise approfondie dans la conception, le d\u00e9veloppement et la gestion d'API, en particulier dans les environnements SOAP, UI et REST. Ce r\u00f4le n\u00e9cessite la capacit\u00e9 de travailler avec des API existantes, de les \u00e9valuer, de les am\u00e9liorer et de proposer des solutions innovantes pour r\u00e9pondre aux besoins de notre client.\nLe profil recherch\u00e9 devra g\u00e9rer la configuration de notre outil d'injection de donn\u00e9es, automatiser les scripts de population de donn\u00e9es, coordonner les activit\u00e9s de traitement des rejets et construire des vues de surveillance.\nResponsabilit\u00e9s :\nConcevoir, d\u00e9velopper et impl\u00e9menter des API performantes et \u00e9volutives.\nMettre en place de l'outil de transformation et d'injection interne existant en d\u00e9finissant le s\u00e9quen\u00e7age des appels API et le mappage des donn\u00e9es avec les appels API en tenant compte des exigences du client.\nCollaborer avec les \u00e9quipes techniques pour int\u00e9grer efficacement les API dans nos applications et syst\u00e8mes.\nAnalyser et am\u00e9liorer les API existantes pour optimiser les performances et la s\u00e9curit\u00e9.\nProposer des solutions innovantes pour r\u00e9soudre les probl\u00e8mes et am\u00e9liorer l'exp\u00e9rience utilisateur.\nAssurer la documentation compl\u00e8te des API d\u00e9velopp\u00e9es ou modifi\u00e9es.\nR\u00e9diger un plan de test principal et concevoir des cas de test pour valider la configuration de l'outil, d\u00e9velopper des scripts de cas de test automatis\u00e9s le cas \u00e9ch\u00e9ant et enrichir les suites de tests de r\u00e9gression sur la base du plan de test d\u00e9fini.\nAnalyser les API des services web et la documentation des \u00e9crans d'interface utilisateur pour \u00e9laborer des documents de cartographie d'interface.\nMaintenir des scripts pour la population et la migration des donn\u00e9es, en utilisant Python et VBA.\nValider et v\u00e9rifier les configurations livr\u00e9es \u00e0 nos clients.\nSuivre les donn\u00e9es pour les KPI afin de mesurer l'effort de l'\u00e9quipe, et contribuer \u00e0 la cr\u00e9ation de rapports.\nAppliquer le mod\u00e8le de gouvernance concernant la propri\u00e9t\u00e9 des donn\u00e9es, l'acc\u00e8s aux donn\u00e9es et le cycle de vie des changements de donn\u00e9es.\nComp\u00e9tences Requises :\nExp\u00e9rience pratique significative dans le d\u00e9veloppement d'API, y compris SOAP, UI et REST.\nMa\u00eetrise des langages de programmation courants pour le d\u00e9veloppement d'API (comme Python, Java, Node.js, etc.).\nCompr\u00e9hension approfondie des bonnes pratiques de conception d'API, de la s\u00e9curit\u00e9 et de la gestion du cycle de vie des API.\nCapacit\u00e9 \u00e0 travailler efficacement dans un environnement agile, en \u00e9quipe multidisciplinaire.\nSolides comp\u00e9tences en r\u00e9solution de probl\u00e8mes et capacit\u00e9 \u00e0 travailler de mani\u00e8re autonome.\nQualifications Additionnelles :\nDipl\u00f4me (bac+5 ou dipl\u00f4me d'ing\u00e9nieur) en informatique, g\u00e9nie logiciel, ou exp\u00e9rience \u00e9quivalente.\nExp\u00e9rience pr\u00e9alable dans le d\u00e9veloppement de solutions de donn\u00e9es ou d'int\u00e9gration.\nInformations Compl\u00e9mentaires :\nCe poste offre l'opportunit\u00e9 de travailler dans un environnement stimulant, o\u00f9 l'innovation et la collaboration sont encourag\u00e9es. Si vous \u00eates passionn\u00e9 par le d\u00e9veloppement d'API et la gestion de donn\u00e9es, et que vous souhaitez contribuer \u00e0 des projets stimulants et agiles, alors vous \u00eates la personne que nous recherchons !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "R\u00e9solution de probl\u00e8mes",
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Tech Lead Data Engineer",
        "company": "AXA en France",
        "location": "Hauts-de-Seine, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/tech-lead-data-engineer-at-axa-en-france-3905641945?position=5&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=GT9b1dyWWtVSzrqUxtYdmg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Environnement\nEn tant que\nTech Lead Data Engineer F/H\n, vous allez contribuer directement aux projets des directions m\u00e9tier (ex : fraude sant\u00e9, multi\u00e9quipements, pricing IARD, optimisation du lead management, fragilit\u00e9 auto, \u2026) d\u2019AXA France et \u00e0 la construction du socle technique Big Data.\nVous allez int\u00e9grer une \u00e9quipe d'une dizaine de personne compos\u00e9e de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus m\u00e9tier de la Direction Transformation Digital Tech et DATA (DT2).\nLa Direction Transfo. & Tech. d'AXA France en quelques mots :\nUne organisation agile en feature teams : tribus, guildes, squads\nDes projets sur des applications innovantes \u00e0 fort trafic (web, mobile\u2026)\nDes m\u00e9thodologies craft (TDD, BDD, clean code, code review\u2026) et DevOps\nUne communaut\u00e9 de partage de bonnes pratiques (BBL, dojo, meetup, conf\u2026)\nVotre r\u00f4le et vos missions\nVous aurez pour missions principales de d\u00e9velopper les projets Big Data demand\u00e9s par le m\u00e9tier, et notamment :\nD\u2019accompagner techniquement les Data Engineer de l\u2019\u00e9quipe (coaching, code review, pair programming\u2026)\nPasser de la donn\u00e9e brute \u00e0 de la donn\u00e9e exploitable, expos\u00e9e sous forme de tables requ\u00eatables dans le datalake\nConsolider ces donn\u00e9es au fur et \u00e0 mesure de leur alimentation r\u00e9currente dans le data lake\nLes exploiter pour atteindre la finalit\u00e9 business (exposition de business view, r\u00e9int\u00e9gration des r\u00e9sultats dans le SI, service de scoring, \u2026)\nDe travailler \u00e0 la cr\u00e9ation du socle technique Big Data et industrialiser le cycle de d\u00e9veloppement de l'\u00e9quipe\nDe mettre en place et de garantir le respect dans la dur\u00e9e d'un processus qualit\u00e9 sur l'ensemble du cycle de DEV (documents, tests unitaires / int\u00e9gration / fonctionnels, commentaires, versionning, etc.)\nVotre profil\nD'une formation sup\u00e9rieure en informatique ou scientifique (Master ou Dipl\u00f4me d'ing\u00e9nieur), vous justifiez de plusieurs exp\u00e9riences significatives (+ de 7 ans)\nsur du d\u00e9veloppement big data, en particulier sur du PySpark.\nComp\u00e9tences techniques :\nConnaissances avanc\u00e9es en d\u00e9veloppement en\nPySpark (Spark avec le langage Python)\nMaitrise de l'environnement\nMicrosoft Azure\nConnaissances avanc\u00e9es d'outils de BI comme\nPowerBI\nComp\u00e9tences transverses :\nCapacit\u00e9 \u00e0 interagir avec des parties prenantes diverses : Business analyst, Architectes, M\u00e9tier\nExp\u00e9rience en mode de delivery Agile (Scrum, Kanban, etc...)\nDriver et accompagner des Data Engineer sur le plan op\u00e9rationnel\nEt Id\u00e9alement :\nAvoir une exp\u00e9rience en tant que lead\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA\nMaitrise des Traitements Big Data en mode Streaming avec Kafka\nMaitrise des Bases de donn\u00e9es relationnelles et NoSQL\nUne exp\u00e9rience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nMais pourquoi AXA France ?\nNous sommes persuad\u00e9s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs ! Les avantages que nous proposons \u00e0 nos salari\u00e9s sont nombreux.\nNous choisir, c\u2019est b\u00e9n\u00e9ficier par exemple :\nD\u2019un package de r\u00e9mun\u00e9ration complet comprenant un salaire fixe, un compl\u00e9ment de r\u00e9mun\u00e9ration variable, des primes, de la participation et de l\u2019int\u00e9ressement, la possibilit\u00e9 d\u2019acqu\u00e9rir des actions AXA, ou encore des solutions d\u2019\u00e9pargne avantageuses ;\nEquilibre vie Pro / Perso. : D\u2019un cadre de travail flexible jusqu\u2019\u00e0 3 jours de t\u00e9l\u00e9travail possible par semaine, des tickets restaurant pour les jours t\u00e9l\u00e9travaill\u00e9s ou encore une participation \u00e0 l\u2019achat d\u2019un \u00e9cran ou fauteuil ergonomique ;\nD\u2019une politique visant \u00e0 concilier vie personnelle et vie professionnelle avec 28 jours de cong\u00e9s pay\u00e9s, entre 14 et 16 RTT selon les ann\u00e9es, des formules de travail \u00e0 temps partiel ou encore des jours d\u2019absence r\u00e9mun\u00e9r\u00e9es pour la rentr\u00e9e scolaire ou un d\u00e9m\u00e9nagement par exemple ;\nDe la possibilit\u00e9 de s\u2019engager pour une cause qui vous tient \u00e0 c\u0153ur gr\u00e2ce \u00e0 nos associations telles que AXA Atout C\u0153ur, AXA Comp\u00e9tences Solidaires ou encore AXA Pr\u00e9vention ;\nEt bien plus encore ! Perspectives de d\u00e9veloppement des comp\u00e9tences et de carri\u00e8res immenses, CE, conciergerie, offres privil\u00e8ges, soutien en cas d\u2019\u00e9preuve personnelle\u2026On s\u2019arr\u00eate l\u00e0, la liste est longue\nQui sommes nous ?\nAXA est un des leaders de l\u2019assurance et de la gestion d\u2019actifs dans le monde.\nNous aidons nos 108 millions de clients \u00e0 traverser les petites et grandes difficult\u00e9s de la vie.\nChaque jour, nous agissons ensemble pour inventer la meilleure mani\u00e8re de les prot\u00e9ger et voulons donner \u00e0 chacun les moyens de vivre une vie meilleure.\nUn challenge qui donne le sourire et envie de se lever le matin !\nChez AXA, nous sommes persuad\u00e9s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C\u2019est pour cette raison que nous menons une politique RH engag\u00e9e qui favorise la diversit\u00e9, qui pr\u00e9serve l\u2019\u00e9quilibre vie priv\u00e9e-vie professionnelle et acc\u00e9l\u00e8re le d\u00e9veloppement des comp\u00e9tences et des carri\u00e8res.\nAinsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une v\u00e9ritable culture d\u2019expertise, acc\u00e9l\u00e9rant le d\u00e9veloppement des comp\u00e9tences de chacun et proposant une r\u00e9mun\u00e9ration attractive.\nPourquoi nous rejoindre ?\nVous \u00eates porteur d\u2019id\u00e9es et d\u2019initiatives innovantes ? Vous proposez des solutions et \u00eates au service du client ? Faites partie de notre grande famille en rejoignant\nUn leader mondial offrant des opportunit\u00e9s de carri\u00e8res int\u00e9ressantes\nUne entreprise qui donne une place de choix \u00e0 l\u2019innovation, \u00e0 l\u2019initiative et aux actions solidaires (notamment via l\u2019association AXA Atout C\u0153ur)\nUn environnement inclusif \u00e0 tous les niveaux (mixit\u00e9, handicap, initiatives pour favoriser l\u2019insertion des jeunes, orientation sexuelle, etc.)\nUn acc\u00e8s \u00e0 de multiples avantages (cong\u00e9s, temps partiel, t\u00e9l\u00e9travail, etc.)\nUn cadre stimulant, qui permet de rencontrer des collaborateurs performants et d\u2019enrichir ses comp\u00e9tences\nVictime ou t\u00e9moin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination \u00e0 alerte.discrimination.harcelement@axa.fr\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "JIRA",
                "Teams"
            ],
            "Other": [
                "DevOps",
                "Big Data"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [
                "3",
                "3"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "7",
                "7",
                "7"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data engineer H/F",
        "company": "Akkodis",
        "location": "Greater Lille Metropolitan Area",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-akkodis-3890779946?position=6&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=Oa7lMHaesBbGmnh3YiFQig%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La ligne de service Consulting & Solutions d\u2019Akkodis France renforce ses \u00e9quipes en r\u00e9gion Hauts-de-France et recrute un\nData engineer H/F\nen\nCDI\nsur la\nm\u00e9tropole lilloise\n:\nDescription de la mission :\nConcevoir, mettre en oeuvre et maintenir des pipelines de donn\u00e9es efficaces et \u00e9volutifs dans un environnement cloud (comme AWS, Azure, Google Cloud Platform\u2026)\nAssurer la qualit\u00e9 des donn\u00e9es et des mod\u00e8les\nD\u00e9finir les bonnes pratiques de d\u00e9veloppement en impl\u00e9mentant des outils de CI/CD\nAssurer une veille technologique sur les technologies Cloud\nCapacit\u00e9 \u00e0 interagir avec des parties prenantes diverses : business analyst, architecte, m\u00e9tier\u2026\nVeiller au bon fonctionnement des pipelines en production\nProfil :\nDe formation\nBac +4/5 en informatique\nou issu d'une\n\u00e9cole d'ing\u00e9nieur\n, vous poss\u00e9dez une exp\u00e9rience de\n3 ans\nminimum en tant que data engineer ainsi que les comp\u00e9tences suivantes :\nUne bonne connaissance des \u00e9cosyst\u00e8mes li\u00e9s \u00e0 la data (Kafka, ETL, base de donn\u00e9es\u2026)\nUne premi\u00e8re exp\u00e9rience sur un cloud provider (AWS, Azure, GCP)\nUne bonne maitrise de langages de programmation tels que SQL, Python, Scala\nAkkodis accompagne ses clients dans la mondialisation de leurs projets, aussi un anglais courant est requis pour l\u2019ensemble de nos collaborateurs.\nProcessus de recrutement :\nUne charg\u00e9e de recrutement vous contacte pour \u00e9changer sur votre projet professionnel\nVous \u00e9changez ensuite avec un.e manager sur les aspects techniques, les projets\nChez Akkodis nous sommes convaincus que de l\u2019intelligence collective na\u00eet le succ\u00e8s. Il n\u2019existe pas qu\u2019un mod\u00e8le, nous valorisons l\u2019agilit\u00e9 et l\u2019excellence, l\u2019audace et la cr\u00e9ativit\u00e9.\nEt si nous parlions ensemble de vos ambitions pour les prochaines ann\u00e9es ?\nAkkodis est une entreprise handi-engag\u00e9e et inclusive. Tous nos postes sont ouverts aux handicaps et \u00e0 la diversit\u00e9. Tous diff\u00e9rents, tous comp\u00e9tents !\nAkkodis, est un acteur mondial de l\u2019ing\u00e9nierie et de l\u2019IT et un leader dans la smart industrie. Nous accompagnons nos clients dans leurs projets de transformation digitale via 4 lignes de service : Consulting, Solutions, Talent et Academy. Akkodis est un partenaire technologique de confiance pour ses clients \u00e0 l\u2019\u00e9chelle internationale. Nous co-cr\u00e9ons et nous imaginons des solutions de pointe pour r\u00e9pondre aux d\u00e9fis majeurs de notre soci\u00e9t\u00e9, qu'il s'agisse d'acc\u00e9l\u00e9rer la transition \u00e9nerg\u00e9tique et de d\u00e9velopper la mobilit\u00e9 verte, ou encore de construire des approches centr\u00e9es sur les utilisateurs.\nDot\u00e9s d\u2019une forte culture de l\u2019inclusion et de la diversit\u00e9, nos 50 000 experts en IT et en ing\u00e9nierie, pr\u00e9sents dans 30 pays, allient les meilleures comp\u00e9tences technologiques \u00e0 une connaissance transverse de toutes les industries pour fa\u00e7onner un futur plus durable. Nous sommes passionn\u00e9s par l\u2019id\u00e9e d\u2019inventer ensemble un avenir meilleur.\nAkkodis en France, ce sont pr\u00e8s de 9.000 experts en IT et en ing\u00e9nierie r\u00e9partis sur l'ensemble du territoire, des collaborateurs partageant des valeurs fortes d'honn\u00eatet\u00e9, de respect, d'\u00e9quit\u00e9 et d'inclusion. Notre engagement : leur permettre au quotidien d'\u00eatre eux-m\u00eames au travail, et acteurs de leur vie et de leur d\u00e9veloppement au sein d'Akkodis.\n*Akkodis est une marque commerciale sous laquelle les entit\u00e9s AKKA et Modis op\u00e8rent\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Cr\u00e9ativit\u00e9"
            ],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Mobiskill | WEFY Group",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-mobiskill-wefy-group-3907391938?position=7&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=eT3vGYULy5bSpvwf%2FX9gYw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La soci\u00e9t\u00e9 ?\nCette startup a \u00e9t\u00e9 cr\u00e9\u00e9e en 2018 et vise \u00e0 aider la prise de d\u00e9cision de ses clients qui sont principalement dans le secteur du retail ou de l'alimentaire.\nIls permettent d'enrichir la donn\u00e9e afin d'am\u00e9liorer la strat\u00e9gie de vente et marketing d'une entreprise gr\u00e2ce \u00e0 leur plateforme Saas bas\u00e9e sur des algorithmes d'IA.\nIls ont besoin de renforcer leur \u00e9quipe en Data Engineering pour g\u00e9rer au mieux leur volum\u00e9trie.\nLes missions ?\n- Editer le cahier des charges des donn\u00e9es \u00e0 collecter aupr\u00e8s de nos partenaires distributeurs\n- Prendre en main la gestion de la donn\u00e9e dans le cloud de la soci\u00e9t\u00e9 pour optimiser les co\u00fbts et l\u2019efficacit\u00e9 des analyses effectu\u00e9es par l\u2019\u00e9quipe Analytics\n- Anticiper les \u00e9volutions et participer aux choix structurants de la soci\u00e9t\u00e9 li\u00e9s \u00e0 la gestion de la data\nLe profil recherch\u00e9 ?\n- Avoir 2/3 ans d'exp\u00e9rience en Data Engineering (hors stage et alternance)\n- Avoir pu travaill\u00e9 en Python comme langage de programmation\nAvoir travaill\u00e9 au moins deux ans et si possible sur des sujets d'optimisation avec Spark !\n- La ma\u00eetrise des outils tels Airflow, Kafka et Snowflake seraient un plus appr\u00e9ci\u00e9\n- Ma\u00eetriser un des cloud providers et si possible avoir une exp\u00e9rience sur Azure\nPourquoi les rejoindre ?\n- Une soci\u00e9t\u00e9 stable financi\u00e8rement (fonds propres uniquement)\n- Une startup en pleine croissance\n- Une r\u00e9mun\u00e9ration en fonction de votre s\u00e9niorit\u00e9\n- Volum\u00e9trie de donn\u00e9es incroyable, il y a de quoi s'amuser !\n- Faire parti de l'unique retail-tech qui a un impact \u00e9cologique positif (fin des prospectus, \u00e9viter le g\u00e2chis alimentaire)\nH\u00e2te de vous en dire plus rapidement !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer / Data Ops",
        "company": "FRG Technology Consulting",
        "location": "\u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-data-ops-at-frg-technology-consulting-3913842168?position=8&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=cQ8SitVQDCHcJKbOT0F5og%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous \u00eates un expert passionn\u00e9 par la Data et \u00e0 la recherche de d\u00e9fis excitants ? Mon client recherche actuellement un\nData Engineer\n/ Data Ops\ntalentueux pour rejoindre une \u00e9quipe dynamique et humaine.\nMissions principales :\nParticipation active au d\u00e9ploiement de la nouvelle plateforme sur Azure & Snowflake\nForte autonomie et gestion compl\u00e8te des projets data\nAnalyse des besoins actuels et futurs\nCr\u00e9ation de sp\u00e9cifications fonctionnelles et techniques\nMod\u00e9lisation de donn\u00e9es\nD\u00e9veloppement de packages SSIS\nInt\u00e9gration des donn\u00e9es dans SnowFlake & Azure,\nCr\u00e9ation de rapports avec Power BI et Excel\nProfil recherch\u00e9 :\n3 \u00e0 4 ans d'exp\u00e9rience\nminimum\ndans la BI (SSIS, SQLServer, SSAS, SSRS) et/ou le cloud (Azure , Snowflake) ainsi qu'en SQL\nComp\u00e9tences en\narchitecture sur Snowflake\nfortement appr\u00e9ci\u00e9es\n1 \u00e0 2 ans d'exp\u00e9rience en tant que DevOps ( CI/CD ; GitLab)\nAutonome, rigoureux et anglais courant\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [
                "3"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "4",
                "4",
                "4"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data engineer H/F",
        "company": "Extia",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-extia-3599188121?position=9&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=pq5NdvpIJcK61%2FJVu1Ybeg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous souhaitez rejoindre une entreprise qui place l\u2019humain au c\u0153ur de ses pr\u00e9occupations ? On vous attend chez\nExtia\n!\nSoci\u00e9t\u00e9 de conseil sp\u00e9cialis\u00e9e dans les m\u00e9tiers de l\u2019IT, de l\u2019ing\u00e9nierie et du digital, Extia privil\u00e9gie depuis sa cr\u00e9ation en 2007 une approche qui allie performance et bien-\u00eatre au travail. Une vision de l\u2019entreprise partag\u00e9e aujourd\u2019hui par plus de 2 500 Extiens en France et \u00e0 l'international et r\u00e9compens\u00e9e par le label Great Place to Work\u00ae depuis 13 ans, notamment en\n2024 o\u00f9 les Extiens se hissent \u00e0 la premi\u00e8re place du palmar\u00e8s Best Workplaces France\n!\nChez Extia, c\u2019est \u00ab D\u2019abord qui, ensuite quoi \u00bb alors, allons-y !\nD'abord qui\nVous \u00eates habitu\u00e9 \u00e0 travailler aussi bien avec des m\u00e9ta-donn\u00e9es qu\u2019avec des donn\u00e9es non-structur\u00e9es. A cet effet vous maitrisez un ou plusieurs des concepts comme l\u2019ETL, le Data mining le Machine learning, les Big data ou encore la Th\u00e9orie des graphes par exemple,\nVous maitrisez les bases de l\u2019analyse statistique,\nVous \u00eates apte \u00e0 r\u00e9diger des scripts en Python et/ou R, et une connaissance d'autres langages de programmation comme Java, Scala ou SAS est un plus,\nVous maitrisez Spark et Hadoop\nVous \u00eates familiaris\u00e9 avec l\u2019environnement Linux,\nUne exp\u00e9rience avec les outils de Stockage de fichiers volumineux (HDFS, Data Lake, S3, stockage Blob), la connaissance des infrastructures cloud AWS ou GCP et des bases en streaming temps r\u00e9el seront aussi de r\u00e9els atouts.\nEnsuite quoi\nVous aurez le r\u00f4le de support technique aux \u00e9quipes d\u2019analyse : structurer les donn\u00e9es, r\u00e9aliser des analyses \u00ab statistiques \u00bb ou \u00ab techniques \u00bb sur les donn\u00e9es, d\u00e9velopper des outils d\u2019analyse\u2026\nVous m\u00e8nerez des \u00e9tudes afin d\u2019\u00e9valuer les nouvelles technologies dans le domaine du Big Data, Data Mining ou Machine Learning afin d\u2019identifier les solutions les plus pertinentes.\nVous serez en charge de :\nParticiper \u00e0 la d\u00e9finition des besoins et \u00e0 la r\u00e9daction des User Stories,\nCollaborer avec les Data Scientists au d\u00e9veloppement des modules d\u2019analyse de donn\u00e9e,\nConcevoir et construire des architectures de donn\u00e9es,\nInt\u00e9grer des sources de donn\u00e9es,\nVous assurez que les donn\u00e9es sont facilement accessibles et que leur exploitation fonctionne comme demand\u00e9, m\u00eame dans des circonstances hautement \u00e9volutives,\nEx\u00e9cuter des processus ETL (extraire / transformer / charger) \u00e0 partir d'ensembles de donn\u00e9es complexes et / ou volumineux\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [
                "Statistiques"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [
                "Linux"
            ],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Machine Learning",
                "Statistiques",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "13",
                "13",
                "13"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (Snowflake)",
        "company": "MindPal",
        "location": "Lyon, Auvergne-Rh\u00f4ne-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3910994899?position=10&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=JBJP3r8cDISpd7T04G1vsQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer Talend F/H",
        "company": "Orange Business",
        "location": "Greater Lille Metropolitan Area",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-talend-f-h-at-orange-business-3916552363?position=1&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=B7xw7hTucHearR4LMUncOg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "L\u2019ambition d\u2019Orange Business est de devenir l\u2019int\u00e9grateur r\u00e9seau et num\u00e9rique de r\u00e9f\u00e9rence en Europe, en nous appuyant sur nos forces autour des solutions de connectivit\u00e9 nouvelle g\u00e9n\u00e9ration, du cloud et de la cybers\u00e9curit\u00e9.\nNos 30 000 femmes et hommes pr\u00e9sents dans 65 pays, dont chaque voix compte, sont tous anim\u00e9s par la m\u00eame d\u00e9termination et le m\u00eame esprit d\u2019\u00e9quipe, pour construire les solutions digitales d\u2019aujourd\u2019hui et de demain et cr\u00e9er un impact positif pour nos clients, pour leurs salari\u00e9s et pour la plan\u00e8te.\nNous offrons des opportunit\u00e9s passionnantes gr\u00e2ce \u00e0 des projets innovants dans la data et le digital, le cloud, l\u2019IA, la cybers\u00e9curit\u00e9, l\u2019IoT, ou encore le digital workspace et le big data.\nVenez vivre cette aventure avec nous !\nAfin de d\u00e9velopper notre \u00e9quipe lilloise, nous recherchons aujourd'hui, un Ing\u00e9nieur DATA \u00e0 m\u00eame d\u2019accompagner nos clients dans la structuration de leurs SI autour de la donn\u00e9e.\nVos principales missions seront les suivantes\n:\n- Concevoir des solutions de traitement et collecter des volumes importants de donn\u00e9es.\n- Participer \u00e0 des \u00e9tudes de cadrage pour collecter le besoin m\u00e9tier et concevoir les solutions qui r\u00e9pondent au besoin du client.\n- Apporter son expertise sur des probl\u00e9matiques pr\u00e9cises rencontr\u00e9es chez les clients.\n- Participer \u00e0 la veille technologique\n- R\u00e9aliser les\nd\u00e9veloppements TALEND\n- Rester inform\u00e9 et former sur les nouvelles solutions DATA\n- Contribuer aux phases d'avant-vente et au d\u00e9veloppement business.\n- Participer \u00e0 la conception, l'\u00e9volution et la pr\u00e9sentation de nos offres DATA.\nVous\n:\n- \u00cates issu(e) de formation bac+5 ?\n- Vous justifiez d'au moins 3 ans d'exp\u00e9riences en qualit\u00e9 d'Ing\u00e9nieur DATA sur la solution TALEND Enterprise (Data Integration) et avez id\u00e9alement une connaissance des solutions Cloud d'AWS et d'AZURE ?\n- Vous \u00eates intervenu sur des projets int\u00e9grant des pratiques DevOps et AGILE ?\nAlors postulez, ce poste est fait pour vous !\nVos comp\u00e9tences cl\u00e9s\n:\n- Expertise sur l'outil\nETL TALEND\nEnterprise (administration et d\u00e9veloppement)\n- Fortes connaissances des solutions de bases de donn\u00e9es (SQL, NoSQL\u2026)\n- Connaissances en langages objets ou scripts (notamment Java mais aussi Javascript, Scala, Python\u2026)\n- Divers syst\u00e8mes d'exploitation : UNIX, Windows\nAutonomie, rigueur, curiosit\u00e9, dynamisme et sens du service sont des qualit\u00e9s n\u00e9cessaires pour ce poste.\nLes comp\u00e9tences compl\u00e9mentaires qui seraient appr\u00e9ci\u00e9es :\n- Connaissances d'autres modules Talend (MDM, ESB, Data Quality, Cloud\u2026)\n- Ma\u00eetrise des technologies du Big Data (Hadoop, Spark, Kafka\u2026)\n- Expertise sur d'autres outils ETL (Informatica, SSIS, DataStage...)\n- Notions en architecture des Syst\u00e8mes d'Information\n- Ma\u00eetrise de l'anglais (oral et \u00e9crit)\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "JavaScript"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [
                "Windows"
            ],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data engineer AWS/Azure",
        "company": "Apside",
        "location": "\u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-aws-azure-at-apside-3825012802?position=2&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=tCEzMqlfv%2BbUvDPus74BWA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83d\udca5\nD\u00e9couvrez la Vie Apsidienne\n\ud83d\udcf9\net vous aussi, devenez Apsidien\nOn aurait pu demander \u00e0 Chat GPT de vous d\u00e9montrer en quoi\nApside est l\u2019ESN qu\u2019il vous faut,\nmais on pr\u00e9f\u00e8re que vous le d\u00e9couvriez vous-m\u00eames \ud83d\udc47\ud83d\ude0f\n\ud83d\udd25\nD\u00e9couvrez votre future mission\n\ud83d\udc49\nContexte\nRejoignez notre Practise Cloud/Data, afin d\u2019intervenir sur des sujets \u00e0 haute valeur ajout\u00e9e !\nSecteur\n: Banque/Finance\nM\u00e9thode de travail\n: Agile\nL\u2019\u00e9quipe Data de l\u2019un de nos clients grands compte du secteur bancaire vise \u00e0 faciliter la construction de parcours transversaux par les DSI en proximit\u00e9 des M\u00e9tiers en leur proposant des solutions technologiques sur \u00e9tag\u00e8re, qu'elles pourront assembler rapidement, de mani\u00e8re agile.\nDes solutions informatiques bas\u00e9es sur les technologies BigData sont donc mises en place dont une qui est un framework de contr\u00f4les. Celle-ci est propos\u00e9e aux applications du groupe afin de les aider dans l'impl\u00e9mentation des contr\u00f4les de Data Quality sur les plates formes Bigdata on premise, ainsi que cloud Azure et AWS.\nOr ils souhaitent aujourd\u2019hui mettre en oeuvre plusieurs \u00e9volutions de son socle technique.\n\ud83d\ude0e Mission\nL'ajout de fonctionnalit\u00e9s sur le moteur de calcul\nIHM de param\u00e9trage\nCompatibilit\u00e9 avec la plateforme Azure et AWS\nD\u00e9veloppements des \u00e9volutions sur le moteur des contr\u00f4les\nTests (en TDD) en mode Agile\nContribution \u00e0 la validation de l'usage de solution en production pour les nouvelles applications utilisatrices du framework.\nEnvironnement technique\n:\nAmazon Web Services\nGitHub\nHadoop\nKubernetes\nMS Azure\nPython\nScala\nSpark\n\ud83d\udcb0\nLe package salarial que nous vous proposons\nContrat :\nCDI\nAvantages groupe :\ncarte ticket restaurant Swile, prime de mobilit\u00e9, RTT, accord t\u00e9l\u00e9travail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle \u00e0 100% etc\u2026\nAvantages agence :\nint\u00e9gration de la Practise Cloud/Data, afterworks, communaut\u00e9 techlead\nFormation :\ncertifications techniques, cours particuliers d\u2019anglais en interne, acc\u00e8s \u00e0 un catalogue de formations gr\u00e2ce \u00e0 notre plateforme e-learning (\nAcademy by Apside\n) ou via nos organismes partenaires.\n\ud83d\udd2e\n\u00d4 vous futur Apsidien, qui \u00eates-vous ?\nAu moins 5 ans d'exp\u00e9rience en tant que Data Engineer\nMaitrise de l\u2019environnement cloud AWS ou Azure\nForce de proposition, bon relationnel et autonome\n\ud83d\ude0f\nApside a suscit\u00e9 votre curiosit\u00e9 ?\nDans un environnement marqu\u00e9 par une acc\u00e9l\u00e9ration des \u00e9volutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients \u00e0 cr\u00e9er de la valeur et \u00e0 adresser leurs enjeux strat\u00e9giques en leur mettant \u00e0 disposition des expertises technologiques (\nData / IA, Cloud, Cyber\n) et une exp\u00e9rience sectorielle (\nIndustrie, Banque, Assurance, Service, Secteur Public\n). Pour un accompagnement global, le groupe propose des offres transverses autour du\nHandicap\n(Apsid\u2019EA), du\nDigital Learning\n, et du\nConseil\n.\n\ud83e\udd14\nEt votre place dans tout \u00e7a ?\n\ud83d\udc49 Notre volont\u00e9\nest de vous accompagner dans la construction et l\u2019\u00e9panouissement de votre carri\u00e8re\nen nous appuyant notamment\nsur 3 piliers :\nUne\nr\u00e9mun\u00e9ration\n\u00e0 hauteur de vos investissements et de vos comp\u00e9tences\nUne\ntrajectoire professionnelle\nstimulante sur mesure\nUn\nengagement\nautour des valeurs Apsidiennes : la qualit\u00e9 de vie et des conditions de travail au c\u0153ur de nos enjeux\nEngag\u00e9e pour\nun monde plus inclusif et plus responsable\n, Apside r\u00e9invente l\u2019ESN et propose l\u2019Engagement Soci\u00e9tal et Num\u00e9rique. D\u00e9couvrez notre d\u00e9marche RSE ainsi que notre vision de l\u2019Entreprise Engag\u00e9e.\nConvaincu ? A vous de jouer, envoyez-nous votre CV !\nRejoignez l\u2019aventure Apsidienne et d\u00e9couvrez notre vision d\u2019une ESN singuli\u00e8re et r\u00e9siliente\n\ud83d\ude80\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [
                "100"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "HarfangLab",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-harfanglab-3849760187?position=3&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=skdkpLczYGSCzfAm1n2LAw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Who we are?\nHarfangLab\nis a\ncybersecurity scale-up\n, and we have developed an\nEndpoint Detection and Response\n(EDR) software to\ndetect and mitigate modern cyberattacks\non a company's workstations and servers. Our algorithms detect abnormal behaviors and generate security alerts or block program execution.\nFrom 50 to 100 employees in 2023, HarfangLab is experiencing hypergrowth and has already achieved several significant milestones: winners of the Ministry of Defense's cyber challenge in 2019, recipients of the BPI's I-Nov competition in 2020, and software certified by ANSSI in 2021.\nOur initial clients include CAC40 industrial companies and government entities. We completed our\nfirst funding round of \u20ac5 million in 2021 and our second funding round of \u20ac25 millions in 2023\n, which will enable us to strengthen our teams, and to expand internationally in Europe.\nOur mission is to\nprotect businesses and government agencies from modern cybersecurity threats\n(cybercrime, data theft, influence)\nthat endanger the economic health of companies and the security of the nation\n.\nWhat you will do with us?\nYou will work within the\nArtificial Intelligence team\n, consisting of 5 individuals, under the direct and daily supervision of the team lead.\nThis team designs, implements, and deploys supervised algorithms for detecting malicious behavior.\nAs a\nData Engineer\nyou will:\nGather requirements from stakeholders,\nManage data for the AI and CTI departments,\nDesign, develop, and maintain the existing data warehouse,\nImplement a data lake if deemed appropriate,\nCreate data pipelines using ELT processes,\nDesign tools for data visualization.\nAbout You\nHard Skills\nMaster\u2019s degree in Computer Science, Engineering, or a related field,\nProven experience as a Data Engineer, 2 years minimum,\nProficient in Python,\nSQL: Strong in SQL syntax and query optimization, NoSQL will be a big plus,\nCompetence in data warehousing and data lake architecture,\nProficiency in at least one ELT tool and strong understanding of related processes.\nSoft Skills\nStrong communication and teamwork skills,\nExcellent problem-solving and attention to detail,\nYou enjoy learning and sharing your knowledge with others,\nYou demonstrate initiative - when an opportunity arises to improve existing processes, you seize it.\nAbout Us\nOur office and Team Life:\nOffices located in the heart of Paris, near Bourse (75002),\nHigh-quality equipment based on preferences and needs (PC, Mac, additional screens, etc.),\nThanks to our Office Manager, we regularly organize events such as seminars, happy hours, themed evenings, and more,\nAn onboarding process to welcome each new colleague with an explanation of the roles and a mentor to support you during your early days!\"\nA great team that always seeks to improve their skills\nAnd more:\nAn attractive package: Base salary + profit sharing,\nFlexible remote work options,\nA mentor to guide you throughout your probationary period,\nHealth insurance: The best health insurance with Alan and Moka Care, a mental health at work app,\nMeal vouchers: We use the Swile card and also have access to a discount platform through our works council,\n7 to 11 additional days off (RTT) per year, in addition to the 25 days of paid vacation. GymLib subscription, covered 80% by HarfangLab,\nAccess to training and events of your choice and according to your professional needs.\nThe recruitment process\nA 30-minutes call with our Talent Acquisition Manager,\nA 30-minutes visio interview with the Hiring Manager,\nA 1 hour on-site interview + 30 minutes with the team for a team fit assessment,\nA psychometric test to assess your motivations and soft skills,\nA final HR video appointment to review your soft skills and motivations.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication",
                "Teamwork",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": [],
            "Salary": [
                "7",
                "7"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Lead Data Engineer",
        "company": "Ippon Technologies",
        "location": "Greater Paris Metropolitan Region",
        "link": "https://fr.linkedin.com/jobs/view/lead-data-engineer-at-ippon-technologies-3851535052?position=4&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=SDvZzVRJfCa5AUR0lcL0eQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Cabinet de conseil et d'expertise en technologie, international et ind\u00e9pendant.\nEn quelques mots : 700 passionn\u00e9s de tech, 12 agences dans le monde, 6 communaut\u00e9s d\u2019expertise d\u2019excellence, contributeur actif et sponsor de l'\u00e9cosyst\u00e8me num\u00e9rique, des publications soutenues et reconnues sur nos r\u00e9seaux.\nRejoignez notre communaut\u00e9 de 70 experts en data, dont 30 \u00e0 Paris, o\u00f9 la collaboration dynamique entre data engineers, data analysts et data architects est le moteur de notre succ\u00e8s. Avec une communication proactive sur des canaux internes, restez constamment inform\u00e9 des derni\u00e8res tendances, participez \u00e0 des discussions stimulantes et contribuez \u00e0 l'organisation d'\u00e9v\u00e9nements passionnants (dataday, datap\u00e9ro, datalunch\u2026).\nFaites partie d'une \u00e9quipe o\u00f9 l'innovation et l'engagement sont les cl\u00e9s de notre excellence collective !\nNotre sp\u00e9cialit\u00e9 ? construire des data platforms dans le cloud public avec les meilleures technos du moment.\nEn tant que tech lead, tu interviendras sur la cr\u00e9ation d'un entrep\u00f4t de donn\u00e9es pour les KPIs d\u2019un grand groupe dans le secteur de l\u2019\u00e9nergie. Le but \u00e9tant de leur permettre de superviser leurs activit\u00e9s afin de supporter leurs d\u00e9cisions strat\u00e9giques.\nTon r\u00f4le :\nIntervenir sur l\u2019architecture et le d\u00e9veloppement d\u2019une pipeline d'alimentation de donn\u00e9es\nTravailler sur la mod\u00e9lisation et l\u2019impl\u00e9mentation de l'entrep\u00f4t de donn\u00e9es\nConseiller et accompagner les \u00e9quipes dans la r\u00e9alisation des dashboards de suivi des KPIs\nDevOps: projet enti\u00e8rement Terraform\u00e9 (ressources + droits), CI/CD Gitlab, administration GCP\nFaire une veille technologique active et partager tes connaissances en interne\nTravailler en collaboration avec les m\u00e9tiers et les data analysts pour leur fournir un support \u00e0 l\u2019industrialisation de leurs travaux (tests, int\u00e9gration continue, scalabilit\u00e9 des mod\u00e8les, craftsmanship etc\u2026)\nSi tu le souhaites, tu pourras \u00e9galement :\nParticiper aux \u00e9v\u00e8nements internes \u00e0 la communaut\u00e9 data (BBL, webinar, datap\u00e9ro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups)\nCapitaliser sur les missions et les diff\u00e9rents \u00e9v\u00e8nements de la communaut\u00e9 au travers d\u2019articles de blogs, REX, BBL interne.\nTes connaissances :\nTu ma\u00eetrises le d\u00e9veloppement en Python\nTu as de l\u2019exp\u00e9rience dans la mise en place de pipeline de donn\u00e9es jusqu\u2019en production (CI/CD Gitlab, Terraform)\nTu as une exp\u00e9rience dans un environnement Cloud (GCP de pr\u00e9f\u00e9rence, AWS, Azure)\nTu as une bonne connaissance d\u2019un outil de visualisation (Looker Studio, Power BI)\nTu accompagnes des data engineers dans la mise en place des bonnes pratiques\nTu es capable de proposer/challenger la stack technique\nIppon c\u2019est aussi :\nTravailler en \u00e9quipe au sein d'une communaut\u00e9 data \u00e0 la pointe des \u00e9volutions\nUn suivi de proximit\u00e9 r\u00e9alis\u00e9 par ton manager (expert data)\nDevenir ceinture noire en data gr\u00e2ce \u00e0 notre programme d\u2019accompagnement de carri\u00e8re Blackbelt\nParticiper \u00e0 nos ap\u00e9ros et divers \u00e9v\u00e8nements internes pour consolider la coh\u00e9sion d\u2019\u00e9quipe\nNotre process de recrutement :\nPr\u00e9qualification t\u00e9l\u00e9phonique - 20 min\nUn entretien RH / Sales - 1H00\nUn entretien technique avec 2 consultants data\nSi le match est bon des deux c\u00f4t\u00e9s : Hadjim\u00e9 ! Tu te lanceras sur le tatami Ippon !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Big Data",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Communication",
                "Collaboration",
                "Organisation"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer \u2013 Alternance H/F",
        "company": "HUTTOPIA Jobs",
        "location": "Saint-Genis-les-Olli\u00e8res, Auvergne-Rh\u00f4ne-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-alternance-h-f-at-huttopia-jobs-3902055574?position=5&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=eGiPNGddRzkmDAThGsqSWg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Huttopia, op\u00e9rateur international reconnu du tourisme durable et acteur du d\u00e9veloppement territorial, poursuit un d\u00e9veloppement soutenu en France et \u00e0 l\u2019international (Pays-Bas, Espagne, Canada, USA, Chine\u2026). Pr\u00e9sent dans les domaines de l\u2019Hospitality avec 127 camping-nature exploit\u00e9s, les activit\u00e9s industrielles avec la fabrication d\u2019h\u00e9bergements en bois et toile, et dans le num\u00e9rique, le groupe Huttopia a g\u00e9n\u00e9r\u00e9 en 2023 un chiffre d\u2019affaires de 160 M\u20ac avec plus de 700 collaborateurs permanents et plus de 1800 personnes l\u2019\u00e9t\u00e9.\nPour accompagner son fort d\u00e9veloppement, Huttopia recrute ses talents de demain !\nNous recherchons notre futur\nData Engineer H/F\nen alternance \u00e0 compter de\nseptembre 2024.\nVOS MISSIONS :\nSous la responsabilit\u00e9 hi\u00e9rarchique du Responsable Data et sous la responsabilit\u00e9 technique de la Data Engineer, vous interviendrez comme :\nCo-responsable de\nl\u2019exploitation des donn\u00e9es\n:\nProduire et livrer des tableaux de bord (sous Tableau)\nParticiper \u00e0 l\u2019administration de l\u2019outil de Data Viz\nAccompagner et former des utilisateurs (lecteurs et d\u00e9veloppeurs)\nProduire des demandes de chiffres ad-hoc en interrogeant la BDD centrale (SQL),\nParticiper aux d\u00e9veloppements des algorithmes d\u2019exploitation des donn\u00e9es (en Python)\nParticipant actif \u00e0\nla r\u00e9cup\u00e9ration et la structuration des donn\u00e9es\n:\nD\u00e9velopper des flux de r\u00e9cup\u00e9ration des donn\u00e9es entre une source et la BDD centrale\nR\u00e9aliser la maintenance et monitoring des flux d\u00e9j\u00e0 d\u00e9velopp\u00e9s\nVOTRE PROFIL :\nVous \u00eates le candidat id\u00e9al pour rejoindre notre \u00e9quipe si \u2026\nVous pr\u00e9parez une formation sup\u00e9rieure en M1 ou M2, sp\u00e9cialis\u00e9e en DATA dans le cadre de votre alternance.\nVous avez des connaissances en langage de programmation : SQL, Python et Java. Vous souhaitez d\u00e9velopper vos comp\u00e9tences sur les ETL type Talend et les outils de Data Viz (Tableau, Power BI, Qlik..) vous sont familiers.\nAu-del\u00e0 de vos comp\u00e9tences, nous nous int\u00e9ressons \u00e0 vous, \u00e0 votre personnalit\u00e9.\nDot\u00e9 d\u2019un bon relationnel, vous \u00eates reconnu pour votre rigueur, votre sens du service et votre curiosit\u00e9 technique. Vous appr\u00e9ciez travailler en \u00e9quipe et vous \u00eates adaptable.\nEnfin, vous \u00eates attir\u00e9 par le secteur de l\u2019outdoor et notamment du tourisme !\nNous sommes l\u2019entreprise qu\u2019il vous faut si \u2026\nVous \u00eates pr\u00eat \u00e0 relever de nouveaux d\u00e9fis en rejoignant une entreprise aux collaborateurs engag\u00e9s et autonomes.\nVous souhaitez \u00e9voluer dans des bureaux en bois o\u00f9 il fait bon travailler avec des espaces pour se d\u00e9tendre et des \u00e9v\u00e8nements festifs\u2026 Parce que chez Huttopia on aime travailler s\u00e9rieusement sans se prendre au s\u00e9rieux.\nLES PLUS HUTTOPIA :\nTicket restaurant : 9.92\u20ac/jour travaill\u00e9\nCh\u00e8ques culture mensuels\nUne bonne mutuelle sant\u00e9\nLES POINTS PRATIQUES :\nContrat apprentissage ou contrat de professionnalisation\nDate de d\u00e9marrage : Septembre 2024\nDur\u00e9e du contrat : 12 mois\nM\u00e9tropole Lyonnaise \u00e0 Saint Genis les Olli\u00e8res \u2013 \u00e0 5 minutes de Tassin-La-Demi-Lune\nPrise en charge \u00e0 hauteur de 50% de l\u2019abonnement de transports en commun.\nNOTRE PROCESS DE RECRUTEMENT :\nB\u00e9rang\u00e8re, notre charg\u00e9e de recrutement vous contactera pour un premier \u00e9change en visio, vous passerez ensuite un entretien dans nos bureaux.\nVous serez inform\u00e9 \u00e0 chaque \u00e9tape de l\u2019\u00e9volution de votre candidature.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer H/F",
        "company": "Lincoln France",
        "location": "Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-lincoln-france-3829857168?position=6&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=SmxRFRPcnUVxFqCIJQZpiw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "DATA ENGINEER H/F\nCDI\n3 ans minimum\nChez Lincoln\n, nous formons une communaut\u00e9 d'innovateurs passionn\u00e9s qui red\u00e9finissent l'analyse de donn\u00e9es depuis\nplus de 30 ans\n. En tant que\nPure Player Data\n, notre expertise est reconnue dans les domaines\nde la Modern BI, du Big Data et de la Science des donn\u00e9es\n.\nNotre mission ?\nTransformer les donn\u00e9es en solutions concr\u00e8tes pour nos clients grands comptes dans divers secteurs tels que la banque, le retail, les t\u00e9l\u00e9coms, l'industrie, la sant\u00e9, etc.\nDescription de poste\nNous recherchons un\nData Engineer H/F\npour accompagner nos clients dans leurs projets strat\u00e9giques.\nVos missions :\nConcevoir et d\u00e9velopper des pipelines de donn\u00e9es robustes et \u00e9volutifs.\nInt\u00e9grer et transformer des donn\u00e9es provenant de diff\u00e9rentes sources.\nD\u00e9velopper et mettre en \u0153uvre des algorithmes de traitement de donn\u00e9es avanc\u00e9s.\nCollaborer \u00e9troitement avec les \u00e9quipes clients pour comprendre leurs besoins et fournir des solutions adapt\u00e9es.\nAssurer la qualit\u00e9 et la fiabilit\u00e9 des solutions d\u00e9velopp\u00e9es.\nPr\u00e9requis :\nMa\u00eetrise des langages de programmation (\nPython, Scala, etc\n.).\nConnaissance approfondie des bases de donn\u00e9es et des technologies\nCloud (GCP, AWS, Azure, Snowflake, etc.)\nExp\u00e9rience avec\nMySQL, PostgreSQL, MongoDB.\nMaitrise ETL/ELT (Talend, Stambia, etc.)\nSolides comp\u00e9tences en conception et en optimisation de pipelines de donn\u00e9es.\nExp\u00e9rience de travail en\nm\u00e9thode Agile\npour la gestion de projet et le d\u00e9veloppement de solutions.\nCapacit\u00e9 \u00e0 travailler de mani\u00e8re autonome et en \u00e9quipe.\nExcellentes comp\u00e9tences en communication et en r\u00e9solution de probl\u00e8mes.\nLes plus du poste :\nEnvironnement Collaboratif\n: projets innovants favorisant le partage des connaissances.\nAccompagnement individualis\u00e9 et de proximit\u00e9\n: formations certifiantes, attribution d\u2019un Career Manager pour vous orienter dans votre trajectoire professionnelle, opportunit\u00e9s d\u2019\u00e9volution de carri\u00e8re.\nFlexibilit\u00e9 du Travail\n: T\u00e9l\u00e9travail et horaires flexibles pour votre \u00e9quilibre vie professionnelle-personnelle.\nR\u00e9mun\u00e9ration Comp\u00e9titive\n: Salaire comp\u00e9titif avec des avantages sociaux attrayants.\nMobilit\u00e9\n: Possibilit\u00e9 de mobilit\u00e9 \u00e0 Paris, Lyon ou Aix-en-Provence offrant des exp\u00e9riences diversifi\u00e9es au sein de Lincoln.\nNotre processus de recrutement :\nun entretien RH (1h) et entretien technique (1h)\nCette annonce n\u2019est pas faite pour vous si :\nVous \u00eates freelance et vous comptez le rester !\nToujours l\u00e0 ? Postulez et rejoignez nos\n400 experts en Data\n\ud83d\ude09.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                " MongoDB"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [
                "MySQL",
                "PostgreSQL",
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Communication",
                "R\u00e9solution de probl\u00e8mes",
                "Flexibilit\u00e9"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [
                "400"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "3",
                "3",
                "3"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Alternance - Data Engineer H/F",
        "company": "Herm\u00e8s",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/alternance-data-engineer-h-f-at-herm%C3%A8s-3889716412?position=7&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=wkNcO1RWrxC1NFezsJWZyA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "El\u00e9ments de contexte\nHerm\u00e8s Digital Ventes et Services recherche pour sa direction Data & Performance :\nUn Alternant Data Engineer (H/F)\nContrat d'alternance de 12 mois\nA partir de Septembre 2024\nBas\u00e9 \u00e0 Paris\nPrincipales activit\u00e9s\nVous \u00eates rattach\u00e9 au Data manager.\nVous avez pour principale mission d\u2019accompagner l\u2019\u00e9quipe Data dans les t\u00e2ches quotidiennes :\nReporting et statistiques de ventes et trafic (notamment via l\u2019outil Google Analytics et Google BigQuery)\nAnalyse des leviers d\u2019acquisition de traffic SEA/SEO/Referral\nCr\u00e9ation de Dashboard via l\u2019outil Google Data Studio\nParticipation aux travaux de CRO (Conversion Rate Optimization) et d\u2019AB testing\nMise en place d\u2019\u00e9tude pr\u00e9dictive sur les donn\u00e9es des sites Ecommerce\nProfil\nEtudiant en \u00e9cole d\u2019ing\u00e9nieur poss\u00e9dant une forte culture Internet et une sensibilit\u00e9 aux probl\u00e9matiques digitales e-commerce, vous avez une premi\u00e8re exp\u00e9rience en entreprise\nProfil technique ou ais\u00e9 avec la technique, une sp\u00e9cialisation en digital est en plus\nOrganis\u00e9, rigoureux, curieux, autonome, bonne expression \u00e9crite et aisance relationnelle\nMa\u00eetrise du Pack Office indispensable, ayant d\u00e9j\u00e0 utilis\u00e9 Google Analytics\nLa connaissance d\u2019outils de BI / Datavisualisation serait appr\u00e9ci\u00e9e (Google Data Studio, Tableau Software, Bime ou QlikView/QlikSense, PowerBI), de Base de Donn\u00e9es (SQL, MySQL, BigQuery)\nUne app\u00e9tence pour la Data, ses languages (Python, R) et ses technologies (Notebooks, mod\u00e9lisation statistique, Machine learning) est fortement appr\u00e9ci\u00e9e.\nAnglais courant souhait\u00e9\nSensible aux produits hauts de gamme, vous souhaitez vous investir dans un stage riche et formateur\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "PowerBI"
            ],
            "Statistics": [
                "Statistiques"
            ],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "MySQL",
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Machine Learning",
                "Statistiques"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer F/H",
        "company": "Valeuriad",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-valeuriad-3741223009?position=8&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=vksQPYG8IfOb6g1PzOjDwQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Rejoins la\nTeam Data\ncr\u00e9\u00e9e par\nNicolas Greffard,\nDocteur en Intelligence Artificielle\n, d\u00e9j\u00e0 compos\u00e9e de\n20\nData Engineers\net\nDatascientists\ntalentueux \ud83d\ude0d\nNous recherchons de\nnouvelles p\u00e9pites\npour rejoindre notre \u00e9quipe de choc et r\u00e9pondre aux\nmultiples probl\u00e9matiques Big Data\nde nos\nclients nantais\nmais \u00e9galement\ncontribuer \u00e0 nos projets de R&D\net travailler sur des\nconf\u00e9rences incroyables\n(DevFest, Salon de la Data)\n\ud83e\udd29\nTa future mission si tu l'acceptes\n\ud83d\ude09\nNous te proposons d'intervenir au sein de nos\ngrandes DSI clientes\n, sur des sujets de\ncollecte\n, d\n'alimentation\net de\ntransformation de donn\u00e9es\nsur un environnement\nBig Data\nApache\n(\nHadoop, Spark, Ambari, Hive\n) sur les technologies suivantes :\nHadoop, Apache Ambari, RabbitMQ, Java, Scala, YarnApplication, Teradata, Squoop, Kudu, Hue, Hive, Impala, Dataiku, Flink, Kafka, Spark, Kibana, Oozie, Git, GitLabCI, Jenkins\n,\nAWS.\nLe job en d\u00e9tail\n\ud83e\udd29\n\u00c9tude, conception et r\u00e9alisation de traitements Big Data ;\n\u00c9change avec les architectes, les PO et PPO, les d\u00e9veloppeurs et la gouvernance de donn\u00e9es ;\nExploration des donn\u00e9es et des usages des utilisateurs avec Impala ;\nImport de donn\u00e9es (SFTP, Kafka, RabbitMQ) ;\nAlimentation du cluster Hadoop via des composants d\u00e9velopp\u00e9 en Java avec le Framework Spark sur IntelliJ ;\nUtilisation d\u2019Apache Ambari pour g\u00e9rer et surveiller un cluster Hadoop, visualisation des jobs en cours via YarnApplication et des flux Oozie ;\nCollecte des donn\u00e9es depuis Teradata via l\u2019outil Sqoop dans une base de donn\u00e9es Hive ;\nTransformation des donn\u00e9es avec Spark (HDFS, Hive, Kafka, Hbase, Phoenix) ;\nUtilisation de Apache Kudu afin d'optimiser les requ\u00eates utilisateurs sur les donn\u00e9es chaudes ;\nExposition de donn\u00e9es sur Dataiku pour la cr\u00e9ation de mod\u00e8le de DataScience ;\nR\u00e9alisation en Java \u2013 Flink pour g\u00e9rer les traitements complexe et volumineux ;\nGestion de configuration sous Git avec GitLab ;\nInt\u00e9gration continue avec Jenkins et Sonar ;\nLecture de fichier parquet depuis un r\u00e9pertoire S3 sous AWS ;\nRequ\u00eatage de bases de donn\u00e9e depuis l'outil Athena d'AWS ;\nTransformation des donn\u00e9es et calcul d'indicateurs sous Hive ;\nUtilisation de Oozie pour l\u2019ordonnancement de flux ;\nUtilisation de Kibana pour visualiser et mesurer la volum\u00e9trie de traitements quotidien et en streaming.\nPourquoi choisir Valeuriad ?\n\ud83d\ude0a\nEn plus d\u2019\u00eatre aujourd\u2019hui un acteur nantais reconnu de l\u2019expertise IT, nous nous inscrivons depuis notre cr\u00e9ation dans une d\u00e9marche d'entreprise\nOpale\net\nHolacratique\n, o\u00f9 l'ensemble de nos prises de d\u00e9cisions et projets sont r\u00e9alis\u00e9s par et avec l'ensemble de nos\n119 co\u00e9quipiers\n\ud83d\udcaa\nRejoindre Valeuriad, c'est\npouvoir s'investir dans la co-construction de l'entreprise\n:\nPar un r\u00f4le, avec une fiche de poste et un temps d\u00e9di\u00e9 (gestionnaire des Ci\u2019s, porteur des partenariats \u00e9coles, organisateur d\u2019\u00e9v\u00e9nements, PO des projets internes, gestion de l'Acad\u00e9mie Valeuriad\u2026).\nPar les projets strat\u00e9giques (200 jours mis \u00e0 disposition pour les co\u00e9quipiers chaque ann\u00e9e) pour cr\u00e9er et faire grandir des projets structurants (cr\u00e9ation de nouveaux avantages \u00e0 l'anciennet\u00e9, cr\u00e9ation d'indicateurs mensuels pour \u00eatre toujours plus transparents, m\u00e9c\u00e9nat de comp\u00e9tences pour des associations caritatives...).\nPar les projets cagnottes (150\u20ac par co\u00e9quipiers et par an) pour r\u00e9aliser des projets collaboratifs qui te tiennent \u00e0 c\u0153ur avec d'autres Valeurieux (d\u00e9couverte du c\u00e9cifoot, challenge \u00e9cologique, challenges sportifs pour des dons \u00e0 des associations humanitaires, borne photo...).\nPar les ateliers collaboratifs, chaque mois des brainstorming et ateliers de travail sont propos\u00e9s par les diff\u00e9rents porteurs de projets et sont ouverts \u00e0 tous les volontaires.\nMais avant-tout nous sommes une\n\u00e9quipe soud\u00e9e\n, des coll\u00e8gues qui appr\u00e9cient\npasser du temps ensemble\nlors de nos\nsoir\u00e9es hebdomadaires\net se cr\u00e9er des\nsouvenirs inoubliables\n\ud83e\udd29 C'est pour \u00e7a que chez Valeuriad, le plus important pour nous reste le\nsavoir-\u00eatre\n:\ndes passionn\u00e9s, du dynamisme, des sourires, de l'\u00e9coute et le sens de la f\u00eate\n\ud83d\ude09\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark",
                "Flink"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "DATA ENGINEER",
        "company": "Apside",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-apside-3909772916?position=9&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=aYYNuTDFLAFR6HvwcvR%2FAg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Offre d'Emploi : DATA ENGINEER H/F chez Apside\nDescription du poste :\nNous sommes \u00e0 la recherche d'un Data Engineer passionn\u00e9 pour rejoindre notre \u00e9quipe dynamique. Si vous avez une expertise dans le Big Data, la Data Science, l'analyse de donn\u00e9es et l'architecture de donn\u00e9es, cette opportunit\u00e9 est faite pour vous. Int\u00e9grer notre communaut\u00e9 Data, c\u2019est l\u2019assurance de progresser, innover, partager, vous certifier et rendre service \u00e0 nos clients.\nVos missions :\nD\u00e9veloppement des jobs Spark pour la collecte et la transformation des donn\u00e9es comptables disponibles dans les bucket S3.\nOptimisation des jobs Spark.\nD\u00e9veloppement des batchs Java et \u00e9criture des donn\u00e9es au formats comptables.\n\u00c9criture et ordonnancement des DAGs Airflow.\nSupport du d\u00e9veloppement Spark Scala.\nMaintenance applicative.\nProduction des \u00e9v\u00e9nements d\u00e9di\u00e9s \u00e0 la plateforme de donn\u00e9es.\n.\nVotre r\u00f4le, vos comp\u00e9tences :\nVous ma\u00eetrisez au minimum un langage de programmation appliqu\u00e9 \u00e0 l\u2019analyse de donn\u00e9es (SQL, Scala, Python, Java).\nVous \u00eates passionn\u00e9 par le Big Data et le Machine Learning.\nVous concevez et mettez en \u0153uvre des strat\u00e9gies s\u00e9curis\u00e9es d'acquisition et d'int\u00e9gration de donn\u00e9es.\nVous configurez des r\u00e9f\u00e9rentiels de donn\u00e9es \u00e0 la pointe de la technologie dans des environnements distribu\u00e9s, majoritairement dans le cloud (Google Cloud Platform, Azure, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks).\nEnvironnement technique :\nSQL\nPython/Spark\nCloud AWS: AWS Glue, AWS Lambda (possibilit\u00e9 de vous former sur AWS)\nStockage objet (AWS S3)\nOrchestration et scheduling de t\u00e2ches (Apache Airflow)\nBases analytiques et bases NoSQL (ElasticSearch, AWS Athena)\nVotre profil :\nFort de 4 ann\u00e9es d\u2019exp\u00e9rience en Data Engineer/ DATA ANALYST\nTitulaire d\u2019une formation sup\u00e9rieure IT.\nCapacit\u00e9 \u00e0 s\u2019int\u00e9grer dans un cadre technique client tout en \u00e9tant \u00e0 m\u00eame de proposer des pistes d\u2019am\u00e9liorations pertinentes.\nAutonome dans la gestion des projets.\nCurieux et impliqu\u00e9, vous \u00eates bon communicant avec les clients et les acteurs de culture technique diff\u00e9rente.\nDe bonnes raisons de rejoindre Apside ?\nUn esprit start-up avec la stabilit\u00e9 d\u2019un grand groupe, qui favorise l\u2019agilit\u00e9, le travail d\u2019\u00e9quipe et la proximit\u00e9. Alors qu\u2019Apside ne cesse d\u2019agrandir sa famille d\u00e9j\u00e0 forte de plus de 3000 consultants, nous sommes \u00e0 la recherche de nos nouveaux talents !\nCDI + package salarial avantageux (Mutuelle offerte, RTT, Tickets Restaurant, Int\u00e9ressement ...)\nParticipez et animez nos soir\u00e9es techniques (Project Lab, Test Lab\u2026),\nDevenez speaker (Devoxx, DevFest, NCraft\u2026),\nFormez vous avec l\u2019Academy By Apside (e-learning, formation, certification).\nD\u00e9veloppez votre r\u00e9seau (Soir\u00e9es trimestrielles, Afterwork, Soir\u00e9es d\u2019int\u00e9gration\u2026),\nInt\u00e9grez notre Communaut\u00e9s d\u2019Experts et testez les derni\u00e8res innovations techniques sur notre Bac \u00e0 Sable !\nApside s\u2019engage en faveur de l\u2019emploi des personnes en situation de handicap avec sa filiale Apsid\u2019EA : 1\u00e8re entreprise adapt\u00e9e totalement int\u00e9gr\u00e9e \u00e0 une ESN !\nPour aller plus loin avec APSIDE !\nhttps://www.apside.com/fr/nos-offres-emploi/?_joboffer-agency=17833&_paged=2\nCe poste de DATA ENGINEER est fait pour vous !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL",
                "Elasticsearch"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark",
                "Apache Airflow"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure",
                "Google Cloud Platform"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Machine Learning",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [
                "17833"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer \u2013 Grenoble",
        "company": "Capgemini",
        "location": "Grenoble, Auvergne-Rh\u00f4ne-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-grenoble-at-capgemini-3905836212?position=10&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=KDNA8Rx%2B%2FhO7ZsBno6DlMw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise o\u00f9 vous serez en mesure de fa\u00e7onner votre carri\u00e8re selon vos aspirations. Avec le soutien et l'inspiration d'une communaut\u00e9 d\u2019experts dans le monde entier, vous pourrez r\u00e9\u00e9crire votre futur. Rejoignez-nous pour red\u00e9finir les limites de ce qui est possible, contribuer \u00e0 lib\u00e9rer la valeur de la technologie pour les plus grandes organisations et participer \u00e0 la construction d\u2019un monde plus durable et inclusif.\nVos missions :\nEn tant que Data Engineer au sein d'une \u00e9quipe multidisciplinaire, vos responsabilit\u00e9s principales seront les suivantes :\nIntervenir sur les diff\u00e9rentes phases d'un projet dans un environnement Cloud et Agile.\nContribuer \u00e0 la gestion de la qualit\u00e9 des donn\u00e9es et extraction et analyse de celle-ci, ainsi qu\u2019\u00e0 la pr\u00e9sentation des donn\u00e9es dans leur forme raffin\u00e9e.\nProposer des nouvelles lectures de donn\u00e9es via un travail de fouille sur les gisements d\u2019information, notamment client.\nAdopter une posture de consultant : proposer de nouvelles solutions et accompagner le client dans ses choix.\nVotre profil :\nTitulaire d'un Bac+5 en \u00e9cole d\u2019ing\u00e9nieur ou en universit\u00e9.\nConnaissances approfondies des ETL (Talend, Informatica ou SSIS), du traitement de donn\u00e9es (Spark, Python, Scala) ainsi que des bases de donn\u00e9es (Oracle, SQL Server, Postgres).\nFacult\u00e9 pour se montrer curieux, autonome et proactif dans la r\u00e9alisation de ses t\u00e2ches.\nCapacit\u00e9 \u00e0 faire preuve de rigueur et \u00e0 travailler en \u00e9quipe.\nBon niveau d\u2019anglais (B2 minimum).\n3 raisons de nous rejoindre :\nQualit\u00e9 de vie au travail\n: accord de t\u00e9l\u00e9travail en France et \u00e0 l\u2019international, accord sur l\u2019\u00e9galit\u00e9 professionnelle, la parentalit\u00e9, l\u2019\u00e9quilibre des temps et la mobilit\u00e9 durable.\nApprentissage en continu\n: certifications et formations en libre acc\u00e8s, accompagnement sur mesure avec votre career manager, parcours d\u2019int\u00e9gration sur 9 mois.\nAvantages groupe & CSE\n: plan actionnariat, tarif pr\u00e9f\u00e9rentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorit\u00e9s\n:\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int\u00e9grez un collectif qui valorise la diversit\u00e9, d\u00e9veloppe le potentiel de ses talents, s\u2019engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r\u00e9duire son impact environnemental sur tous ses sites et aupr\u00e8s de ses clients.\n\u00c0 propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr\u00e8s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d\u2019exp\u00e9rience, nous sommes un partenaire strat\u00e9gique des entreprises pour la transformation de leurs activit\u00e9s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp\u00e9tuelle \u00e9volution tels que le cloud, la data, l\u2019Intelligence Artificielle, la connectivit\u00e9, les logiciels, l\u2019ing\u00e9nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Oracle",
                "SQL Server"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Cloud"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5"
            ],
            "Experience": [
                "a",
                "n",
                "s",
                "55",
                "55",
                "55"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer  H/F",
        "company": "Groupe INGENA",
        "location": "Greater Paris Metropolitan Region",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-groupe-ingena-3883689479?position=1&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=9Wc3V%2BMHM4IlRePP6%2BcZVA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Le groupe INGENA promeut la transition num\u00e9rique en \u00e9tant acteur d\u2019un monde souhaitable.\nVotre mission :\nConcevoir, d\u00e9velopper et tester des algorithmes de collecte et de traitement de gros volumes de donn\u00e9es sous Scala, Python ou Java\nAutomatiser et optimiser les flux de donn\u00e9es et leurs visualisations en dashboards\nIndustrialiser les traitements, la qualit\u00e9 et l\u2019int\u00e9grit\u00e9 des donn\u00e9es\nParticiper \u00e0 la Mod\u00e9lisation et \u00e0 la Gouvernance des donn\u00e9es (process, normalisation, r\u00e9f\u00e9rentiel,\u2026)\nContribuer \u00e0 la scalabilit\u00e9, la s\u00e9curit\u00e9, la stabilit\u00e9 et la disponibilit\u00e9 des donn\u00e9es de la plateforme\nAnalyser les donn\u00e9es pour r\u00e9pondre aux questions m\u00e9tiers et participer \u00e0 l\u2019\u00e9volution de l\u2019architecture Big Data\nConcevoir, D\u00e9velopper et Industrialiser des mod\u00e8les de Machine Learning, Deep Learning, en collaboration avec les Data Scientists\nAppliquer une d\u00e9marche CI/CD (Git, Jira, Jenkins)\nLes comp\u00e9tences techniques n\u00e9cessaires sont :\nExp\u00e9rience de 5 ans minimum en d\u00e9veloppements Scala, Python ou Java\nExp\u00e9rience de 2 ans minimum sur SPARK et sur le traitement des flux en streaming\nExpertise sur Hadoop (Hive, HBase, HDFS) sous distributions MapR ou Hortonworks\nExp\u00e9rience souhait\u00e9e sur ELK, Terraform, NoSQL,\u2026\nFort background en Mod\u00e9lisation de donn\u00e9es ou ETL\nMa\u00eetrise des briques analytiques des clouds AWS, GCP ou Azure\nSensibilisation \u00e0 la d\u00e9marche CI/CD tools (Git, Jenkins)\nLa connaissance de Docker, Kubernetes et Ansible est un plus\nMise en \u0153uvre des m\u00e9thodes Agile (Scrum, Kanban,\u2026)\nAnglais souhait\u00e9\nGroupe INGENA\n:\nLe Groupe INGENA est sp\u00e9cialis\u00e9 en Conseil M\u00e9tier et en Int\u00e9gration pour les march\u00e9s de l\u2019assurance, de la banque et de la Finance. INGENA intervient notamment sur les projets associ\u00e9s \u00e0 la Data, aux Risques et \u00e0 la Distribution.\nLe groupe comprend \u00e9galement la soci\u00e9t\u00e9 DRiMS sp\u00e9cialis\u00e9e en Finance de March\u00e9.\nNos valeurs : Engagement, Int\u00e9grit\u00e9 et Bienveillance.\nLa mise en pratique du monde souhaitable, c\u2019est pour nous une entreprise \u00e9co-responsable, \u00e9thique, inclusive, sociale, soucieuse du bien-\u00eatre, de l\u2019\u00e9volution et de l\u2019\u00e9panouissement de ses \u00e9quipes. Ce sont aussi des offres pour un monde durable comme la ma\u00eetrise des risques ou l\u2019ESG.\nDans un esprit convivial et engag\u00e9, nous faisons en sorte que chacun puisse \u00eatre acteur de l\u2019INGENA souhaitable.\nBureau \u00e0 Paris 9\u00e8me (M\u00e9tro Le Peletier). Clients \u00e0 Paris ou tr\u00e8s proche banlieue.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL",
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker",
                "Jenkins"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Ansible",
                "Kubernetes"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "Machine Learning",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer - Stream Data Processing - Distributed Data Processing",
        "company": "Pathway",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-stream-data-processing-distributed-data-processing-at-pathway-3887662141?position=2&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=ikwLbQi4nhCOigonivq9UA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "About Pathway\nDeeptech start-up, founded in March 2020.\nOur primary developer offering is an ultra-performant Data Processing Framework (unified streaming + batch) with a Python API, distributed Rust engine, and capabilities for data source integration & transformation at scale (Kafka, S3, databases/CDC,...)\nThe single-machine version is provided on a free-to-use license (`pip install pathway`)\nMajor data use cases are around event-stream data (including real-world data such as IoT), and graph data that changes over time\nOur enterprise offering is currently used by leaders of the logistics industry, such as DB Schenker or La Poste, and tested across multiple industries. Pathway has been featured in Gartner's market guide for Event Stream Processing\nLearn more at http://pathway.com/ and https://github.com/pathwaycom/\nPathway is VC-funded, with amazing BAs from the AI space and industry. We have operations across Europe and in the US. We are headquartered in Paris, with significant support from the French ecosystem (BPI, Agoranov, WILCO,...).\nThe Team\nPathway is built by and for overachievers. Its co-founders and employees have worked in the best AI labs in the world (Microsoft Research, Google Brain, ETH Zurich), worked at Google, and graduated from top universities (Polytechnique, ENSAE, Sciences Po, HEC Paris, PhD obtained at the age of 20, etc...). Pathway's CTO is a co-author with Goeff Hinton and Yoshua Bengio. The management team also includes the co-founder of Spoj.com (1M+ developer users) and NK.pl (13.5M+ users) and experienced growth leader who has scaled companies with multiple exits.\nThe opportunity\nWe are searching for a person with a Data Processing or Data Engineering profile, willing to work with live client datasets, and to test, benchmark, and showcase our brand-new stream data processing technology.\nThe end-user of our product are mostly developers and data engineers working in a corporate environment. Our development framework is one day expected to become for them a part of their preferred development stack for analytics projects at work - their daily bread & butter.\nYou Will\nYou will be working closely with our CTO, Head of Product, as well as key developers. You will be expected to:\nImplement the flow of data from their location in client's warehouses up to Pathway's ingress\nSet up CDC interfaces for change streams between client data stores and i/o data processed by Pathway; ensuring data persistence for Pathway outputs\nDesign ETL pipelines within Pathway\nContribute to benchmark framework design (throughput / latency / memory footprint; consistency), including in a distributed system setup.\nContribute to building open-source test frameworks for simulated streaming data scenarios on public datasets\nRequirements\nInside-out understanding of at least one major distributed data processing framework (Spark, Dask, Ray,...)\n6 months+ experience working with a streaming dataflow framework (e.g.: Flink, Kafka Streams or ksqldb, Spark in streaming mode, Beam/Dataflow)\nAbility to set up distributed dataflows independently\nExperience with data streams: message queues, message brokers (Kafka), CDC\nWorking familiarity with data schema and schema versioning concepts; Avro, Protobuf, or others\nFamiliarities with Kubernetes\nFamiliarity with deployments in both Azure and AWS clouds\nGood working knowledge of Python\nGood working knowledge of SQL\nExperienced in working for an innovative tech company (SaaS, IT infrastructure or similar preferred), with a long-term vision\nWarmly disposed towards open-source and open-core software, but pragmatic about licensing\nBonus Points\nKnow the ways of developers in a corporate environment\nPassionate about trends in data\nProficiency in Rust\nExperience with Machine Learning pipelines or MLOps\nFamiliarity with any modern data transformation workflow tooling (dbt, Airflow, Dagster, Prefect,...)\nFamiliarity with Databricks Data Lakehouse architecture\nFamiliarity with Snowflake's data product vision (2022+)\nExperience in a startup environment\nBenefits\nWhy You Should Apply\nIntellectually stimulating work environment. Be a pioneer: you get to work with a new type of stream processing framework\nWork in one of the hottest data startups in France, with exciting career prospects\nResponsibilities and ability to make significant contribution to the company' success\nCompensation: annual salary of \u20ac60K-\u20ac100K + Employee stock option plan.\nInclusive workplace culture\nFurther details\nType of contract: Permanent employment contract\nPreferable joining date: early 2023\nCompensation: annual salary of \u20ac60K-\u20ac100K + Employee stock option plan\nLocation: Remote work from home. Possibility to work or meet with other team members in one of our offices:\nParis - Agoranov (where Doctolib, Alan, and Criteo were born) near Saint-Placide Metro (75006)\nParis Area - Drahi X-Novation Center, Ecole Polytechnique, Palaiseau\nWroclaw - University area\nCandidates based anywhere in the EU, United States, and Canada will be considered.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark",
                "Databricks",
                "Flink"
            ],
            "MachineLearning": [],
            "DataSerialization": [
                "Avro"
            ],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes",
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "ML",
                "Machine Learning",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": [],
            "Salary": [
                "60K"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer H/F",
        "company": "ALFI : Financial Markets Consultancy Services",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-alfi-financial-markets-consultancy-services-3916559634?position=3&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=gTjggiRztds529DCE7H89w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ALFI est une soci\u00e9t\u00e9 de conseil et services sp\u00e9cialis\u00e9e en syst\u00e8mes d\u2019information. Depuis plus de 20 ans, ALFI est un acteur unique qui m\u00eale technologie et humain pour accompagner les transformations num\u00e9riques sur les march\u00e9s de l\u2019Asset Management, la banque d\u2019Investissement et les Services aux Investisseurs.\nAvec plus de 46 r\u00e9f\u00e9rencements de rang un, ALFI est reconnu comme incontournable sur le secteur BFA. Nous avons plus de 35 clients grands comptes actifs tels que HSBC, Soci\u00e9t\u00e9 G\u00e9n\u00e9rale, BNP Paribas, Cr\u00e9dit Agricole, Axa\u2026\nDepuis 2015, ALFI a int\u00e9gr\u00e9 le groupe\nMoOngy\n, qui compte plus de 6000 salari\u00e9s dans toute l\u2019Europe\nMissions :\nPour l'un de nos clients grands comptes, nous vous proposons d'intervenir sur une fonction de Consultant Data engineer.\nLes principales missions sont :\nComprendre les besoins des utilisateurs et les traduire de mani\u00e8re analytique\nD\u00e9veloppement de solutions permettant de traiter des volumes importants de donn\u00e9es\nConception, collection et fabrication des donn\u00e9es brutes\nD\u00e9velopper des algorithmes permettant de r\u00e9pondre aux probl\u00e8mes pos\u00e9s et veiller \u00e0 leur industrialisation\nS\u00e9curisation des Pipelines donn\u00e9es pour les Data Scientists et les Data Analysts\nConstruire des bases de donn\u00e9es robustes\nOrganisation de l\u2019architecture du cloud\nProfil recherch\u00e9 :\nVous \u00eates issu d'une formation Bac +5 Ecole scientifique ou informatique.\nVous disposez d'une premi\u00e8re exp\u00e9rience en d\u00e9veloppement et dans la data.\nVous disposez d'un niveau d'anglais op\u00e9rationnel.\nJava, Python, C++\nSQL\nDevops (Jenkins, Kubernetes, Docker)\nConform\u00e9ment \u00e0 la r\u00e8glementation, et \u00e0 notre politique d\u2019\u00e9galit\u00e9 professionnelle, tous nos postes sont ouverts aux personnes en situation de handicap;\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "C++",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Docker",
                "Jenkins"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "20",
                "20",
                "20"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "MindPal",
        "location": "Marseille, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-mindpal-3896991761?position=4&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=mK%2Fm3m7kwlYQFk2YmDT9sA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for\nData Engineer!\nResponsibilities\nDesigning, creating, and maintaining data processing systems\nAnalyzing and optimizing data processing workflows\nCollaborating with the team to ensure data quality and efficiency\nTesting and implementing new solutions\nRequirements\nAt least 2 years of experience in designing and creating data processing systems\nProficiency in tools and programming languages related to data engineering (e.g. Hadoop, Spark, Scala, Python)\nExcellent knowledge of databases and SQL language\nAbility to work in a team and communicate effectively with other departments\nCommunicative English skills\nExperience with AWS/AWS Glue is a plus\nWe Offer\nB2B contract\nFull-time job\nRemote work and flexible hours\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer  H/F",
        "company": "METEOJOB by CleverConnect",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-meteojob-by-cleverconnect-3895135654?position=5&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=G%2BraCwgpd5v46E1jI26QuA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Entreprise\nQui sommes-nous ?\nNous sommes passionn\u00e9s par les nouvelles technologies, et vous ?\nRejoindre Amiltone, c'est int\u00e9grer des \u00e9quipes dynamiques et soud\u00e9es dans le cadre de projets novateurs et ambitieux. Nous relevons les challenges techniques de nos clients et les accompagnons dans leur transformation digitale.\nPourquoi choisir Amiltone\u202f?\nAmiltone, plus qu'une entreprise, un \u00e9tat d'esprit !\nNotre objectif ? Votre \u00e9panouissement professionnel !\nNous Avons \u00e0 C\u0153ur De\nVous accompagner au mieux au travers d'un suivi personnalis\u00e9\nVous faire monter en comp\u00e9tences en vous proposant des formations tout au long de votre carri\u00e8re\nComprendre vos besoins et respecter nos engagements\nVous proposer des missions de qualit\u00e9 avec des technologies innovantes\nCultiver votre potentiel gr\u00e2ce \u00e0 notre programme de d\u00e9veloppement personnel Addvise\nVotre bien-\u00eatre passe aussi par des activit\u00e9s extraprofessionnelles, c'est pourquoi nous vous proposons des s\u00e9ances sportives anim\u00e9es par nos coachs, soir\u00e9es pour se retrouver et animations (\u00e0 l'agence ou en visio), Gaming nights...\nDescription Du Poste\nLes missions d'un Amiltonien :\nEn tant que Data Engineer\n(H/F)\n, vous serez en charge des missions suivantes :\nConcevoir et d\u00e9velopper les futures fonctionnalit\u00e9s de la plateforme Big Data sous Google Cloud Platform.\nConcevoir les flux d'alimentation et les tables (structure de donn\u00e9e).\nAutomatiser et industrialiser les flux.\nAssurer le run applicatif, le cas \u00e9ch\u00e9ant.\nLa Stack Technique\nMa\u00eetrise des langages suivants : SQL, Talend, BigQuery\nConnaissances de Google (GCP)\nNotion de programmation fonctionnelle\nDescription Du Profil\nLe profil d'un Amiltonien :\nDipl\u00f4m\u00e9 Bac+4/5 (Ecole d'ing\u00e9nieur/Master), vous disposez de 2 ann\u00e9es d'exp\u00e9rience dans le d\u00e9veloppement de data.\nToujours sur le qui-vive des nouveaut\u00e9s technologiques, vous \u00eates force de proposition sur des technos, des outils ou des process qui permettent d'am\u00e9liorer la qualit\u00e9 du code et la stabilit\u00e9 de nos applications.\nOutre vos comp\u00e9tences techniques, nous nous int\u00e9ressons \u00e9galement \u00e0 votre potentiel et votre motivation.\nNos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Senior Data Engineer - Valbonne",
        "company": "Capgemini",
        "location": "Valbonne, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-engineer-valbonne-at-capgemini-3888072146?position=6&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=MuhVomcbD2tDd3DYDYZk7w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise o\u00f9 vous serez en mesure de fa\u00e7onner votre carri\u00e8re selon vos aspirations. Avec le soutien et l'inspiration d'une communaut\u00e9 d\u2019experts dans le monde entier, vous pourrez r\u00e9\u00e9crire votre futur. Rejoignez nous pour red\u00e9finir les limites de ce qui est possible, contribuer \u00e0 lib\u00e9rer la valeur de la technologie pour les plus grandes organisations et participer \u00e0 la construction d\u2019un monde plus durable et inclusif.\nVos missions :\nEn tant que Senior Data Engineer au sein d'une \u00e9quipe multidisciplinaire, vos responsabilit\u00e9s principales seront les suivantes :\nParticiper \u00e0 des ateliers clients.\nAcqu\u00e9rir des donn\u00e9es et optimiser le stockage.\nCr\u00e9er de flux de donn\u00e9es optimis\u00e9s et \u00e9laborer des algorithmes de transformation.\nTraiter et analyser pour la visualisation et le machine learning.\nEncadrer des ing\u00e9nieurs juniors et contribuer \u00e0 la communaut\u00e9 Data.\nVotre profil :\nVous poss\u00e9dez un dipl\u00f4me d'ing\u00e9nieur informatique et/ou Master avec une sp\u00e9cialit\u00e9 data.\nVous parlez couramment fran\u00e7ais et anglais.\nVous poss\u00e9dez au minimum 6 ans d'exp\u00e9rience sur un r\u00f4le similaire.\nVous ma\u00eetrisez les outils Spark, Python, Scala ainsi qu'une bonne compr\u00e9hension des syst\u00e8mes d'extraction, de transformation et de changement (ETL).\nVous avez un certain leadership et un esprit d'\u00e9quipe, id\u00e9alement dans un cadre agile.\n3 raisons de nous rejoindre :\nQualit\u00e9 de vie au travail :\naccord de t\u00e9l\u00e9travail en France et \u00e0 l'international, accord sur l'\u00e9galit\u00e9 professionnelle, la parentalit\u00e9, l'\u00e9quilibre des temps et la mobilit\u00e9 durable.\nApprentissage en continu\n: certifications et formations en libre acc\u00e8s, accompagnement sur mesure avec votre carrer manager, parcours d'int\u00e9gration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, activit\u00e9s \u00e0 tarifs pr\u00e9f\u00e9rentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorit\u00e9s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int\u00e9grez un collectif qui valorise la diversit\u00e9, d\u00e9veloppe le potentiel de ses talents, s'engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r\u00e9duire son impact environnemental sur tous ses sites et aupr\u00e8s de ses clients.\nA Propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr\u00e8s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d\u2019exp\u00e9rience, nous sommes un partenaire strat\u00e9gique des entreprises pour la transformation de leurs activit\u00e9s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp\u00e9tuelle \u00e9volution tels que le cloud, la data, l\u2019Intelligence Artificielle, la connectivit\u00e9, les logiciels, l\u2019ing\u00e9nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Machine Learning",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Leadership",
                "Organisation"
            ],
            "EnSoftSkils": [
                "Leadership",
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Junior",
                "Senior"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "6",
                "6",
                "6"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Alternance Data Engineer - Lille (F/H)",
        "company": "METEOJOB by CleverConnect",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/alternance-data-engineer-lille-f-h-at-meteojob-by-cleverconnect-3901971331?position=7&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=5Xzo%2B%2FEkXS0PjKMcS6ocHQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Entreprise\nL'ISCOD, sp\u00e9cialiste de la formation en Digital Learning, recherche pour son entreprise partenaire, une ESN agile et un groupe international, son Data Engineerq, en contrat d'apprentissage, pour pr\u00e9parer l'une de nos formations dipl\u00f4mantes reconnues par l'Etat de niveau 5 \u00e0 niveau 7 (Bac+2, Bachelor/Bac+3 ou Mast\u00e8re/Bac+5).\nOptez pour l'alternance nouvelle g\u00e9n\u00e9ration avec l'ISCOD !\nDescription Du Poste\nDurant cette alternance, tu auras l'opportunit\u00e9 de :\nCollaborer avec les \u00e9quipes pour d\u00e9finir les besoins ;\nOrganiser et traiter le flux de donn\u00e9es quotidien ;\nEffectuer la visualisation des donn\u00e9es ;\nOrganiser, synth\u00e9tiser l'information\nLivrer les r\u00e9sultats (rapports, pr\u00e9sentations, tableaux de bord...) ;\nDescription Du Profil\n\u00c9tudiant en derni\u00e8re ann\u00e9e, tu suis une formation ax\u00e9e sur le Data Engineering et tu recherches une alternance.\nTu as une grande app\u00e9tence pour le domaine de la Data et tu ma\u00eetrises un des langages de programmation suivants: SQL, Python, Java \u2026\nTu connais le fonctionnement des ETL et les outils de visualisation, notamment Power BI et des gestion de donn\u00e9es, notamment de MS Excel.Poste bas\u00e9 \u00e0 LilleR\u00e9mun\u00e9ration selon niveau d'\u00e9tudes + \u00e2ge\nFormation prise en charge \u00e0 100% par l'entreprise\nCe poste vous int\u00e9resse ? Envoyez vite votre candidature !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau",
                "Power BI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [
                "Bac+5",
                "Bac+3"
            ],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "ALTERNANT DATA ENGINEER (H/F)",
        "company": "METEOJOB by CleverConnect",
        "location": "Pont-\u00e0-Mousson, Grand Est, France",
        "link": "https://fr.linkedin.com/jobs/view/alternant-data-engineer-h-f-at-meteojob-by-cleverconnect-3868016363?position=8&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=hZRnMmL0q03FEFk%2BycezAg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Entreprise\nSaint-Gobain\ncon\u00e7oit, produit et distribue des mat\u00e9riaux et des solutions pens\u00e9s pour le bien-\u00eatre de chacun et l'avenir de tous. Rejoignez une communaut\u00e9 innovante, passionn\u00e9e et entreprenante pour am\u00e9liorer le monde de demain.\nActeur mondial de r\u00e9f\u00e9rence et leader europ\u00e9en de solutions compl\u00e8tes de canalisation en fonte ductile,\nSaint-Gobain PAM Canalisation\ncon\u00e7oit, produit et commercialise un \u00e9ventail complet de solutions d\u00e9di\u00e9es au\ntransport de l'eau\n.\nSaint-Gobain PAM Canalisation dispose dans la r\u00e9gion Grand-Est, \u00e0 Maidieres, d'un\nCentre de Recherche\nunique dont les comp\u00e9tences et le savoir-faire contribuent \u00e0 apporter \u00e0 nos clients des\nsolutions innovantes\net \u00e0 valeur ajout\u00e9e dans le\ncontr\u00f4le et la gestion patrimoniale de l'eau.\nDescription Du Poste\nL'entreprise Saint-Gobain PAM Canalisation, sp\u00e9cialis\u00e9e dans le d\u00e9veloppement, la production et la vente d'\u00e9quipements de canalisation (tuyaux, robinetteries et regards de voirie) recrute pour son Si\u00e8ge \u00e0 PONT-A-MOUSSON :\nUN ALTERNANT DATA ENGINEER (H/F)\nInt\u00e9gr\u00e9.e Aux \u00c9quipes De La Direction Des Syst\u00e8mes D'Information PAM DIGITAL & IT Et Rattach\u00e9.e \u00e0 Son Domaine DIGITAL TECHNOLOGIES, Vous Serez Notamment Amen\u00e9(e) \u00e0\nIdentifier les sources des donn\u00e9es et mettre en place leur int\u00e9gration dans SnowFlake via notre Cloud AZURE,\nConcevoir et mod\u00e9liser des datawarehouses et des data hubs en fonction des Use Cases sur lesquels vous travaillerez,\nRestituer des donn\u00e9es par visualisation (avec PowerBI) ou APIsation (avec Microsoft Data Factory),\nR\u00e9aliser des projets sur des technologies innovantes,\nMonitorer et maintenir des plateformes d'\u00e9changes h\u00e9bergeant ces flux.\nContrat d'alternance\nLe poste est bas\u00e9 \u00e0 Pont-\u00e0-Mousson ( \u00e0 30km de Nancy et Metz )\nDescription Du Profil\nVous pr\u00e9parez un Master (Bac+ 5),\nVous \u00eates passionn\u00e9 par le Big data, la Business intelligence, la Valorisation de donn\u00e9es,\nVous avez des bases concernant les langages : Python , SQL et Java (Spring Batch) ainsi que les bases de donn\u00e9es SQL,\nVous connaissez Snowflake,\nVous avez envie de faire de la DataViz avec PowerBI.\nVous \u00eates reconnu\u00b7e par vos coll\u00e8gues pour :\nVotre aptitude \u00e0 aller vers les autres, communiquer et \u00e9couter\nVotre capacit\u00e9 \u00e0 travailler en \u00e9quipe, dans un environnement international\nVotre rigueur, et votre autonomie.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Analytics Engineer",
        "company": "Vestiaire Collective",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/analytics-engineer-at-vestiaire-collective-3875996826?position=9&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=aKEX6pSYETA3IWaU83cDlg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vestiaire Collective is the leading global online marketplace for desirable pre-loved fashion. Our mission is to transform the fashion industry for a more sustainable future by empowering our community to promote the circular fashion movement. Vestiaire was founded in 2009 and is headquartered in Paris with offices in London, Berlin, New York, Singapore, Ho Chi Minh and Hong Kong and warehouses in Tourcoing (France), Crawley (UK), Hong Kong and New York.\nWe currently have a diverse global team of 700 employees representing more than 50 nationalities. Our values are Activism, Transparency, Dedication and Greatness and Collective. We are proud to be a BCorp.\nAbout The Role\nThis role is central to our data strategy and requires a balance of technical expertise and business acumen. As a Junior Analytics Engineer, you will be at the heart of our data-driven initiatives, working closely with cross-functional teams to transform raw data into a single source of truth data mart. Your work will directly influence key decisions in finance, payment systems and business performance.\nWhat You'll Do\nDesign, implement, and maintain efficient and reliable data pipelines using a modern data stack: Airflow, Snowflake, DBT\nDevelop advanced data models to support complex analytics, including financial reconciliations, cost effectiveness and profitability models. Collaborate with finance, payments and tech teams to understand their data requirements and translate these into sophisticated technical solutions\nEnsure scalability and performance of our data infrastructure to handle large-scale, multi-faceted data sets from diverse sources\nImplement and maintain data quality checks and monitoring systems for accuracy and consistency\nInnovate and integrate new technologies and methodologies to enhance data capabilities across finance domains\nAssist the finance team in building key dashboards in Tableau to enable data driven decision making\nWho You Are\nRequired Qualifications:\nBachelor\u2019s/Master\u2019s in Computer Science, Engineering, Statistics, or related field\nAt least one previous experience in analytics engineering, with strong skills in ETL and data modeling, an awareness of data warehousing and dataOps practices\nProficient in SQL and programming languages like Python or R\nExperience with cloud data technologies and big data tools\nDesirable Skills:\nApache Airflow: an understanding of workflow management\nGit: Solid knowledge in version control and CI/CD integration\nCloud Service: AWS, Snowflake or similar cloud experience\nData Visualization Tools: Proficiency in tools like Tableau, Looker, Snowsight\nPrevious experience in DBT for data modeling\nWhat we offer\n\ud83c\udf81\nA meaningful job with an impact on the way people consume fashion and promote sustainability\nFlexible work possibilities\nThe opportunity to do career-defining work in a fast-growing French-born scale up\nThe possibility to work as part of a globally diverse team with more than 50 nationalities\nTwo days to help Project - reinforcing your activist journey and volunteer for an association\nSignificant investment in your learning and growth\nCompetitive Compensation And Benefits Package\nAs full member of our entrepreneurial project, you will be eligible to free shares\nVestiaire Collective is an equal opportunities employer\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau"
            ],
            "Statistics": [
                "Statistics"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "Snowflake",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Big Data",
                "Cloud",
                "CI/CD"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [
                "Full",
                "Junior"
            ],
            "TypeContract": [],
            "Salary": [
                "Package"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data engineer",
        "company": "Harnham",
        "location": "Greater Paris Metropolitan Region",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-harnham-3860004924?position=10&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=8Jke7%2FnB8H6WLoCi6KALqw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data engineer\nMinimum 2 ans\nParis\n2j tt\nCDI\nUP TO 55k\u20ac\nRejoignez une \u00e9quipe dynamique au c\u0153ur de l'innovation dans le domaine du retail !\nStack technique : Python, Pandas, SQL, Tableau\nVous utiliserez votre connaissance du d\u00e9veloppement et de la testabilit\u00e9 pour am\u00e9liorer la conception, promouvoir des d\u00e9cisions d'ing\u00e9nierie pr\u00e9cises et mettre en \u0153uvre des strat\u00e9gies de pr\u00e9vention des bogues \u00e0 la fois \u00e9volutives et maintenables dans ce ensemble de responsabilit\u00e9s cl\u00e9s :\nCoop\u00e9rer avec divers d\u00e9partements tels que la Recherche et le D\u00e9veloppement, l'Infrastructure, l'Ing\u00e9nierie et le Backend pour garantir la qualit\u00e9 et la ponctualit\u00e9 des livraisons de produits.\n\u00c9laborer et mettre en \u0153uvre des sc\u00e9narios de test, \u00e0 la fois manuels et automatis\u00e9s, pour les applications logicielles afin d'assurer la fiabilit\u00e9 des pipelines de donn\u00e9es, des processus ETL et des transformations de donn\u00e9es.\nConcevoir et automatiser des tableaux de bord de qualit\u00e9 pour surveiller en continu la qualit\u00e9 des donn\u00e9es.\nEffectuer des tests fonctionnels, d'int\u00e9gration, de r\u00e9gression et de performance des syst\u00e8mes de bases de donn\u00e9es en utilisant des technologies standard de l'industrie telles que SQL, Python, etc.\nCr\u00e9er et maintenir la documentation d\u00e9taill\u00e9e des plans de test, des cas de test et des r\u00e9sultats des tests.\nContribuer activement au succ\u00e8s du d\u00e9ploiement europ\u00e9en en assurant la fiabilit\u00e9 et la qualit\u00e9 des produits livr\u00e9s.\nVotre profil :\nSolide exp\u00e9rience dans le d\u00e9veloppement et les tests de logiciels, avec une compr\u00e9hension approfondie des pipelines de donn\u00e9es, des processus ETL et des transformations de donn\u00e9es.\nMa\u00eetrise des technologies standard de l'industrie telles que SQL, Python, etc., avec une capacit\u00e9 av\u00e9r\u00e9e \u00e0 \u00e9laborer et ex\u00e9cuter des cas de test manuels et automatis\u00e9s.\nComp\u00e9tences avanc\u00e9es en mati\u00e8re de surveillance et de garantie de la qualit\u00e9 des donn\u00e9es, y compris la cr\u00e9ation et l'automatisation de tableaux de bord de qualit\u00e9.\nCapacit\u00e9 \u00e0 travailler efficacement en collaboration avec diverses \u00e9quipes, y compris la Recherche et le D\u00e9veloppement, l'Infrastructure et le Backend, pour garantir la qualit\u00e9 et la ponctualit\u00e9 des livraisons de produits.\nExcellentes comp\u00e9tences en communication et en documentation, avec une capacit\u00e9 \u00e0 maintenir des rapports d\u00e9taill\u00e9s des plans de test, des cas de test et des r\u00e9sultats des tests.\nInt\u00e9ress\u00e9(e) ? Postulez !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "Pandas",
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Tableau"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [],
            "FrSoftSkills": [
                "Communication",
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "2",
                "2",
                "2"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer | ML Engineer | Up to 75k | Boulogne",
        "company": "Talent-R",
        "location": "Greater Paris Metropolitan Region",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-ml-engineer-up-to-75k-boulogne-at-talent-r-3904965048?position=1&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=goHKTsw2R1FcHVw1qNEXpg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83d\udccd\nLocalisation\n: R\u00e9gion Parisienne & Remote Flexible (60%) - CDI\n\ud83d\udd0d\nSeniorit\u00e9\n: Mid/Senior (>4/5 years of Data)\n\ud83d\udcb0\nSalaire\n: Up to 75K\u20ac fixe + Prime de participation, prime vacances et bonus...\nL'entreprise\n\ud83d\udcbc\nLe groupe entre dans une nouvelle \u00e8re gr\u00e2ce \u00e0 la strat\u00e9gie qui place\nL\u2019IA\nau c\u0153ur du business. Acteur incontournable de ce nouveau cycle ils participent activement \u00e0 relever les challenges des\nnouvelles mobilit\u00e9s et de l\u2019industrie 4.0.\nP\u00f4le Architecture et Data :\nL'objectif est de mettre en place les bases de\nla plateforme IA\nafin de r\u00e9pondre aux nouveaux besoins m\u00e9tiers.\nLes missions\n\u2699\ufe0f\nDans ce r\u00f4le, vous travaillerez en \u00e9troite collaboration avec les \u00e9quipes m\u00e9tiers et les autres membres du P\u00f4le Architecture & Data (Data Analysts, Scientists, architectes, etc.), en exploitant des quantit\u00e9s massives de donn\u00e9es (flux d'\u00e9v\u00e9nements en continu, traitements par lots et en temps r\u00e9el, ainsi que les appels aux APIs).\nL'objectif est notamment d'alimenter des mod\u00e8les d'apprentissage automatique pour des t\u00e2ches telles que la segmentation des clients et la d\u00e9tection automatique des pannes des v\u00e9hicules.\nLes avantages\n\ud83d\ude0d\nVariable de 6% (Objectifs individuels / Performance du groupe)\nPrime int\u00e9ressement (\u00e0 peu pr\u00e8s un mois de salaire)\nPrime vacances (1% de salaire annuel)\nTarif pr\u00e9f\u00e9rentiels achats de v\u00e9hicules\nAvantage CE (200 - 400\u20ac de ch\u00e8ques cadeaux)\nT\u00e9l\u00e9 travail : 3 jours / semaine\nMat\u00e9riel IT + participation frais d\u2019internet\n10 jours de RTT\nLe stack technique\n\ud83d\udc49\ud83c\udffb\nGoogle Cloud Platform (BigQuery, PubSub, Dataflow, Vertex AI)\nAirflow\nTerraform\nPython\nLooker\nDataiku\nKubernetes, SQL, Git\nPostulez si et seulement si\nVous disposez d'au moins\n5\nans\nd\u2019exp\u00e9rience en data\nVous disposez d\u2019une solide exp\u00e9rience en d\u00e9veloppement\nPython\net\nframework ML\n(Vertex, Tensorflow, Scikit, PyTorch\u2026)\nVous poss\u00e9dez une exp\u00e9rience de d\u00e9veloppement et orchestration de chaines ETL complexes via\nAirflow\nou \u00e9quivalent\nVous savez utiliser des services cloud (pr\u00e9f\u00e9rablement\nGCP\n)\nVous \u00eates capable d\u2019\u00e9changer en\nanglais\ntechnique \u00e9crit et oral\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [
                "TensorFlow",
                "PyTorch"
            ],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes",
                "Airflow"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [
                "Remote",
                "Senior"
            ],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [
                "75K"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "StackEase",
        "location": "Marseille, Provence-Alpes-C\u00f4te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-stackease-3906618983?position=2&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=CTOOVDi7B%2FDx%2BYw6tjYbRA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Context :\nIt is challenging in many ways to develop a net zero electricity grid. Adding renewable energies means more need for storage and for grid balancing. Grid-scale batteries handle these issues but their operation is not quite simple.\nA battery has access to many different revenue streams, each being specific. No revenue alone is enough to make it economically viable. All revenues stacked and optimized together ensure profitability in the long run and a fast deployment.\nStackEase\u2019s ambition is to develop optimization and trading algorithms that merge the battery revenue streams and ensure their sustainable development. The final tool will be an autopilot that makes the best decisions in real time, controls the battery accordingly and sends the corresponding orders to\u00a0 the market.\nAbout StackEase:\nStackEase is a deeptech spinoff from the INRIA (French Institute for Research in Computer Science). It was created in August 2023 and secured its first fundings. Members are located in Marseille and Paris.\nOur values are innovation, customer satisfaction, merit and sustainability. The company's purpose is to leverage Machine Learning and Mathematical Optimization to accelerate the energy transition.\nMissions :\nDefine and develop the backend architecture of StackEase\nSet up databases and data pipelines collecting battery and market data\nDeploy and maintain optimisation algorithms and forecasts\nDevelop a robust and scalable SaaS platform for 24/7 battery management with high cyber standards\nParticipate in the UI/UX product definition\nSkills Wishlist :\nScientific BS/MS/PhD with 2+ years of experience in software engineering\nExperience with the common backend tools: Python, Git, Kubernetes, SQL/NoSQL \u2026 Knowledge of the AWS environment is a plus\nEnthusiastic, rigorous, autonomous and willing to be involved in major technical decisions\nKnowledge/Interest in the energy sector and ancillary services\nCompensation :\n45k\u20ac - 60k\u20ac salary range (incl. healthcare, unemployment rate, vacations, \u2026)\nFlexible remote work policies\nYou do not need to meet 100% of the requirements to apply, we will study all applications: please send your resume to jobs@stackease.fr. References and a cover letter are also welcome but not mandatory.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [],
            "Other": [
                "Machine Learning"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": [],
            "Salary": [
                "100"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer Intern",
        "company": "Kyriba",
        "location": "St.-Cloud, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-intern-at-kyriba-3886667187?position=3&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=hDnXaxqe0%2FKcc1Uyp5ctjw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "It's fun to work in a company where people truly BELIEVE in what they're doing!\nWe're committed to bringing passion and customer focus to the business.\nKyriba is the global leader in cloud-based Enterprise Liquidity Management management solutions, delivering Software-as-a-Service (SaaS) financial technology to corporate CFOs and Treasurers. More than 2,600 global organizations (including Spotify, Ripple, Adecco, Auchan, Adobe, EuropCar, Eurostar International, Expedia, Electronic Arts and Takeda) use Kyriba to enhance their global cash visibility, improve financial controls, and increase productivity across their cash and liquidity, payments, supply chain finance and risk management operations\n.\nBeing an intern at Kyriba means more than simply getting involved in the day-to-day operations of the company. It is an opportunity to introduce you to your potential future work environment so that you can better decide if it meets your career goals\nWe are looking for an enthusiastic\nData Engineer Intern\nto join our Machine Learning Platform team, which is part of our Engineering Department. The main goal of this internship is to participate in building and delivery of Kyriba ML Platform. For this internship, you are going to work with the engineering manager, senior developers of your team, and also QA team.\nKeywords: Data, Python, Java, SQL, Git, Machine Learning (ML), Containers, Orchestration, Kubernetes.\nRequirements\nSolid understanding of basics in Python and Java programming languages\nKnowledge of SQL\nIntermediate (at least) level of English\nAbility to work in team\nMotivation to learn new technologies and tools\nKnowing ML fundamentals will be a plus but is not strictly required\nUnderstanding and basic knowledge of Cloud and Cloud Platforms is also a plus\nResponsibilities\nWork as a part of our development team to build and deliver the Kyriba ML Platform\nFocus on different aspects of the software development and operation lifecycle (designing, coding, test coverage, etc)\nParticipate in all the development-related ceremonies (Pull Requests review, SCRUM activities, grooming sessions, etc)\nLearning Opportunities\nHands-on experience in designing and building cloud-native solutions using microservices-based architecture\nDiscover the world of Machine Learning from the developer\u2019s perspective and exposure to industry-leading tools and technologies in this domain\nPossibility to deep dive in containers\u2019 orchestration\nInsight into the intricacies of the Treasury Management, specifically in a Working Capital business domain\nBenefits\nMentorship from experienced professionals in the area\nPractical experience within an international SaaS provider leveraging public cloud solutions\nGain practical experience in utilizing Jira for project management\nFlexibility in work hours to accommodate academic commitments\nNetworking opportunities within the organization\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "ML",
                "Machine Learning",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": [
                "Organization",
                "Flexibility"
            ]
        },
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer BI",
        "company": "Listen too",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bi-at-listen-too-3903073048?position=4&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=Rx7XexD69MeYfSqLD3%2ByeA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "\ud83d\udce2 Imaginez un lieu o\u00f9 votre voix compte autant que vos comp\u00e9tences\u2026\nChez Listen too, nous sommes convaincus que l\u2019\u00e9coute est indissociable du progr\u00e8s. Cultiv\u00e9e au quotidien, elle devient un catalyseur de croissance et de r\u00e9ussite, tant pour l\u2019entreprise que pour ses collaborateur\u00b7rice\u00b7s.\nListen too est une agence conseil sp\u00e9cialis\u00e9e depuis 2013 dans la digitalisation de la Relation Client & Collaborateur. Gr\u00e2ce \u00e0 la justesse de l\u2019accompagnement offert \u00e0 nos clients comme \u00e0 nos consultant\u00b7e\u00b7s, nous nous sommes construits au fil des ann\u00e9es sur notre march\u00e9 une r\u00e9putation de v\u00e9ritables \u00ab horlogers du conseil \u00bb.\nNos domaines d\u2019expertise ?\n\ud83c\udfaf Le Product Management : pour porter la vision Produit et maximiser la cr\u00e9ation de valeur.\n\ud83c\udfa8 Le Product Design : pour une conception Produit centr\u00e9e utilisateur.\n\ud83d\udd27 Le Product TechOps : pour d\u00e9ployer les meilleures solutions digitales.\n\ud83d\udce2 Le Product Marketing : pour maximiser le succ\u00e8s d\u2019un produit/service sur son march\u00e9.\n\ud83d\udcca Le Product Data : pour mettre la donn\u00e9e au c\u0153ur de la strat\u00e9gie de nos clients.\n\ud83e\udd1d La Gouvernance projet : pour optimiser le mode de management et le cadre organisationnel de nos clients.\n\u00catre listenien\u00b7ne, c\u2019est \u00eatre au c\u0153ur d\u2019une entreprise qui valorise votre voix, votre parcours, et votre \u00e9volution. Ici, nous appliquons les principes du Design Thinking \u00e0 votre carri\u00e8re, co-construisant avec vous une trajectoire o\u00f9 vos expertises, vos aspirations, et vos forces sont au premier plan.\n\ud83d\udcaa Vos futures missions\nNous recherchons un Data Engineer qui aura la capacit\u00e9 de g\u00e9rer \u00e0 la fois les aspects MCO/TMA mais \u00e9galement la partie projets pour les diff\u00e9rentes directions du groupe en collaboration avec le reste de l\u2019\u00e9quipe.\n\u00c0 la suite d\u2019une passation avec notre centre de service, vous devrez \u00e9galement \u00eatre en mesure d\u2019innover et d\u2019am\u00e9liorer les processus existants pour perfectionner l\u2019efficience de l\u2019\u00e9quipe au quotidien dans ses diff\u00e9rentes t\u00e2ches.\nIl s'agit d'un poste pour de l'internalisation.\nConcr\u00e8tement, vous aurez l\u2019opportunit\u00e9 de :\n\u2022 Participer \u00e0 l\u2019analyse du besoin avec la supervision du PPO ou Manager et \u00e0 la conception du mod\u00e8le de donn\u00e9es permettant de r\u00e9pondre aux enjeux.\n\u2022 Utiliser l\u2019analyse des donn\u00e9es pour fournir des \u00e9l\u00e9ments significatifs et \u00e9clairer la conception/mise en \u0153uvre des projets\n\u2022 Concevoir, d\u00e9velopper et maintenir les pipelines de donn\u00e9es pour l\u2019acquisition, la transformation, le stockage et la mise \u00e0 disposition des donn\u00e9es.\n\u2022 Optimiser les performances des d\u00e9veloppements pour assurer une disponibilit\u00e9 et une fiabilit\u00e9 maximale\n\u2022 Identifier et r\u00e9soudre les goulots d\u2019\u00e9tranglement et les probl\u00e8mes de performances dans les flux de donn\u00e9es\n\u2022 Mettre en place des processus et des contr\u00f4les pour garantir la qualit\u00e9 des donn\u00e9es\n\u2022 Concevoir et mettre en \u0153uvre des syst\u00e8mes d\u2019alerte, de monitoring et de reprise automatique efficaces.\n\u2022 R\u00e9aliser la recette technique et animer la recette fonctionnelle avec les utilisateurs.\n\u2022 R\u00e9diger des documents associ\u00e9s au projet (Sp\u00e9cification Fonctionnelles, Sp\u00e9cifications Techniques, Cahier de recette Technique, Document d\u2019exploitation).\n\u2022 D\u2019\u00eatre force de proposition sur l\u2019am\u00e9lioration de notre stack Data.\n\u2022 Faire le reporting d\u2019avancement des travaux.\n\u2022 Support au d\u00e9ploiement.\n\u2022 Assurer la maintenance corrective ou \u00e9volutive.\n\u2022 S\u2019assurer de la coh\u00e9rence des livrables avec les r\u00e8gles et bonnes pratiques d\u00e9finies au sein de l\u2019\u00e9quipe.\n\u2022 S\u2019assurer L\u2019environnement Data dans le cadre de cette mission est actuellement constitu\u00e9 principalement d\u2019une base de donn\u00e9es Teradata (Cloud) ainsi qu\u2019un environnement BigQuery.\n\u2022 Langages/Framework : SQL, BigQuery, Python, Java, Shell\n\u2022 Outils : OpenText, Talend\n\u2022 Base de donn\u00e9es : Teradata, BigQuery, SQL Server, IBM DB2\n\ud83d\ude0e Votre vie de listenien\u00b7ne\nCe qu\u2019on vous propose ?\n\ud83e\udd1c \u00catre membre d\u2019une communaut\u00e9 : partager et enrichir vos comp\u00e9tences au sein d\u2019une \u00e9quipe soud\u00e9e.\n\ud83e\udd1d \u00catre accompagn\u00e9\u00b7e : dans votre progression professionnelle et votre quotidien par votre Consultant Manager, votre Business Manager et notre Responsable Exp\u00e9rience collaborateur\u00b7rice.\n\ud83c\udf93 \u00catre form\u00e9\u00b7e en continu : gr\u00e2ce \u00e0 un plan de d\u00e9veloppement des comp\u00e9tences co-construit et nourri par nos solutions d\u2019e-learning, les formations de la Listen too Academy et nos formations externes.\n\ud83d\ude80 \u00catre intrapreneur\u00b7e : encourag\u00e9 \u00e0 innover, \u00e9changer, entreprendre et ainsi contribuer \u00e0 l\u2019\u00e9panouissement du cabinet.\n\u2618\ufe0f \u00catre engag\u00e9\u00b7e : \u00e0 nos c\u00f4t\u00e9s dans une d\u00e9marche soci\u00e9tale et environnementale responsable et durable.\n\ud83e\udd17 \u00catre \u00ab bien \u00bb ! : parce que le bien-\u00eatre physique et mental de nos collaborateur\u00b7rice\u00b7s est au c\u0153ur de notre r\u00e9ussite.\nConcr\u00e8tement ?\n\ud83c\udf89 De multiples occasions de passer de bons moments avec notamment notre fameux s\u00e9mineige, des afterworks et des teambuildings.\n\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Du sport et de la sant\u00e9 avec des \u00e9v\u00e8nements sportifs, nos partenaires\nGymlib\n,\nZenride\n,\nMoka.care\net le programme\nVitality\n.\n\ud83d\ude0e Du confort avec du t\u00e9l\u00e9travail indemnis\u00e9.\n\ud83e\udd17 Toujours plus liens avec un onboarding aux petits oignons, notre programme de parrainage, notre appli interne Mylistentoo, nos newsletters, nos podcasts et nos webinaires.\n\ud83d\udc4d De nombreux avantages avec notre CSE, des tickets restaurant, une prime vacances, des primes de cooptation, de participation, de d\u00e9veloppement\u2026\n\ud83d\ude80 Profil recherch\u00e9\nVous avez au minimum 5 ans d\u2019exp\u00e9rience.\nVous disposez d\u2019une exp\u00e9rience confirm\u00e9e chez un Grand Compte.\nVous maitrisez l\u2019anglais dans un contexte professionnel tant \u00e0 l\u2019\u00e9crit qu\u2019\u00e0 l\u2019oral.\nVous avez envie de progresser et d\u2019\u00e9voluer au sein d\u2019une soci\u00e9t\u00e9 o\u00f9 votre voix compte.\nVous r\u00eavez de vous investir au sein d\u2019une communaut\u00e9 soud\u00e9e et passionn\u00e9e !\nD\u00e9roulement des entretiens ?\n\ud83d\ude4b\ud83c\udffc\u200d\u2640\ufe0f Un premier entretien t\u00e9l\u00e9phonique ou en visio avec l'une de nos charg\u00e9es de recrutement (Ines, Aur\u00e9lie ou Yasmine) pour voir si \u00e7a colle entre nous !\n\ud83d\udc81\ud83c\udffb\u200d\u2640\ufe0f Un deuxi\u00e8me en visio ou en pr\u00e9sentiel avec Aur\u00e9lien, notre Directeur de R\u00e9gion ou un de nos Business Manager (Lauranne, Hugo), pour valider notre premi\u00e8re impression.\n\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f Un dernier \u00e9change sur mesure avec Florent, Co-fondateur de l'agence, pour confirmer ce que nous savions d\u00e9j\u00e0 ! \ud83d\ude09\nVous l\u2019aurez compris, cette phase de recrutement est avant tout l\u2019occasion de s\u2019\u00e9couter et d\u2019\u00e9changer. Chez Listen too, nous c\u00e9l\u00e9brons la diversit\u00e9 et l\u2019inclusion, convaincus que chaque talent, quelle que soit son origine ou son parcours, est une richesse pour notre \u00e9quipe et contribue \u00e0 notre r\u00e9ussite commune.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "SQL Server",
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "ML",
                "Cloud"
            ],
            "FrSoftSkills": [
                "Collaboration",
                "Organisation"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        },
        "details": {
            "JobDetail": [
                "Confirm\u00e9"
            ],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data engineer python",
        "company": "FINAXYS",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-at-finaxys-3887107285?position=5&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=f3RCu7zpUVWESqBI4Y3PBg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LE CONTEXTE\nLeader en\nIT\n, dans les domaines Banque\n,\nFinance\net\nAssurance\n,\nFinaxys\nest un cabinet de\nconseil\ncr\u00e9\u00e9 en 2008. Nous accompagnons au quotidien les plus grandes banques du pays sur leur\ntransformation digitale\n(BNP Paribas, Soci\u00e9t\u00e9 G\u00e9n\u00e9rale, Cr\u00e9dit Agricole, Natixis, etc.)\nNos clients bancaires travaillent \u00e9galement dans des contextes Big Data sur des applications centrales rattach\u00e9es aux Datalakes.\nLES MISSIONS\nD\u00e9veloppement et traitements sur des applications Big Data (Python)\n\u00catre force de proposition sur les choix techniques les plus pertinents\nMaintenir la qualit\u00e9 des solutions, mesure de cette qualit\u00e9, alerte sur les non-conformit\u00e9s et validation des solutions d\u00e9finitives.\nAnalyser des risques li\u00e9s aux solutions envisag\u00e9es et proposition des actions de rem\u00e9diation.\nApporter des solutions IT r\u00e9pondant au mieux aux besoins du business port\u00e9 par la/le Product Owner (M\u00e9tiers/Fonctions) en cherchant toujours la maximisation de la valeur g\u00e9n\u00e9r\u00e9e\nAccompagner les \u00e9quipes dans les migrations Cloud\nENVIRONNEMENT TECHNIQUE\nPython\nPandas\nScirpting Big Data\nCulture DevOps (Jenkins, Maven, Ansible)\nPROFIL\nComp\u00e9tences Techniques et Fonctionnelles requises\nMaitrise obligatoire de l\u2019anglais\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "Pandas",
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Ansible"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps",
                "Big Data",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Alternant - Data Engineer H/F",
        "company": "ALLIANCE EMPLOI",
        "location": "La Couture, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/alternant-data-engineer-h-f-at-alliance-emploi-3913991795?position=6&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=IUN5rmntBuBv0NURN0B9ZA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La campagne \" alternance2024 \" est lanc\u00e9e ! \u00cates-vous pr\u00eat(e) \u00e0 monter en comp\u00e9tences et acqu\u00e9rir de l'exp\u00e9rience ? Avec 25 ans d'expertise, 2000 salari\u00e9s et notre r\u00e9seau de 400 entreprises issues des secteurs industriel, agroalimentaire, automobile, m\u00e9tallurgie, pharmaceutique, sid\u00e9rurgie ou encore logistique, nous sommes la destination id\u00e9ale pour celles et ceux qui cherchent une alternance.\nLaur\u00e9at 2021 des P\u00e9pites de l'alternance, notre mission est simple : apporter la bonne comp\u00e9tence au bon moment. Et c'est l\u00e0 que vous entrez en jeu !\nNous sommes \u00e0 la recherche d'un Alternant - Data Engineer (H/F) en alternance pour une entreprise partenaire sp\u00e9cialis\u00e9e dans la chimie pour une dur\u00e9e de 12 \u00e0 24 mois au sein du Utilit\u00e9s qui a en charge la fourniture d'utilit\u00e9s pour l'ensemble du site.\nLe poste et les missions ?\nInformatique\nModernisation des pratiques de collecte de donn\u00e9es industrielles,\nCr\u00e9ation d'interface saisie d'encours de production\nRefonte des rapports d'exploitation\nAlgorithme de num\u00e9risation des rapports PDF\nAutomatisation des archivages\nGestion des datas consolid\u00e9es\nStatisitques\nInt\u00e9gration des statistiques au coeur des syst\u00e8mes de production\nCartes de contr\u00f4les\nPr\u00e9visions statistiques\nEtude d'implantation de machine learning\nAlgorithme de traitement et nettoyage des donn\u00e9es (analyse de la d\u00e9rive)\nOrganisationnel\nD\u00e9ploiement des solutions de power BI au sein du service\nOutils d'affiche, de partage et d'analyse de donn\u00e9es\nAnalyse fonctionnelle des flux de donn\u00e9es\nR\u00e9daction des logigrammes de gestion de donn\u00e9es\nR\u00e9daction des bonnes pratiques d'archivage et de traitement de don\u00e9nes\nFormation des utilisateurs et propri\u00e9taires\nCe que nous allons aimer chez vous ?.\nVous avez le sens de l'organisation et du service, une capacit\u00e9 \u00e0 s'int\u00e9grer dans un milieu technique de terrain, vous avez le sens de l'organisation et des priorit\u00e9s, vous \u00eates rigoureux\nMais aussi :\nVous pr\u00e9parez un dipl\u00f4me d'ing\u00e9nieur Bac +4 / 5 en informatique et analyse de donn\u00e9es\nVous ma\u00eetrisez le d\u00e9veloppement informatique (base de donn\u00e9es, Python, Java)\nLes avantages de rejoindre Alliance Emploi\nContrat : ALTERNANCE de 12 \u00e0 24 mois\nD\u00e9but : Septembre 2024\nLieu : LESTREM [ site non accessible en transport en commun]\nR\u00e9mun\u00e9ration : Selon le bar\u00e8me de l'alternance\nEt aussi : int\u00e9ressement, mutuelle, pr\u00e9voyance, CSE, formations qualifiantes\nEt ce n'est pas tout ! En choisissant Alliance Emploi, vous vivrez une exp\u00e9rience bas\u00e9e sur la confiance, la solidarit\u00e9 et l'engagement. Vous d\u00e9velopperez vos comp\u00e9tences \u00e0 travers une diversit\u00e9 de missions et notre r\u00e9seau d'entreprises, et nous nous engageons \u00e0 vous proposer un accompagnement personnalis\u00e9 pour booster votre carri\u00e8re.\nAlors convaincu(e) ?\nN'attendez plus pour postuler et venez d\u00e9couvrir la diff\u00e9rence Alliance Emploi !\nLa diversit\u00e9 est une force. Nous sommes engag\u00e9s pour l'inclusion en offrant des opportunit\u00e9s de carri\u00e8re \u00e0 toutes les personnes, ind\u00e9pendamment de leur genre ou de leur situation de handicap.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Java",
                "R",
                "Go"
            ],
            "DataBase": [],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "Power BI"
            ],
            "Statistics": [
                "Statistiques"
            ],
            "CloudComputing": [],
            "DevTools": [],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Machine Learning",
                "Statistiques"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "25",
                "25",
                "25"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Consultant Data Engineer",
        "company": "WHIZE",
        "location": "Neuilly-sur-Seine, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/consultant-data-engineer-at-whize-3916769237?position=7&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=tBWGhRh06hBOkPIFde2ODg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Offre d\u2019emploi pour un CDI : Consultant Data Engineer\nWHIZE est sp\u00e9cialis\u00e9e dans le d\u00e9veloppement de solutions sur mesure en architecture Serverless (Azure, Amazon WS, Google CP) et de solutions bas\u00e9es sur l'\u00e9cosyst\u00e8me Microsoft 365 (SharePoint, Teams, Power Platform).\nVos missions :\nConcevoir des solutions de traitement de volume tr\u00e8s important de donn\u00e9es.\nD\u00e9veloppement de flux de donn\u00e9es et pr\u00e9paration de leur analyse.\nPr\u00e9paration des donn\u00e9es pour l'analyse des donn\u00e9es collect\u00e9es.\nProfil recherch\u00e9 :\n2 ans minimum d\u2019exp\u00e9rience.\nMa\u00eetrise du langage Python et Scala\nConnaissance d'un ou plusieurs ETL du march\u00e9 (Talent , SSIS, Azure Data Factory, ...)\nForte expertise en SQL\n\u00catre \u00e0 l\u2019aise avec un ou plusieurs outils Devops (Jenkins, git, GitHub, gitlab, docker, kubernetes, etc\u2026)\nConnaissances appr\u00e9ci\u00e9es :\nHadoop, Spark, Kafka\nConnaissance des syst\u00e8mes NoSQL : Elasticsearch, HBase, Cassandra, Redshift\nConnaissance de l'offre data d'un des providers Cloud (GCP, Azure, AWS)\nQu\u2019attendez vous pour nous rejoindre ?\nVous ferez partie d\u2019une soci\u00e9t\u00e9 \u00e0 taille humaine et qui b\u00e9n\u00e9ficie des avantages d'un grand groupe. Nous adressons une centaine de clients en direct dont la moiti\u00e9 sont des grands comptes.\nVous serez accompagn\u00e9(e) et manag\u00e9(e) par le CEO de WHIZE (THE WHIZE MAN).\nVous allez compl\u00e9ter notre \u00e9quipe dynamique et travailler avec nous dans une ambiance Start-up et conviviale.\nVous occuperez des postes int\u00e9ressants et \u00e9volutifs.\nVous b\u00e9n\u00e9ficierez des \u00e9v\u00e8nements internes organis\u00e9s pour parler tech, business et projets.\nVous r\u00e9aliserez des projets \u00e0 forte valeur ajout\u00e9e.\n\ud83d\udccd : Neuilly-Sur-Seine+ T\u00e9l\u00e9travail\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL",
                "Cassandra",
                "HBase",
                "Elasticsearch"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker",
                "Jenkins"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "DevOps",
                "Cloud"
            ],
            "FrSoftSkills": [],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [
                "CDI"
            ],
            "Salary": [],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "2",
                "2",
                "2"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Tech Lead Data Engineer",
        "company": "AXA en France",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/tech-lead-data-engineer-at-axa-en-france-3837261944?position=8&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=ZUYIbU0zZpg2YW%2FH1I6apw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Environnement\nEn tant que\nTech Lead Data Engineer F/H\n, vous allez contribuer directement aux projets des directions m\u00e9tier (ex : fraude sant\u00e9, multi\u00e9quipements, pricing IARD, optimisation du lead management, fragilit\u00e9 auto, \u2026) d\u2019AXA France et \u00e0 la construction du socle technique Big Data.\nVous allez int\u00e9grer une \u00e9quipe d'une dizaine de personne compos\u00e9e de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus m\u00e9tier de la Direction Transformation Digital Tech et DATA (DT2).\nLa Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :\n- Une organisation agile en feature teams : tribus, guildes, squads\n- Des projets sur des applications innovantes \u00e0 fort trafic (web, mobile\u2026)\n- Des m\u00e9thodologies craft (TDD, BDD, clean code, code review\u2026) et DevOps\n- Une communaut\u00e9 de partage de bonnes pratiques (BBL, dojo, meetup, conf\u2026)\nVotre r\u00f4le et vos missions\nVous aurez pour missions principales de d\u00e9velopper les projets Big Data demand\u00e9s par le m\u00e9tier, et notamment :\nD\u2019accompagner techniquement les Data Engineer de l\u2019\u00e9quipe (coaching, code review, pair programming\u2026)\nPasser de la donn\u00e9e brute \u00e0 de la donn\u00e9e exploitable, expos\u00e9e sous forme de tables requ\u00eatables dans le datalake\nConsolider ces donn\u00e9es au fur et \u00e0 mesure de leur alimentation r\u00e9currente dans le data lake\nLes exploiter pour atteindre la finalit\u00e9 business (exposition de business view, r\u00e9int\u00e9gration des r\u00e9sultats dans le SI, service de scoring, \u2026)\nDe travailler \u00e0 la cr\u00e9ation du socle technique Big Data et industrialiser le cycle de d\u00e9veloppement de l'\u00e9quipe\nDe mettre en place et de garantir le respect dans la dur\u00e9e d'un processus qualit\u00e9 sur l'ensemble du cycle de DEV (documents, tests unitaires / int\u00e9gration / fonctionnels, commentaires, versionning, etc.)\nVotre profil\nD'une formation sup\u00e9rieure en informatique ou scientifique (Master ou Dipl\u00f4me d'ing\u00e9nieur), vous justifiez de plusieurs exp\u00e9riences significatives (+ de 7 ans)\nsur du d\u00e9veloppement big data, en particulier sur du PySpark.\nComp\u00e9tences techniques :\nConnaissances avanc\u00e9es en d\u00e9veloppement en\nPySpark (Spark avec le langage Python)\nMaitrise de l'environnement\nMicrosoft Azure\nConnaissances avanc\u00e9es d'outils de BI comme\nPowerBI\nComp\u00e9tences transverses :\nCapacit\u00e9 \u00e0 interagir avec des parties prenantes diverses : Business analyst, Architectes, M\u00e9tier\nExp\u00e9rience en mode de delivery Agile (Scrum, Kanban, etc...)\nDriver et accompagner des Data Engineer sur le plan op\u00e9rationnel\nEt Id\u00e9alement :\nAvoir une exp\u00e9rience en tant que lead\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de donn\u00e9es relationnelles et NoSQL\nUne exp\u00e9rience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nMais pourquoi AXA France ?\nNous sommes persuad\u00e9s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs ! Les avantages que nous proposons \u00e0 nos salari\u00e9s sont nombreux.\nNous choisir, c\u2019est b\u00e9n\u00e9ficier par exemple :\nD\u2019un package de r\u00e9mun\u00e9ration complet comprenant un salaire fixe, un compl\u00e9ment de r\u00e9mun\u00e9ration variable, des primes, de la participation et de l\u2019int\u00e9ressement, la possibilit\u00e9 d\u2019acqu\u00e9rir des actions AXA, ou encore des solutions d\u2019\u00e9pargne avantageuses ;\nEquilibre vie Pro / Perso. : D\u2019un cadre de travail flexible jusqu\u2019\u00e0 3 jours de t\u00e9l\u00e9travail possible par semaine, des tickets restaurant pour les jours t\u00e9l\u00e9travaill\u00e9s ou encore une participation \u00e0 l\u2019achat d\u2019un \u00e9cran ou fauteuil ergonomique ;\nD\u2019une politique visant \u00e0 concilier vie personnelle et vie professionnelle avec 28 jours de cong\u00e9s pay\u00e9s, entre 14 et 16 RTT selon les ann\u00e9es, des formules de travail \u00e0 temps partiel ou encore des jours d\u2019absence r\u00e9mun\u00e9r\u00e9es pour la rentr\u00e9e scolaire ou un d\u00e9m\u00e9nagement par exemple ;\nDe la possibilit\u00e9 de s\u2019engager pour une cause qui vous tient \u00e0 c\u0153ur gr\u00e2ce \u00e0 nos associations telles que AXA Atout C\u0153ur, AXA Comp\u00e9tences Solidaires ou encore AXA Pr\u00e9vention ;\nEt bien plus encore ! Perspectives de d\u00e9veloppement des comp\u00e9tences et de carri\u00e8res immenses, CE, conciergerie, offres privil\u00e8ges, soutien en cas d\u2019\u00e9preuve personnelle\u2026On s\u2019arr\u00eate l\u00e0, la liste est longue\nQui sommes nous ?\nAXA est un des leaders de l\u2019assurance et de la gestion d\u2019actifs dans le monde.\nNous aidons nos 108 millions de clients \u00e0 traverser les petites et grandes difficult\u00e9s de la vie.\nChaque jour, nous agissons ensemble pour inventer la meilleure mani\u00e8re de les prot\u00e9ger et voulons donner \u00e0 chacun les moyens de vivre une vie meilleure.\nUn challenge qui donne le sourire et envie de se lever le matin !\nChez AXA, nous sommes persuad\u00e9s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C\u2019est pour cette raison que nous menons une politique RH engag\u00e9e qui favorise la diversit\u00e9, qui pr\u00e9serve l\u2019\u00e9quilibre vie priv\u00e9e-vie professionnelle et acc\u00e9l\u00e8re le d\u00e9veloppement des comp\u00e9tences et des carri\u00e8res.\nAinsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une v\u00e9ritable culture d\u2019expertise, acc\u00e9l\u00e9rant le d\u00e9veloppement des comp\u00e9tences de chacun et proposant une r\u00e9mun\u00e9ration attractive.\nPourquoi nous rejoindre ?\nVous \u00eates porteur d\u2019id\u00e9es et d\u2019initiatives innovantes ? Vous proposez des solutions et \u00eates au service du client ? Faites partie de notre grande famille en rejoignant\nUn leader mondial offrant des opportunit\u00e9s de carri\u00e8res int\u00e9ressantes\nUne entreprise qui donne une place de choix \u00e0 l\u2019innovation, \u00e0 l\u2019initiative et aux actions solidaires (notamment via l\u2019association AXA Atout C\u0153ur)\nUn environnement inclusif \u00e0 tous les niveaux (mixit\u00e9, handicap, initiatives pour favoriser l\u2019insertion des jeunes, orientation sexuelle, etc.)\nUn acc\u00e8s \u00e0 de multiples avantages (cong\u00e9s, temps partiel, t\u00e9l\u00e9travail, etc.)\nUn cadre stimulant, qui permet de rencontrer des collaborateurs performants et d\u2019enrichir ses comp\u00e9tences\nVictime ou t\u00e9moin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination \u00e0 alerte.discrimination.harcelement@axa.fr\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark",
                "Databricks"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [
                "JIRA",
                "Teams"
            ],
            "Other": [
                "DevOps",
                "Big Data"
            ],
            "FrSoftSkills": [
                "Organisation"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [
                "3",
                "3"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "7",
                "7",
                "7"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer",
        "company": "Pictarine",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-pictarine-3911913926?position=9&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=n3HEyd%2F98Jz4oy37fKFd1g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Mission and challenges \ud83c\udfaf\nSi tu es enthousiaste \u00e0 embarquer dans la nouvelle \u00e9quipe data de Pictarine pour la faire rayonner avec tout ton savoir-faire, alors c\u2019est l\u2019aventure qu\u2019il te faut! \ud83c\udfd4\ufe0f\nAvec plus de 1K tables, 2M de clients et 4M de commandes en 2022, les \u00e9quipes de Pictarine ne sont jamais \u00e0 court d\u2019id\u00e9es pour explorer de nouveaux horizons. \ud83d\ude80\nEn tant que Data Engineer chez Pictarine tu vas pouvoir utiliser toutes tes comp\u00e9tences SQL pour garantir la qualit\u00e9 de la data sur GCP, accompagner et challenger les besoins data.\nTu \u00e9volueras au sein de l\u2019\u00e9quipe Engineering, compos\u00e9e des p\u00f4les dev & data.\nTon r\u00f4le comprendra les aspects suivants \ud83d\udc47\ud83c\udffb\nTu es garant de la qualit\u00e9 de la data !\nEn simplifiant la structure de la data et r\u00e9duisant le nombre de tables\nEn transformant les donn\u00e9es pour les rendre facilement utilisables\nEn orchestrant le flux des donn\u00e9es de mani\u00e8re continue et automatique\nTu accompagnes et challenges les \u00e9quipes de Pictarine !\nEn co-construisant des solutions data appropri\u00e9es\nEn \u00e9levant le niveau de jeu des m\u00e9thodes data existantes\nEn faisant rayonner la data autour de bonnes pratiques et d\u2019outillages ad\u00e9quates\nProfil Recherch\u00e9\nAbout you \ud83d\udc8e\nTu as au moins 5 ans d\u2019exp\u00e9rience sur un poste similaire\nTu ma\u00eetrises le data warehouse BigQuery et son langage SQL\nTu es \u00e0 l'aise avec les services GCP\nTu as de bonnes connaissances dans la conception de mod\u00e8les de donn\u00e9es et les strat\u00e9gies d'optimisation des requ\u00eates SQL\nTu as des comp\u00e9tences en DevOps pour le d\u00e9ploiement et la gestion efficace des pipelines de donn\u00e9es\nTu as une bonne ma\u00eetrise de Python & Github\nTu es organis\u00e9, rigoureux et portes une grande attention aux d\u00e9tails\nTu es dot\u00e9 d\u2019excellentes qualit\u00e9s relationnelles, de communication et de vulgarisation\nTu as une passion pour r\u00e9soudre des probl\u00e8mes business avec la programmation\nTu es curieux de tester des nouvelles technologies\nTu es un team player et toujours \u00e0 l'aff\u00fbt de nouvelles id\u00e9es\nWork @ Pictarine\u2728\nUn environnement de travail agile, collaboratif, international et multiculturel\nDes perspectives d\u2019\u00e9volution rapides\nDes locaux tout beaux \u00e0 Lab\u00e8ge avec du mat\u00e9riel dernier cri (mais aussi des snacks \u00e0 profusion et un frigo \u00e0 boissons toujours bien rempli)\nUn apprentissage permanent : conf\u00e9rence, meet-up, Pictarine Academy, cours d\u2019anglais.\nDes events tous les mois : massage, pilates, TGIF, team building .\nUn environnement de travail flexible : horaires, politique de remote hybride.\nUn package de r\u00e9mun\u00e9ration attractif : salaire comp\u00e9titif, RTT, mutuelle & pr\u00e9voyance 100% prise en charge, int\u00e9ressement.\nDes petits + : D\u00e9veloppement de photos gratuit, subvention sport, 3 jours \u201centraide familiale\u201d, jours de cong\u00e9s en plus avec l'anciennet\u00e9... \ud83e\udd2b on ne te d\u00e9voile pas tout !\nRecruitment process \u2699\ufe0f\nTu souhaites nous rejoindre ? Viens rencontrer les gens avec qui tu vas bosser :\n1er \u00e9change pour apprendre \u00e0 se conna\u00eetre avec Marie - Engineering Manager Data (15\u2019)\nEntretien Manager avec Marie (60-90\u2019)\nTest pratique afin de nous montrer tes talents \ud83d\ude42 (3 heures)\nEntretien final avec 2 membres du Codir (90\u2019)\nWelcome aboard !\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "R",
                "Go"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "DevOps"
            ],
            "FrSoftSkills": [
                "Communication"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        },
        "details": {
            "JobDetail": [
                "Hybride",
                "Remote"
            ],
            "TypeContract": [],
            "Salary": [
                "100",
                "100"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s",
                "5",
                "5",
                "5"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "title": "Data Engineer (H/F)",
        "company": "CITECH",
        "location": "Paris, \u00cele-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-citech-3908612761?position=10&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=QDl21Yh58PhaCfqsKPTodg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Description de l'entreprise\nCITECH recrute !\nSi vous souhaitez apporter vos comp\u00e9tences dans la r\u00e9alisation d\u2019un projet important, nous avons LA mission pour vous ! Nous recherchons en effet un(e)\nData Engineer (H/F)\nVotre mission est pour un client reconnu dans le secteur bancaire, implant\u00e9 dans de nombreuses villes en France, il a pour objectif d'acc\u00e9l\u00e9rer sa transformation digitale afin d'offrir toujours plus de solutions et de services innovants.\nDescription du poste\nVous aurez donc les missions principales suivantes :\nSupport de l'application (notamment lors des cl\u00f4tures mensuelles).\nParticiper \u00e0 la maintenance \u00e9volutive.\nParticiper \u00e0 la conception et l'impl\u00e9mentation de nouvelles fonctionnalit\u00e9s.\nParticiper \u00e0 la refonte technique.\nParticiper \u00e0 la migration de Talend vers Spark/scala.\nQualifications\nDe formation sup\u00e9rieure en informatique, vous justifiez de 5 ann\u00e9es d\u2019exp\u00e9rience minimum sur un poste similaire.\n\u2699\ufe0f Les comp\u00e9tences attendues sont les suivantes :\n\u2714\ufe0f Vous ma\u00eetrisez Spark, Talend (Data Int\u00e9gration, Big Data) et Scala.\n\u2714\ufe0fVous avez des comp\u00e9tences en d\u00e9veloppement (Shell unix, Perl, PHP, Python, git, github).\n\u2714\ufe0fVous avez des comp\u00e9tences sur l\u2019environnement technique suivant :\nHadoop (Big Data), Hive, Microsoft PowerBI, Microsoft SQLServer Analysis services (Olap), Integration services, Reporting services, Scripting (GuitHub, Ansible, AWX, shell, vba) et SQL Server Database.\nInformations suppl\u00e9mentaires\nPoste situ\u00e9 \u00e0\nParis\n\u2600\ufe0f\nSalaire :\n45-65 K\u20ac brut/an\nFreelance :\n300-450 \u20ac brut/jour\nR\u00e9f\u00e9rence :\n240424_OUTIL DE PILOTAGE FINANCIER - TALEND / SPARK / SCALA\nPourquoi rejoindre Citech ?\nUne ambiance de travail conviviale avec des afterworks organis\u00e9s r\u00e9guli\u00e8rement !\nDes missions de longues dur\u00e9es\nDes formations adapt\u00e9es \u00e0 vos envies et vos aspirations\nUne mobilit\u00e9 que si vous le souhaitez\nUn accompagnement personnalis\u00e9 avec un suivi r\u00e9gulier (autour d\u2019un caf\u00e9 ou un th\u00e9, c\u2019est vous qui choisissez )\nUne mutuelle avantageuse pour vous mais aussi pour les membres de votre famille\nUne flexibilit\u00e9 sur la gestion de vos repas\nUn statut Cadre et une convention collective SYNTEC\nAlors qu\u2019attendez-vous pour nous rejoindre ?\nCompany Description\nCITECH ce n\u2019est pas une Entreprise de Services du Num\u00e9rique comme les autres : c\u2019est avant tout une aventure humaine. Nous cherchons des personnalit\u00e9s passionn\u00e9es qui nous ressemblent ! Dans un environnement convivial et chaleureux, venez r\u00e9v\u00e9ler vos talents !\nNos fid\u00e8les clients, reconnus \u00e0 l\u2019\u00e9chelle internationale, offrent \u00e0 nos collaborateurs de multiples possibilit\u00e9s de carri\u00e8re. Nos domaines d\u2019interventions sont vari\u00e9s : banque, assurance, automobile, sant\u00e9, transport ou encore la robotique, vous trouverez forc\u00e9ment un secteur \u00e9panouissant, \u00e0 votre image !\nVous avez un projet professionnel ? Nous sommes l\u00e0 pour vous aider \u00e0 le d\u00e9velopper. Pour nous, l\u2019essentiel c\u2019est vous. C\u2019est pourquoi nous assurons un suivi r\u00e9gulier et portons une attention toute particuli\u00e8re \u00e0 votre plan de carri\u00e8re.\nShow more\nShow less",
        "skills": {
            "ProgLanguage": [
                "Python",
                "Scala",
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [],
            "DataSerialization": [],
            "DataVisualization": [
                "PowerBI"
            ],
            "Statistics": [],
            "CloudComputing": [],
            "DevTools": [
                "Git"
            ],
            "Os": [],
            "DBMS": [
                "SQL Server"
            ],
            "SoftBigDataProcessing": [],
            "Automation": [
                "Ansible"
            ],
            "InfrastructureAsCode": [],
            "NetworkSecurty": [],
            "Virtualisation": [],
            "Containers": [],
            "Collaboration": [],
            "Other": [
                "Big Data"
            ],
            "FrSoftSkills": [
                "Flexibilit\u00e9"
            ],
            "EnSoftSkils": []
        },
        "details": {
            "JobDetail": [],
            "TypeContract": [],
            "Salary": [
                "45"
            ],
            "Level": [],
            "Experience": [
                "a",
                "n",
                "s"
            ]
        }
    }
]