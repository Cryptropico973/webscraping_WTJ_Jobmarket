[
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Ille-et-Vilaine, Brittany, France",
        "link": "https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-at-akademija-oxford-3912800588?position=2&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=qxkAxO3kCKPFsjtGrF%2F6AA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous souhaitez intégrer une école à taille humaine ? Vous êtes intéressé.es par la donnée ? Sup de Vinci propose une formation complète, en Mastère Big Data & Intelligence Artificielle (2 années en alternance).\nL’école Sup de Vinci, Rennes, accompagne l’une de ses entreprises partenaires dans son projet de recrutement d’un profil ingénieur data, en alternance pour la rentrée 2022.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Vélizy-Villacoublay, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/hadoop-data-engineer-f-h-at-thales-3890949542?position=3&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=NUzkjoDiCgv%2B68vSb9TBHw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.\nThales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.\nQUI ETES-VOUS ?\nDiplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.\nVOS MISSIONS :\n• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Hadoop.\n• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.\n• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.\n• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.\n• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.\n• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.\n• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.\nInnovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "BigData": [
                "Hadoop"
            ],
            "Other": [
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Air France",
        "location": "Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-d%C3%A9veloppeur-big-data-%23-h-f-at-air-france-3900080172?position=4&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=r6G3xtDtwyoSIYOezOARsw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Description du poste\nIntitulé du poste\nData Engineer / Développeur Big Data # H/F\nMétier\nSystèmes d'informations - Développement\nCatégorie socio-professionnelle\nCadre\nPrésentation du contexte\nVous avez peut-être déjà voyagé avec nous, mais que connaissez-vous de nos métiers et de la richesse des données qu’ils génèrent au quotidien ? Comment le traitement et l’exploitation de ces données peut contribuer à notre stratégie de Revenue Management, ou encore aux multiples opérations à réaliser pour permettre à un vol de partir à l’heure ?\nAir France-KLM fait rêver 104 millions de passagers par an, en les emmenant vers plus de 250 destinations, grâce à une flotte de plus de 500 appareils. Le Groupe emploie 80 000 collaborateurs partout dans le monde :les opportunités sont vastes pour mettre à profit ses compétences, apprendre et se développer !\nLe département de développement DATA, OR & AI d’Air France, au sein de la direction des Systèmes d’Information, intervient dans toute la chaîne de captation et de traitement des données du groupe pour délivrer à nos métiers des solutions applicatives clés en main.\nLe département est également en charge de l’ensemble des outils techniques (ETL, DataLakes, DataWarehouses, Data visualisation) et du développement des talents et compétences de Data Engineering.\nNotre mission ? Transformer la donnée brute en décision intelligente, pour mieux optimiser les métiers d’Air France – KLM !\nPour cela, nous avons chacun un rôle essentiel à jouer, pourquoi le vôtre ne serait pas celui de Data Engineer et de développeur Big Data ?\nDescription de la mission\nAu sein de notre département, vous travaillerez main dans la main avec d’autres Data Engineers et développeurs Big Data ainsi qu’avec des spécialistes des métiers.\nIntégré au sein d’une product team agile passionnée et dynamique :\nVous participez à l’analyse des besoins métiers du commercial, des opérations aériennes, de l’exploitation sol en aéroport, de la maintenance aéronautique ou encore du Cargo.\nVous contribuez à la définition, au développement, à l’industrialisation et à la maintenance d’applications Big Data ou en Business Intelligence\nVous présentez la restitution de vos travaux et accompagnez les utilisateurs d’un point de vue fonctionnel ou méthodologique\nVous serez en contact avec les directions métier du groupe Air France KLM.\nNous attachons beaucoup d'importance au développement des compétences de nos collaborateurs ainsi qu’à leur offrir des conditions de travail favorables à l’autonomie et aux missions à forte valeur ajoutée. L'ouverture, le respect, la bienveillance et le partage sont des valeurs humaines portées par l'entreprise.\nProfil recherché\nVous êtes diplômé de niveau Master ou Ingénieur dans les domaines informatiques, vous avez acquis une expérience professionnelle dans le développement d’applications.\nVous disposez d’une expérience du développement indispensable en Backend / Java\nVous maîtrisez les bases de données relationnelles et le langage SQL\nEn Complément, Vous Avez Une Connaissance Ou Une Expérience Dans Tout Ou Partie Des Concepts Ou Outils Suivants\nEnvironnement Big Data (Spark, Hadoop, Elasticsearch, Kafka, ...)\nBase de données noSQL (MongoDB, HBase, REDIS) ou Data Warehouse Teradata\nOutil de Datavisualisation (Spotfire, PowerBI, Qlik ou Kibana)\nSolutions de Cloud (GCP) et hybride (GCP / AZURE)\n(Ces compétences complémentaires ou manquantes pouvant aussi s'acquérir à travers un parcours de reskilling et de formations aux outils du data engineering dispensé en interne).\nVous avez participé à des projets organisés en Scrum ou Kanban, et avez peut-être même œuvré comme Scrum-Master, ce qui vous permettra de vous intégrer aisément au sein d’une Product Team. Votre esprit de synthèse, votre force de conviction et votre maîtrise de la communication facilitent les décisions avec l’ensemble des collaborateurs de l’équipe, éventuellement en langue anglaise, à l’écrit comme à l’oral.\nVous êtes autonome, rigoureux(se), responsable et curieux(se), vous aimez travailler en équipe. Vous possédez de bonnes capacités d'écoute, d'analyse, de synthèse et de communication.\nEt bien sûr, vous êtes passionné(e), enthousiaste et ingénieux(se)\nCe que nous vous offrons\nDe la création de valeur pour l’ensemble des métiers d’Air France KLM\nDes challenges et problématiques complexes à résoudre\nL’opportunité de déployer des solutions Data industrielles à l’échelle !\nUne grande part de responsabilité dans une structure hiérarchique horizontale\nUn important degré de liberté pour apprendre et développer son expertise au sein de l’équipe\nOn vous attend le plus rapidement possible ! Et pour une durée indéterminée ;)\nType de contrat\nCDI\nTemps partiel possible\nNon\nType d'horaires\nAdministratif\nProfil candidat\nNiveau d'études min. requis\nBac + 5 et plus\nLangue\nAnglais (4 - Confirmé / C1)\nLocalisation du poste\nLocalisation du poste\nFrance, Provence-Alpes-Côte d'Azur, Alpes Maritimes (06)\nSite\nValbonne\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Confirmé"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java"
            ],
            "DataBase": [
                "MongoDB",
                "HBase",
                "Elasticsearch",
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "Azure"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/apprenti-data-engineer-h-f-at-akademija-oxford-3917872047?position=5&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=rmaFYM2oHNOKTV4fxpDKVg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Une de nos entreprises partenaires, ESN située à Paris, recherche un Apprenti Data Engineer (H/F) préparant un bac +4/+5 spécialité Big Data pour la rentrée de Septembre 2021.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/apprenti-e-ing%C3%A9nieur-data-at-akademija-oxford-3912806043?position=6&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=UHqnpuGea9Itifl0Aoj1Lg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous souhaitez intégrer une école à taille humaine ? Vous êtes intéressé par le secteur du développement informatique ? Sup de Vinci propose une formation complète en Mastère Big Data.\nL’école Sup de Vinci à Bordeaux, accompagne l’une de ses entreprises partenaires dans son projet de recrutement d’un profil Ingénieur Data, en alternance pour la rentrée 2022.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "DataScientest.com",
        "location": "Puteaux, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-poei-at-datascientest-com-3909358387?position=7&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=dzcO8ZpLWKV7kVqbDMyaZA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data Engineer (H/F) | POEI\nPuteaux\nCDI\nPostuler\nRetour\nDatascientest Is Hiring!\nData Engineer (H/F) | POEI\nÀ propos\nVous êtes demandeur d'emploi et vivement intéressé(e) par les métiers de la Data ?\nRejoignez DataScientest en intégrant une formation 100% financée par Pôle Emploi afin d’acquérir les compétences clés qui vous permettront de booster votre carrière en tant que Data Engineer Cloud, un métier en tension et en plein essor.\nCette formation est certifiée par l'Ecole des Mines ParisTech, et inclut le passage de certifications éditeurs (AWS, Microsoft Azure ou encore GCP) qui garantissent votre employabilité.\nAprès avoir suivi notre formation de Cloud Data Engineer, vous rejoindrez, en CDI , notre entreprise partenaire.\nLes candidats retenus bénéficieront d’une formation intensive, entièrement prise en charge par le dispositif POEI (Préparation Opérationnelle à l’Emploi Individuel) avec Pôle-Emploi.\nDescriptif du poste\nEn Tant Que Cloud Data Engineer, Vous Aurez Pour Missions De Proposer Les Meilleures Solutions Aux Entreprises Afin D'optimiser Leur Activité, à Travers Les Missions Suivantes\nDéveloppement de solutions permettant de traiter des volumes importants de données,\nConception, collection et fabrication des données brutes,\nCréation d'outils et algorithmes pour le traitement des données,\nPréparation des données pour le Data Analyst,\nSécurisation des Pipelines données pour les Data Analysts et Data Scientists,\nOrganisation de l'architecture du cloud\nProfil recherché\nCe Que Nous Vous Offrons\nUne certification de l'Ecole des Mines ParisTech\nUn CDI auprès d'un de nos partenaires, expert européen dans le traitement et l'exploitation des données\nUn salaire attractif à la clé : 35 000€ à 48 000€ selon le profil\n**Votre profil : **\nIssu(e) d’une filière scientifique ou informatique vous disposez d'un bac+5 ou d’un diplôme d’ingénieur,\nVous disposez idéalement d’une expérience significative en développement informatique, en architecture réseaux ou dans la Data,\nVous maîtrisez un langage objet type Java, Python, C++, etc.\nVous êtes inscrit(e) à Pôle Emploi\nInformations complémentaires\nType de contrat : CDI\nDate de début : 01 septembre 2023\nLieu : Puteaux\nNiveau d'études : Bac +5 / Master\nExpérience : > 1 an\nTélétravail ponctuel autorisé\nSalaire : entre 35000€ et 48000€ / an\nVous êtes intéressé par cette offre ?\nPostuler\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "35",
            "Level": "Bac+5",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-%E2%80%93-paris-france-h-f-at-astek-3470775874?position=8&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=q4j7hA8VWh11yPRBq9Ju3Q%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nParis - France\nPubliée il y a 2 mois\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nNous rejoindre en tant que\nBig Data Engineer (H/F),\nafin d’accompagner un opérateur télécoms, Leader en Europe dans la création d’une infrastructure cloud (IAAS) performante, robuste et sécurisée.\nUn challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !\nVotre Mission, Si Vous L’acceptez :\nQualifier les données et les résultats\nConception technique des solutions\nDécliner les impacts de la stratégie et des innovations technologiques au sein des processus et outils de l’exploitant SI\nAssurer l’accompagnement et le déploiement des évolutions des processus et outils\nContribuer aux programmes de transformation DevOps, Cloud et catalogues des offres SI\nDévelopper des fonctions transverses et les « uses cases »\nAccompagner la phase de mise en production\nVotre Future Équipe :\nVous intégrerez une équipe à la fois technique et fonctionnel, qui œuvre chaque jour pour développer et maintenir en conditions opérationnelles l’ensemble des solutions IT !\nL’équipe est en interaction avec des clients à la fois internes et externes.\nVotre stack de jeu\nSystème d’exploitation : Linux\nEnvironnement Big data : Hadoop, Spark, Scala\nCloud computing : GCP ou AWS\nBase de données : No SQL (Cassandra, Mongo DB)\nDataviz : Power BI ou Kibana\nDes notions en développement feront la différence !\nLes Petits Plus Du Projet :\nVous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.\nVous ?\nDiplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.\nVous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?\nVous faite preuve de proactivité et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?\nAlors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !\nAstek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nNotre projet commun vous plait ?\nPostulez à cette annonce, et soyez transparent !\nMaud, notre Talent Acquisition Referent, vous contactera pour un premier échange.\nPuis vous rencontrerez Yoram, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .\nEnfin, vous rencontrerez Anthime, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nMots-clés :\ningénieur – ingénieure – consultant – consultante – developpement – Scala – Data\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – developpement – Scala – Data\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala"
            ],
            "DataBase": [
                "SQL",
                "Cassandra"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP",
                "AWS"
            ],
            "OS": [
                "Linux"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bordeaux-at-capgemini-3889788624?position=9&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=qJ4q4DZbIEQJgFfJAS3paA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une\ncommunauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la\nvaleur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.\nVos missions :\nVous êtes passionné par le domaine de la Data, vous souhaitez prendre part à des projets d'envergure, concevoir des solutions, les implémenter et les faire évoluer?\nAlors rejoignez notre équipe Data Engineering Services au sein de Capgemini Cloud Infrastructure Services en tant que Data Engineer.\nVous avez acquis une expérience solide dans le développement, la mise en œuvre et l’optimisation de solutions pour le traitement d'un grand volume de données, vous êtes capable de créer des solutions qui répondent aux besoins métiers et IT, alors rejoignez notre équipe d’experts.\nEn qualité de Data engineer, vos missions sont les suivantes :\n▪ Concevoir et développer des solutions Data/IA.\n▪ Accompagner les Métier dans la compréhension et la mise en œuvre de solution orientées données.\n▪ Collaborer avec les Dev, les Ops, les experts infrastructures dans la construction de solutions et d’infrastructures axées sur les données.\n▪ Gérer un écosystème de partenaires data et assurer un haut niveau d'expertise\n▪ Assurer un rôle de veille technologique sur tous les outils autours de la data, de l’IA et de la BI.\nVotre profil :\nVous êtes issu d’une formation ingénieur ou équivalent bac+5 informatique spécialisée en DATA et vous justifiez d’une expérience de 3 à 5 ans dans un rôle similaire. Expert dans une technologie de base de données relationnelle (PostgreSQL, Oracle...)\nExpert dans une technologie de base NoSQL (MongoDB, Cassandra...)\nVous maitrisez un framework de manipulation de données (Hadoop, Spark, Kafka...)\nVous maitrisez les concepts DevOps et avez de bonnes notions en scripting et développement\nVous avez une expérience des outils BI et de data visualisation (Kibana, PowerBI...)\nLa maitrise de l'anglais est nécessaire.\n3 raisons de nous rejoindre :\nQualité de vie au travail :\naccord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu :\ncertifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, tarif préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culture\nNos engagements et priorités :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nÀ propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "MongoDB",
                "Cassandra",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DBMS": [
                "PostgreSQL"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Lincoln France",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-h-f-at-lincoln-france-3834466740?position=10&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=2dHhajJA1zlXIU1bYbcHBA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Poste CDI : Consultant Big Data Engineer - H/F (Hadoop, Spark, PySpark, Databricks, Kafka, Python, Scala, Java, Hive, MongoDB, etc.)\nLincoln Pure Player Data\n💡: Réinventant l'analyse\ndepuis 30 ans\n. Experts en Modern BI, Big Data et Science des données 📊. Nous transformons les données en solutions pour les grands comptes, des secteurs bancaire, retail, télécoms, industriel, santé, et plus encore 💼.\nDescription du poste\n🎯\nMissions\n:\nAnalyser les besoins clients en matière d'analyse de données\nDévelopper et optimiser des pipelines de données distribués\nConcevoir et mettre en œuvre des solutions Big Data adaptées\nIntégrer les solutions dans les environnements existants\nFournir un support technique et une expertise tout au long des projets\n🔍\nPrérequis\n:\nExpertise avérée en Hadoop, Spark, PySpark, Databricks, Kafka, Hive\nMaîtrise de Python, Scala, Java\nConnaissance des bases de données NoSQL (MongoDB, etc.)\nCapacité à travailler en équipe et à communiquer efficacement avec les clients\n🌟\nAvantages :\nEnvironnement collaboratif et innovant\nFormations certifiantes et accompagnement individualisé\nTélétravail et horaires flexibles\nRémunération compétitive avec avantages sociaux attrayants\nPossibilité de mobilité à Lille, Lyon ou Aix-en-Provence\n✨\nProcessus de recrutement\n: 2 entretiens (RH et technique)\nSi vous êtes passionné par les défis de la Data et que vous souhaitez rejoindre une équipe dynamique et innovante,\npostulez dès maintenant et contribuez à redéfinir l'avenir de l'analyse de données chez Lincoln! 😉\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "MongoDB",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Talan",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-databricks-senior-h-f-cdi-at-talan-3909664424?position=11&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=JOUu%2BSuh85U2DmX%2B5oozDw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Talan est un groupe international de conseil en transformation et en innovation par la technologie, créé en 2002.\nNos 5000 consultantes et consultants partagent à travers le monde l’audace d’innover, le goût de l’excellence, et l’envie de relever les défis les plus complexes.\nNous accompagnons les entreprises dans des secteurs variés : énergie, industrie, transport, finance, luxe… à travers 3 grandes expertises :\nLe Conseil en Management et Innovation (320 Consultants en France)\nLa valorisation des données, leurs structurations, et leurs usages (Data et Technologies)\nL’intégration de solutions logicielles (Cloud et Applications Services)\nNos valeurs : engagement, respect, partage, esprit d’équipe et optimisme.\nTalan est une entreprise responsable, reconnue par ses collaborateurs et attachée à la diversité. Des aménagements peuvent être proposés si vous êtes en situation de handicap.\nRetrouvez nos engagements RSEiciet nos actions en faveur de la diversitéici\nJob Description\nNous sommes à la recherche d’un Big Data Engineer Databricks Sénior qui sera en charge de l’intégration des données: acquisition, préparation, modélisation et stockage, exposition, . Vous devrez faire preuve d’un état d’esprit à la fois innovant, méthodique, orienté solution (et non problème!), et communiquant.\nResponsabilités\nManager des Big Data Engineer et Cloud Engineer\nCoacher techniquement les membres de l’équipe: solution et code review sur site, recommandation sur les formations à suivre, certifications à réaliser, …\nAnalyse des besoins techniques métiers, définition de l’architecture solution et logiciel, référent technique, développement et optimisation, code review, maintenir les pratiques Devops “You build IT, You run IT”, support à recette et mise en production, documentation, et parfois assumer le rôle de Scrum Master,…\nBenchmark de solutions et conseil auprès de notre client sur les solutions technologiques à adopter, en lien avec leurs besoins\nPartage de connaissances et formations interne\nQualifications\nIssu(e) d’une formation supérieure (école d’ingénieur, master,…)\nVous disposez d’au moins 4 années d’expérience dans le domaine du Big Data (et particulièrement sur le framework Spark), et au moins 6 années d’expérience dans le développement logiciel\nVous maîtrisez ledéveloppement logiciel (Scala, Python …), et vous disposez de solides expériences dans la mise en place de pipelines de données\nVous maîtrisez leFramework Spark (idéalement sur Databricks) etson optimisation\nExpérience sur une plateforme Cloud serait un plus et idéalement AWS\nExpérience sur des flux temps réelserait un plus : Kafka + Spark Streaming\nVous maîtrisez les bases de données SQL et le langage SQL\nVous avez de l'expérience sur les méthodes de stockage: HDFS, S3,,…\nVous avez de bonnes connaissances en devOps : Jenkins, Gitlab, Maven, …\nLa connaissance des concepts suivants serait un +: DataOps, DataVault, DataMesh..\nConnaissance de l’Agilité\nAutonome\nOrganisé(e)\nSens du partage\nBonne communication\nOrientation produit et solution\nAdditional Information\nAVANTAGES\n:\nPlan de formation pour accompagner votre carrière (formations éditeurs, certifications) grâce à nos partenariats nous accordant une position de partenaire privilégié, et management de proximité par des experts\nLocaux modernes en centre-ville\nTop 5 du Palmarès Great Place to Work\nTélétravail jusqu’à 5 jours selon les missions, prime d’équipement de 100€\nMobilité en France et à l’étranger\nTop 1% des entreprises évaluées par Ecovadis dans le domaine social, environnemental et éthique\nTickets restaurant, prime vacances, 50% transport (abonnement transport public), mutuelle\nPermanence handicap (consultant dédié aux collaborateurs en situation de handicap et aux proches aidants)\nActionnariat salarié\nPrime de cooptations\nRTT\nPROCESS RECRUTEMENT\n:\nL’équipe recrutement s’engage à vous proposer un processus de recrutement rapide et fluide\n1 entretien RHpar Teams (45min)\n1 test technique\n1 entretien technique par Teams (1heure)\n1 entretien opérationnel avec le responsable de domaine, au siège (1heure)\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "digiRocks recrute ✅",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-digirocks-recrute-%E2%9C%85-3903481080?position=12&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=DIzHfjsZbaLVQRKIaO5kzQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "😎 Envie d'accompagner des organisations dans leurs stratégies, Fan de data?\nRejoins un jeune cabinet de conseil en stratégie spécialisé en data. Le cabinet a été créé il y a 4 ans pas des anciens de grands cabinets de conseil en stratégie qui ne se reconnaissaient plus dans ce qu'est devenu le \"consulting\". Cependant ils n'ont pas perdu espoir de pouvoir apporter du conseil à haute valeur ajoutée dans une ambiance friendly, façon start-up, sans sacrifier l'excellence.\nJean-Patrick recrute un(e) Consultant Data Engineer à Paris en CDI\n✅ MISSION :\nVous serez responsable de la mise en œuvre de bout en bout de la pile de données, de la collecte au reporting, avec un accent sur l'infrastructure et les processus techniques. Vous travaillerez avec des Consultants en Stratégie & Data et les soutiendrez dans la résolution des défis liés aux données de leurs clients. Vous contribuerez à la définition des stratégies de données, à la mise en œuvre des systèmes de données et vous soutiendrez l'exploitation des données dans des projets transformationnels. En général, vous serez responsable de comprendre intimement les problèmes, de concevoir une stratégie technique pour les adresser et de faciliter une exécution technique de haute qualité.\n✅ RÉSULTATS ATTENDUS :\n🚀 Résultat 1: Unificateur de Données : Architecturer, assembler, assimiler, nettoyer et conformer de grands ensembles de données complexes pour livrer des insights commerciaux et alimenter les expériences de produits de données.\n🚀 Résultat 2: Agent de Sécurité des Données : Concevoir et construire une infrastructure de données fiable et évolutive avec les techniques de confidentialité et de sécurité de pointe pour protéger les données.\n🚀 Résultat 3: DataOps : Posséder la pile de données de bout en bout, y compris la collecte d'événements, la gouvernance des données, les intégrations de données et la modélisation.\n🚀 Résultat 4: Gardien des Données : Assurer la cohérence et la qualité de l'environnement technique et de la structure des données à travers des métriques, de la documentation, des processus, des tests de données et de la formation.\nRequirements\n✅ PROFIL RECHERCHÉ :\nDiplômé d'une Grande Ecole de Commerce ou d'ingénieur, avec une première expérience réussie comme Data Engineer, idéalement dans un contexte similaire au Conseil,\nConnaissance des services de Data Warehouses Cloud. Expérience avec Google BigQuery, Snowflake, AWS Redshift/Athena, Looker, Azure SQL DWH, ou Azure Databricks est très souhaitable.\nConnaissance des architectures de données relationnelles et de grandes données, de l'entreposage de données, de l'intégration de données, de la modélisation de données, de l'optimisation de données et des techniques d'analyse de données.\nExpérience dans la construction de pipelines de données de bout en bout en utilisant des plateformes de données sur site ou basées sur le cloud.\nExpérience pratique dans la livraison de solutions comprenant des bases de données, SQL avancé et développement logiciel dans des langues telles que Python.\nIntéressé et connaissant les technologies Big Data et les technologies de l'écosystème Apache telles que Beam, Spark, Kafka, Airflow, bases de données, intégration, gestion des données de référence, assurance qualité, manipulation de données et technologies de gouvernance des données.\nExpérience avec les plateformes cloud publiques et l'infrastructure cloud qui est essentielle.\nExposé aux outils ETL/ELT et de gouvernance.\nIntéressé par les technologies et principes IA et ML.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "ML",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-%E2%80%93-lille-france-h-f-at-astek-3839097187?position=13&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=g6Iw%2FVj01V51GKbNbzngpQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nLille - France\nPubliée il y a 2 mois\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nNous rejoindre en tant que\nBig Data Engineer (H/F),\nafin d’accompagner un opérateur télécoms, Leader en Europe dans la création d’une infrastructure cloud (IAAS) performante, robuste et sécurisée.\nUn challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !\nVotre Mission, Si Vous L’acceptez :\nQualifier les données et les résultats\nConception technique des solutions\nDécliner les impacts de la stratégie et des innovations technologiques au sein des processus et outils de l’exploitant SI\nAssurer l’accompagnement et le déploiement des évolutions des processus et outils\nContribuer aux programmes de transformation DevOps, Cloud et catalogues des offres SI\nDévelopper des fonctions transverses et les « uses cases »\nAccompagner la phase de mise en production\nVotre Future Équipe :\nVous intégrerez une équipe à la fois technique et fonctionnel, qui œuvre chaque jour pour développer et maintenir en conditions opérationnelles l’ensemble des solutions IT !\nL’équipe est en interaction avec des clients à la fois internes et externes.\nVotre stack de jeu\nSystème d’exploitation : Linux\nEnvironnement Big data : Hadoop, Spark, Scala\nCloud computing : GCP ou AWS\nBase de données : No SQL (Cassandra, Mongo DB)\nDataviz : Power BI ou Kibana\nDes notions en développement feront la différence !\nLes Petits Plus Du Projet :\nVous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.\nVous ?\nDiplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.\nVous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?\nVous faite preuve de proactivité et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?\nAlors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !\nAstek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nNotre projet commun vous plait ?\nPostulez à cette annonce, et soyez transparent !\nMaud, notre Talent Acquisition Referent, vous contactera pour un premier échange.\nPuis vous rencontrerez Yoram, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .\nEnfin, vous rencontrerez Anthime, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nMots-clés :\ningénieur – ingénieure – consultant – consultante – developpement – Scala – Data\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – developpement – Scala – Data\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala"
            ],
            "DataBase": [
                "SQL",
                "Cassandra"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP",
                "AWS"
            ],
            "OS": [
                "Linux"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Talan",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-databricks-confirm%C3%A9-h-f-cdi-at-talan-3902693062?position=14&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=YqfrGktg2oFxsRwyJ9Dmfg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Talan est un cabinet de conseil en innovation et transformation par la technologie.\nDepuis 20 ans, Talan conseille les entreprises et les administrations, les accompagne et met en œuvre leurs projets de transformation et d’innovation en France et à l'international. Présent sur cinq continents, le groupe prévoit de réaliser un chiffre d'affaires de 600 millions d'euros en 2022 pour plus de 6000 consultant·e·s et vise à dépasser la barre du milliard d’€ de CA à horizon 2024.\nLe Groupe met l'innovation au cœur de son développement et intervient dans les domaines liés aux mutations technologiques des grands groupes, comme le Big Data, l'IoT, la Blockchain et l'Intelligence Artificielle.\nPrésent dans les événements incontournables du secteur, comme Viva Technology, Talan prend régulièrement la parole sur les enjeux de ces technologies révolutionnaires aux côtés d'acteurs majeurs du secteur et de parlementaires (Syntec Numérique, Forum de l'intelligence artificielle, French Fab Tour, Forum de Giverny…).\nTalan est une entreprise responsable, attachée à la diversité. Des aménagements de poste peuvent être organisés pour tenir compte des personnes en situation de handicap.\nRetrouvez nos engagementsRSEiciet nos actions en faveur de la diversitéici\nJob Description\nNous sommes à la recherche d’un Big Data Engineer Databricks confirmé qui sera en charge de l’intégration des données: acquisition, préparation, modélisation et stockage, exposition, . Vous devrez faire preuve d’un état d’esprit à la fois innovant, méthodique, orienté solution (et non problème!), et communiquant.\nResponsabilités\nAnalyse des besoins techniques métiers, définition de l’architecture solution et logiciel, référent technique, développement et optimisation, code review, maintenir les pratiques Devops “You build IT, You run IT”, support à recette et mise en production, documentation, et parfois assumer le rôle de Scrum Master,…\nPartager techniquement les membres de l’équipe: solutions et code reviews, recommandations, certifications à réaliser, …\nParticipation à des meet-up, coding dogo,…\nCommunication: écriture d’articles, retours d’expérience…\nQualifications\nIssu d’une formation supérieure (école d’ingénieur, master,…)\nVous disposez d’au moins 3 années d’expérience dans le domaine du Big Data et particulièrement sur le framework Spark (idéalement Databricks)\nMaîtrise du développement logiciel (Scala, Python,…) et vous disposez de solides expériences dans la mise en place de pipeline de données\nExpérience sur une plateforme Cloud serait un plus et idéalement AWS\nExpérience sur des flux temps réelserait un plus : Kafka + Spark Streaming\nMaitrise du langage SQL\nExpérience sur des méthodes de stockage: HDFS, S3, ,…\nBonnes connaissances en devOps : Jenkins, Gitlab, Maven, …\nConnaissance de l’Agilité\nAutonomie, organisation, sens du partage\nBonne communication\nOrientation produit et solution\nAdditional Information\nAVANTAGES\n:\nPlan de formation pour accompagner votre carrière (formations éditeurs, certifications) grâce à nos partenariats nous accordant une position de partenaire privilégié, et management de proximité par des experts\nLocaux modernes en centre-ville\nTop 5 du Palmarès Great Place to Work\nTélétravail jusqu’à 5 jours selon les missions, prime d’équipement de 100€\nMobilité en France et à l’étranger\nTop 1% des entreprises évaluées par Ecovadis dans le domaine social, environnemental et éthique\nTickets restaurant, prime vacances, 50% transport (abonnement transport public), mutuelle\nPermanence handicap (consultant dédié aux collaborateurs en situation de handicap et aux proches aidants)\nActionnariat salarié\nPrime de cooptations\nRTT\nPROCESS RECRUTEMENT\n:\nL’équipe recrutement s’engage à vous proposer un processus de recrutement rapide et fluide\n1 entretien RHpar Teams (45min)\n1 entretien opérationnel avec le responsable de domaine, au siège (1heure)\n1 entretien avec le directeur de pôle, au siège(1heure)\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "20 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-sql-gcp-f-h-at-orange-business-3916557264?position=15&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=0rGuS34ssXebpVfpSRYJ7w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?\nNous l’avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les étapes du voyage de la donnée.\nDepuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’ESN d’Orange Business alliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …\nDescription du poste\nOrange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.\nVotre quotidien ?\nEn intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :\nAu démarrage du projet :\nRecueillir et analyser les besoins du client\nRédiger les spécifications fonctionnelles et techniques\nEstimre les charges\nPendant la phase de réalisation :\nModéliser des datawarehouses et datamart (intégration de flux et consolidation des données)\nDévelopper les procédures d’alimentation (ETL)\nDévelopper en SQL\n/ PLSQL / Shell\nGarantir la qualité des données et leur disponibilité\nConcevoir et développer des solutions frontend BI à des fins analytics & dashboarding\nRéaliser la recette et les tests\nSuivre et mettre en production\nEn fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !\nQualifications\nVous possédez 3 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels et ingénierie ou analyse data.\nVous avez de\nsolides compétences en développement SQL\n(job, scripting, déploiement), vous avez l’habitude de travailler dans un\nenvironnement Google Cloud Plateform\nainsi qu’avec\nPower BI\n.\nEnvie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?\nOutre l’aspect technique, c’est une personnalité qui est recherchée !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Vélizy-Villacoublay, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-thales-3908228180?position=16&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=SR3wNiEahgqu4gp2aczA2g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.\nThales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.\nQUI ETES-VOUS ?\nDiplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.\nVOS MISSIONS :\n• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Hadoop.\n• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.\n• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.\n• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.\n• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.\n• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.\n• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.\nInnovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "BigData": [
                "Hadoop"
            ],
            "Other": [
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-lille-at-capgemini-3914228495?position=17&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=tWBPN1TDM2mx5aVvM0uDjQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini en quelques mots\nChoisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.\nVos missions\nVous maitrisez au minimum un langage de programmation appliqué à l’analyse de données\n(Java, Python, Scala et les environnements Spark et / ou Hadoop).\nVous êtes passionné par le Big Data et le Machine Learning et l’analyse de données\nVous concevez et mettez en œuvre des stratégies sécurisées d'acquisition et d'intégration de données\nVous configurez des référentiels de données à la pointe de la technologie dans des environnements distribués\nVous construisez des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée\nVotre profil\nDiplômé(e) de Bac+5 en informatique\n4 ans d’expérience\n(au sein d’une ESN ou chez un intégrateur) en conseil clientèle\nUne solide culture technologique\nUn bon niveau d’anglais\n3 raisons de nous rejoindre\nQualité de vie au travail :\naccord de télétravail en France et à l’international, accord sur l’égalité\nprofessionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu :\ncertifications et formations en libre accès, accompagnement sur mesure avec\nvotre carreer manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, activités à tarifs préférentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel\nNos engagements et priorités\nLe groupe Capgemini encourage une\nculture inclusive dans un cadre multiculturel et handi-accueillant.\nEn nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des\ninitiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nCapgemini\nest un\nleader mondial\n, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de\n55 ans d’expérience,\nnous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le\ncloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "EarthDaily Agro",
        "location": "Balma, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-earthdaily-agro-3883708013?position=18&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=OstO0NnkVGTi6x0rYb4beg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "About Us\nEarthDaily Agro provides space age data and analytics to the organizations and people who feed the planet!\nWith 35 years of industry experience, EarthDaily Agro provides customers with the data, analysis and knowledge they need to make more efficient and effective decisions. B2B services range from global risk management and monitoring of agricultural commodities to the marketing of inputs and precision agriculture consulting, using the latest research in agronomy, information technologies and remote sensing.\nEarthDaily Agro also develops highly customized business solutions for agricultural lenders, insurers, input suppliers and food companies, with easy-to-use analytics, that help reduce the daily risks of agriculture.\nEarthDaily Agro is headquartered in Minneapolis, MN, USA, with offices in France, Brazil, Australia and Switzerland and is a division of EarthDaily Analytics Corp.\nEarthDaily Analytics Corp., a vertically-integrated data processing and analytics company, is launching a new constellation of earth observation satellites. The EarthDaily satellite constellation will significantly enhance geospatial analytics capabilities in agriculture, forestry, environment, financial services, and intelligence, among many other verticals.\nMain Job Tasks And Responsibilities\nAs a EarthDailyAgro Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and infrastructure specialized for geospatial and remote sensing applications. You will work closely with data scientists, geospatial analysts, remote sensing experts, software engineers, and DevOps teams to ensure the successful deployment and scaling of data pipeline to feed geospatial data machine learning models. Your role will be crucial in optimizing the geospatial machine learning ecosystem and ensuring the seamless integration of AI-driven geospatial solutions into real-world applications.\nYour Responsibilities Include\nCloud-based data pipeline Conceptualization, Development and Scaling: Build up pipeline to ingest large volumes of geospatial data, pre-process them and meet data scientists’ requirements, in terms of accessibility, speed, format, quality.\nAutomation and CI/CD: Industrialization of pipeline deployment, orchestration, workflows, and versioning.\nCost & Speed Optimization: Collaborate with infrastructure team to develop, optimize, and fine-tune pipeline.\nCloud and Containerization: Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization tools.\nInfrastructure Management: Utilize containerization technologies and cloud-based services to set up and manage infrastructure, enabling seamless deployment and scalability.\nMonitoring and Anomaly Detection: Implement monitoring systems to track pipeline performance and identify anomalies.\nVersion Control and Data Version Control: Proficient with version control systems like Git and DVC.\nSecurity and Compliance: Ensure the security and privacy of geospatial data, adhering to relevant data protection regulations and industry best practices.\nCollaboration and Communication: Collaborate with interdisciplinary teams to integrate data pipeline into existing applications or develop new geospatial products.\nIssue Resolution and Troubleshooting: Identify and resolve promptly technical issues related to geospatial data processing, performance, or infrastructure.\nEducation, Knowledge And Abilities\nRequirements\nEducation: Master's degree in Computer Science, specialisation in Geomatics and/or Remote sensing would be a plus.\nExperience: 3+ years experiences with data pipeline processes and deployment is a must-have. Proven hands-on experience in setting up pipelines and data processes with opensource tools (e.g., MLFlow, Argo, Kubeflow) is desirable.\nProgramming Skills: Proficiency in Python and with data manipulation frameworks (e.g., dataframe, numpy, pandas, xarray, rasterio) and librairies (e.g., Dask).\nProblem-Solving Skills: Autonomous, and strong analytical and problem-solving abilities to address complex geospatial data and analysis challenges.\nCommunication Skills: Excellent communication and interpersonal skills to collaborate effectively with cross-functional teams and stakeholders.\nFrench mandatory (job based in France). Fluent in English (oral and written): meetings with internal are mostly in English.\nPreferred Additional Skills\nExperience with Earth Observation (EO) data analysis and processing.\nExperience with geospatial data formats (e.g., GeoTIFF, Shapefile, NetCDF).\nSpatial Analysis Techniques: Understanding of spatial analysis techniques and algorithms commonly used in geospatial data manipulation.\nRemote Sensing Integration: Knowledge of remote sensing data sources (e.g., STAC catalog, satellite imagery, LiDAR, SAR) integration into data pipelines for accurate and up-to-date geospatial analysis.\nCONDITIONS\nFull time job based in Balma, near Toulouse, France.\nFixed + Bonuses\nTR / \"Family\" insurance / CSE\nPowered by JazzHR\nWrfSXQ5YJg\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataAnalytics": [
                "NumPy",
                "Pandas"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Interpersonal Skills",
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Lyon, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-thales-3903089036?position=19&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=l0OHDSxc4s3SzSTrRXtmfg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "📢 Nous recherchons un(e) Data Engineer, basé(e) à Lyon\n👉Quelques mots sur les activités numériques de Thales Lyon :\nLes activités numériques représentent une entité rattachée au groupe Thales, spécialisée dans l’IT et présente au national.\nL’agence de Lyon adresse divers sujets d’expertise : ingénierie logiciels, cybersécurité, infogérance des infrastructures et transformation digitale.\n🎯\nVotre rôle et missions\nEn nous rejoignant, vous intégrerez le centre de compétences\nAugmented data\n,\nspécialisé dans la conception, le développement et l’évolution d’applications data centrées. Vous y boosterez votre carrière en travaillant sur des technologies telles que\nSpark, Elasticsearch, Kube ...\nle plus souvent dans un environnement\nAgile\n.\nDans le cadre des projets que nous opérons aujourd’hui :\n- Vous contribuerez à la conception, au maintien, à la scalabilité des plateformes d’analyse de données au travers de votre expertise sur les sujets data (base de données, gestion de flux, ETL …)\n- Vous contribuerez à la conception et à la mise en production des pipelines d’analyses et de transformations de données en veillant à leur bonne adaptation aux besoins métiers et aux contraintes techniques du client\n- Vous pourrez intervenir sur des sujets de visualisations, dans le but de notamment accompagnées nos clients sur la conception de Dashboard métier intelligent …\n- Vous serez également amenées à échanger directement avec des DevOps/Datascientist pour la mise en place, l’intégration des pipelines et l’élaboration des algorithmes de traitements de données.\n- A l’échelle du département, Vous serez un acteur majeur du développement de notre activité et du lancement de nouveaux projets de valorisation de données.\n🙋‍♀️ 🙋‍♂️\nVotre profil\nDe formation Bac +5 en informatique (école d’ingénieur, Master ou équivalent), vous justifiez d’une première expérience réussie sur un projet data ? Vous souhaitez participer à la conception et intervenir sur des solutions de récupération et d’exploitation de données métiers dans des contextes critiques et hautement sécurisés ?\nAutonome, dynamique, organisé(e) et proactif(ve), vous souhaitez évoluer au sein d’équipes passionnées par l’exploration et l’intégration des technologies nouvelles au service des métiers de nos clients ?\nVous avez des compétences qui couvrent les domaines suivants :\nMise en place et gestion de base de données (SQL, Elasticsearch, Clickhouse ...)\nLangages de programmations (Java, Python)\nGestion de flux (Kafka, flink, logstash …)\nEnvironnements big Data (Spark/hadoop )\nPrincipes et outils de type ETL\nVous êtes de plus intéressé(e):\nPar les environnements containerisés (docker, kubernetes, helm ...)\nLes concepts DevOps (Ansible, CI/CD...)\nLes sujets de Datavisualisation (Vega, Kibana, python librairies...)\nVous aimez travailler en équipe ? Vous êtes reconnu(e) pour vos qualités relationnelles et vos capacités de vulgarisation ?\nAlors notre poste d’Ingénieur(e) Data(H/F) est fait pour vous !\n🙌\nVotre carrière chez Thales\nDifférentes opportunités vous permettront de découvrir d'autres domaines ou sites. Vous pourrez évoluer et développer vos compétences dans différents domaines.\nExplorez un espace attentif au développement personnel.\nDéveloppez vos talents dans un autre domaine du groupe Thales, en découvrant de nouveaux produits, de nouveaux clients, un nouveau pays ou en vous orientant vers une solution plus complexe.\nChoisissez entre une expertise technique ou un parcours de leadership.\nVous travaillerez dans une entreprise résolument humaine avec des valeurs fortes comme la sécurité au travail, l’égalité Homme/Femme et l’équilibre vie personnelle/professionnelle (Accord Télétravail).\nRattaché(e) à la Convention métallurgie, vous bénéficierez aussi de ses multiples avantages (…)\nVous souhaitez en savoir plus ?\nN’hésitez pas à contacter notre équipe de recrutement ou nos équipes directement.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Flink",
                "Spark"
            ],
            "DevTools": [
                "Docker"
            ],
            "Automation": [
                "Kubernetes",
                "Ansible"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Leadership"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Octopus IT - Expert du recrutement tech",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-spark-hadoop-sp%C3%A9cialis%C3%A9-en-big-data-paris-ou-remote-partiel-at-octopus-it-expert-du-recrutement-tech-3837194913?position=20&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=pVu5TEtTTwGCsZughQT0Dw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La société\nCréée il y a 7 ans, cette entreprise de conseil en hyper croissance, se compose d'environ 90 personnes. Elle est devenue experte en Data et IA (NLP, Deep Learning, Machine Learning) et accompagne leurs clients sur l’ensemble de leurs projets data à travers la valorisation de leurs données.\nLeur valeur ajoutée ? Leur spécialisation en Data ce qui leur permet d'offrir 3 expertises métiers distinctes : la Data Science, la Data Engineering et le Machine Learning Engineering. Autour de ces expertises gravitent bien sûr les métiers de Lead et d'Architecte.\nUne autre de leur force est leur formation interne (avec des profils de seniors ou d'architectes) et externe (avec des partenaires pour passer les certifications).\nChez eux, le collaborateur est placé au centre des préoccupations, permettant ainsi de créer une cohésion et une véritable culture au sein de l'entreprise. Par exemple la majorité des projets se font en équipe et non seul.\nConnu et reconnu pour leur expertise en Big Data, ils sont devenu le partenaire principal d'un grand groupe du CAC 40 et ont pris le lead sur tous les sujets touchant à la transformation Big Data de ce groupe.\nPour poursuivre leur croissance, répondre à leurs ambitions et développer de nouveaux marchés, nous recherchons plusieurs profils pour renforcer leurs effectifs.\nLe poste\nEn les rejoignant vous travaillerez sur les problématiques suivantes :\nMise en place et/ou scale d'architectures\nConstruction de Datalake\nMise en production de model de ML\nPipelining de données\nStreaming de données et temps réel\nLa stack sur laquelle vous travaillerez :\nPython, Scala, Spark, Architectures distribuées : Hadoop, HDFS, Cloud : Aws, GCP, Azure\nVotre profil\nA partir de 3 ans d'expérience en CDI\nVous avez une expérience significative sur des problématiques Big Data\nTrès bonne compétences en Python et/ou Scala et en Spark\nVous êtes familier avec Hadoop, Hive, Hbase\nUne logique cloud (Aws, GCP ou Azure)\nLe salaire & avantages\n50-60 K€ selon expérience\nRTT\nCarte Swile & Mutuelle\n3/4 jours de télétravail par semaine\nEt plus encore…\nCe qu’on préfère\nÊtre impliqué à fond dans une aventure avec de nombreux challenges techniques\nBelles opportunités d'évolutions sur des postes d'Architecte, de Lead ou de Ml Ops\nTrès bonne ambiance, équipe solidaire et orientée partage d’informations\nBeaucoup de workshops en interne et catalogue de formations à votre guise\nCe poste a été soigneusement choisi par votre coach. Powered by Octopus IT, cabinet d’Experts en Recrutement Tech (CDI et clients finaux uniquement) – Visitez nous pour plus d’opportunités :\nwww.octopusit.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "CDI",
            "Salary": "50",
            "Level": "",
            "Experience": "7 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "HBase"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Other": [
                "ML",
                "Machine Learning",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-talend-f-h-at-orange-business-3916552363?position=21&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=nWMEAJ97Rkf7axHPJgu4ww%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "L’ambition d’Orange Business est de devenir l’intégrateur réseau et numérique de référence en Europe, en nous appuyant sur nos forces autour des solutions de connectivité nouvelle génération, du cloud et de la cybersécurité.\nNos 30 000 femmes et hommes présents dans 65 pays, dont chaque voix compte, sont tous animés par la même détermination et le même esprit d’équipe, pour construire les solutions digitales d’aujourd’hui et de demain et créer un impact positif pour nos clients, pour leurs salariés et pour la planète.\nNous offrons des opportunités passionnantes grâce à des projets innovants dans la data et le digital, le cloud, l’IA, la cybersécurité, l’IoT, ou encore le digital workspace et le big data.\nVenez vivre cette aventure avec nous !\nAfin de développer notre équipe lilloise, nous recherchons aujourd'hui, un Ingénieur DATA à même d’accompagner nos clients dans la structuration de leurs SI autour de la donnée.\nVos principales missions seront les suivantes\n:\n- Concevoir des solutions de traitement et collecter des volumes importants de données.\n- Participer à des études de cadrage pour collecter le besoin métier et concevoir les solutions qui répondent au besoin du client.\n- Apporter son expertise sur des problématiques précises rencontrées chez les clients.\n- Participer à la veille technologique\n- Réaliser les\ndéveloppements TALEND\n- Rester informé et former sur les nouvelles solutions DATA\n- Contribuer aux phases d'avant-vente et au développement business.\n- Participer à la conception, l'évolution et la présentation de nos offres DATA.\nVous\n:\n- Êtes issu(e) de formation bac+5 ?\n- Vous justifiez d'au moins 3 ans d'expériences en qualité d'Ingénieur DATA sur la solution TALEND Enterprise (Data Integration) et avez idéalement une connaissance des solutions Cloud d'AWS et d'AZURE ?\n- Vous êtes intervenu sur des projets intégrant des pratiques DevOps et AGILE ?\nAlors postulez, ce poste est fait pour vous !\nVos compétences clés\n:\n- Expertise sur l'outil\nETL TALEND\nEnterprise (administration et développement)\n- Fortes connaissances des solutions de bases de données (SQL, NoSQL…)\n- Connaissances en langages objets ou scripts (notamment Java mais aussi Javascript, Scala, Python…)\n- Divers systèmes d'exploitation : UNIX, Windows\nAutonomie, rigueur, curiosité, dynamisme et sens du service sont des qualités nécessaires pour ce poste.\nLes compétences complémentaires qui seraient appréciées :\n- Connaissances d'autres modules Talend (MDM, ESB, Data Quality, Cloud…)\n- Maîtrise des technologies du Big Data (Hadoop, Spark, Kafka…)\n- Expertise sur d'autres outils ETL (Informatica, SSIS, DataStage...)\n- Notions en architecture des Systèmes d'Information\n- Maîtrise de l'anglais (oral et écrit)\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "OS": [
                "Windows"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Octopus IT - Expert du recrutement tech",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-azure-ia-machine-learning-paris-ou-remote-partiel-at-octopus-it-expert-du-recrutement-tech-3664568765?position=22&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=moygXwPkYJVD30cyjrlNXw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La sociét��\nCréée il y a plus de 2 ans, cette startup est la première base de connaissance intelligente dédiée aux services clients. Leur mission ? En finir avec la frustration lorsque l'on contact un Help Desk.\nPour cela, elle propose aux entreprises la possibilité de délivrer une expérience client d'exception : rapide et de qualité. Grâce à leur moteur de recherche intelligent, cette entreprise est capable de centraliser toute la connaissance interne de l'entreprise (procédures, produits, modes opératoires, etc.) et la diffuse intelligemment dans les outils de production des conseillers de service client.\nRésultat :\nPlus besoin de chercher l'information\nDes réponses instantanées et de meilleures qualitées\nUne autonomie totale des collaborateurs\nAprès une croissance fulgurante, elle a su séduire à la fois de nombreuses scale up (Luko, OpenClassrooms, Japhy...) et grands groupes (BNP Paribas, La Poste, Fnac Darty...).\nAprès le recrutement de leur Lead Data (réaliser ensemble) et suite à l'annonce de leur levée de 2,5M€ pour tripler la taille de ses équipes, le but est maintenant de s'imposer très vite comme la base de connaissance de référence en France et en Europe. Pour ce faire, nous recherchons un Data Engineer.\nLe poste\nEn travaillant main dans la main avec le Lead Data, ta mission sera de développer et de maintenir des flux de données complexes et robustes. La donnée étant au coeur de l' entreprise, dans le produit comme dans la stratégie, tu seras amené à travailler avec un panel d’interlocuteurs très variés :\nData Scientists sur des sujets comme le monitoring des modèles de production et l’enrichissement des données d’entrainement.\nProduct Team sur des sujets de performance et d’acheminement de données au service de fonctionnalités produit telles que le dashboard d’analytics à destination de nos clients.\nCustomer Success / Strategy sur des sujets de pilotage comme le suivi de l’utilisation de notre plateforme ou la mise en place de KPIs de performance.\nTu travailleras sur les problématiques suivantes :\nTu seras responsable de notre architecture de données et de son outillage, mais aussi de la mise en place de pipelines de données complexes et robustes.\nTu seras amené à mettre en place des outils de monitoring et d’alerting pour suivre de près nos nombreuses pipelines de donnée.\nTu seras garant de la qualité de nos données en assurant l’application des guidelines de code et des tests automatisés pour chacune de nos pipelines.\nTu seras amené à mettre en place des outils de reporting / insights à destination d’interlocuteurs variés (Data Science, Product, Customer Success, Clients, etc.).\nTu créeras et développeras des pipelines de données avec des outils de scheduling et d’orchestration.\nLa stack sur laquelle vous travaillerez :\nLangage : Python, Javascript\nFramework data : PyTorch, Transformers (Hugging Face), FastAPI\nDatabase : PostgreSQL, MongoDB, ElasticSearch, Redis\nInfrastructure : Azure, Docker, Kubernetes, Spark, RabbitMQ, Serverless, Terraform\nEnvironnement / Test : PyTest, Gitlab (git + ci/cd)\nBI : Metabase, Superset\nVotre profil\nEntre 1 et 3 ans d'expérience en CDI\nTu as une expérience significative sur des problématiques de Data engineering\nTu es quelqu'un de pragmatique\nUn très bon niveau en Python et une très bonne rigueur dans le code\nBonne pratique de dev : clean code, TDD, BDD\nUne bonne culture Ops\nUne logique cloud (Aws, GCP ou Azure)\nLe salaire & avantages\n50-7O K€ selon expérience\nRTT\nCarte Swile & Mutuelle\n2/3 jours de télétravail par semaine\nEt plus encore…\nCe qu’on préfère\nÊtre impliqué à fond dans une aventure avec de nombreux challenges techniques\nBelles opportunités d'évolutions sur des postes d'Architecte, de Lead ou de Ml Ops\nBeaucoup de workshops en interne et catalogue de formations à votre guise\nUne opportunité de travailler sur un produit unique qui a déjà séduit de très beaux clients (BNP Paribas, Fnac Darty, Luko, OpenClassrooms)\nLa possibilité de travailler sur une stack très moderne, des problématiques complexes aussi bien en traitement de données, qu'en DevOps\nUn plan de BSPCE (actions de l'entreprise) très intéressant et motivant !\nUne culture d'entreprise fondée sur l'apprentissage, l'autonomie, la bienveillance et l'exigence\nLe fait de travailler au quotidien avec des fondateurs passionnés par leur domaine d'expertise\nCe poste a été soigneusement choisi par votre coach. Powered by Octopus IT, cabinet d’Experts en Recrutement Tech (CDI et clients finaux uniquement) – Visitez nous pour plus d’opportunités :\nwww.octopusit.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "50",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "MongoDB"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [
                "PyTorch"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "DBMS": [
                "PostgreSQL"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "ML",
                "Cloud",
                "DevOps",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-mod%C3%A9lisation-sql-f-h-at-orange-business-3916551577?position=23&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=mXyowhaRMxCj2BN%2Bzc7rrw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?\nNous l’avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les étapes du voyage de la donnée.\nDepuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’ESN d’Orange Business alliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …\nDescription du poste\nOrange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.\nVotre quotidien ?\nEn intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :\nAu démarrage du projet :\nRecueillir et analyser les besoins du client\nRédiger les spécifications fonctionnelles et techniques\nEstimer les charges\nPendant la phase de réalisation :\nModéliser des datawarehouses et datamart (intégration de flux et consolidation des données)\nDévelopper les procédures d’alimentation (ETL)\nDévelopper en SQL / PLSQL / Shell\nGarantir la qualité des données et leur disponibilité\nRéaliser la recette et les tests\nSuivre et mettre en production\nEn fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !\nQualifications\nVous possédez 5 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels et en modélisation.\nVous avez de s\nolides compétences en développement SQL\n(job, scripting, déploiement) ainsi que sur Python.\nEnvie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?\nOutre l’aspect technique, c’est une personnalité qui est recherchée !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "SFR",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-sfr-3879318123?position=24&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=FMZDHlMn1O%2FB9TWDKjU5bw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "En tant que Data Ingénieur expérimenté, vous occuperez un rôle essentiel dans notre équipe Data Science.\nVous serez responsable de la conception, du développement et de la maintenance des pipelines de données ainsi que de l'intégration de sources de données multiples.\nVotre expertise sera cruciale pour garantir une gestion efficace des flux de données, ainsi que pour faciliter l'analyse et la visualisation des données en plus du support aux data scientists vos missions seront les suivantes :\nArchitecture projet des données\n: Concevoir et développer des architectures projet de données robustes, évolutives et performantes pour intégrer et gérer de grandes quantités de données provenant de sources multiples. Assurer la fiabilité, l'évolutivité et la sécurité des flux de données entrant d’un projet Data Science.\nIntégration des données\n: Élaborer des pipelines de données efficaces pour l'extraction, la transformation et le chargement des données (via notre Framework ELT/ETL interne) provenant de différentes sources. Mettre en place des processus d'intégration automatisés et veiller à la qualité des données.\nGestion des bases de données\n: Concevoir et optimiser des bases de données pour répondre aux besoins analytiques et de reporting. Assurer la performance, la disponibilité et la sécurité des bases de données, ainsi que la gestion efficace des requêtes.\nCollaboration interfonctionnelle\n: Support des Data Scientists, vous travaillerez avec les équipes business pour comprendre leurs besoins et fournir des conseils et des recommandations basés sur les données.\nOptimisation des performances\n: Surveiller et optimiser les performances des pipelines de données, des bases de données et des requêtes. Identifier les goulots d'étranglement et les points d'optimisation, et proposer des améliorations pour garantir des performances optimales.\nSécurité et conformité\n: Veiller à ce que les données soient traitées et stockées conformément aux normes de sécurité et de confidentialité. Mettre en place des mécanismes de sécurité pour protéger les données sensibles et garantir la conformité aux réglementations en vigueur.\nVotre profil :\nVous avez un\nDiplôme universitaire en informatique, en génie logiciel, en science des données ou dans un domaine connexe et vous avez à minima 5 ans d'expérience en tant que Data Ingénieur.\nVous possédez également une solide maîtrise des technologies et des outils suivants :\nHadoop, Spark, SQL, Kafka, GCP BigQuery,\nDe plus vous avez une bonne compréhension des architectures, des modèles et des concepts de base de donnés avec une expérience avancée dans la mise en œuvre de pipelines ETL et dans la gestion de bases de données.\nVos connaissances en matière de sécurité des données, de conformité aux réglementations ainsi que vos compétences en programmation scripting et en développement logiciel seront un plus.\nVos excellentes compétences en communication seront des qualités appréciées et\nun niveau d'anglais (appliquée au domaine technique) est un plus.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "fifty-five",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-cloud-engineer-h-f-at-fifty-five-3910028674?position=25&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=VCniSLmV9FdFUbTN8DKrDw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data & Cloud Engineer\nfifty-five est une data-company d'un genre nouveau qui aide les marques à exploiter les données pour améliorer le marketing, les médias et l'expérience client grâce à une combinaison de services de conseil et de technologie spécialisés.\nEn tant que pilier data et marketing du Brandtech Group, nous offrons des services qui combinent le conseil en stratégie, les services de cloud, le conseil en média et l'expérience client.\nfifty-five, c'est plus de 400 experts du numérique. Des digital consultants, des spécialistes du tracking et du média, des ingénieurs et des data scientists, travaillent tous en étroite collaboration pour fournir des conseils marketing de haut niveau et une assistance technique aux marques, dans tout type d'industrie, partout dans le monde.\nPartenaire des annonceurs de la collecte à l'activation et l'exploitation des données, nous aidons les organisations à devenir de véritables entités omnicanales maîtrisant l'efficacité de leur écosystème digital et ses synergies avec le monde physique.\nBasé à Paris, nous opérons sur 3 fuseaux horaires depuis nos 10 bureaux, situés à Paris, Londres, Genève, Milan, Shanghai, Hong Kong, Shenzhen, Taipei, Singapour et New York. fifty-five attache une importance particulière au bien-être de ses collaborateurs, ce qui lui a permis de figurer dans le classement Best Workplaces France en 2018.\nContexte :\nL'équipe d'ingénierie développe et met en œuvre les solutions techniques permettant la réalisation de pipelines de données et l'implémentation de data platform pour nos clients : récupération de datas sur de multiples sources de données (APIs, files, etc.), data cleaning, data processing, automation et monitoring de l'ensemble. L'équipe s'appuie sur des technologies récentes (docker, kubernetes, terraform, notebooks, etc.) et met en place ses projets dans les différents clouds du marché (GCP, Azure, AWS...).\nMission :\nNous sommes à la recherche d'une personne capable de réaliser des projets techniques pour répondre aux besoins de nos clients (par exemple: système de recommandations de produits, détection d'anomalies, ranking). Les activités vont du chiffrage et du sizing technique à la mise en œuvre des architectures, en passant par la revue des spécifications fonctionnelles et la production de code. Le Data & Cloud Engineer sera épaulé par un Lead dans ses missions. Il sera également amené à participer à la R&D et à accompagner les équipes transverses dans la mise en place d'outils de travail internes (librairies pour les data scientists, environnement Notebooks pour les data analysts et data scientists, développement de frameworks sur différents cloud providers, etc.).\nNous souhaitons trouver la bonne personne pour faire évoluer ou créer de nouvelles solutions dans ce cadre. Les missions comprennent aussi bien du prototypage rapide pour des démonstrateurs, que de la production de code robuste qui tourne en production tous les jours.\nCompétences et expériences :\n2 ans d'expérience en tant que Data Engineer\nMaîtrise de Python, SQL\nMaîtrise des environnements Cloud. Idéalement certifié GCP, Azure ou AWS\nBonne connaissance de Docker/Kubernetes\nBonne connaissance d'au moins un data warehouse (BigQuery, Snowflake, etc)\nConnaissance autour des Notebooks (Jupyter)\nA l'aise avec des concepts liés aux APIs (OAuth, REST, etc.)\nA l'aise avec les notions d'Infrastructure as Code (Terraform)\nAu courant des pratiques GitOps et connaissances des concepts autour du CI/CD\nLa maîtrise d'un orchestrateur, comme Apache Airflow, est un plus\nEsprit d'équipe (collaborer aux tests unitaires, revue de code, partage de code, sprints)\nBon niveau en français et en anglais\nA déjà travaillé en mode projet avec des interlocuteurs variés (consultant, data analyst, data scientist)\nUne expérience en marketing digital est un plus\nNous proposons :\nun bureau au centre de Paris avec terrasse et jardin\nun environnement multiculturel avec des collaborateurs aux nationalités multiples (France, Royaume-Uni, Etats-Unis, Chine, Tunisie, Italie et plus)\ndes projets avec nos bureaux à Londres, Hong Kong, New York, Shanghai, Genève, Shenzhen et Taipei\ndes TGIF et supers soirées\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Grenoble, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-grenoble-at-capgemini-3905836212?position=26&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=phBx1%2BYFP%2BE7q4Dxg28eJw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une communauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.\nVos missions :\nEn tant que Data Engineer au sein d'une équipe multidisciplinaire, vos responsabilités principales seront les suivantes :\nIntervenir sur les différentes phases d'un projet dans un environnement Cloud et Agile.\nContribuer à la gestion de la qualité des données et extraction et analyse de celle-ci, ainsi qu’à la présentation des données dans leur forme raffinée.\nProposer des nouvelles lectures de données via un travail de fouille sur les gisements d’information, notamment client.\nAdopter une posture de consultant : proposer de nouvelles solutions et accompagner le client dans ses choix.\nVotre profil :\nTitulaire d'un Bac+5 en école d’ingénieur ou en université.\nConnaissances approfondies des ETL (Talend, Informatica ou SSIS), du traitement de données (Spark, Python, Scala) ainsi que des bases de données (Oracle, SQL Server, Postgres).\nFaculté pour se montrer curieux, autonome et proactif dans la réalisation de ses tâches.\nCapacité à faire preuve de rigueur et à travailler en équipe.\nBon niveau d’anglais (B2 minimum).\n3 raisons de nous rejoindre :\nQualité de vie au travail\n: accord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu\n: certifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE\n: plan actionnariat, tarif préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorités\n:\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nÀ propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "55 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DBMS": [
                "SQL Server"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "eXalt Value",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=27&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=NbB%2F19lo6taC1wWs893b8w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "eXalt\nest un cabinet de conseil IT\nPure player Data\n& IA basé à Paris.\nNotre offre s’articule autour de 4 piliers réunis au sein d’une même communauté pour un accompagnement à 360° alliant une expertise technique et méthodologique à une approche conseil métier:\nData Gouvernance & Project\nData Engineering & Big Data\nData Performance & Analytics\nData Science & IA\nFiliale du groupe eXalt, créé en 2018,\nregroupant plus de\n950 collaborateurs en France\n(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)\net à l’international\n(Colombie, Etats-Unis, Espagne, Belgique),\neXalt Value\napporte une\nexpertise approfondie\ndans le domaine de la Data & IA et conseille les entreprises dans le déploiement de leurs stratégies data-driven.\nBénéficiant du support du groupe eXalt\n(1er dans la catégorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value\nest en pleine croissance et regroupe aujourd’hui une communauté d’expertise de plus de 60 collaborateurs en région parisienne.\nNos consultants interviennent sur d\nes projets d’envergure\ndans divers secteurs d’activité,\nBanque & Assurance, Médias, Transports, Retail, Tourisme, etc.\nNous recherchons un\nData Engineer Confirmé H/F (minimum 4 ans d'expérience dans la fonction)\npour rejoindre notre communauté sur le\npilier Data Engineering & Big Data.\nVos missions:\nConcevoir et développer des pipelines et des flux de données.\nIntégrer et transformer des données provenant de différentes sources.\nDévelopper et mettre en œuvre des algorithmes de traitement de données avancés.\nCollaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.\nAssurer la qualité et la fiabilité des solutions développées.\nConseiller les équipes clients sur les solutions à mettre en place.\nLes Prérequis :\nTitulaire d'un Bac+5, Ecole d'Ingénieur\nMaîtrise d'un ou plusieurs langages de programmation (\nPython, Scala, Spark, etc\n.).\nExpérience approfondie des technologies\nBig Data (Hadoop, Spark, Kafka, Talend, etc.)\nExpérience avérée\nen\nenvironnement Cloud (AWS, GCP, ou Azure)\n.\nSolides compétences en conception et en optimisation de pipelines de données.\nExpérience de travail en\nméthode Agile\nCapacité à travailler de manière autonome et en équipe.\nExcellentes compétences en communication et en résolution de problèmes.\nMaîtrise de l’anglais (oral & écrit dans un contexte international professionnel).\nVotre environnement eXalté:\nUn environnement de travail Collaboratif\nfavorisant les initiatives et projets transverses à la Practice Data & IA (Lab IA, Data Hub, etc.).\nUn collectif de consultants passionnés,\ns’intéressant aux tendances innovantes du secteur.\nUne Practice de proximité,\nprivilégiant la montée en compétence de ses collaborateurs (formations, coachings, mentorats, etc.)\nUn suivi individualisé et de proximité\npar un.e Data Sales Manager référent du compte client, un.e Chargé.e RH et un.e Practice Manager\nUne équipe ouverte et dynamique,\nqui privilégie les moments de partage et de convivialité (séminaires, eXaltemps, meet-up, déjeuners d’équipe, etc.)\nNotre processus de recrutement :\nUn entretien RH avec Estelle,\nà la suite duquel vous saurez tout (ou presque) d’eXalt Value,\nUn entretien technique avec un Manager assorti d’un test technique,\nlors duquel vous aurez l’occasion de démontrer vos talents mais aussi d’apprendre avant même de dire oui,\nUn entretien final avec la Directrice Associée ou le Directeur Opérationnel,\npour finir de vous convaincre de nous rejoindre 😊\nNous avons hâte de recevoir vos CV, et de faire votre connaissance!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Octopus IT - Expert du recrutement tech",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-spark-hadoop-sp%C3%A9cialis%C3%A9-en-big-data-paris-ou-remote-partiel-at-octopus-it-expert-du-recrutement-tech-3685740787?position=28&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=95UUn0EtXS4JBMe178TvPw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La société\nCréée il y a 7 ans, cette entreprise de conseil en hyper croissance, se compose d'environ 90 personnes. Elle est devenue experte en Data et IA (NLP, Deep Learning, Machine Learning) et accompagne leurs clients sur l’ensemble de leurs projets data à travers la valorisation de leurs données.\nLeur valeur ajoutée ? Leur spécialisation en Data ce qui leur permet d'offrir 3 expertises métiers distinctes : la Data Science, la Data Engineering et le Machine Learning Engineering. Autour de ces expertises gravitent bien sûr les métiers de Lead et d'Architecte.\nUne autre de leur force est leur formation interne (avec des profils de seniors ou d'architectes) et externe (avec des partenaires pour passer les certifications).\nChez eux, le collaborateur est placé au centre des préoccupations, permettant ainsi de créer une cohésion et une véritable culture au sein de l'entreprise. Par exemple la majorité des projets se font en équipe et non seul.\nConnu et reconnu pour leur expertise en Big Data, ils sont devenu le partenaire principal d'un grand groupe du CAC 40 et ont pris le lead sur tous les sujets touchant à la transformation Big Data de ce groupe.\nPour poursuivre leur croissance, répondre à leurs ambitions et développer de nouveaux marchés, nous recherchons plusieurs profils pour renforcer leurs effectifs.\nLe poste\nEn les rejoignant vous travaillerez sur les problématiques suivantes :\nMise en place et/ou scale d'architectures\nConstruction de Datalake\nMise en production de model de ML\nPipelining de données\nStreaming de données et temps réel\nLa stack sur laquelle vous travaillerez :\nPython, Scala, Spark, Architectures distribuées : Hadoop, HDFS, Cloud : Aws, GCP, Azure\nVotre profil\nA partir de 3 ans d'expérience en CDI\nVous avez une expérience significative sur des problématiques Big Data\nTrès bonne compétences en Python et/ou Scala et en Spark\nVous êtes familier avec Hadoop, Hive, Hbase\nUne logique cloud (Aws, GCP ou Azure)\nLe salaire & avantages\n50-60 K€ selon expérience\nRTT\nCarte Swile & Mutuelle\n3/4 jours de télétravail par semaine\nEt plus encore…\nCe qu’on préfère\nÊtre impliqué à fond dans une aventure avec de nombreux challenges techniques\nBelles opportunités d'évolutions sur des postes d'Architecte, de Lead ou de Ml Ops\nTrès bonne ambiance, équipe solidaire et orientée partage d’informations\nBeaucoup de workshops en interne et catalogue de formations à votre guise\nCe poste a été soigneusement choisi par votre coach. Powered by Octopus IT, cabinet d’Experts en Recrutement Tech (CDI et clients finaux uniquement) – Visitez nous pour plus d’opportunités :\nwww.octopusit.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "CDI",
            "Salary": "50",
            "Level": "",
            "Experience": "7 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "HBase"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Other": [
                "ML",
                "Machine Learning",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MP DATA",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-mp-data-3908719610?position=29&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=wOEylaAHBy4c%2BGNI5PQ6tg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "MP DATA est une société spécialisée dans l’acquisition, le traitement, et la valorisation des données.\nDepuis sa création en 2015, MP DATA accompagne ses clients, majoritairement industriels, dans le management de leur performance et l’exploitation de leurs données.\nLes collaborateurs, tous issus de grandes écoles, incarnent au quotidien les valeurs d’Excellence, de Partage et d’Engagement.\nIls associent savoir-faire technique, méthodologie et passion et mettent leurs compétences au service de missions et projets au sein de grands groupes français.\nMP DATA accompagne ses clients sur toute la chaine au travers de 3 pôles d’expertise : Conseil et Stratégie, Infrastructure & CloudOPS, Data Science.\nChez MP DATA, les équipes commerciales cherchent des missions en fonction des envies des collaborateurs et non pas l’inverse. Les consultants sont accompagnés dans tous leurs projets, de la mobilité géographique, au changement de secteur d’activité en passant par le développement de nouvelles compétences.\nRejoindre MP DATA, c’est la garantie de travailler sur des sujets passionnants avec un cadre technique fort.\nDescriptif du poste :\nNous recherchons un Data Engineer expérimenté pour rejoindre notre équipe.\nEn tant que Data Engineer, vous serez responsable de la conception, du développement et de la mise en œuvre de pipelines de traitement de données en temps réel à grande échelle.\nVous travaillerez avec des technologies telles que Kafka, Flink, Kinesis et vous utiliserez les services du cloud AWS pour stocker et traiter les données.\nVos responsabilités :\nUtiliser Kafka pour le traitement de flux de données en temps réel à grande échelle, en travaillant avec les producteurs, les consommateurs et les topics.\nMettre en œuvre des pipelines de traitement de données en streaming avec Flink, en appliquant des transformations complexes et en gérant les états.\nÉcrire du code efficace et maintenable en Java / Python pour manipuler et analyser les données en temps réel.\nUtiliser Kubernetes pour déployer et gérer des applications conteneurisées à grande échelle, en assurant la résilience et l’évolutivité des services.\nUtiliser les services AWS tels que Amazon S3, AWS Lambda, Elastic Kubernetes Service (EKS), Elastic Container Service (ECS) et Elastic Compute Cloud (EC2) pour le stockage, le traitement et le calcul des données en temps réel.\nSuivre les meilleures pratiques pour une utilisation efficace du cloud, en assurant la gestion des coûts, la sécurité des données et la disponibilité des services.\nCollaborer avec l’équipe de développement logiciel et la gestion de projets pour assurer un flux de développement fluide et une livraison efficace des fonctionnalités.\nBon à savoir :\nCDI / ASAP / Toulouse\nProfil recherché:\nNous recherchons un candidat diplômé d'une grande école d'Ingénieur avec une première expérience.\nCompétences nécessaires :\nExpérience significative dans un environnement industriel en mode DevOps, avec des outils tels que CICD, gitlab, Jenkins, Sonar, Nexus, XLdeploy, Camunda, etc.\nMaîtrise des langages de programmation tels que Python, Java et expertise dans l’écriture et l’optimisation du code SQL\nMaîtrise du français et bonne maitrise de l’anglais.\nCapacité à travailler en équipe et esprit d’équipe.\nLe processus de recrutement se déroule en 3 entretiens :\nPrise de contact\n1er entretien : Présentation et projet du candidat + présentation MP DATA\n2ème entretien : Entretien de qualification technique\n3ème entretien : Rencontre avec les équipes dans les locaux MP DATA + Proposition de collaboration\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Flink"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Technology & Strategy",
        "location": "Lyon, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-technology-strategy-3881556102?position=30&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=6Bqksdqvzmsz9cNBZny65A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Découvrez Novencia\n:\nExpert en Data et Intelligence Artificielle, nous aidons nos clients à exploiter et à valoriser leurs données sous toutes ses formes en les accompagnant sur des projets de Data Analyse, Data Gourvernance, Data Architecture, Data Science, et Data Engineering…\nVous avez une solide expérience de minimum 2 ans dans l'ingénierie des données et vous êtes à la recherche de nouveaux défis ? Bouclez votre ceinture, la suite est pour vous !\nType de contrat : CDI\nLieu : Lyon\nEn qualité de Data Engineer (H/F), votre rôle sera :\nConcevoir et proposer les solutions de développement répondant aux besoins fonctionnels et techniques des projets big data.\nTu participes à la conception de solutions permettant le traitement de volumes importants de pipelines données.\nRéaliser ces solutions par l’écriture de code, en respectant les méthodes et procédures qualités définies au sein du département Technique.\nMise à disposition sécurisé et lisible de la data.\nS’assurer de la conformité fonctionnelle et technique de ces réalisations en effectuant les tests automatisés nécessaire et la mise en place de monitoring (système et qualité).\nAssurer la maintenance des applicatifs / plateforme data science\nAssurer une veille technologique\nVous disposez des compétences suivantes :\nMaitrise des plateformes Cloud (AWS, GCP ou Azure), de Scala et de SQL.\nUn.e touche à tout : possédant des compétences en langage Python/Spark, de bonnes capacités de modélisation, une forte appétence pour le Big Data\nFin.e connaisseur.euse : Data Engineer convaincu, très peu de secrets pour les clusters et pour les calculs parallèles\nExplorateur.trice : découvre de nouvelles technos grâce à une veille régulière\nDébrouillard.e : relève de nouveaux défis\nNotre objectif commun est de co-construire votre carrière en fonction de vos aspirations et de vos compétences.\nContactez-moi en message privé ou par mail à s.ziki@technologyandstrategy.com !\nLet's make it possible #together\n*Nos postes sont ouverts aux personnes bénéficiant d’une Reconnaissance de la Qualité de Travailleur Handicapé (RQTH). T&S Groupe encourage la diversité et l’égalité sur le lieu de travail. Tous les candidats qualifiés H/F/* sont pris en considération pour un emploi sur un même pied d'égalité.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Withings",
        "location": "Issy-les-Moulineaux, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/stage-data-engineer-ml-h-f-at-withings-3613476264?position=31&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=KmvpsR%2BWm3jN8lzOCziUBA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Chez Withings, nous développons des appareils de santé connectée : nos balances connectées, montres hybrides, tensiomètres, moniteurs de sommeil et tous les dispositifs de notre gamme sont aujourd'hui utilisés par des millions d'utilisateurs. Notre objectif est de permettre la prévention, le dépistage et l'accompagnement d'un certain nombre de maladies chroniques via des produits et des services innovants afin de révolutionner la manière dont on prend soin de notre santé.\nAu sein de l'équipe Machine Learning, nous développons des algorithmes pour extraire des informations physiologiques et médicales pour nos utilisateurs tels que le SPO2, la fréquence cardiaque, la détection de diverses pathologies comme la fibrillation atriale, l'apnée du sommeil...\nIntégré.e au sein de l'équipe Machine Learning, tu auras une ou plusieurs des responsabilités suivantes :\nDévelopper un outil de monitoring de la dette technique, des mauvaises pratiques de code, des failles de sécurité ;\nConstruire des dashboards de visualisation ;\nConstruire un système d'alerte pour notifier les contributeurs d'éventuels problèmes ;\nDévelopper des outils permettant de corriger les éventuels problèmes de façon automatisée ;\nRequirements\nÀ la recherche d'un stage d'une durée de 3 à 6 mois ;\nPréparation d'un Master en école d'ingénieur ou équivalent / année de césure possible ;\nMaîtrise de Python ;\nMaîtrise de Debian ou de Ubuntu, de Shell et de l'environnement Linux ;\nPremière expérience sur du développement logiciel ;\nCulture DevOps (omniprésence du monitoring, automatisation des tâches, ...)\nCompréhension de la culture et des besoins des différents membres de l'équipe ;\nRigueur, autonomie, prise d'initiative, curiosité\nBenefits\nRejoindre l'aventure Withings, c'est :\nIntégrer un des pionniers et leaders mondiaux de la santé connectée, plusieurs fois primé au Consumer Electronic Show\nContribuer à des projets innovants et ambitieux pour la santé de demain dans un environnement agile et en constante évolution\nIntégrer une entreprise internationale, membre de la FrenchTech 120, dont les équipes sont basées à Issy-les-Moulineaux, Boston, Hong-Kong et Shenzhen\nParticiper à l'amélioration continue de nos produits et services en les bêta-testant avant leur sortie, notamment lors de nos nombreuses sessions sportives entre collègues\nParticiper à la Withings Med Academy en assistant à des conférences de professionnels de santé afin de renforcer ses connaissances dans le domaine médical\nCollaborer avec des collègues passionnés et célébrer ensemble chacune de nos réussites !\nToutes les candidatures reçues sont étudiées indépendamment de l'origine ethnique, des croyances, de la religion, du genre, de l'orientation sexuelle ou de la santé des candidats. Withings aspire à offrir et garantir l'égalité des chances aux candidats et seules les personnes habilitées (RH et Management) auront accès aux informations concernant votre candidature.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "OS": [
                "Linux"
            ],
            "Other": [
                "Machine Learning",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Ollioules, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-thales-3902424527?position=32&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=Fv5mtAMxP79Qn3C6UTqogg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.\nThales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.\nQUI ETES-VOUS ?\nDiplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez a\nu moins 3 ans d'expérience\ndans les technologies Big Data.\nPassionné par le\nsecteur de la Défense et du Naval.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que\nData Engineer,\nvous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.\nVOS MISSIONS :\n• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies\nHadoop, Spark, Scala\n.\n• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.\n• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.\n• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.\n• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.\n• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.\nInnovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=33&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=AYRkKdnEYMdhl33uV621Ow%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini\nChoisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.\nVos missions :\nIntégré(e) au sein d'une équipe projets intervenant pour des clients dans des secteurs d'activités variées, vous serez notamment en charge des missions suivantes :\nConcevoir et mettre en oeuvre des stratégies sécurisées d'acquisition et d'intégration de données,\nConfigurer des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),\nConstruire des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée.\nVotre profil :\nDiplôme d’ingénieur ou équivalent universitaire\nMinimum 3 ans d'expérience\nAnglais courant\nMaîtrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.\nExpertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks…\n3 raisons de nous rejoindre :\nQualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité\nprofessionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorités :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nCapgemini\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "DGSE - Direction Générale de la Sécurité Extérieure",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-%E2%80%93-ing%C3%A9nieur-des-donn%C3%A9es-massives-h-f-at-dgse-direction-g%C3%A9n%C3%A9rale-de-la-s%C3%A9curit%C3%A9-ext%C3%A9rieure-3778473628?position=34&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=C68Ytx6N93azegb462EtDw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Introduction\nLa Direction Générale de la Sécurité Extérieure, DGSE, recrute Big Data engineer – Ingénieur des données massives (H/F).\nLe poste est situé à Paris.\nLa nationalité française est obligatoire.\nDomaine métier\nSciences et Technologies\nVotre environnement de travail\nLe flux de données traitées par la DGSE est équivalent à celui des GAFAM. Ces données sont au centre du travail des analystes de renseignement, qui doivent pouvoir compter sur des systèmes leur permettant de rechercher, croiser, traiter ces données, en temps réel ou en batch. Dans ce contexte, la DGSE cherche à renforcer ses équipes de traitement de la donnée massive.\nAu sein d'un service centré sur le stockage, l'exploitation et la valorisation des données, nous vous proposons d'intégrer les équipes en charge des plateformes de stockage ou des traitements temps réel des données. Ces équipes pluridisciplinaires développent et maintiennent de bout en bout diverses plateformes reposant sur les technologies Kafka, Yarn, Hadoop, HBase ou encore Elasticsearch. Plus spécifiquement, l’équipe Stockage administre des entrepôts Big Data ainsi que des couches d’accès à leurs données. L’équipe Temps réel conçoit des algorithmes répondant à des besoins de temps de réaction très courts (levée d’alertes, enrichissement à la volée, réponse à des besoins opérationnels).\nEn nous rejoignant, vous découvrirez :\nun environnement unique, qu'aucune autre structure ne peut vous proposer,\nun métier proche du renseignement et de l'opérationnel,\nune action sur l'intégralité de la chaîne, du développement au déploiement en production,\nun minimum de 48 jours de congés par an,\nune ambiance propice à l’épanouissement professionnel.\nVos missions\nLes missions des équipes auxquelles vous serez amenés à contribuer seront déterminées en fonction de votre expérience et de vos appétences.\nVous serez en charge de plusieurs activités parmi les suivantes :\nconcevoir, implémenter et optimiser des algorithmes de traitement de données distribués (Scala, Spark, Java),\ngarantir le bon fonctionnement, la disponibilité et la performance des plateformes de traitement,\nparticiper à l’évolution de l’architecture, en intégrant de nouveaux composants (frameworks, bibliothèques, …) permettant de mieux répondre aux besoins,\nassurer une veille technologique constante pour rester au plus haut niveau et garantir une adéquation des clusters existants avec l’état de l’art du domaine,\ncontribuer à l'amélioration continue de l'équipe,\ninteragir avec l’équipe SRE/Devops pour améliorer la fiabilité des architectures, l’automatisation des déploiements et l'observabilité des systèmes mis en œuvre.\nVotre profil\nVous êtes titulaire d’un diplôme en informatique, niveau master ou école d’ingénieur, ou pouvez démontrer une expérience équivalente.\nVous devez posséder les compétences et qualités suivantes :\nbonnes connaissances fondamentales logicielles (structures de données, algorithmique, architecture),\nmaîtrise des langages Scala, Java ou python, vous n'avez pas peur de monter en compétences sur ceux que vous ne maîtrisez pas,\nadepte de l'intégration continue, vous êtes familier de Gitlab CI, Github Actions ou Jenkins,\nfamilier avec les bonnes pratiques de développement collaboratif (usage de git, pratique de relecture de code).\nEn bonus :\npremière expérience avec un framework de traitement en streaming (SparkStreaming, KStream, Storm, Flink, ...),\nconvaincu de l'importance de l'observabilité des systèmes qui regroupe métrologie, logging et tracing, vous avez déjà mis en place une stack de ce type (Prometheus, Telegraph, OpenTelemetry, Jaeger, ELK, …),\nfamilier avec un outil de gestion de configuration (Ansible, Puppet, ...),\nexpérience sur les clusters Kafka, Hadoop, HBase ou Elasticsearch de plusieurs nœuds.\nLes plus de l’offre\nContexte d’activités unique\nDiversité des projets\nTechnologies à la pointe\nContact\nEnvoyez-nous votre candidature à l’adresse :\ndgse-macandidature.cer.fct@intradef.gouv.fr\nPlus d’information sur www.dgse.gouv.fr > Nous rejoindre.\nRESTEZ DISCRET SUR VOTRE CANDIDATURE A LA DGSE\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "HBase"
            ],
            "BigData": [
                "Hadoop",
                "Flink",
                "Spark"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Puppet",
                "Ansible"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Aubay",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=35&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=h3dJGitbn9AqvYtDTfW1hQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Passionné par la Data, tu souhaites rejoindre une communauté d’experts dans le domaine afin de développer tes compétences en Data Engineering. Aubay renforce ses équipes Data et recherche des Data Engineers pour intégrer des dispositifs de projets pointus et variés.\nTon quotidien en tant que Data Engineer chez Aubay, :\nDéfinition de la stratégie de stockage et mise en œuvre des technologie appropriées (base de données SQL, NoSQL, stockage distribué,…)\nIngestion des données (structurées, semi-structurées ou non-structurées) selon différentes fréquences : batch, micro-batch ou temps réel\nConception et mise en œuvre de pipelines de données afin de fournir des données prêtes à l’emploi aux consommateurs : uniformisation, mise en qualité, enrichissement, calcul d’indicateurs,…\nConception et développement d’API pour exposer les données auprès d’applications tierces\nAppui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning\nPréparation et animation d’ateliers de travail avec des interlocuteurs variés : recueil/approfondissement des besoins métiers, avancement/restitution des travaux, transfert de compétences,…\nTon profil :\nTu dispose d’une formation niveau BAC+5 (Master 2 ou école d’ingénieur) spécialisée en informatique\nTu as déjà une première expérience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data\nLes technologies telles que Hadoop, Spark ou Kafka sont tes technologies de prédilection\nLa programmation n’a plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python\nTu maîtrises les tenants et aboutissants de la philosophie DevOps et des outils orientés CI/CD\nTu es soucieux de la qualité et de la performance de tes développements et tu t'intéresse à l’innovation frugale\nTu es un expert technique dans ton domaine sans pour autant oublier l’importance d’une communication orale et écrite de qualité et adaptée à chacun de tes interlocuteurs\nTu travaille au quotidien en mode agile et tu en maitrise les fondements\nCe qui nous caractérise :\nDes missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs variés (Banque, Assurance, Telecom, Industrie,…) qui permettent à nos collaborateurs de monter en compétences et de devenir des experts Data reconnus\nDe l’apprentissage en continu avec des formations et des certifications sur les technologies Data d’aujourd’hui et de demain\nDes experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets\nDes communautés de savoir-faire Data proposant de manière régulière aux collaborateurs d’Aubay du contenu et des évènements de partage (webinar, meetup/afterwork, BBL,…) sur les thématiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,…\nAubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.\nTa carrière chez Aubay :\nTu auras la possibilité de développer et certifier tes compétences sur les dernières technologies Data avec un focus fort sur les plateformes Data Cloud telles qu’Azure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks\nTu pourras rejoindre la BU d’excellence Data et évoluer au sein d’un environnement humain et professionnel de haut niveau. Tu profiteras d’un management sur-mesure pour t'accompagner dans ta trajectoire de carrière\nAu sein de la BU d’excellence, de multiples perspectives s’offriront à toi :\nRôle de « Lead » : Vous pourrez gagner en responsabilité sur le plan technologique et devenir un référent auprès de nos clients et des collaborateurs de la communauté Data Engineering\nRôle de « Champion » : Vous représenterez Aubay auprès d’un ou plusieurs de nos partenaires éditeurs stratégiques et vous participerez activement à l’animation de la relation sur le plan technologique\nRôle de « Head » : Vous pourrez prendre la responsabilité du savoir-faire Data Engineering et de ses offres et en assurer le développement au sens large (développement business, recrutement, management de collaborateurs, définition de la stratégie et animation de la communauté au sein du groupe Aubay,…)\nBesoin d’en savoir plus sur le processus de recrutement ?\nUn échange macro au niveau RH avec Doriane\nUn entretien technique avec Marius ou Peter, deux de nos référents techniques\nUn échange managérial avec le Directeur de la BU Modern BI & Data\nA savoir que l’ordre des étapes peut varier selon tes envies (ex : échange managérial avec l’échange technique)\nAubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "CI/CD",
                "Big Data",
                "Machine Learning",
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "United Robotics Group",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=36&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=nFxE35kmFCphA0TLbxr9wg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Bienvenue chez\nAldebaran\n, leader européen de la robotique\nau sein du groupe\nUnited Robotics Group\n.\nNous concevons et industrialisons des robots innovants avec une vision sociétale ambitieuse pour façonner un monde plus humain. Depuis 2005, nous sommes à l'avant-garde de l'interaction homme-robot avec des produits emblématiques tels que NAO et Pepper.\nNotre dernier-né,\nPlato\n,\nincarne notre engagement envers la technologie de pointe et la sécurité,\nfabriqué en France avec des composants européens.\nRejoignez nos équipes multiculturelles et dynamiques pour être au cœur de la révolution de la robotique.\nSi vous êtes passionné.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer à façonner l'avenir, nous vous offrons une expérience enrichissante et stimulante.\nEn tant que membre de notre équipe, vous bénéficierez d'une culture d'entreprise axée sur le sens de ce que nous faisons et valorisant la responsabilité sociale et environnementale.\nChez Aldebaran, nous valorisons l'innovation, la diversité et l'égalité et encourageons chacun.e à être ouvert.e, authentique, courageux.se, responsable et engagé.e.\nFinalité du poste\nAu sein de l'équipe Cloud-Online Services, le Data engineer intégrera l'équipe Data, responsable du développement des produits destinés à la collecte, aux process et à l'exploitation des données de nos robots.\nIl aura pour rôle de définir et d'implémenter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui gèrent les robots du groupe.\nMissions principales\nLe Data engineer aura pour responsabilités de :\névaluer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,\nconcevoir et développer des services Data en respectant la spécification fonctionnelle et la méthodologie agile,\nagréger et stocker de grandes quantités de données,\nmettre en place des solutions de data processing,\nintégrer/développer des outils de visualisation de données et analyser les KPI,\ndévelopper, tester, sélectionner et mettre en production des algorithmes qui permettent de répondre aux besoins,\nréaliser des analyses de données,\nmettre en place des tests de charge et fonctionnels pour les solutions Data,\ninvestiguer et corriger les bugs remontés par les utilisateurs,\ncontribuer à la mise en place de l'infrastructure et outil de déploiement (CI/CD)\nRejoignez-nous pour faire partie d'une aventure passionnante où Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !\nRequirements\nPour la bonne exécution des missions confiées, vous témoignez d'au moins 6 ans d'expérience en tant que développeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.\nCompétences demandées :\nBonne compréhension des technologies d'infrastructure et de déploiement,\nCompétences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,\nBonne compréhension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,\nBonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)\nBonne connaissance et une expérience pratique de Scrum\\Scrumban et des méthodes agiles,\nUne certification AWS sera appréciée,\nUn niveau de français et d'anglais courant est indispensable,\nDes expériences dans des environnements fortement internationaux sont un plus\nBenefits\nNos principaux avantages :\nUne culture du bien-être en entreprise qui a fait ses preuves (budget célébration et moments de convivialité par équipes et directions, restauration collective de qualité, environnement de travail agréable)\nUn engagement fort en matière de responsabilité sociale et environnementale (promotion de l'égalité professionnelle, performance de notre plan diversité et inclusion, référent handicap, fresque du numérique)\nUne culture du télétravail encadrée de manière appropriée !\nTous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "6 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Chantelle",
        "location": "Cachan, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-chantelle-3909775663?position=37&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=B7M58zT%2F%2FcerBjMK2k9%2F7g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La Direction des Systèmes d'Information et du Digital du groupe Chantelle recherche son/sa futur.e Data Engineer H/F, pour le lancement du grand chantier de rénovation de l'architecture Data : la bascule de l'intégralité de son Data Warehouse vers Google Big Query.\nNous souhaitons recruter un Data Engineer H/F confirmé.e, chargé.e de contribuer à la définition de la feuille de route de la Chantelle Data Plaform. En tant que Data Engineer vous travaillerez au sein de l'équipe Data Intégration en charge de la Chantelle Data Platform.\nVos Missions :\n- Mettre en œuvre une infrastructure autour de Google Cloud Platform permettant de collecter (airbyte, API, ...) , transformer (dataform, Bigquery ...), exposer (dataviz, API, applications, ...) et historiser les données générées par l'entreprise.\n- Travailler en étroite proximité avec les responsables des différents domaines fonctionnels (Référentiels, Supply Chain, Manufacturing, B2B, Retail & e-commerce, Finance, ...), avec notre équipe de Data Analysts ainsi qu'avec l'équipe technique en charge des infrastructures transverses\n- Être force de proposition sur tous les sujets d'architecture et de modélisation (choix de mise en place de pipeline temps réels ou au contraire de flux de données en mode batch, ou bien encore stockage sur Big Query / Big Table en fonction des cas d'usage).\n- Définir les éléments structurants, en justifiant vos choix, et les mettre en œuvre.\n- Rationaliser et moderniser notre architecture d'intégration inter-applicative; se projeter sur la création d'un modèle de données de type Datamesh.\n- Faire la refonte de la BI de nombreux use cases tels que le pilotage de nos stocks, personnalisation de nos sites e-commerce en temps réel en fonction de nos profils client, etc…\nStack technique : Google Cloud Platform, BigQuery, DataForm, DataFlow, PubSub, Airbyte, Github ...\nBonne maîtrise des langages Python et SQL\nPourquoi travailler chez Chantelle ?\nUne flexibilité dans votre lieu de travail, selon la politique de télétravail de l'entreprise.\n11 jours de RTT/an ainsi qu'un 13ème mois.\nUne culture d'entreprise familiale basée sur des valeurs de respect, de créativité, de durabilité et de transparence\nUne aventure dans laquelle vous pourrez vous épanouir, apprendre et entreprendre, avec une grande variété de missions et beaucoup d'autonomie\nDes équipes ressources humaines et des managers à votre écoute pour vous accompagner dans votre parcours professionnel\nDes réductions sur nos produits et des ventes au personnel\nDes avantages dans votre qualité de vie au travail : une conciergerie complète proposant un large panel de services, des activités en interne, un CSE.\nVous souhaitez rejoindre un Groupe familial, innovant, engagé et leader dans son secteur en France comme à l'international et vous souhaitez apporter votre expertise et authenticité pour guider votre équipe vers le succès : postulez et rejoignez le Groupe Chantelle !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DBMS": [
                "Big Query",
                "BigQuery"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Siderlog Conseil",
        "location": "Niort, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-developpeur-talend-at-siderlog-conseil-3861714639?position=38&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=OI1YfXE1D5T6TJRZFOucYA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Siderlog est un cabinet de conseil spécialisé implanté à Niort depuis 2004 qui accompagne les directions métiers et SI sur des projets de:\n- Business et Data Analyse\n- Management de projets\n- Conduite du changement\nPour soutenir notre croissance, nous prévoyons à Niort le recrutement de 20 consultants d'ici 2025.\nNos consultants bénéficient d'un modèle qui favorise l'épanouissement professionnel et le bien être:\n🍃Un processus d'intégration spécifique et un suivi régulier\n🍃Une écoute active des attentes, notamment en terme de formations, certifications\n🍃Des déjeuners et évènements mensuels\n🍃Un management et un accompagnement de proximité\n🍃Un package salarial attractif\n🍃La possibilité de contribuer aux projets d'entreprise ( RSE, communautés métiers, pôle conseil et expertise)\n🍃Entreprise labellisée Happy At Work, charte Télétravail...\n🍃De nombreux autres avantages que nous vous invitons à venir découvrir\nSiderlog recherche pour renforcer son équipe, à Niort un(e) consultant(e) Data Engineer / Developpeur Talend.\nDans ce cadre vous devrez :\n✔️Concevoir et développer des traitements/job de données complexes à l'aide de Talend pour l'ingestion, le nettoyage, la transformation et la distribution des données.\n✔️Collaborer étroitement avec les équipes métier pour comprendre les besoins en matière de données et concevoir des solutions adaptées.\n✔️Mettre en œuvre des bonnes pratiques de développement ETL, y compris la documentation, les tests unitaires et l'intégration continue.\n✔️Assurer la surveillance et la maintenance des traitements/job de données en production, en résolvant les incidents et en effectuant des mises à jour si nécessaire.\n📋 Qualifications et compétences :\n👉Expérience avérée dans le développement de solutions de gestion et d'intégration de données, sur Talend.\n👉Maîtrise des langages de requête SQL pour l'extraction et la manipulation des données.\n👉Connaissance approfondie des bases de données relationnelles et des entrepôts de données.\n👉Compétences en programmation avec Java, Python ou d'autres langages similaires.\n👉Capacité à travailler de manière autonome tout en collaborant efficacement avec les membres de l'équipe.\n👉Excellentes compétences en communication écrite et verbale.\n👉Maitrise de l'outil ETL Talend.\n👉Expérience avec d'autres outils d'intégration de données tels que Informatica, BODS, Altéryx.\n👉Certification Talend serait un plus.\n👉Expérience dans le domaine de l'assurance souhaitée\nCette offre vous intéresse ! Postulez !\n🏆🙏🚀🎉 !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Boulanger",
        "location": "Lesquin, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-boulanger-3854554057?position=39&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=rzwJ7eGfBAYHY19wsvY09w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Au sein de la direction informatique, le pôle DATA a pour missions de maximiser la mise en valeur des données de BOULANGER ,ELECTRO-DEPOT et KREFEL/HIFI afin d’aider nos décideurs à agir sur les leviers de leur performance par des processus décisionnels efficients.\nAu sein de ce pôle, tu prendras en charge un large domaine métier qu'il te faudra maitriser de bout en bout : de la données brutes, sa transformation jusqu'à son exposition dans les reporting. Cela afin d'en assurer le bon fonctionnement, les évolutions constantes et sa pérennité\nTes tâches principales portent sur :\nLe pilotage et la mise en œuvre de projets DATA.\nLa collecte, le stockage et l’exploitation fluides des données par le développement de solutions\nMissions\nMaitriser les règles fonctionnelles et les KPI de ton domaine afin de challenger les métiers dans les évolutions et les nouveaux projets\nAccompagner des équipes métiers dans leurs travaux d’identification et expression des besoins sur la data\nParticiper aux ateliers de conception et développement des applications data\nModéliser la solution à mettre en œuvre\nConcevoir et mettre (ou faire mettre) en œuvre des flux les pipelines d’intégration (en mode batch ou fil de l'eau) de données structurées/semi-structurées\nTransformer les données : consolider, enrichir et optimiser les données, qui seront exploitées par le métier\nCréer, faire évoluer et optimiser les restitutions\nSuivre et animer les développeurs (ETL, restitution, self-BI internes ou externes)\nGérer le RUN\nMaitrise le SQL et la base de données (Oracle, Snowflake)\nMaîtrise d’outils de restitution (tel que Business Object (BO), PowerBI…)\nCapacité relationnelle, rigueur et dynamisme\nMaîtrise un ou plusieurs outils de préparation et traitement de la donnée (DataStage, Stambia, ...)\nCapacité à s’adapter à tout type d’interlocuteurs (technique, métiers, Direction)\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Vélizy-Villacoublay, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-talend-f-h-at-thales-3890948785?position=40&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=joJb400uS2PS7%2FhJB7d5wA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.\nThales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.\nQUI ETES-VOUS ?\nDiplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.\nVOS MISSIONS :\n• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Talend.\n• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.\n• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.\n• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.\n• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.\n• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.\n• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.\nInnovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "BigData": [
                "Hadoop"
            ],
            "Other": [
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896992742?position=41&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=tSBp1Wb79voGnfrm%2F3nyZg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Key Performance Consulting (KPC)",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-key-performance-consulting-kpc-3915036342?position=42&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=87iaB3Zrtg5V1NO0NwSbCQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Rejoignez la plus grosse équipe SNOWFLAKE certifiée en France !\nKPC est Partner Elite Snowflake, le plus haut niveau de certification.\nVous souhaitez intégrer une ESN à taille humaine vous permettant de travailler sur des projets challengeant et de monter en compétences ? Ce poste est pour vous !\nVOS MISSIONS :\nElaboration d'architectures optimisées dans un contexte Snowflake,\nConception et mise en place des ingestions de données (temps réel, Kafka, Snowpipe),\nModélisation de données (Star Schéma, DataVault, DataMesh, virtualisation) - DataOps\nMise en oeuvre des transformations et de la valorisation des données (SQL, Python, Java, Scala) - DataOps\nOptimisation des performances et des coûts d utilisation Snowflake (FinOps)\nVOTRE PROFIL :\nVous êtes issu d'une école d'ingénieur ou d'un Master 2\nVous avez une première expérience sur du Snowflake ou au moins 2 ans de SQL\nDEVENEZ NOTRE PROCHAIN(E) Data Engineer Snowflake !\nVous recherchez une entreprise où vous pouvez télétravailler tout en gardant un lien de proximité, qui laisse de l’autonomie, et où il fait bon travailler, alors nous vous souhaitons la bienvenue chez KPC.\nVous avez une expérience d'au moins 2 ans en développement SQL ou une première expérience sur Snowflake ?\nVous souhaitez travailler au sein d’une équipe d'experts technico-fonctionnels ?\nRejoignez l’entreprise KPC ! Une entreprise à taille humaine avec un mode de management dynamique et de proximité.\nNotre cœur de métier de KPC : la business intelligence. Nous sommes intégrateurs de solutions GOLD PARTENAIRE de SAP, Qlik, Microsoft BI, Tableau, Snowflake, Semarchy etc.\nNotre gold partenariat nous permet d'offrir des formations certifiantes pour nos collaborateurs, d'avoir des liens de proximités avec les éditeurs et de revendre leurs solutions.\nRejoignez la plus grosse équipe SNOWFLAKE certifiée en France ! KPC est Partner Elite Snowflake, le plus haut niveau de certification.\nVous souhaitez intégrer une ESN à taille humaine vous permettant de travailler sur des projets challengeant et de monter en compétences ? Ce poste est pour vous !\nVOS MISSIONS :\nElaboration d'architectures optimisées dans un contexte Snowflake,\nConception et mise en place des ingestions de données (temps réel, Kafka, Snowpipe),\nModélisation de données (Star Schéma, DataVault, DataMesh, virtualisation) - DataOps\nMise en oeuvre des transformations et de la valorisation des données (SQL, Python, Java, Scala) - DataOps\nOptimisation des performances et des coûts d utilisation Snowflake (FinOps)\nVOTRE PROFIL :\nVous êtes issu d'une école d'ingénieur ou d'un Master 2\nVous avez une première expérience sur du Snowflake ou deux/trois ans de SQL\nPROCESSUS DE RECRUTEMENT :\nVous pensez être celui, celle qu’il nous faut et vous vous êtes reconnu dans notre organisation, alors venez vivre votre première expérience KPC en postulant à cette offre :\nVous serez appelé(e) par Ludivine (chargée de recrutement) pour une première prise de contact\nNous pourrons poursuivre les échanges avec Olivier (Directeur Sud-Ouest) pour l’approche projet et technique\nPour clore ce processus de recrutement, nous vous inviterons à rencontrer Gabriel (Directeur Sud-Ouest)\nEt tout ça dans un temps record 😊 : 15 jours en moyenne pour allier réactivité et efficacité.\nNous garantissons l’égalité des chances pour toutes et tous car pour nous la diversité est une force !\nKPC EN QUELQUES MOTS ?\nNous sommes une entreprise spécialisée dans la Data.\nDepuis treize ans, nous accompagnons nos clients à valoriser leurs données de manière innovante et efficace pour développer leur performance, améliorer leurs processus et expériences utilisateurs. Nous intervenons en mode projet (50% régie, 50% forfait)\nNous avons développé 3 grandes activités :\nANALYTICS (BI, Data Science/Big Data, Data Gouvernance, EPM, Digital)\nERP SAP\nCRM\nPour cela, nous travaillons en partenariat avec les plus grands éditeurs du marché tels que : SAP, QLIK, DATAGALAXY, SALESFORCES, SNOWFLAKE, TALEND, MICROSOFT, ONESTREAM, IMAGINO.\nEn forte croissance, nous cherchons de nouveaux talents pour participer à cette aventure humaine au service des entreprises de demain.\nKPC EN QUELQUES CHIFFRES :\n300 collaborateurs\n20 % Croissance annuelle\n8 agences en France (Aix-en-Provence, Nice, Montpellier, Lyon, Toulouse, Bordeaux, Paris et Nantes)\nDes grands groupes en tant que clients : CMA CGM, PERNOD RICARD, AIRBUS, CDISCOUNT, CULTURA, GIFI, POLE EMPLOI, L'OREAL ...\nVOS AVANTAGES :\nOrganisation du travail 100% flexible avec du télétravail, participation aux frais téléphonique et internet et un forfait équipement fourniture\nUn parcours d'intégration\nDes formations et des certifications avec les éditeurs sur les technos de pointe\nIK voiture, vélo\nCarte resto, mutuelle, prévoyance santé\nPrime « Vacances »\nPOURQUOI NOUS REJOINDRE ?\nUne entreprise à taille humaine\nUn mode de management dynamique, agile et de proximité\nUne vie d'agence animée, engagée et conviviale dans des locaux sympas\nUne attention particulière à un équilibre de vie pro / perso\nUne entreprise qui encourage les initiatives et l'autonomie\nUne entreprise certifiée Ecovadis Silver pour des actions concrètes en termes de RSE\nUn cadre de travail agréable prenant en compte les enjeux sociétaux et environnementaux\nVous êtes ou voulez être consultant(e), chef de projet, expert(e) technique, manager ? Notre promesse : vous accompagner de façon personnalisée et continue quel que soit votre projet à court, moyen et long terme.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Chef"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "iPepperGroup",
        "location": "Valbonne, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ipeppergroup-3894091360?position=43&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=tEwLeqmjPSQfhAClCp4pfA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "iPepper recrute pour l'un de ses clients une PME éditeur de logiciel dans le domaine du voyage, un\nData Engineer (H/F)\npassionné(e) et expérimenté(e) pour rejoindre une équipe dynamique.\nEn tant qu'Ingénieur(e) Data, vous serez en charge d'extraire et de transformer des données, de construire et d'optimiser des pipelines de données, ainsi que de concevoir des visualisations de données intuitives et informatives.\nResponsabilités :\nConcevoir, construire et maintenir des pipelines de données évolutifs et efficaces pour transférer des données entre des bases de données SQL et NoSQL.\nDévelopper et mettre en œuvre des processus ETL pour extraire, transformer et charger des données à partir de différentes sources dans notre entrepôt de données.\nCollaborer avec des équipes pluridisciplinaires pour comprendre les besoins en données et garantir la fourniture réussie de solutions de données.\nOptimiser et ajuster les pipelines de données existants pour la performance et la fiabilité.\nConcevoir et développer des visualisations de données et des tableaux de bord pour fournir des insights exploitables aux parties prenantes.\nSurveiller et résoudre les problèmes de pipelines de données, en veillant à la qualité et à l'intégrité des données.\nProfil recherché :\nDiplôme universitaire en informatique, en ingénierie ou dans un domaine connexe.\nExpérience avérée en tant que Data Engineer ou dans un rôle similaire, avec un accent particulier sur la construction de pipelines de données et de processus ETL.\nCompréhension solide des bases de données\nSQL\net\nNoSQL\n, y compris la modélisation des données et la conception de schémas.\nMaîtrise des langages de programmation tels que\nPython, Java ou Scala.\nExpérience avec des outils de visualisation de données tels que\nTableau, Power BI.\nSolides compétences en analyse et en résolution de problèmes, avec la capacité de traduire des données complexes en insights exploitables.\nExcellentes compétences en communication et en collaboration, avec la capacité de travailler efficacement dans un environnement d'équipe pluridisciplinaire.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "DataScientest.com",
        "location": "Puteaux, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-h-f-poei-at-datascientest-com-3909360157?position=44&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=FHcjn5vGWt2bAC0sneS4jg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Ingénieur Data (H/F) | POEI\nPuteaux\nCDI\nPostuler\nRetour\nDatascientest Is Hiring!\nIngénieur Data (H/F) | POEI\nÀ propos\nDATASCIENTEST ? LA REFERENCE EN DATASCIENCE\nCréée en 2017, DataScientest révolutionne la formation en Data Science et devient leader en France !\nNotre école compte plus de 6 000 apprenants à son actif, et a séduit 50 entreprises dont une trentaine du CAC40 et des leaders internationaux (BCG, Allianz, Christian Dior, Axa…),\nNous formons aussi les demandeurs d'emploi en leur offrant un CDI au sein de nos entreprises partenaires\nPrésents en Espagne et en Allemagne, notre pédagogie repose sur une structure hybride :\nQui allie l’adaptabilité du distanciel avec une plateforme entièrement conçue par nous-même et\nLa motivation du présentiel avec des séances de coaching animées par nos enseignants data scientists !\nDescriptif du poste\nEn Tant Qu'Ingénieur Data, Vous Serez Chargé(e) De Proposer Les Meilleures Solutions à L'entreprise En Leur Permettant D'optimiser Leur Activité, à Travers Quelques Missions Principales\nDévelopper des solutions pour traiter des volumes importants de données,\nConcevoir, collecter et fabriquer des données brutes,\nCréer des outils et algorithmes pour le traitement des données,\nPréparer des données pour le Data Analyst,\nSécuriser des Pipelines données pour les Data Analysts et Data Scientists,\nOrganiser l'architecture du cloud,\nContribuer à l'effort d'animation technique, de veille technologique et d'innovation\nProfil recherché\nEt si nous parlions de vous ?\nIssu(e) d’une filière scientifique bac+5 ou d’un diplôme d’ingénieur,\nVous disposez idéalement d’une expérience significative en développement informatique, en architecture réseaux ou dans la Data,\nVous maîtrisez un langage objet type Java, Python, C++, etc.\nVous êtes demandeur d'emploi\nN'attendez plus, envoyez nous votre CV, nos équipes se feront un plaisir de vous contacter et de vous accompagner pour préparer vos entretiens avec notre entreprise partenaire.\nInformations complémentaires\nType de contrat : CDI\nLieu : Puteaux\nNiveau d'études : Bac +5 / Master\nExpérience : > 1 an\nTélétravail ponctuel autorisé\nSalaire : entre 38000€ et 48000€ / an\nVous êtes intéressé par cette offre ?\nPostuler\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride"
            ],
            "TypeContract": "CDI",
            "Salary": "38000",
            "Level": "Bac+5",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Inetum",
        "location": "Courbevoie, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-devops-h-f-at-inetum-3843956952?position=45&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=LN1KQRS61sKKEPaDT%2FUBBA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Détail de l'offre\nInformations générales\nEntité de rattachement\nNous sommes une ESN agile, un groupe international certifié Top Employer Europe 2024.\nA l'ère de la post-transformation digitale, nous mettons tout en œuvre pour que chacun de nos 28 000 athlètes du digital puisse se renouveler perpétuellement, en vivant positivement son propre flow digital.\nChacun de nos talents peut ainsi façonner son parcours de carrière selon ses appétences, entreprendre de manière pragmatique avec ses clients pour un monde à impact positif, innover localement dans 27 pays et harmoniser son investissement professionnel et son bien-être personnel.\nRejoignez Inetum. Live your positive digital flow.\nTous nos postes sont ouverts aux personnes en situation de handicap.\nDescription du poste\nMétier\nConseil et Intégration - Business Consulting\nIntitulé du poste\nData Engineer DevOps H/F\nContrat\nCDI\nDescription De La Mission\nQui sommes-nous ?\nNous sommes une ESN agile et un groupe international. A l'ère de la post-transformation digitale, nous mettons tout en œuvre pour que chacun de nos 27 000 collaborateurs puisse se renouveler perpétuellement, en vivant positivement son propre flow digital. Chacun d'entre eux peut ainsi façonner son parcours de carrière selon ses appétences, entreprendre de manière pragmatique avec ses clients pour un monde plus positif, innover localement dans 26 pays et harmoniser son investissement professionnel et son bien-être personnel.\nRejoignez\nCapital Market, entité Inetum en Finance de Marché\n. Nous accompagnons les acteurs majeurs du secteur de la finance en France et à l’International.\nCultivant la double compétence technique et fonctionnelle, nous intervenons sur des projets innovants à haute valeur ajoutée.\nQuelles sont nos valeurs ?\n🏆 Excellence Notre culture de l’excellence naît de notre audace.\n🤝 Engagement S’associer et grandir ensemble !\n🛰 Innovation Nos FabLab au service de la transformation digitale de nos clients.\nMissions proposées\nPour accompagner notre forte croissance, nous recherchons des\nData Engineer DevOps\npour le compte d’un acteur majeur de la finance de marché en Europe et dans le monde. Dans ce contexte international et exigeant, vous travaillez sur la conception de solutions Big Data afin de répondre aux besoins des opérationnels métiers.\nPour mener à bien ce projet, vous aurez pour responsabilités de\nComprendre les enjeux des équipes Data et les accompagner. Faire le lien entre les environnements (datalake, datawarehouse et environnement de déploiement du modèle) grâce à des pipelines sophistiqués\nÊtre référent et garant des bonnes pratiques pour le développement des langages utilisés par l'équipe. Accompagner les Data Scientists dans l'optimisation de leurs algorithmes\nAssurer la viabilité des solutions de datamining et de machine learning de l'équipe Data et les mettre en production.Construire et optimiser des pipelines de données complexes (ETL et ELT)\nCoordonner le développement et les opérations grâce à l’automatisation des flux de travail, la création de services Web prédictifs.\nDéployez ces modèles en utilisant les dernières techniques et pratiques (API REST, Docker, Tensorflow Serving, etc.)\nAnalyser et résoudre les anomalies liées aux performances et à l’évolutivité des solutions Cloud BI et Big Data\nProfil\nProfil souhaité\nDe formation Ingénieur Grande Ecole ou équivalent, vous possédez une première expérience réussite de trois ans minimum sur un poste équivalent idéalement en banque d’investissement ou asset management.\nVous êtes familier avec l’environnement Big Data (data grids, compute grids, REST based architectures, SGBDR, No-SQL Databases, GPUs)\nVous avez déjà travaillé avec la méthodologie Agile\nUne certaine aisance technique est également requise (Jenkins, Docker, Ansible, Git, Scala, Kubernetes, Python/Java, Maven)\nUne double compétence Cloud (AWS, Google Cloud, Azure) serait un véritable plus\nEvoluant dans un contexte international, la maîtrise de l'Anglais est nécessaire.\nL’aisance relationnelle, de l’autonomie, la gestion des priorités, des capacités d’analyse et de synthèse, … le savoir-être est une composante importante dans notre processus de recrutement.\nTous nos postes sont ouverts aux personnes en situation de handicap.\nEt pourquoi Inetum Capital Market ?\n😄 Des missions intéressantes\n🤩 Des perspectives d'évolutions professionnelles et financières\n😎 Les avantages d'un grand groupe international\n😉 Un suivi régulier\n✈️ Une aide à la mobilité géographique que vous soyez localisé en France ou à l'étranger\n👨‍🎓 Des formations certifiantes\n🥳 Des moments de FUN !\nLocalisation du poste\nLocalisation du poste\nFrance, Ile-de-France, 75 Paris\nVille\nCourbevoie\nCritères candidat\nNiveau d'études min. requis\nBac+5\nNiveau d'expérience min. requis\nPlus de 2 ans\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker",
                "Jenkins"
            ],
            "Automation": [
                "Kubernetes",
                "Ansible"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Inetum",
        "location": "St.-Ouen, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/d%C3%A9veloppeur-big-data-junior-h-f-at-inetum-3887272015?position=46&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=GY4l3RNWLP3B0lXdrBu2dw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Détail de l'offre\nInformations générales\nEntité de rattachement\nInetum est un leader européen des services numériques. Pour les entreprises, les acteurs publics et la société dans son ensemble, les 28 000 consultants et spécialistes du groupe visent chaque jour l'impact digital des solutions qui contribuent à la performance, à l'innovation et au bien commun.\nPrésent dans 19 pays au plus près des territoires, et avec ses grands partenaires éditeurs de logiciels, Inetum répond aux enjeux de la transformation digitale avec proximité et flexibilité.\nPorté par son ambition de croissance et d'industrialisation, Inetum a généré en 2023 un chiffre d'affaires de 2,5 milliards d'€.\nPour répondre à un marché en croissance continue depuis plus de 30ans, Inetum a fait le choix délibéré de se recentrer sur 4 métiers afin de gagner en puissance et proposer des solutions sur mesure, adaptées aux besoins spécifiques de ses clients le conseil (Inetum Consulting), la gestion des infrastructures et applications à façon (Inetum Technologies), l'implémentation de progiciels (Inetum Solutions) et sa propre activité d'éditeur de logiciels (Inetum Software). Inetum a conclu des partenariats stratégiques avec 4 grands éditeurs mondiaux - Salesforce, ServiceNow, Microsoft et SAP et poursuit une stratégie d'acquisitions dédiée afin d'entrer dans le top 5 européen sur ces technologies et proposer la meilleure expertise à ses clients.\nTous nos postes sont ouverts aux personnes en situation de handicap.\nDescription du poste\nMétier\nConseil et Intégration - Conseil Technique\nIntitulé du poste\nDéveloppeur Big Data Junior H/F\nContrat\nCDI\nDescription De La Mission\nNous sommes une ESN agile, un groupe international certifié Top Employer Europe 2023.\nA l'ère de la post-transformation digitale, nous mettons tout en œuvre pour que chacun de nos 28 000 athlètes du digital puisse se renouveler perpétuellement, en vivant positivement son propre flow digital.\nChacun de nos talents peut ainsi façonner son parcours de carrière selon ses appétences, entreprendre de manière pragmatique avec ses clients pour un monde à impact positif, innover localement dans 27 pays et harmoniser son investissement professionnel et son bien-être personnel.\nRejoignez Inetum. Live your positive digital flow.\nTous nos postes sont ouverts aux personnes en situation de handicap\nA la recherche de développeurs Big Data juniors pour rejoindre notre équipe. Les candidats doivent être des ingénieurs Big Data passionnés et motivés, capables de travailler en équipe et de résoudre des problèmes complexes.\nResponsabilités\nParticiper à la conception et au développement de solutions Big Data\nTravailler en étroite collaboration avec les équipes de développement pour intégrer les solutions Big Data dans les applications existantes\nParticiper à la mise en place de l'architecture Big Data\nDévelopper des scripts et des programmes pour automatiser les tâches de traitement de données\nParticiper à la maintenance et à l'amélioration des solutions Big Data existantes\nProfil\nCompétences requises\nConnaissance des technologies Big Data telles que Hadoop, Hive, Iceberg, Kafka, Spark, Cloudera, Databricks, Snowflake\nMaîtrise d'au moins un langage de programmation parmi Scala et Java\nConnaissance des environnements Cloud tels que AWS, Azure serait un plus\nDiplôme d'ingénieur ou Master 2 en informatique ou en statistiques\nUne première expérience en développement Big Data serait un plus\nProfil recherché\nNous recherchons des candidats passionnés par les technologies Big Data, ayant une bonne capacité d'analyse et de résolution de problèmes. Les candidats doivent être capables de travailler en équipe et de communiquer efficacement avec les autres membres de l'équipe.\nPourquoi nous rejoindre ?\nRejoindre Inetum, Certifié TOP EMPLOYER EUROPE 2024, c'est\nFaire partie d'une équipe à taille humaine, soudée, encourageant la diversité des profils et des expériences et favorisant l'autonomie et l'initiative de chacun ;\nAvoir un impact chez nos clients en étant responsabilisé dans le cadre de missions à forts enjeux et sur un large panel d'activités ;\nS'intégrer dans une dynamique de croissance et de développement de notre marque de conseil.\nNous vous proposons\nUne trajectoire de carrière personnalisée et adaptée à vos souhaits d'évolution grâce à une implantation à l’international (26 pays, 7 Fablab), des formations ciblées et des projets couvrant l’ensemble de la chaîne de valeur IT (+25 filières métiers)\nIntégrer un collectif d’experts partageant des valeurs de solidarité et d’excellence\nUne culture de la proximité au sein de nos 45 agences en France\nLocalisation du poste\nLocalisation du poste\nFrance, Ile-de-France, 75 Paris\nVille\n5-7 rue Touzet Gaillard - 93400 Saint-Ouen-sur-Seine\nCritères candidat\nNiveau d'études min. requis\nBac+5\nNiveau d'expérience min. requis\nMoins de 2 ans\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Junior"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "30 an(s)"
        },
        "title": "big data developer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Statistiques",
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Initiative",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "FINAXYS",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-at-finaxys-3887107285?position=47&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=eBg06AwxyNxRUXDsMZTakg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LE CONTEXTE\nLeader en\nIT\n, dans les domaines Banque\n,\nFinance\net\nAssurance\n,\nFinaxys\nest un cabinet de\nconseil\ncréé en 2008. Nous accompagnons au quotidien les plus grandes banques du pays sur leur\ntransformation digitale\n(BNP Paribas, Société Générale, Crédit Agricole, Natixis, etc.)\nNos clients bancaires travaillent également dans des contextes Big Data sur des applications centrales rattachées aux Datalakes.\nLES MISSIONS\nDéveloppement et traitements sur des applications Big Data (Python)\nÊtre force de proposition sur les choix techniques les plus pertinents\nMaintenir la qualité des solutions, mesure de cette qualité, alerte sur les non-conformités et validation des solutions définitives.\nAnalyser des risques liés aux solutions envisagées et proposition des actions de remédiation.\nApporter des solutions IT répondant au mieux aux besoins du business porté par la/le Product Owner (Métiers/Fonctions) en cherchant toujours la maximisation de la valeur générée\nAccompagner les équipes dans les migrations Cloud\nENVIRONNEMENT TECHNIQUE\nPython\nPandas\nScirpting Big Data\nCulture DevOps (Jenkins, Maven, Ansible)\nPROFIL\nCompétences Techniques et Fonctionnelles requises\nMaitrise obligatoire de l’anglais\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataAnalytics": [
                "Pandas"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Automation": [
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Renault Digital",
        "location": "Boulogne-Billancourt, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-renault-digital-3911202728?position=48&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=ZkQXgjIjA2SgSdRtYuvyYA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Contexte :\nDans le cadre de son programme Industrie 4.0, Renault développe depuis 2017 sa propre plateforme pour connecter et agréger les données industrielles des 22 sites du Groupe et de plus de 2500 machines.\nFort de partenariats stratégiques signés avec Google Cloud (stack data full GCP), Renault Digital est à la recherche d’un(e) Data Engineer au sein du Pôle Architecture et Data pour mettre en place des chaînes de traitement de données répondant à de nouveaux besoins métiers.\nVous collaborerez au jour le jour avec les équipes métiers ainsi qu’avec les autres fonctions du Pôle Architecture & Data (Data Analysts et Scientists, architectes, …), exploitant des téraoctets de données (événements en mode streaming, traitements en batch et temps réels et les appels aux APIs) afin entre autres d’alimenter des modèles de machine learning (segmentation clients, détection automatiquement des pannes des véhicules, …).\nResponsabilités principales :\nVous participez aux phases de framing, MVP et release des produits, services et APIs orientés data ;\nVous argumentez les choix d’architecture des projets et de la plateforme datalake sur GCP ;\nVous contribuez à la valeur métier des produits orientés Data s’appuyant sur le Datalake, en mettant en place des chaînes bout en bout de traitement de la data, de l’ingestion à l’exposition d’APIs et à la visualisation des données et des solutions ML/DS ;\nVous êtes garant de la qualité des données transformées dans le Datalake, du bon fonctionnement des chaînes de traitement et de l’optimisation de l’utilisation des ressources des ressources cloud ;\nVous proposez des standards d’architecture et de développement ;\nVous êtes force de proposition, innovant(e) et bienveillant(e).\nEnvironement technique :\nSpark, Scala, Python, Java, Airflow, SQL, Google Cloud Platform (BigQuery, Cloud Storage, PubSub, Beam, Dataflow, Cloud ML, TensorFlow, Kubernetes), Git, Docker, JSON, Bash, Spotfire\nProfil recherché :\nVous avez minimum 5 ans d’expérience en tant que Data Engineer ;\nVous disposez d’une expérience en développement Spark, Scala, Python et requêtage SQL sur des gros volumes de données ;\nVous avez une appétence pour la data : validation, transformation, analyse, valorisation ;\nVous possédez une expérience de développement et orchestration de chaines ETL complexes via Airflow ou équivalent ;\nVous pratiquez la méthodologie agile (Agile Scrum et/ou Kanban) ;\nVous utilisez les services cloud (préférablement GCP) ;\nVous êtes capable d’échanger en anglais technique écrit et oral.\nInformations complémentaires :\nVotre poste sera basé à Boulogne-Billancourt (France) en CDI (temps plein)\nVous bénéficiez de 2 à 3 jours de télétravail par semaine\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Temps plein",
                "Full"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Bash",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "DataSerialization": [
                "Json"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "ML"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Equativ",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-internship-at-equativ-3821045783?position=49&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=%2FuGqbRz65o7d93Q2HdgHZQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "👫 About the team\nAt Equativ, we’re on a mission to develop advertising technologies that empower our customers to reach their digital business goals. This means that we rely on massively scalable, widely distributed, highly available, and efficient software systems; the platform deals with over 3 millions requests per second managed by 3,000 servers.\nOur innovation team based in Paris, Nantes, Limoges, Krakow and Berlin is composed of 100+ straightforward and energetic engineers working in an Agile environment and ready to tackle the most complex technical challenges.\nOur data engineering team is composed of 8 skilled engineers and is based in Paris. We are part of the R&D department which is composed of 120+ engineers spread across Paris, Nantes, Limoges, Krakow and Berlin all working in an Agile environment and ready to tackle the most complex technical challenges.\nThe data engineers are split in two sub-teams working in close collaboration:\nPipeline team: Maintaining and enhancing the operationality of our on-premise and cloud data pipelines which feed our warehouses and APIs\nFeature team: Apply best in class data modeling and orchestrating data transformations in our warehouses, leading the day-to-day management of these warehouses\nOur Mission 👇\nOur Data Engineering team is central to Equativ’s data centric business and is responsible to ingest, transform, model and redistribute all data coming from our ad tech platform.\nWe aim at building scalable and robust Big Data platforms from ingestion to business actionable consumption. Our Big Data ecosystem must handle huge volumes (15 Tb per day), short & long term data storage, complex data modeling, real-time and batch ELT as well as providing external access through dedicated APIs.\nWe enhance and deliver Equativ data directly to our customers and throughout the company whether it is for BI analysis, data science models feeding, customer reporting, invoicing and more.\nWe rely on a top tier on-premise & cloud stack (Kafka, Flink, ClickHouse, Bigquery, Dataflow, Airflow, DBT…) and work hard to increase reporting capabilities, lower maintenance time, improve performances and simplify the access to our raw data.\nWhat you'll do ✏️\nAs a data engineer intern, you will be supporting one of the two sub-team (pipeline or feature) in their tasks and projects:\nTake a leading part on a data engineering project such as but not limited to:\nProof of Concept of Clickhouse Cloud\nImprovement of the data transformation process with DBT, BigQuery and Airflow\nDevelopment of new functionalities on our internal tools (APIs, software applications)\nSetup a data lineage application (castor doc)\nSupport the data engineering team in their day-to-day activities:\nEnhance our DevOps process with CI/CD and testing framework\nMonitor performances and workflow of our applications using reporting tool (Grafana)\nTake part in improving and deploying data engineering standards, documentation and operational guidelines around data usage at Equativ\n💪 About you\nMaster degree in Computer Science or similar technical field of study\nPrior experience in data or software development related environment is desired\nExperience with a cloud datawarehouse (BigQuery, Snowflake, Databrick,..) is a plus\nGood knowledge of SQL and one other data programming language (Java preferred, Python, Scala..).\nKnowledge on the software development process (Git, CI/CD, test, scrum)\nWorking proficiency and communication skills in verbal and written English\nStrong interest in big data and cloud computing technologies.\n👋 About us\nEquativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus — four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.\nHeadquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.\nThe company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times’ FT 1000: Europe’s Fastest-Growing Companies.\nEquativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.\nCome and lead the charge with us in building a transparent ecosystem based on quality!\nEquativ is an equal opportunity employer. Equal access to employment, services, and programs are available to everyone, regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you require reasonable accommodation throughout the application and/or interview process, please contact the recruitment team at ta-team@equativ.com\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Flink"
            ],
            "DevTools": [
                "Git"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "METEOJOB by CleverConnect",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-lille-cdi-at-meteojob-by-cleverconnect-3858145866?position=50&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=Yq6xoHndecJYfbsKu%2F1jGg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Entreprise\nChez LJE Solutions, nous plaçons l’humain au cœur de chaque projet. Au-delà des compétences, nous valorisons les\naspirations\net les\nvaleurs\nde chaque individu.\nNous intervenons dans tous les secteurs d'activité en France et en Suisse.\nDescription Du Poste\nLJE Solutions recherche pour un de ses clients basé à Lille, un/une Data Engineer.\nNotre client est une\nESN dynamique basée à Lille, qui se distingue dans l'intégration et la restitution de données. Partenaire privilégié de technologies de pointe comme Power BI, Tableau et Qlik, il recherche des talents désireux de participer à notre aventure entrepreneuriale.\nNous recherchons un Data Engineer curieux et motivé pour jouer un rôle clé dans l'organisation et le développement de l'agence. Ce poste offre une opportunité unique de travailler directement avec les fondateurs, experts en technologies, et de contribuer significativement à la formation interne et à l'expertise chez nos clients.\nVos Responsabilités\nTravailler en étroite collaboration avec les fondateurs sur des projets d'intégration et de restitution de données,\nParticiper activement à la croissance de l'entreprise en apportant des idées innovantes et en prenant part à des projets variés,\nMonter en compétence techniquement, avec la possibilité d'évoluer vers des rôles de Team Lead ou Tech Lead selon vos aspirations.\nCette entreprise offre un environnement convivial et ambitieux, encourageant la prise d'initiative. Leur structure à taille humaine valorise chaque collaborateur, avec une approche personnalisée et une hiérarchie plate qui favorise l'expression et la participation active de tous.\nRémunération Et Avantages\nPoste basé à Lille, avec possibilité de télétravail partiel,\nRémunération compétitive basée sur l'expérience, fourchette indicative de 44k à 48k € en fixe, + variables,\nTickets restaurant,\nMutuelle d'entreprise.\nDescription Du Profil\nPassion pour les technologies de la data, avec une expertise ou un intérêt pour XDi et Talend, sans exclure d'autres ETL du marché,\nPlus de 4 ans d'expérience dans le domaine de la data engineering,\nCuriosité intellectuelle, agilité, excellent savoir-être, forte capacité de travail en équipe et de partage de connaissances,\nLocalisation à Lille ou disposition à déménager, avec une préférence pour les candidats de la région pour faciliter la collaboration et le partage au sein de notre agence physique.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "EnSoftSkils": [
                "Initiative",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Digital Waffle",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-digital-waffle-3913824888?position=51&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=8uQpOuoHSTAxlgIAc6XcKw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Digital Waffle is proud to have partnered with an innovative tech startup in Paris, who are looking for a talented Data Engineer to join their growing team!\nThey are made up of a powerhouse of experts, combining\ndata engineers, business process gurus, and Project managers\nwho leverage the most advanced solutions available; utilising process mining, automation tools, and smart execution systems.\nLooking for an experienced Data Engineer (3-5 years)\nWhat You'll Do:\nThis is a full-time,\nhybrid role (Paris-based)\nwhere you'll wear many hats: data exploration, system integration, data prep, data modeling, and implementing data solutions.\nExperience:\nExpertise in data engineering, data modeling, and ETL (Extract, Transform, Load) processes\nData warehousing and data analytics skills\nExperience handling large, complex datasets\nProficiency in SQL and programming languages like Python or Java\nStellar problem-solving and analytical skills\nTop-notch communication and collaboration abilities\nBachelor's or Master's in Computer Science, Information Systems, or a similar field (a plus for process mining or intelligent process automation experience)\nIf you are an experienced and driven Data Engineer, please apply here!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ippon Technologies",
        "location": "Nantes",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=52&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=DqSfWwLB6EnRE5u%2BjZ6DkA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre la communauté DATA la plus dynamique de France ?\nNotre spécialité est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.\nMembre de la Practice Data, le/la futur(e) Data Engineer sera intégré(e) à nos équipes de conseil et sera suivi(e) par un(e) mentor qui l’aidera à monter en compétences.\nVotre champs d’expertise :\nIntervenir sur les data platforms de nos clients pour développer de nouveaux pipelines de données (ingestion, traitement, exposition).\nTravailler en collaboration avec les métiers et les data scientists pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship etc…)\nDéployer des infrastructures cloud full\ninfra-as-code\n(Terraform, CloudFormation).\nParticiper aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).\nCapitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne.\nVos connaissances :\nUn framework de calcul distribué tel que Spark, Storm, Flink.\nUn ou plusieurs langages de programmation (Python, Scala, Java...)\nDifférents systèmes de stockage de données (SQL ou NoSQL) et bien sûr le langage SQL.\nLa connaissance de Snowflake est bienvenue ;-)\nUn framework de streaming de données tel que Kafka ou Amazon Kinesis.\nUne expérience sur les technologies Cloud : AWS, GCP, Azure\nLe delivery et les projets en production faisant partie de notre ADN, vous êtes capable de livrer du code de qualité dans des environnements agiles.\nDe plus en plus de nos projets se font en remote avec des clients du monde entier, il devient nécessaire d’être à l’aise en Anglais.\nIppon technologies c’est aussi :\n👍 Bénéficier d'un suivi de proximité réalisé par votre manager technique : points réguliers pour votre suivi en mission, votre formation et votre évolution de carrière\n✌️ Rejoindre une entreprise où les valeurs du sport sont nos leitmotiv : dépassement de soi, travail en équipe, bienveillance.\n🗒️ Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL\n😁 Travailler en pair programming ou avec un.e mentor pour gravir les échelons !\n💪 Pouvoir participer à une aventure humaine au sein de notre Fondation Ippon pour réduire la fracture numérique dans le monde !\n🤝 Participer à nos apéros et divers évènements internes pour consolider la cohésion d’équipe\nEt après ?\nEt oui alors ? Que se passe-t-il une fois que vous êtes convaincu d’avoir lu l’offre d’emploi qui vous correspond bien ?\nNous vous proposons de prendre contact et de nous rencontrer !\nLes Next Steps :\n1 call RH\n1 échange RH\n1 échange Technique\nSi le match est bon des deux côtés : Hadjimé ! Vous vous lancerez sur le tatami Ippon !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Flink",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "InfrastructureAsCode": [
                "CloudFormation",
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Antibes, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-antibes-france-h-f-at-astek-3909192086?position=53&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=ln5VxaOXXVRJsDTyWf0TrQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nAntibes - France\nPubliée il y a 2 semaines\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nIntervenir dans la conception, le développement, les tests unitaires, la qualification, l’intégration continue et la mise en production d’évolutions sur les projets du pôle produits scoring (un pôle visant à développer des solutions permettant de générer des scores ou des segments d’information pertinents dans divers domaines, notamment : profiling TV, PUB, SAB, MMDM, Voscastview) chez l’un de nos partenaires spécialisé dans le secteur des télécoms.\nVotre Mission, Si Vous L’acceptez :\nEn collaboration avec les autres membres de l’équipe, vous devrez prendre en charge le RUN des applications du pôle produit scoring.\nConception d’une solution se basant sur les développements existants et les besoins métiers remontés par le Product Owner.\nRéalisation et développement de nouvelles fonctionnalités sur les composants des applications du pôle produits scoring et environnement CGP.\nVotre Future Équipe :\nAu sein d’un environnement riche et complexe, vous évoluerez avec des experts passionnés à la fois techniques et fonctionnels (Ingénieurs spécialisés, chef de projet, scrum master, product owner, analystes …).\nVotre stack de jeu\nD ans un environnement SAFE sous cloud GCP, Big Query, OnPrime, Grafana, Python et Ansible.\nVous ?\nDe formation Ingénieur, vous justifiez d’une première expérience sur un poste de Data engineer. Vous possédez des compétences d’autonomie et d’adaptabilité et vous avez une capacité à communiquer efficacement au sein d’une équipe.\nLe Groupe Astek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies, présent sur les 5 continents. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de ses 7800 collaborateurs qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde et ayant réalisé un chiffre d’affaires hors\nacquisitions de 600M€ en 2023.\nTous les détails sur le Groupe sur le site\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Data engineer – Big Data\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Data engineer – Big Data\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Automation": [
                "Chef",
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LCL",
        "location": "Villejuif, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-profils-exp%C3%A9riment%C3%A9s-h-f-at-lcl-3888403052?position=54&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=B2xAp1M9k1B%2FmH5%2FSOf%2BVg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "🏦 LCL, c’est LA banque urbaine du Groupe Crédit Agricole - avec nous, accompagnez la transformation, le développement et le maintien technologiques de nos outils avec une vision business et de satisfaction de nos 6 millions de clients.\nEn tant qu’acteur majeur de la banque de détail, nous nous adaptons chaque jour aux nouveaux modes de consommation et les projets de nos de clients internes et externes tout en garantissant le besoin de sécurité et de développement technologique qu’impliquent nos activités.\n💡Organisées en mode Agile, les 8 squads de la tribu DATA (6 squads Métier et 2 squads transverses) œuvrent au quotidien pour répondre à un enjeu majeur pour la banque : la collecte, le stockage, la gestion et l’usage de la donnée. En interaction permanente avec les autres tribus IT et les métiers, elles étudient et proposent les solutions et architectures à déployer pour répondre au mieux aux stratégies de développement et de pilotage de l’ensemble des métiers de la banque.\nRejoignez-nous si vous souhaitez participer aux réflexions et au développement de la trajectoire technique et DataCentric du SI LCL et plus largement du Groupe CA. Vous côtoierez et serez au cœur de l’implémentation de technologies variées telles que les plateformes Teradata, les solutions d’architecture applicative des technologies BigData ou IA, des environnements analytiques ou encore des solutions de datavisualisation. Vous assurerez le traitement de données en temps réel ou en batch et exposerez les données sous différentes formes.\nQue vous souhaitiez devenir expert sur les socles technologiques ou relever le challenge de la gestion de projets Métier, nous vous aiderons à atteindre vos propres objectifs.\nVous rejoindrez une équipe pluridisciplinaire, clairement orientée vers le développement de ses collaborateurs à de nouvelles technologies !\n🎯 En tant que Data Engineer :\n· Vous aimez analyser les besoins avec les métiers, challenger, identifier les sources de données dans les différents univers technologiques, industrialiser des algorithmes, concevoir et développer des Datalab ou des Datamart sur les plateformes ? Vous saurez relever les challenges proposés par les squads métier !\n· Vous préférez travailler à l’architecture et au déploiement de nouvelles plateformes, à la levée de la dette technologique ou encore réaliser de la veille au service de notre trajectoire ? La squad Socles Data est faite pour vous !\n· Au-delà des projets que vous gérerez, garant du bon fonctionnement de votre parc applicatif, vous attacherez une grande attention à la mise en œuvre de solutions optimisées.\n· La rigueur, la communication, l’esprit d’équipe mais aussi la curiosité et la créativité font partie de vos soft skills ! ils vous permettront de répondre aux enjeux de sécurité, de qualité, de transmission de la connaissance et contribueront à l’atteinte des objectifs de l’IT et plus largement de LCL, au service de ses clients.\n💻 Voici les principales technologies utilisées au sein de la tribu, si certaines vous sont familières, nous vous aiderons à monter en compétence sur d’autres !\nLangages utilisés : SQL, Python, Scala\nSGBD : Teradata et utilitaires (TPT, BTEQ, …)\nStreaming : Kafka\nSearch : ElasticSearch, SolR\nEnvironnement : Unix\nSolutions Big Data : Hadoop Cloudera, DataIku, HDFS, Hive, Impala,\nDevops : GitLab, Jenkins, Nexus\nOutils de visualisation : MS BI (SSIS, SSAS, SSRS) Qlik Sens, BO\nModélisation : MEGA\nOutils collaboratifs : GIT, Jira, Confluence, Teams\n⚡Si les nouveaux enjeux bancaires vous intéressent, que vous souhaitez intégrer une équipe Agile au service des métiers dans laquelle vous serez force de proposition et que vous aimez travailler dans un environnement motivant et dynamique, rejoignez-nous, cette offre est faite pour vous !\n🔥 Les + de notre entreprise :\nAccès au Plan d’épargne Groupe, intéressement et participation aux bénéfices de l’entreprise + abondement\nPrix préférentiels bancaires et avantages CSE\nParcours évolutif dans l’entreprise et/ou dans le Groupe CA.S.A\nTélétravail (jusqu'à 2 jours de télétravail par semaine)\nDe multiples commodités sur le campus (restaurants d'entreprise, salle de sport, crèche, centre médical, médiathèque...)\nForfait et avantages pratiques « mobilité durable » pour les velotafeurs\nDes équipes aussi diversifiées que structurées dans une dynamique de transformation\nLCL s’engage en faveur de la diversité et nous encourageons tout(e) candidat(e) ayant l’expérience requise à postuler à nos offres. Tous nos postes sont ouverts aux personnes en situation de handicap.\nNous avons encore de nombreuses raisons à vous présenter pour vous convaincre de nous rejoindre mais pour cela, il faudra postuler ici !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL"
            ],
            "BigData": [
                "Hadoop"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Collaboration": [
                "Confluence",
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-confirm%C3%A9-%E2%80%93-paris-france-h-f-at-astek-3839098103?position=55&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=Zls1eavoxiIyF27%2BQNvkaw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nParis - France\nPubliée il y a 2 mois\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nNous rejoindre en tant que\nBig Data Engineer Confirmé (H/F),\nafin d’accompagner un opérateur télécoms, Leader en Europe dans l’assistance et le support applicatif de niveau 3 (résolution des problèmes utilisateurs, exploitation des environnements hors production).\nUn challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !\nVotre Mission, Si Vous L’acceptez :\nSupervision et détection et résolution des problèmes utilisateurs (développeurs, exploitants et data exploreurs)\nDéveloppement de solutions de self-service ou d’une solution de résolutions automatiques des problèmes\nQualifier les données et les résultats\nConception technique des solutions\nAssurer l’accompagnement et le déploiement des évolutions des processus et outils\nAccompagner la phase de mise en production\nVotre Future Équipe :\nVous intégrerez une équipe à la fois technique et fonctionnel, qui œuvre chaque jour pour développer et maintenir en conditions opérationnelles l’ensemble des solutions IT !\nL’équipe est en interaction avec des clients à la fois internes et externes.\nVotre stack de jeu\nSystème d’exploitation : Linux\nOutils des distributions : HDP, HDF, ELK\nEnvironnement Big data : Hadoop, Spark,\nLangage : Scala, Shell, Python\nCloud computing : GCP ou AWS\nBase de données : No SQL (Cassandra, Mongo DB), Shell, Ansible\nDataviz : Power BI ou Kibana\nDes notions en Réseau et Systèmes feront la différence !\nLes Petits Plus Du Projet :\nVous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.\nVous ?\nDiplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.\nVous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?\nVous faite preuve de proactivité et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?\nAlors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !\nAstek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nNotre projet commun vous plait ?\nPostulez à cette annonce, et soyez transparent !\nMaud, notre Talent Acquisition Referent, vous contactera pour un premier échange.\nPuis vous rencontrerez Martin, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .\nEnfin, vous rencontrerez Jérémy, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Hadoop – Scala – Data\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Hadoop – Scala – Data\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "Cassandra"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP",
                "AWS"
            ],
            "OS": [
                "Linux"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Automation": [
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Logic@l Conseils",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-lille-at-logic%40l-conseils-3811575649?position=56&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=dy9XohBb6wp2uWIn1KF0hw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Dans le cadre du développement de nos activités sur la métropole Lilloise, nous recherchons un\nconsultant data engineer\n(H/F) pour intervenir chez l'un de nos grands comptes clients.\nVos missions :\nRecueillir\nles besoins métiers et des équipes data\nConcevoir et mettre en place les\ntraitements de données\nRéaliser les\ntests de validation\nAssurer\nl’alimentation du dataware\nRéaliser les\nordonnancements des traitements\nEtre garant de la\nmise en place\n, du\nsuivi\net de l’\nexploitation\ndes outils déployés\nAssurer\nune veille technologique\nrégulière\nEnvironnement technique :\nDéveloppement :\nPython, Scala, R, Java,\nFramework :\nSpark,\nHadoop,\nOutils Big data :\nYarn, Pig, Hive, Kafka, Splunk\nBases de données :\nMongoDB, HBase, Cassandra\nETL :\nTalend, Stambia\nPlateforme :\nHortonworks, Cloudera, Map Reduce\n,\nAWS, GCP, Azure\nVotre profil :\nVous disposez d’une expérience\nd’au moins 2 ans en tant que data engineer\nou dans le domaine de l’analyse et du traitement de données.\nVéritable\npassionné de la data\n, vous êtes\nforce de proposition\nsur les solutions techniques à mettre en œuvre. Vous maitrisez l’anglais dans un contexte professionnel.\nCompétences requises :\nAnalyses qualitatives et quantitatives (Intermédiaire)\nAnglais (Intermédiaire)\nArchitecture fonctionnelle SI (Débutant)\nDéveloppement d'ouvrages, produits ou événements (Débutant)\nGestion des contrôles, tests et diagnostics (Débutant)\nGestion des risques (Intermédiaire)\nMaîtrise des logiciels (Intermédiaire)\nMise en exploitation / Production et maintenance (Débutant)\nNos valeurs\nNous avons décidé de renverser la pyramide du management pour placer nos collaborateurs en tête des priorités de l’entreprise.\nEn effet, attaché à des valeurs fortes, telles que la proximité, la sincérité, la fidélité, la confiance et le respect, nous sommes persuadés que la réussite réside dans le bien-être de nos collaborateurs.\nCela se traduit par un accompagnement de proximité, de la transparence sans langue de bois, des échanges réguliers avec les managers référents, un accompagnement dans le développement de carrière qui est construit et jalonné avec les formations et certifications nécessaires et les missions en adéquation, pour mener à bien l’évolution de carrière.\nPour vous convaincre de nous rejoindre, nos avantages salariés complémentaires :\nEnvironnement bienveillant et stimulant au sein de 3 pôles d’expertises\nFormations et Certifications à la demande\nTickets restaurants : 13€ par ticket\nRemboursement à 100 % des abonnements de transports en commun\nMutuelle frais de santé avec de hautes garanties\nPrise en charge à 100% de l’assurance Prévoyance\nChèque Cadeau Culture 120 €\nCompte CSE avec une cagnotte de 390 €\nCompte CE : billetterie, voyages, culture, sorties, à des tarifs préférentiels\nDes évènements chaque mois : activités associatives, sportives, afterwork, séminaire,\nPartenariat Losc (participation aux match dans la loge VIP logical conseils – (Une Vingtaine de match par an)\nPossibilité de télétravail\nEn intégrant Logic@l Conseils, vous participez à une réelle aventure humaine, alors pour postuler, il suffit de cliquer ci-dessous !\nTous nos postes sont ouverts, à compétences égales, aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "MongoDB",
                "Cassandra",
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "SoftBigDataProcessing": [
                "Cassandra",
                "HBase"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "CGI",
        "location": "Niort, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-cgi-3902057928?position=57&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=nya1Wu5YHzOoOY6TKfTIXA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Description de poste\nBig Data, Data Science, Data analyse, Data architecture ... Ça n’a pas de secret pour vous ?\nQue vous commenciez votre carrière professionnelle ou que vous soyez spécialiste de l’une de ces disciplines, intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.\nSi vous souhaitez intégrer nos équipes à Niort et accompagner les plus grands acteurs du secteur des Assurances, cette annonce est susceptible de vous intéresser.\nEn tant que Data Engineer, vous serez responsable de la conception, du développement, de la gestion et de l'intégration des systèmes basés sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop. Ce rôle implique la mise en place d'architectures évolutives et hautement disponibles pour répondre aux besoins de traitement et de stockage de données de l'entreprise.\nFonctions et responsabilités\nVos responsabilités seront les suivantes:\n-Maintenir et développer des solutions basées sur les services AWS pour le stockage, le traitement et l'analyse de données\n-Utiliser les services AWS appropriés tels que Amazon EC2, S3, RDS, Lambda, etc., pour répondre aux exigences du projet.\n-Créer et maintenir les configurations Terraform pour la gestion de l'infrastructure en tant que code (IaC) sur AWS\n-Participer à la maintenance et à la mise en place d'environnements OpenShift pour l'hébergement d'applications et de services\n-Gérer et administrer les clusters Kafka pour garantir la disponibilité, la performance et la sécurité du système de messagerie\nParticiper à l’assistance utilisateurs sur les briques de la plateforme Hadoop Cloudera Data\n-Travailler avec les projets et les devOps pour assurer un traitement efficace des données\nEn rejoignant CGI, vous bénéficiez notamment d’une offre complète de formations (techniques, métiers, développement personnel,…), de flexibilité grâce à notre accord télétravail (jusqu’à 3 jours de télétravail par semaine), d’une politique de congés avantageuse (27 jours de congés payés, RTT, congés ancienneté et enfant malade,…) et d’un package d’avantages intéressant (régime d’achats d’actions, participation, CSE,...).\nQualités requises pour réussir dans ce rôle\nAyant une première expérience en tant que Data Engineer, vous avez une première expérience relative aux points suivants:\n-Développement et intégration sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop\n-Connaissance avancée de l'administration Kafka, y compris la configuration, la gestion et la résolution des problèmes\n-Mise en œuvre de l'infrastructure en tant que code à l'aide de Terraform\n-Bonne compréhension des bonnes pratiques de sécurité pour les systèmes cloud, les clusters Kafka et les plateformes Hadoop\nCGI est un employeur inclusif et attentif aux candidatures des personnes en situation de handicap, à l’évolution de carrières des hommes et des femmes et au bien-être de nos salariés LGBT+. Dans un souci d’accessibilité et de clarté, le point médian n’est pas utilisé dans cette annonce. Tous les termes employés se réfèrent aussi bien au genre féminin que masculin.\nEnsemble, en tant que propriétaires, mettons notre savoir-faire à l’œuvre.\nLa vie chez CGI est ancrée dans l’actionnariat, le travail d’équipe, le respect et un sentiment d’appartenance. Chez nous, vous pourrez exploiter votre plein potentiel parce que…\nNous vous invitons à devenir propriétaire dès le jour 1 alors que nous travaillons ensemble à faire de notre rêve une réalité. C’est pourquoi nous nous désignons comme associés de CGI, plutôt que comme employés. Nous tirons profit des retombées de notre succès collectif et contribuons activement à l’orientation et à la stratégie de notre entreprise.\nVotre travail crée de la valeur. Vous élaborerez des solutions novatrices et développerez des relations durables avec vos collègues et clients, tout en ayant accès à des capacités mondiales pour concrétiser vos idées, saisir de nouvelles opportunités, et bénéficier d’une expertise sectorielle et technologique de pointe.\nVous ferez évoluer votre carrière en vous joignant à une entreprise bâtie pour croître et durer. Vous serez soutenus par des leaders qui ont votre santé et bien-être à cœur et qui vous permettront de saisir des occasions afin de parfaire vos compétences et élargir les horizons.\nJoignez-vous à nous, l’une des plus importantes entreprises de conseil en technologie de l’information (TI) et en management au monde.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "1",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "BigData": [
                "Hadoop"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "OpenShift"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ternair",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ternair-3915757963?position=58&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=dWBVkZEg5jT0dB6SX9Q7Mg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "👨‍🚀 MISSION : 👩‍🚀\nEn cohérence avec la stratégie d’entreprise et la roadmap data, vous aurez pour principales missions de :\nEn lien avec l’équipe DevOps, construire, maintenir et faire évoluer la plateforme de données;\nDéfinir et piloter la cohérence de la collecte, la gestion et l’alimentation des données internes et externes, en différents modes : batch, streaming, API (architecture micro-services);\nPréparer et mettre en qualité les données pour les rendre disponibles dans les différents environnements de travail (datalake, datawarehouse, datamart);\nVérifier la qualité des données, de leur bonne et régulière exécution ainsi que de leur utilisation adéquate (gestion des coûts);\nTravailler en étroite collaboration avec les data analysts, scientists et data stewards et business de l’entreprise ;\nEn lien avec l’IT et la sécurité, veiller aux règles d'intégrité et de sécurité des données;\nVeille technologique.\n🧮 Les outils :\nPlateforme data : Google Cloud Platform (Big Query, Airflow)\nDéveloppement : Github/GitLab, Docker, Terraform, Python\nAnalytiques : Qlik\nGestion de projet s: Jira, Confluence, Miro, Drive, Docs, Sheets, Slides\n🤩 Profil recherché : 🤩\nExpérience d'au moins 4-5 ans (après études) en data ingénierie (flux, modélisation, run)\nA l’aise avec l’environnement Cloud et les infrastructures digitales\nCommuniquant, pédagogue et fortes capacités relationnelles\nAnglais (à l’écrit)\nRémunération : 42-60 k€ en package selon expérience\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker"
            ],
            "Collaboration": [
                "Confluence",
                "JIRA"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LVMH",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=59&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=Ii5OaSH%2Bx4fLih9wC%2BWMmA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.\nWe are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.\nIf you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,\nIf you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,\nIf you like building tech solutions having direct impacts on billion-dollar-valued businesses,\nIf you have good communication skills and like sharing your knowledge,\nApply now, and join us!\nThe mission\nThe Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.\nOur team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.\nYou will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).\nMain responsibilities\nYou will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).\nYou will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.\nYou will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.\nApplying the quality and security standards. Making them evolve if necessary.\nProducing realistic, understandable and documented solutions following the group guidelines.\nSharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.\nTaking responsibility as member of the team on the product performances (delivery and long term usage)\nRequired expertise and knowledge\nAbility to build technical solutions answering concrete usage (User Stories) and communicate them to the team.\nDimension and evaluate complexity for technical solution productions.\nExtensive knowledge and experience with good learning and sharing abilities.\nEvaluate quickly risks and opportunities about technical choices.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProblem-solving skills on Data and AI, coding and software development\nTech lover\nFeedback taker and giver\nTeam player\nKey benefits to join our team\nAttractive packages\nOffices in the 8th arrondissement near the Champs Elysées\nFlexibility on the working hours\nRemote work possible (~40%)\n7 weeks of holidays (congés payés + RTT)\nLVMH brands exclusive private sales\nGreat employee committee and health insurance (CE, mutuelle)\nLast generation MacBooks\nPart of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!\nYou’re eligible if\nYou have a strong experience (3+ years) in cloud data architecting or consultancy.\nYou graduated from an engineering (or equivalent) with a master’s degree. Computer Science knowledge is mandatory.\nExperience on data stacks and/or Google Cloud (built in components) is a huge plus.\nFrench and English both written and oral (Maisons are all over the world)\nYou’re thrilled to support the #1 luxury group to get even better.\nHiring Process\nCall with our HR partner dedicated to the Tech Team\nTechnical interview with the Solution Engineering Manager\nTechnical test\nInterview with the Head of Engineering\nStill here? Apply now!!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "40",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Flexibility",
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-%E2%80%93-secteur-t%C3%A9l%C3%A9com-%E2%80%93-paris-france-h-f-at-astek-3832149765?position=60&pageNum=0&refId=TDTc9UZJy78L%2BED7EV%2BMrg%3D%3D&trackingId=Z8j5uhBScZ1Pfkab7NeYTg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nParis - France\nPubliée il y a 2 semaines\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nNous rejoindre en tant que\nBig Data Engineer (H/F),\nafin d’accompagner un opérateur télécoms, Leader en Europe dans la gestion de ses portails clients grands publics (multimarques mobile et internet).\nUn challenge portant sur des millions d’utilisateurs dans un environnement technique innovant et stratégique.\nVotre Mission, Si Vous L’acceptez :\nQualifier les données et les résultats\nConception technique des solutions\nDécliner les impacts de la stratégie et des innovations technologiques au sein des processus et outils de l’exploitant SI\nAssurer l’accompagnement et le déploiement des évolutions des processus et outils\nContribuer aux programmes de transformation DevOps, Cloud et catalogues des offres SI\nDévelopper des fonctions transverses et les « uses cases »\nAccompagner la phase de mise en production\nVotre Future Équipe :\nAu sein d’un environnement riche et complexe, vous évoluerez avec des experts passionné(e)s à la fois techniques et fonctionnels (Ingénieurs spécialisées, chef de projet, scrum master, product owner, analystes…).\nL’équipe est en interaction avec des clients à la fois internes et externes.\nVotre stack de jeu\nSystème d’exploitation Linux\nBig data (Hadoop, Spark, Scala)\nCloud computing (GCP…)\nS QL, No SQL (Cassandra, Mongo DB)\nDataviz : Power BI ou Kibana\nDes notions en développement feront la différence !\nLes Petits Plus Du Projet :\nVous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.\nVous ?\nDiplômé(e) d’une\nécole d’ingénieur\nou équivalent de niveau Bac+5. Vous justifiez idéalement d’une expérience d’au moins 2 ans sur un poste similaire.\nCe descriptif vous interpelle ?\nAlors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !\nAstek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nVous vous êtes reconnu(e) sur l’annonce et Astek vous plaît !\nJulie , Talent Acquisition Officer vous contactera pour en savoir plus sur vous.\nPar La Suite, 2 Échanges Maximum :\nLe premier avec Mathieu (votre futur N+1, avec lequel vous échangerez autour d’ASTEK, de votre parcours, de vos attentes et de la mission)\nLe second avec Anthime (Notre Directeur d’agence pour valider votre intérêt pour le poste et vous présenter les éléments contractuels).\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nBienvenue dans la team ! Allez-y, maintenant c’est à vous de jouer !\nMots-clés :\ningénieur – ingénieure – consultant – consultante – big data engineer\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – big data engineer\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala"
            ],
            "DataBase": [
                "SQL",
                "Cassandra"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "OS": [
                "Linux"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Automation": [
                "Chef"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Grenoble, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-grenoble-at-capgemini-3905836212?position=1&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=rVM%2FWT4txhii7kUBqMd4jw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une communauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.\nVos missions :\nEn tant que Data Engineer au sein d'une équipe multidisciplinaire, vos responsabilités principales seront les suivantes :\nIntervenir sur les différentes phases d'un projet dans un environnement Cloud et Agile.\nContribuer à la gestion de la qualité des données et extraction et analyse de celle-ci, ainsi qu’à la présentation des données dans leur forme raffinée.\nProposer des nouvelles lectures de données via un travail de fouille sur les gisements d’information, notamment client.\nAdopter une posture de consultant : proposer de nouvelles solutions et accompagner le client dans ses choix.\nVotre profil :\nTitulaire d'un Bac+5 en école d’ingénieur ou en université.\nConnaissances approfondies des ETL (Talend, Informatica ou SSIS), du traitement de données (Spark, Python, Scala) ainsi que des bases de données (Oracle, SQL Server, Postgres).\nFaculté pour se montrer curieux, autonome et proactif dans la réalisation de ses tâches.\nCapacité à faire preuve de rigueur et à travailler en équipe.\nBon niveau d’anglais (B2 minimum).\n3 raisons de nous rejoindre :\nQualité de vie au travail\n: accord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu\n: certifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE\n: plan actionnariat, tarif préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorités\n:\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nÀ propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "55 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DBMS": [
                "SQL Server"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "eXalt Value",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=2&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=iUyZScP6JnVgtR9o6D%2Bz8w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "eXalt\nest un cabinet de conseil IT\nPure player Data\n& IA basé à Paris.\nNotre offre s’articule autour de 4 piliers réunis au sein d’une même communauté pour un accompagnement à 360° alliant une expertise technique et méthodologique à une approche conseil métier:\nData Gouvernance & Project\nData Engineering & Big Data\nData Performance & Analytics\nData Science & IA\nFiliale du groupe eXalt, créé en 2018,\nregroupant plus de\n950 collaborateurs en France\n(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)\net à l’international\n(Colombie, Etats-Unis, Espagne, Belgique),\neXalt Value\napporte une\nexpertise approfondie\ndans le domaine de la Data & IA et conseille les entreprises dans le déploiement de leurs stratégies data-driven.\nBénéficiant du support du groupe eXalt\n(1er dans la catégorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value\nest en pleine croissance et regroupe aujourd’hui une communauté d’expertise de plus de 60 collaborateurs en région parisienne.\nNos consultants interviennent sur d\nes projets d’envergure\ndans divers secteurs d’activité,\nBanque & Assurance, Médias, Transports, Retail, Tourisme, etc.\nNous recherchons un\nData Engineer Confirmé H/F (minimum 4 ans d'expérience dans la fonction)\npour rejoindre notre communauté sur le\npilier Data Engineering & Big Data.\nVos missions:\nConcevoir et développer des pipelines et des flux de données.\nIntégrer et transformer des données provenant de différentes sources.\nDévelopper et mettre en œuvre des algorithmes de traitement de données avancés.\nCollaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.\nAssurer la qualité et la fiabilité des solutions développées.\nConseiller les équipes clients sur les solutions à mettre en place.\nLes Prérequis :\nTitulaire d'un Bac+5, Ecole d'Ingénieur\nMaîtrise d'un ou plusieurs langages de programmation (\nPython, Scala, Spark, etc\n.).\nExpérience approfondie des technologies\nBig Data (Hadoop, Spark, Kafka, Talend, etc.)\nExpérience avérée\nen\nenvironnement Cloud (AWS, GCP, ou Azure)\n.\nSolides compétences en conception et en optimisation de pipelines de données.\nExpérience de travail en\nméthode Agile\nCapacité à travailler de manière autonome et en équipe.\nExcellentes compétences en communication et en résolution de problèmes.\nMaîtrise de l’anglais (oral & écrit dans un contexte international professionnel).\nVotre environnement eXalté:\nUn environnement de travail Collaboratif\nfavorisant les initiatives et projets transverses à la Practice Data & IA (Lab IA, Data Hub, etc.).\nUn collectif de consultants passionnés,\ns’intéressant aux tendances innovantes du secteur.\nUne Practice de proximité,\nprivilégiant la montée en compétence de ses collaborateurs (formations, coachings, mentorats, etc.)\nUn suivi individualisé et de proximité\npar un.e Data Sales Manager référent du compte client, un.e Chargé.e RH et un.e Practice Manager\nUne équipe ouverte et dynamique,\nqui privilégie les moments de partage et de convivialité (séminaires, eXaltemps, meet-up, déjeuners d’équipe, etc.)\nNotre processus de recrutement :\nUn entretien RH avec Estelle,\nà la suite duquel vous saurez tout (ou presque) d’eXalt Value,\nUn entretien technique avec un Manager assorti d’un test technique,\nlors duquel vous aurez l’occasion de démontrer vos talents mais aussi d’apprendre avant même de dire oui,\nUn entretien final avec la Directrice Associée ou le Directeur Opérationnel,\npour finir de vous convaincre de nous rejoindre 😊\nNous avons hâte de recevoir vos CV, et de faire votre connaissance!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Octopus IT - Expert du recrutement tech",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-spark-hadoop-sp%C3%A9cialis%C3%A9-en-big-data-paris-ou-remote-partiel-at-octopus-it-expert-du-recrutement-tech-3685740787?position=3&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=2wRnF1bj7F62V7tFqLDvzA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La société\nCréée il y a 7 ans, cette entreprise de conseil en hyper croissance, se compose d'environ 90 personnes. Elle est devenue experte en Data et IA (NLP, Deep Learning, Machine Learning) et accompagne leurs clients sur l’ensemble de leurs projets data à travers la valorisation de leurs données.\nLeur valeur ajoutée ? Leur spécialisation en Data ce qui leur permet d'offrir 3 expertises métiers distinctes : la Data Science, la Data Engineering et le Machine Learning Engineering. Autour de ces expertises gravitent bien sûr les métiers de Lead et d'Architecte.\nUne autre de leur force est leur formation interne (avec des profils de seniors ou d'architectes) et externe (avec des partenaires pour passer les certifications).\nChez eux, le collaborateur est placé au centre des préoccupations, permettant ainsi de créer une cohésion et une véritable culture au sein de l'entreprise. Par exemple la majorité des projets se font en équipe et non seul.\nConnu et reconnu pour leur expertise en Big Data, ils sont devenu le partenaire principal d'un grand groupe du CAC 40 et ont pris le lead sur tous les sujets touchant à la transformation Big Data de ce groupe.\nPour poursuivre leur croissance, répondre à leurs ambitions et développer de nouveaux marchés, nous recherchons plusieurs profils pour renforcer leurs effectifs.\nLe poste\nEn les rejoignant vous travaillerez sur les problématiques suivantes :\nMise en place et/ou scale d'architectures\nConstruction de Datalake\nMise en production de model de ML\nPipelining de données\nStreaming de données et temps réel\nLa stack sur laquelle vous travaillerez :\nPython, Scala, Spark, Architectures distribuées : Hadoop, HDFS, Cloud : Aws, GCP, Azure\nVotre profil\nA partir de 3 ans d'expérience en CDI\nVous avez une expérience significative sur des problématiques Big Data\nTrès bonne compétences en Python et/ou Scala et en Spark\nVous êtes familier avec Hadoop, Hive, Hbase\nUne logique cloud (Aws, GCP ou Azure)\nLe salaire & avantages\n50-60 K€ selon expérience\nRTT\nCarte Swile & Mutuelle\n3/4 jours de télétravail par semaine\nEt plus encore…\nCe qu’on préfère\nÊtre impliqué à fond dans une aventure avec de nombreux challenges techniques\nBelles opportunités d'évolutions sur des postes d'Architecte, de Lead ou de Ml Ops\nTrès bonne ambiance, équipe solidaire et orientée partage d’informations\nBeaucoup de workshops en interne et catalogue de formations à votre guise\nCe poste a été soigneusement choisi par votre coach. Powered by Octopus IT, cabinet d’Experts en Recrutement Tech (CDI et clients finaux uniquement) – Visitez nous pour plus d’opportunités :\nwww.octopusit.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "CDI",
            "Salary": "50",
            "Level": "",
            "Experience": "7 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "HBase"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Other": [
                "ML",
                "Machine Learning",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MP DATA",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-mp-data-3908719610?position=4&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=B0719RxYRCpvjHVP51zIXQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "MP DATA est une société spécialisée dans l’acquisition, le traitement, et la valorisation des données.\nDepuis sa création en 2015, MP DATA accompagne ses clients, majoritairement industriels, dans le management de leur performance et l’exploitation de leurs données.\nLes collaborateurs, tous issus de grandes écoles, incarnent au quotidien les valeurs d’Excellence, de Partage et d’Engagement.\nIls associent savoir-faire technique, méthodologie et passion et mettent leurs compétences au service de missions et projets au sein de grands groupes français.\nMP DATA accompagne ses clients sur toute la chaine au travers de 3 pôles d’expertise : Conseil et Stratégie, Infrastructure & CloudOPS, Data Science.\nChez MP DATA, les équipes commerciales cherchent des missions en fonction des envies des collaborateurs et non pas l’inverse. Les consultants sont accompagnés dans tous leurs projets, de la mobilité géographique, au changement de secteur d’activité en passant par le développement de nouvelles compétences.\nRejoindre MP DATA, c’est la garantie de travailler sur des sujets passionnants avec un cadre technique fort.\nDescriptif du poste :\nNous recherchons un Data Engineer expérimenté pour rejoindre notre équipe.\nEn tant que Data Engineer, vous serez responsable de la conception, du développement et de la mise en œuvre de pipelines de traitement de données en temps réel à grande échelle.\nVous travaillerez avec des technologies telles que Kafka, Flink, Kinesis et vous utiliserez les services du cloud AWS pour stocker et traiter les données.\nVos responsabilités :\nUtiliser Kafka pour le traitement de flux de données en temps réel à grande échelle, en travaillant avec les producteurs, les consommateurs et les topics.\nMettre en œuvre des pipelines de traitement de données en streaming avec Flink, en appliquant des transformations complexes et en gérant les états.\nÉcrire du code efficace et maintenable en Java / Python pour manipuler et analyser les données en temps réel.\nUtiliser Kubernetes pour déployer et gérer des applications conteneurisées à grande échelle, en assurant la résilience et l’évolutivité des services.\nUtiliser les services AWS tels que Amazon S3, AWS Lambda, Elastic Kubernetes Service (EKS), Elastic Container Service (ECS) et Elastic Compute Cloud (EC2) pour le stockage, le traitement et le calcul des données en temps réel.\nSuivre les meilleures pratiques pour une utilisation efficace du cloud, en assurant la gestion des coûts, la sécurité des données et la disponibilité des services.\nCollaborer avec l’équipe de développement logiciel et la gestion de projets pour assurer un flux de développement fluide et une livraison efficace des fonctionnalités.\nBon à savoir :\nCDI / ASAP / Toulouse\nProfil recherché:\nNous recherchons un candidat diplômé d'une grande école d'Ingénieur avec une première expérience.\nCompétences nécessaires :\nExpérience significative dans un environnement industriel en mode DevOps, avec des outils tels que CICD, gitlab, Jenkins, Sonar, Nexus, XLdeploy, Camunda, etc.\nMaîtrise des langages de programmation tels que Python, Java et expertise dans l’écriture et l’optimisation du code SQL\nMaîtrise du français et bonne maitrise de l’anglais.\nCapacité à travailler en équipe et esprit d’équipe.\nLe processus de recrutement se déroule en 3 entretiens :\nPrise de contact\n1er entretien : Présentation et projet du candidat + présentation MP DATA\n2ème entretien : Entretien de qualification technique\n3ème entretien : Rencontre avec les équipes dans les locaux MP DATA + Proposition de collaboration\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Flink"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Technology & Strategy",
        "location": "Lyon, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-technology-strategy-3881556102?position=5&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=Tp%2BxV8D3CpC1FtkRjSsGJA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Découvrez Novencia\n:\nExpert en Data et Intelligence Artificielle, nous aidons nos clients à exploiter et à valoriser leurs données sous toutes ses formes en les accompagnant sur des projets de Data Analyse, Data Gourvernance, Data Architecture, Data Science, et Data Engineering…\nVous avez une solide expérience de minimum 2 ans dans l'ingénierie des données et vous êtes à la recherche de nouveaux défis ? Bouclez votre ceinture, la suite est pour vous !\nType de contrat : CDI\nLieu : Lyon\nEn qualité de Data Engineer (H/F), votre rôle sera :\nConcevoir et proposer les solutions de développement répondant aux besoins fonctionnels et techniques des projets big data.\nTu participes à la conception de solutions permettant le traitement de volumes importants de pipelines données.\nRéaliser ces solutions par l’écriture de code, en respectant les méthodes et procédures qualités définies au sein du département Technique.\nMise à disposition sécurisé et lisible de la data.\nS’assurer de la conformité fonctionnelle et technique de ces réalisations en effectuant les tests automatisés nécessaire et la mise en place de monitoring (système et qualité).\nAssurer la maintenance des applicatifs / plateforme data science\nAssurer une veille technologique\nVous disposez des compétences suivantes :\nMaitrise des plateformes Cloud (AWS, GCP ou Azure), de Scala et de SQL.\nUn.e touche à tout : possédant des compétences en langage Python/Spark, de bonnes capacités de modélisation, une forte appétence pour le Big Data\nFin.e connaisseur.euse : Data Engineer convaincu, très peu de secrets pour les clusters et pour les calculs parallèles\nExplorateur.trice : découvre de nouvelles technos grâce à une veille régulière\nDébrouillard.e : relève de nouveaux défis\nNotre objectif commun est de co-construire votre carrière en fonction de vos aspirations et de vos compétences.\nContactez-moi en message privé ou par mail à s.ziki@technologyandstrategy.com !\nLet's make it possible #together\n*Nos postes sont ouverts aux personnes bénéficiant d’une Reconnaissance de la Qualité de Travailleur Handicapé (RQTH). T&S Groupe encourage la diversité et l’égalité sur le lieu de travail. Tous les candidats qualifiés H/F/* sont pris en considération pour un emploi sur un même pied d'égalité.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Withings",
        "location": "Issy-les-Moulineaux, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/stage-data-engineer-ml-h-f-at-withings-3613476264?position=6&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=wgKY5I%2FqN%2BXtVHXYWR5xEw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Chez Withings, nous développons des appareils de santé connectée : nos balances connectées, montres hybrides, tensiomètres, moniteurs de sommeil et tous les dispositifs de notre gamme sont aujourd'hui utilisés par des millions d'utilisateurs. Notre objectif est de permettre la prévention, le dépistage et l'accompagnement d'un certain nombre de maladies chroniques via des produits et des services innovants afin de révolutionner la manière dont on prend soin de notre santé.\nAu sein de l'équipe Machine Learning, nous développons des algorithmes pour extraire des informations physiologiques et médicales pour nos utilisateurs tels que le SPO2, la fréquence cardiaque, la détection de diverses pathologies comme la fibrillation atriale, l'apnée du sommeil...\nIntégré.e au sein de l'équipe Machine Learning, tu auras une ou plusieurs des responsabilités suivantes :\nDévelopper un outil de monitoring de la dette technique, des mauvaises pratiques de code, des failles de sécurité ;\nConstruire des dashboards de visualisation ;\nConstruire un système d'alerte pour notifier les contributeurs d'éventuels problèmes ;\nDévelopper des outils permettant de corriger les éventuels problèmes de façon automatisée ;\nRequirements\nÀ la recherche d'un stage d'une durée de 3 à 6 mois ;\nPréparation d'un Master en école d'ingénieur ou équivalent / année de césure possible ;\nMaîtrise de Python ;\nMaîtrise de Debian ou de Ubuntu, de Shell et de l'environnement Linux ;\nPremière expérience sur du développement logiciel ;\nCulture DevOps (omniprésence du monitoring, automatisation des tâches, ...)\nCompréhension de la culture et des besoins des différents membres de l'équipe ;\nRigueur, autonomie, prise d'initiative, curiosité\nBenefits\nRejoindre l'aventure Withings, c'est :\nIntégrer un des pionniers et leaders mondiaux de la santé connectée, plusieurs fois primé au Consumer Electronic Show\nContribuer à des projets innovants et ambitieux pour la santé de demain dans un environnement agile et en constante évolution\nIntégrer une entreprise internationale, membre de la FrenchTech 120, dont les équipes sont basées à Issy-les-Moulineaux, Boston, Hong-Kong et Shenzhen\nParticiper à l'amélioration continue de nos produits et services en les bêta-testant avant leur sortie, notamment lors de nos nombreuses sessions sportives entre collègues\nParticiper à la Withings Med Academy en assistant à des conférences de professionnels de santé afin de renforcer ses connaissances dans le domaine médical\nCollaborer avec des collègues passionnés et célébrer ensemble chacune de nos réussites !\nToutes les candidatures reçues sont étudiées indépendamment de l'origine ethnique, des croyances, de la religion, du genre, de l'orientation sexuelle ou de la santé des candidats. Withings aspire à offrir et garantir l'égalité des chances aux candidats et seules les personnes habilitées (RH et Management) auront accès aux informations concernant votre candidature.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "OS": [
                "Linux"
            ],
            "Other": [
                "Machine Learning",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Ollioules, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-thales-3902424527?position=7&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=0nf93AXX17MnZHS4OFuw%2Fw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.\nThales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.\nQUI ETES-VOUS ?\nDiplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez a\nu moins 3 ans d'expérience\ndans les technologies Big Data.\nPassionné par le\nsecteur de la Défense et du Naval.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que\nData Engineer,\nvous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.\nVOS MISSIONS :\n• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies\nHadoop, Spark, Scala\n.\n• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.\n• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.\n• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.\n• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.\n• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.\nInnovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=8&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=Mef8ntH2ikGhIaILEuhGJw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini\nChoisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.\nVos missions :\nIntégré(e) au sein d'une équipe projets intervenant pour des clients dans des secteurs d'activités variées, vous serez notamment en charge des missions suivantes :\nConcevoir et mettre en oeuvre des stratégies sécurisées d'acquisition et d'intégration de données,\nConfigurer des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),\nConstruire des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée.\nVotre profil :\nDiplôme d’ingénieur ou équivalent universitaire\nMinimum 3 ans d'expérience\nAnglais courant\nMaîtrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.\nExpertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks…\n3 raisons de nous rejoindre :\nQualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité\nprofessionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorités :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nCapgemini\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "DGSE - Direction Générale de la Sécurité Extérieure",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-%E2%80%93-ing%C3%A9nieur-des-donn%C3%A9es-massives-h-f-at-dgse-direction-g%C3%A9n%C3%A9rale-de-la-s%C3%A9curit%C3%A9-ext%C3%A9rieure-3778473628?position=9&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=6VmCvsOgk1%2BOB3QFaDw2dg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Introduction\nLa Direction Générale de la Sécurité Extérieure, DGSE, recrute Big Data engineer – Ingénieur des données massives (H/F).\nLe poste est situé à Paris.\nLa nationalité française est obligatoire.\nDomaine métier\nSciences et Technologies\nVotre environnement de travail\nLe flux de données traitées par la DGSE est équivalent à celui des GAFAM. Ces données sont au centre du travail des analystes de renseignement, qui doivent pouvoir compter sur des systèmes leur permettant de rechercher, croiser, traiter ces données, en temps réel ou en batch. Dans ce contexte, la DGSE cherche à renforcer ses équipes de traitement de la donnée massive.\nAu sein d'un service centré sur le stockage, l'exploitation et la valorisation des données, nous vous proposons d'intégrer les équipes en charge des plateformes de stockage ou des traitements temps réel des données. Ces équipes pluridisciplinaires développent et maintiennent de bout en bout diverses plateformes reposant sur les technologies Kafka, Yarn, Hadoop, HBase ou encore Elasticsearch. Plus spécifiquement, l’équipe Stockage administre des entrepôts Big Data ainsi que des couches d’accès à leurs données. L’équipe Temps réel conçoit des algorithmes répondant à des besoins de temps de réaction très courts (levée d’alertes, enrichissement à la volée, réponse à des besoins opérationnels).\nEn nous rejoignant, vous découvrirez :\nun environnement unique, qu'aucune autre structure ne peut vous proposer,\nun métier proche du renseignement et de l'opérationnel,\nune action sur l'intégralité de la chaîne, du développement au déploiement en production,\nun minimum de 48 jours de congés par an,\nune ambiance propice à l’épanouissement professionnel.\nVos missions\nLes missions des équipes auxquelles vous serez amenés à contribuer seront déterminées en fonction de votre expérience et de vos appétences.\nVous serez en charge de plusieurs activités parmi les suivantes :\nconcevoir, implémenter et optimiser des algorithmes de traitement de données distribués (Scala, Spark, Java),\ngarantir le bon fonctionnement, la disponibilité et la performance des plateformes de traitement,\nparticiper à l’évolution de l’architecture, en intégrant de nouveaux composants (frameworks, bibliothèques, …) permettant de mieux répondre aux besoins,\nassurer une veille technologique constante pour rester au plus haut niveau et garantir une adéquation des clusters existants avec l’état de l’art du domaine,\ncontribuer à l'amélioration continue de l'équipe,\ninteragir avec l’équipe SRE/Devops pour améliorer la fiabilité des architectures, l’automatisation des déploiements et l'observabilité des systèmes mis en œuvre.\nVotre profil\nVous êtes titulaire d’un diplôme en informatique, niveau master ou école d’ingénieur, ou pouvez démontrer une expérience équivalente.\nVous devez posséder les compétences et qualités suivantes :\nbonnes connaissances fondamentales logicielles (structures de données, algorithmique, architecture),\nmaîtrise des langages Scala, Java ou python, vous n'avez pas peur de monter en compétences sur ceux que vous ne maîtrisez pas,\nadepte de l'intégration continue, vous êtes familier de Gitlab CI, Github Actions ou Jenkins,\nfamilier avec les bonnes pratiques de développement collaboratif (usage de git, pratique de relecture de code).\nEn bonus :\npremière expérience avec un framework de traitement en streaming (SparkStreaming, KStream, Storm, Flink, ...),\nconvaincu de l'importance de l'observabilité des systèmes qui regroupe métrologie, logging et tracing, vous avez déjà mis en place une stack de ce type (Prometheus, Telegraph, OpenTelemetry, Jaeger, ELK, …),\nfamilier avec un outil de gestion de configuration (Ansible, Puppet, ...),\nexpérience sur les clusters Kafka, Hadoop, HBase ou Elasticsearch de plusieurs nœuds.\nLes plus de l’offre\nContexte d’activités unique\nDiversité des projets\nTechnologies à la pointe\nContact\nEnvoyez-nous votre candidature à l’adresse :\ndgse-macandidature.cer.fct@intradef.gouv.fr\nPlus d’information sur www.dgse.gouv.fr > Nous rejoindre.\nRESTEZ DISCRET SUR VOTRE CANDIDATURE A LA DGSE\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "HBase"
            ],
            "BigData": [
                "Hadoop",
                "Flink",
                "Spark"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Puppet",
                "Ansible"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Aubay",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=10&pageNum=2&refId=EunNu8klDuKd8PhUvgvx0Q%3D%3D&trackingId=Fi2i%2BPsUT%2FrkOzoKgGwkpQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Passionné par la Data, tu souhaites rejoindre une communauté d’experts dans le domaine afin de développer tes compétences en Data Engineering. Aubay renforce ses équipes Data et recherche des Data Engineers pour intégrer des dispositifs de projets pointus et variés.\nTon quotidien en tant que Data Engineer chez Aubay, :\nDéfinition de la stratégie de stockage et mise en œuvre des technologie appropriées (base de données SQL, NoSQL, stockage distribué,…)\nIngestion des données (structurées, semi-structurées ou non-structurées) selon différentes fréquences : batch, micro-batch ou temps réel\nConception et mise en œuvre de pipelines de données afin de fournir des données prêtes à l’emploi aux consommateurs : uniformisation, mise en qualité, enrichissement, calcul d’indicateurs,…\nConception et développement d’API pour exposer les données auprès d’applications tierces\nAppui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning\nPréparation et animation d’ateliers de travail avec des interlocuteurs variés : recueil/approfondissement des besoins métiers, avancement/restitution des travaux, transfert de compétences,…\nTon profil :\nTu dispose d’une formation niveau BAC+5 (Master 2 ou école d’ingénieur) spécialisée en informatique\nTu as déjà une première expérience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data\nLes technologies telles que Hadoop, Spark ou Kafka sont tes technologies de prédilection\nLa programmation n’a plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python\nTu maîtrises les tenants et aboutissants de la philosophie DevOps et des outils orientés CI/CD\nTu es soucieux de la qualité et de la performance de tes développements et tu t'intéresse à l’innovation frugale\nTu es un expert technique dans ton domaine sans pour autant oublier l’importance d’une communication orale et écrite de qualité et adaptée à chacun de tes interlocuteurs\nTu travaille au quotidien en mode agile et tu en maitrise les fondements\nCe qui nous caractérise :\nDes missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs variés (Banque, Assurance, Telecom, Industrie,…) qui permettent à nos collaborateurs de monter en compétences et de devenir des experts Data reconnus\nDe l’apprentissage en continu avec des formations et des certifications sur les technologies Data d’aujourd’hui et de demain\nDes experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets\nDes communautés de savoir-faire Data proposant de manière régulière aux collaborateurs d’Aubay du contenu et des évènements de partage (webinar, meetup/afterwork, BBL,…) sur les thématiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,…\nAubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.\nTa carrière chez Aubay :\nTu auras la possibilité de développer et certifier tes compétences sur les dernières technologies Data avec un focus fort sur les plateformes Data Cloud telles qu’Azure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks\nTu pourras rejoindre la BU d’excellence Data et évoluer au sein d’un environnement humain et professionnel de haut niveau. Tu profiteras d’un management sur-mesure pour t'accompagner dans ta trajectoire de carrière\nAu sein de la BU d’excellence, de multiples perspectives s’offriront à toi :\nRôle de « Lead » : Vous pourrez gagner en responsabilité sur le plan technologique et devenir un référent auprès de nos clients et des collaborateurs de la communauté Data Engineering\nRôle de « Champion » : Vous représenterez Aubay auprès d’un ou plusieurs de nos partenaires éditeurs stratégiques et vous participerez activement à l’animation de la relation sur le plan technologique\nRôle de « Head » : Vous pourrez prendre la responsabilité du savoir-faire Data Engineering et de ses offres et en assurer le développement au sens large (développement business, recrutement, management de collaborateurs, définition de la stratégie et animation de la communauté au sein du groupe Aubay,…)\nBesoin d’en savoir plus sur le processus de recrutement ?\nUn échange macro au niveau RH avec Doriane\nUn entretien technique avec Marius ou Peter, deux de nos référents techniques\nUn échange managérial avec le Directeur de la BU Modern BI & Data\nA savoir que l’ordre des étapes peut varier selon tes envies (ex : échange managérial avec l’échange technique)\nAubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "CI/CD",
                "Big Data",
                "Machine Learning",
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Digital Waffle",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-digital-waffle-3913824888?position=1&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=CIbs7PPg0iqB6Iyk37LUGw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Digital Waffle is proud to have partnered with an innovative tech startup in Paris, who are looking for a talented Data Engineer to join their growing team!\nThey are made up of a powerhouse of experts, combining\ndata engineers, business process gurus, and Project managers\nwho leverage the most advanced solutions available; utilising process mining, automation tools, and smart execution systems.\nLooking for an experienced Data Engineer (3-5 years)\nWhat You'll Do:\nThis is a full-time,\nhybrid role (Paris-based)\nwhere you'll wear many hats: data exploration, system integration, data prep, data modeling, and implementing data solutions.\nExperience:\nExpertise in data engineering, data modeling, and ETL (Extract, Transform, Load) processes\nData warehousing and data analytics skills\nExperience handling large, complex datasets\nProficiency in SQL and programming languages like Python or Java\nStellar problem-solving and analytical skills\nTop-notch communication and collaboration abilities\nBachelor's or Master's in Computer Science, Information Systems, or a similar field (a plus for process mining or intelligent process automation experience)\nIf you are an experienced and driven Data Engineer, please apply here!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ippon Technologies",
        "location": "Nantes",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=2&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=GH7vg4huWWuTwG0Xe%2FwWPg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre la communauté DATA la plus dynamique de France ?\nNotre spécialité est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.\nMembre de la Practice Data, le/la futur(e) Data Engineer sera intégré(e) à nos équipes de conseil et sera suivi(e) par un(e) mentor qui l’aidera à monter en compétences.\nVotre champs d’expertise :\nIntervenir sur les data platforms de nos clients pour développer de nouveaux pipelines de données (ingestion, traitement, exposition).\nTravailler en collaboration avec les métiers et les data scientists pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship etc…)\nDéployer des infrastructures cloud full\ninfra-as-code\n(Terraform, CloudFormation).\nParticiper aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).\nCapitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne.\nVos connaissances :\nUn framework de calcul distribué tel que Spark, Storm, Flink.\nUn ou plusieurs langages de programmation (Python, Scala, Java...)\nDifférents systèmes de stockage de données (SQL ou NoSQL) et bien sûr le langage SQL.\nLa connaissance de Snowflake est bienvenue ;-)\nUn framework de streaming de données tel que Kafka ou Amazon Kinesis.\nUne expérience sur les technologies Cloud : AWS, GCP, Azure\nLe delivery et les projets en production faisant partie de notre ADN, vous êtes capable de livrer du code de qualité dans des environnements agiles.\nDe plus en plus de nos projets se font en remote avec des clients du monde entier, il devient nécessaire d’être à l’aise en Anglais.\nIppon technologies c’est aussi :\n👍 Bénéficier d'un suivi de proximité réalisé par votre manager technique : points réguliers pour votre suivi en mission, votre formation et votre évolution de carrière\n✌️ Rejoindre une entreprise où les valeurs du sport sont nos leitmotiv : dépassement de soi, travail en équipe, bienveillance.\n🗒️ Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL\n😁 Travailler en pair programming ou avec un.e mentor pour gravir les échelons !\n💪 Pouvoir participer à une aventure humaine au sein de notre Fondation Ippon pour réduire la fracture numérique dans le monde !\n🤝 Participer à nos apéros et divers évènements internes pour consolider la cohésion d’équipe\nEt après ?\nEt oui alors ? Que se passe-t-il une fois que vous êtes convaincu d’avoir lu l’offre d’emploi qui vous correspond bien ?\nNous vous proposons de prendre contact et de nous rencontrer !\nLes Next Steps :\n1 call RH\n1 échange RH\n1 échange Technique\nSi le match est bon des deux côtés : Hadjimé ! Vous vous lancerez sur le tatami Ippon !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Flink",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "InfrastructureAsCode": [
                "CloudFormation",
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Antibes, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-antibes-france-h-f-at-astek-3909192086?position=3&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=UlDBekVCS59%2Ffa2Hrk0odA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nAntibes - France\nPubliée il y a 2 semaines\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nIntervenir dans la conception, le développement, les tests unitaires, la qualification, l’intégration continue et la mise en production d’évolutions sur les projets du pôle produits scoring (un pôle visant à développer des solutions permettant de générer des scores ou des segments d’information pertinents dans divers domaines, notamment : profiling TV, PUB, SAB, MMDM, Voscastview) chez l’un de nos partenaires spécialisé dans le secteur des télécoms.\nVotre Mission, Si Vous L’acceptez :\nEn collaboration avec les autres membres de l’équipe, vous devrez prendre en charge le RUN des applications du pôle produit scoring.\nConception d’une solution se basant sur les développements existants et les besoins métiers remontés par le Product Owner.\nRéalisation et développement de nouvelles fonctionnalités sur les composants des applications du pôle produits scoring et environnement CGP.\nVotre Future Équipe :\nAu sein d’un environnement riche et complexe, vous évoluerez avec des experts passionnés à la fois techniques et fonctionnels (Ingénieurs spécialisés, chef de projet, scrum master, product owner, analystes …).\nVotre stack de jeu\nD ans un environnement SAFE sous cloud GCP, Big Query, OnPrime, Grafana, Python et Ansible.\nVous ?\nDe formation Ingénieur, vous justifiez d’une première expérience sur un poste de Data engineer. Vous possédez des compétences d’autonomie et d’adaptabilité et vous avez une capacité à communiquer efficacement au sein d’une équipe.\nLe Groupe Astek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies, présent sur les 5 continents. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de ses 7800 collaborateurs qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde et ayant réalisé un chiffre d’affaires hors\nacquisitions de 600M€ en 2023.\nTous les détails sur le Groupe sur le site\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Data engineer – Big Data\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Data engineer – Big Data\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Automation": [
                "Chef",
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LCL",
        "location": "Villejuif, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-profils-exp%C3%A9riment%C3%A9s-h-f-at-lcl-3888403052?position=4&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=gIHYgeFCEYY8kCc8uBBh3Q%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "🏦 LCL, c’est LA banque urbaine du Groupe Crédit Agricole - avec nous, accompagnez la transformation, le développement et le maintien technologiques de nos outils avec une vision business et de satisfaction de nos 6 millions de clients.\nEn tant qu’acteur majeur de la banque de détail, nous nous adaptons chaque jour aux nouveaux modes de consommation et les projets de nos de clients internes et externes tout en garantissant le besoin de sécurité et de développement technologique qu’impliquent nos activités.\n💡Organisées en mode Agile, les 8 squads de la tribu DATA (6 squads Métier et 2 squads transverses) œuvrent au quotidien pour répondre à un enjeu majeur pour la banque : la collecte, le stockage, la gestion et l’usage de la donnée. En interaction permanente avec les autres tribus IT et les métiers, elles étudient et proposent les solutions et architectures à déployer pour répondre au mieux aux stratégies de développement et de pilotage de l’ensemble des métiers de la banque.\nRejoignez-nous si vous souhaitez participer aux réflexions et au développement de la trajectoire technique et DataCentric du SI LCL et plus largement du Groupe CA. Vous côtoierez et serez au cœur de l’implémentation de technologies variées telles que les plateformes Teradata, les solutions d’architecture applicative des technologies BigData ou IA, des environnements analytiques ou encore des solutions de datavisualisation. Vous assurerez le traitement de données en temps réel ou en batch et exposerez les données sous différentes formes.\nQue vous souhaitiez devenir expert sur les socles technologiques ou relever le challenge de la gestion de projets Métier, nous vous aiderons à atteindre vos propres objectifs.\nVous rejoindrez une équipe pluridisciplinaire, clairement orientée vers le développement de ses collaborateurs à de nouvelles technologies !\n🎯 En tant que Data Engineer :\n· Vous aimez analyser les besoins avec les métiers, challenger, identifier les sources de données dans les différents univers technologiques, industrialiser des algorithmes, concevoir et développer des Datalab ou des Datamart sur les plateformes ? Vous saurez relever les challenges proposés par les squads métier !\n· Vous préférez travailler à l’architecture et au déploiement de nouvelles plateformes, à la levée de la dette technologique ou encore réaliser de la veille au service de notre trajectoire ? La squad Socles Data est faite pour vous !\n· Au-delà des projets que vous gérerez, garant du bon fonctionnement de votre parc applicatif, vous attacherez une grande attention à la mise en œuvre de solutions optimisées.\n· La rigueur, la communication, l’esprit d’équipe mais aussi la curiosité et la créativité font partie de vos soft skills ! ils vous permettront de répondre aux enjeux de sécurité, de qualité, de transmission de la connaissance et contribueront à l’atteinte des objectifs de l’IT et plus largement de LCL, au service de ses clients.\n💻 Voici les principales technologies utilisées au sein de la tribu, si certaines vous sont familières, nous vous aiderons à monter en compétence sur d’autres !\nLangages utilisés : SQL, Python, Scala\nSGBD : Teradata et utilitaires (TPT, BTEQ, …)\nStreaming : Kafka\nSearch : ElasticSearch, SolR\nEnvironnement : Unix\nSolutions Big Data : Hadoop Cloudera, DataIku, HDFS, Hive, Impala,\nDevops : GitLab, Jenkins, Nexus\nOutils de visualisation : MS BI (SSIS, SSAS, SSRS) Qlik Sens, BO\nModélisation : MEGA\nOutils collaboratifs : GIT, Jira, Confluence, Teams\n⚡Si les nouveaux enjeux bancaires vous intéressent, que vous souhaitez intégrer une équipe Agile au service des métiers dans laquelle vous serez force de proposition et que vous aimez travailler dans un environnement motivant et dynamique, rejoignez-nous, cette offre est faite pour vous !\n🔥 Les + de notre entreprise :\nAccès au Plan d’épargne Groupe, intéressement et participation aux bénéfices de l’entreprise + abondement\nPrix préférentiels bancaires et avantages CSE\nParcours évolutif dans l’entreprise et/ou dans le Groupe CA.S.A\nTélétravail (jusqu'à 2 jours de télétravail par semaine)\nDe multiples commodités sur le campus (restaurants d'entreprise, salle de sport, crèche, centre médical, médiathèque...)\nForfait et avantages pratiques « mobilité durable » pour les velotafeurs\nDes équipes aussi diversifiées que structurées dans une dynamique de transformation\nLCL s’engage en faveur de la diversité et nous encourageons tout(e) candidat(e) ayant l’expérience requise à postuler à nos offres. Tous nos postes sont ouverts aux personnes en situation de handicap.\nNous avons encore de nombreuses raisons à vous présenter pour vous convaincre de nous rejoindre mais pour cela, il faudra postuler ici !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL"
            ],
            "BigData": [
                "Hadoop"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Collaboration": [
                "Confluence",
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-confirm%C3%A9-%E2%80%93-paris-france-h-f-at-astek-3839098103?position=5&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=%2Fgv3ywdDRdhT0vOlps7OZw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nParis - France\nPubliée il y a 2 mois\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nNous rejoindre en tant que\nBig Data Engineer Confirmé (H/F),\nafin d’accompagner un opérateur télécoms, Leader en Europe dans l’assistance et le support applicatif de niveau 3 (résolution des problèmes utilisateurs, exploitation des environnements hors production).\nUn challenge portant sur des millions d’utilisateurs dans un environnement technique innovant, stratégique et où l’entraide et la bonne humeur priment !\nVotre Mission, Si Vous L’acceptez :\nSupervision et détection et résolution des problèmes utilisateurs (développeurs, exploitants et data exploreurs)\nDéveloppement de solutions de self-service ou d’une solution de résolutions automatiques des problèmes\nQualifier les données et les résultats\nConception technique des solutions\nAssurer l’accompagnement et le déploiement des évolutions des processus et outils\nAccompagner la phase de mise en production\nVotre Future Équipe :\nVous intégrerez une équipe à la fois technique et fonctionnel, qui œuvre chaque jour pour développer et maintenir en conditions opérationnelles l’ensemble des solutions IT !\nL’équipe est en interaction avec des clients à la fois internes et externes.\nVotre stack de jeu\nSystème d’exploitation : Linux\nOutils des distributions : HDP, HDF, ELK\nEnvironnement Big data : Hadoop, Spark,\nLangage : Scala, Shell, Python\nCloud computing : GCP ou AWS\nBase de données : No SQL (Cassandra, Mongo DB), Shell, Ansible\nDataviz : Power BI ou Kibana\nDes notions en Réseau et Systèmes feront la différence !\nLes Petits Plus Du Projet :\nVous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.\nVous ?\nDiplômé(e) d’une école d’ingénieur ou équivalent de niveau Bac+5.\nVous justifiez idéalement d’une expérience d’au moins 3 ans d’expériences sur un poste similaire ?\nVous faite preuve de proactivité et d’esprit d’équipe, êtes doté(e) d’un excellent sens de l’organisation et vous aimez les challenges et la résolution de problème ?\nAlors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !\nAstek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nNotre projet commun vous plait ?\nPostulez à cette annonce, et soyez transparent !\nMaud, notre Talent Acquisition Referent, vous contactera pour un premier échange.\nPuis vous rencontrerez Martin, votre futur manager, avec lequel vous échangerez autour d’Astek, de votre parcours, de vos attentes et de votre future mission .\nEnfin, vous rencontrerez Jérémy, notre Directeur d’agence avec lequel vous pourrez valider votre intérêt et adéquation pour le poste et finaliser les éléments contractuels.\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Hadoop – Scala – Data\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Hadoop – Scala – Data\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "Cassandra"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP",
                "AWS"
            ],
            "OS": [
                "Linux"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Automation": [
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Logic@l Conseils",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-lille-at-logic%40l-conseils-3811575649?position=6&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=lsnSMaZYOtp54jPBFAW7cQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Dans le cadre du développement de nos activités sur la métropole Lilloise, nous recherchons un\nconsultant data engineer\n(H/F) pour intervenir chez l'un de nos grands comptes clients.\nVos missions :\nRecueillir\nles besoins métiers et des équipes data\nConcevoir et mettre en place les\ntraitements de données\nRéaliser les\ntests de validation\nAssurer\nl’alimentation du dataware\nRéaliser les\nordonnancements des traitements\nEtre garant de la\nmise en place\n, du\nsuivi\net de l’\nexploitation\ndes outils déployés\nAssurer\nune veille technologique\nrégulière\nEnvironnement technique :\nDéveloppement :\nPython, Scala, R, Java,\nFramework :\nSpark,\nHadoop,\nOutils Big data :\nYarn, Pig, Hive, Kafka, Splunk\nBases de données :\nMongoDB, HBase, Cassandra\nETL :\nTalend, Stambia\nPlateforme :\nHortonworks, Cloudera, Map Reduce\n,\nAWS, GCP, Azure\nVotre profil :\nVous disposez d’une expérience\nd’au moins 2 ans en tant que data engineer\nou dans le domaine de l’analyse et du traitement de données.\nVéritable\npassionné de la data\n, vous êtes\nforce de proposition\nsur les solutions techniques à mettre en œuvre. Vous maitrisez l’anglais dans un contexte professionnel.\nCompétences requises :\nAnalyses qualitatives et quantitatives (Intermédiaire)\nAnglais (Intermédiaire)\nArchitecture fonctionnelle SI (Débutant)\nDéveloppement d'ouvrages, produits ou événements (Débutant)\nGestion des contrôles, tests et diagnostics (Débutant)\nGestion des risques (Intermédiaire)\nMaîtrise des logiciels (Intermédiaire)\nMise en exploitation / Production et maintenance (Débutant)\nNos valeurs\nNous avons décidé de renverser la pyramide du management pour placer nos collaborateurs en tête des priorités de l’entreprise.\nEn effet, attaché à des valeurs fortes, telles que la proximité, la sincérité, la fidélité, la confiance et le respect, nous sommes persuadés que la réussite réside dans le bien-être de nos collaborateurs.\nCela se traduit par un accompagnement de proximité, de la transparence sans langue de bois, des échanges réguliers avec les managers référents, un accompagnement dans le développement de carrière qui est construit et jalonné avec les formations et certifications nécessaires et les missions en adéquation, pour mener à bien l’évolution de carrière.\nPour vous convaincre de nous rejoindre, nos avantages salariés complémentaires :\nEnvironnement bienveillant et stimulant au sein de 3 pôles d’expertises\nFormations et Certifications à la demande\nTickets restaurants : 13€ par ticket\nRemboursement à 100 % des abonnements de transports en commun\nMutuelle frais de santé avec de hautes garanties\nPrise en charge à 100% de l’assurance Prévoyance\nChèque Cadeau Culture 120 €\nCompte CSE avec une cagnotte de 390 €\nCompte CE : billetterie, voyages, culture, sorties, à des tarifs préférentiels\nDes évènements chaque mois : activités associatives, sportives, afterwork, séminaire,\nPartenariat Losc (participation aux match dans la loge VIP logical conseils – (Une Vingtaine de match par an)\nPossibilité de télétravail\nEn intégrant Logic@l Conseils, vous participez à une réelle aventure humaine, alors pour postuler, il suffit de cliquer ci-dessous !\nTous nos postes sont ouverts, à compétences égales, aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "MongoDB",
                "Cassandra",
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "SoftBigDataProcessing": [
                "Cassandra",
                "HBase"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "CGI",
        "location": "Niort, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-cgi-3902057928?position=7&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=rAZwnI6BIvOJVViuEWO5Cw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Description de poste\nBig Data, Data Science, Data analyse, Data architecture ... Ça n’a pas de secret pour vous ?\nQue vous commenciez votre carrière professionnelle ou que vous soyez spécialiste de l’une de ces disciplines, intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.\nSi vous souhaitez intégrer nos équipes à Niort et accompagner les plus grands acteurs du secteur des Assurances, cette annonce est susceptible de vous intéresser.\nEn tant que Data Engineer, vous serez responsable de la conception, du développement, de la gestion et de l'intégration des systèmes basés sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop. Ce rôle implique la mise en place d'architectures évolutives et hautement disponibles pour répondre aux besoins de traitement et de stockage de données de l'entreprise.\nFonctions et responsabilités\nVos responsabilités seront les suivantes:\n-Maintenir et développer des solutions basées sur les services AWS pour le stockage, le traitement et l'analyse de données\n-Utiliser les services AWS appropriés tels que Amazon EC2, S3, RDS, Lambda, etc., pour répondre aux exigences du projet.\n-Créer et maintenir les configurations Terraform pour la gestion de l'infrastructure en tant que code (IaC) sur AWS\n-Participer à la maintenance et à la mise en place d'environnements OpenShift pour l'hébergement d'applications et de services\n-Gérer et administrer les clusters Kafka pour garantir la disponibilité, la performance et la sécurité du système de messagerie\nParticiper à l’assistance utilisateurs sur les briques de la plateforme Hadoop Cloudera Data\n-Travailler avec les projets et les devOps pour assurer un traitement efficace des données\nEn rejoignant CGI, vous bénéficiez notamment d’une offre complète de formations (techniques, métiers, développement personnel,…), de flexibilité grâce à notre accord télétravail (jusqu’à 3 jours de télétravail par semaine), d’une politique de congés avantageuse (27 jours de congés payés, RTT, congés ancienneté et enfant malade,…) et d’un package d’avantages intéressant (régime d’achats d’actions, participation, CSE,...).\nQualités requises pour réussir dans ce rôle\nAyant une première expérience en tant que Data Engineer, vous avez une première expérience relative aux points suivants:\n-Développement et intégration sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop\n-Connaissance avancée de l'administration Kafka, y compris la configuration, la gestion et la résolution des problèmes\n-Mise en œuvre de l'infrastructure en tant que code à l'aide de Terraform\n-Bonne compréhension des bonnes pratiques de sécurité pour les systèmes cloud, les clusters Kafka et les plateformes Hadoop\nCGI est un employeur inclusif et attentif aux candidatures des personnes en situation de handicap, à l’évolution de carrières des hommes et des femmes et au bien-être de nos salariés LGBT+. Dans un souci d’accessibilité et de clarté, le point médian n’est pas utilisé dans cette annonce. Tous les termes employés se réfèrent aussi bien au genre féminin que masculin.\nEnsemble, en tant que propriétaires, mettons notre savoir-faire à l’œuvre.\nLa vie chez CGI est ancrée dans l’actionnariat, le travail d’équipe, le respect et un sentiment d’appartenance. Chez nous, vous pourrez exploiter votre plein potentiel parce que…\nNous vous invitons à devenir propriétaire dès le jour 1 alors que nous travaillons ensemble à faire de notre rêve une réalité. C’est pourquoi nous nous désignons comme associés de CGI, plutôt que comme employés. Nous tirons profit des retombées de notre succès collectif et contribuons activement à l’orientation et à la stratégie de notre entreprise.\nVotre travail crée de la valeur. Vous élaborerez des solutions novatrices et développerez des relations durables avec vos collègues et clients, tout en ayant accès à des capacités mondiales pour concrétiser vos idées, saisir de nouvelles opportunités, et bénéficier d’une expertise sectorielle et technologique de pointe.\nVous ferez évoluer votre carrière en vous joignant à une entreprise bâtie pour croître et durer. Vous serez soutenus par des leaders qui ont votre santé et bien-être à cœur et qui vous permettront de saisir des occasions afin de parfaire vos compétences et élargir les horizons.\nJoignez-vous à nous, l’une des plus importantes entreprises de conseil en technologie de l’information (TI) et en management au monde.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "1",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "BigData": [
                "Hadoop"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "OpenShift"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ternair",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ternair-3915757963?position=8&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=fnWL1RHsXOUn1571MFK17A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "👨‍🚀 MISSION : 👩‍🚀\nEn cohérence avec la stratégie d’entreprise et la roadmap data, vous aurez pour principales missions de :\nEn lien avec l’équipe DevOps, construire, maintenir et faire évoluer la plateforme de données;\nDéfinir et piloter la cohérence de la collecte, la gestion et l’alimentation des données internes et externes, en différents modes : batch, streaming, API (architecture micro-services);\nPréparer et mettre en qualité les données pour les rendre disponibles dans les différents environnements de travail (datalake, datawarehouse, datamart);\nVérifier la qualité des données, de leur bonne et régulière exécution ainsi que de leur utilisation adéquate (gestion des coûts);\nTravailler en étroite collaboration avec les data analysts, scientists et data stewards et business de l’entreprise ;\nEn lien avec l’IT et la sécurité, veiller aux règles d'intégrité et de sécurité des données;\nVeille technologique.\n🧮 Les outils :\nPlateforme data : Google Cloud Platform (Big Query, Airflow)\nDéveloppement : Github/GitLab, Docker, Terraform, Python\nAnalytiques : Qlik\nGestion de projet s: Jira, Confluence, Miro, Drive, Docs, Sheets, Slides\n🤩 Profil recherché : 🤩\nExpérience d'au moins 4-5 ans (après études) en data ingénierie (flux, modélisation, run)\nA l’aise avec l’environnement Cloud et les infrastructures digitales\nCommuniquant, pédagogue et fortes capacités relationnelles\nAnglais (à l’écrit)\nRémunération : 42-60 k€ en package selon expérience\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker"
            ],
            "Collaboration": [
                "Confluence",
                "JIRA"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LVMH",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=9&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=IQD19zGtUnDkMOS73%2BJYWQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.\nWe are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.\nIf you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,\nIf you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,\nIf you like building tech solutions having direct impacts on billion-dollar-valued businesses,\nIf you have good communication skills and like sharing your knowledge,\nApply now, and join us!\nThe mission\nThe Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.\nOur team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.\nYou will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).\nMain responsibilities\nYou will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).\nYou will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.\nYou will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.\nApplying the quality and security standards. Making them evolve if necessary.\nProducing realistic, understandable and documented solutions following the group guidelines.\nSharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.\nTaking responsibility as member of the team on the product performances (delivery and long term usage)\nRequired expertise and knowledge\nAbility to build technical solutions answering concrete usage (User Stories) and communicate them to the team.\nDimension and evaluate complexity for technical solution productions.\nExtensive knowledge and experience with good learning and sharing abilities.\nEvaluate quickly risks and opportunities about technical choices.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProblem-solving skills on Data and AI, coding and software development\nTech lover\nFeedback taker and giver\nTeam player\nKey benefits to join our team\nAttractive packages\nOffices in the 8th arrondissement near the Champs Elysées\nFlexibility on the working hours\nRemote work possible (~40%)\n7 weeks of holidays (congés payés + RTT)\nLVMH brands exclusive private sales\nGreat employee committee and health insurance (CE, mutuelle)\nLast generation MacBooks\nPart of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!\nYou’re eligible if\nYou have a strong experience (3+ years) in cloud data architecting or consultancy.\nYou graduated from an engineering (or equivalent) with a master’s degree. Computer Science knowledge is mandatory.\nExperience on data stacks and/or Google Cloud (built in components) is a huge plus.\nFrench and English both written and oral (Maisons are all over the world)\nYou’re thrilled to support the #1 luxury group to get even better.\nHiring Process\nCall with our HR partner dedicated to the Tech Team\nTechnical interview with the Solution Engineering Manager\nTechnical test\nInterview with the Head of Engineering\nStill here? Apply now!!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "40",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Flexibility",
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-%E2%80%93-secteur-t%C3%A9l%C3%A9com-%E2%80%93-paris-france-h-f-at-astek-3832149765?position=10&pageNum=5&refId=63c0Bj1atjsufX74pWwghg%3D%3D&trackingId=%2Fgwdg%2BYdGKwK5zWpSPfDOg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nParis - France\nPubliée il y a 2 semaines\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nNous rejoindre en tant que\nBig Data Engineer (H/F),\nafin d’accompagner un opérateur télécoms, Leader en Europe dans la gestion de ses portails clients grands publics (multimarques mobile et internet).\nUn challenge portant sur des millions d’utilisateurs dans un environnement technique innovant et stratégique.\nVotre Mission, Si Vous L’acceptez :\nQualifier les données et les résultats\nConception technique des solutions\nDécliner les impacts de la stratégie et des innovations technologiques au sein des processus et outils de l’exploitant SI\nAssurer l’accompagnement et le déploiement des évolutions des processus et outils\nContribuer aux programmes de transformation DevOps, Cloud et catalogues des offres SI\nDévelopper des fonctions transverses et les « uses cases »\nAccompagner la phase de mise en production\nVotre Future Équipe :\nAu sein d’un environnement riche et complexe, vous évoluerez avec des experts passionné(e)s à la fois techniques et fonctionnels (Ingénieurs spécialisées, chef de projet, scrum master, product owner, analystes…).\nL’équipe est en interaction avec des clients à la fois internes et externes.\nVotre stack de jeu\nSystème d’exploitation Linux\nBig data (Hadoop, Spark, Scala)\nCloud computing (GCP…)\nS QL, No SQL (Cassandra, Mongo DB)\nDataviz : Power BI ou Kibana\nDes notions en développement feront la différence !\nLes Petits Plus Du Projet :\nVous évoluerez au sein d’une équipe impliquée et réactive et interviendrez sur un projet polyvalent et à forte valeur ajoutée.\nVous ?\nDiplômé(e) d’une\nécole d’ingénieur\nou équivalent de niveau Bac+5. Vous justifiez idéalement d’une expérience d’au moins 2 ans sur un poste similaire.\nCe descriptif vous interpelle ?\nAlors ce poste est fait pour vous, n’hésitez plus et rejoignez l’aventure ASTEK !\nAstek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nVous vous êtes reconnu(e) sur l’annonce et Astek vous plaît !\nJulie , Talent Acquisition Officer vous contactera pour en savoir plus sur vous.\nPar La Suite, 2 Échanges Maximum :\nLe premier avec Mathieu (votre futur N+1, avec lequel vous échangerez autour d’ASTEK, de votre parcours, de vos attentes et de la mission)\nLe second avec Anthime (Notre Directeur d’agence pour valider votre intérêt pour le poste et vous présenter les éléments contractuels).\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nBienvenue dans la team ! Allez-y, maintenant c’est à vous de jouer !\nMots-clés :\ningénieur – ingénieure – consultant – consultante – big data engineer\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – big data engineer\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala"
            ],
            "DataBase": [
                "SQL",
                "Cassandra"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "OS": [
                "Linux"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Automation": [
                "Chef"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Boulogne-Billancourt, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-s%C3%A9curit%C3%A9-h-f-at-akademija-oxford-3917866657?position=1&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=Mk9nataTJV4b3v8yeh%2BlWQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Une entreprise du secteur de la protection de l’environnement et basée dans les Hauts-De-Seine recherche un.e Data Engineer Sécurité dans le cadre d’un contrat d’apprentissage.\nAu sein de son Data Service, votre rôle sera d’établir la stratégie des architectures data sur les aspects sécurité en lien avec la stratégie globale métier et contribuer à la déclinaison des principes du modèle de sécurité globale.\nVous devrez également élaborer des modèles de référence pour les architectures data, mais aussi contribuer à la déclinaison des politiques de sécurité en standards de sécurité opérationnels.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {}
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-mindpal-3919800161?position=2&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=l1UUPOQsgk8%2BcVvdL4FkwA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for\nData Engineer!\nResponsibilities\nDesigning, creating, and maintaining data processing systems\nAnalyzing and optimizing data processing workflows\nCollaborating with the team to ensure data quality and efficiency\nTesting and implementing new solutions\nRequirements\nAt least 2 years of experience in designing and creating data processing systems\nProficiency in tools and programming languages related to data engineering (e.g. Hadoop, Spark, Scala, Python)\nExcellent knowledge of databases and SQL language\nAbility to work in a team and communicate effectively with other departments\nCommunicative English skills\nExperience with AWS/AWS Glue is a plus\nWe Offer\nB2B contract\nFull-time job\nRemote work and flexible hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "AWS"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Web Transition",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=3&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=WhWEfSHP6nlUfjNFUtD%2BOw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Web Transition, c’est qui ?\nFondée en 2011,\nWeb transition\nest une entreprise de services numériques opérant sur le marché de l’IT/Digital !\nConstituant une part essentielle de\nMoOngy Digital Lab\n, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et également en Data !\nNotre objectif : nous implanter comme un acteur principal sur le marché de la Transformation Digitale en accompagnant et valorisant les compétences de nos collaborateurs !\nNous sommes convaincus que le succès de MoOngy Digital Lab réside dans la somme des potentiels de nos équipes 🤝\nTon équipe : La tribu Data\nParce qu’il est indispensable que tu puisses partager tes connaissances mais aussi en acquérir de nouvelles, tu feras partie de l’une de nos tribus : celle de la Data. De plus, cela te permettra d’être acteur dans le développement et la stratégie de Web Transition. Ce système de co-réflexion et co-construction est un fondement essentiel chez nous !\nDans cette aventure, tu :\nT’assures\nde la maîtrise de la donnée et est garant de la qualité de son utilisation (référencement, normalisation, et qualification)\nTravailles\nà la compréhension et l'intégration des données en provenance des différents formats\ndes interfaces de flux\négalement à la définition de la politique de la donnée et à la structuration de son cycle de vie dans le respect des réglementations en vigueur\nla supervision et l'intégration des données de diverse nature qui proviennent de ces sources multiples et vérifie la qualité des données qui entrent dans le Data Lake\nGarantis\nl'accès qualitatif aux sources de données\nFacilites\nl’accès aux données pour tes collègues (data scientists, data analysts…)\nAssistes\nles autres équipes dans l'accès et la compréhension des données des socles.\nRejoins-nous si tu as :\nExpérience d’au-moins 4 ans dans la Data\nAppétence à la qualité des données.\nConnaissance familière des Datawarehouses.\nMaitrise de Python, Oracle SQL, GCP/Power BI\nAisance avec les indicateurs, tu as une bonne capacité d'analyse et de rédaction.\nTon savoir-être :\nOuvert d’esprit\nRigoureux\nAutonome\nRespectueux des différences de chacun\nCurieux\nProactif\nAgile\nPar où on commence ?\nUn premier entretien RH d’1h pour comprendre ton parcours et tes aspirations\nUn second entretien de 45 minutes avec l’un de nos Business Manager afin de valider tes compétences et qu’il se projette sur l’une des missions qu’il pourrait te proposer\nUn troisième entretien de quelques minutes avec notre responsable d’agence pour te proposer d’intégrer notre superbe Team Web !\n3 entretiens en peu de temps, si ton profil correspond tu intègreras très vite nos équipes 😉\nPrêt pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant à cette offre ! Voici les avantages qui t’attendent en tant que Weber :\n🤩 Des collègues incroyables\n🏆 Certifiée Great Place to Work\n🎮 Des bureaux sympas (où vous serez toujours les bienvenus)\n🎉 Des teambuilding et évents tous les mois\n💻 Des tributs métiers pour échanger entre Weber du même métier\nDes missions chez le client qui sont accompagnées et coachées par ton manager\nUn accompagnement dans ton plan de carrière et tes envies de re skilling\n🤓 Un catalogue de formations certifiantes ouvert à tous les salariés\n🍽️ Une carte tickets restaurant MyEdenred\n❤️ Une mutuelle GrasSavoye\n🚎 Une prise en charge des frais de transport à 100%\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "Oracle SQL"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Chantelle",
        "location": "Cachan, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-ops-h-f-at-chantelle-3909858815?position=4&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=L4rwOAa52tSFHDVPvaDZeg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La Direction des Systèmes d'Information et du Digital du groupe Chantelle recherche son/sa futur.e Data Engineer / Ops H/F, pour le lancement du grand chantier de rénovation de l'architecture Data : la bascule de l'intégralité de son Data Warehouse vers Google Big Query.\nVos Missions Sont Les Suivantes\nVous concevez et mettez en œuvre une infrastructure autour de Google Cloud Platform permettant de collecter, transformer, charger et historiser les données générées par l'entreprise.\nVous travaillez en étroite proximité avec le lead data engineer de l'équipe, l'équipe\nintégration en charge du développement des interfaces, avec notre équipe de Data Analysts qui sont en charge d'exposer cette donnée au reste de l'entreprise ainsi qu'avec l'équipe technique en charge des infrastructures transverses.\nVous collaborez avec l'ensemble des domaines fonctionnels de la DSI (MasterData, Supply Chain, Manufacturing, B2B, et B2C, Finance ...) dans le cadre des projets menés par le Groupe.\nVous apportez l'assertivité technique sur tous les sujets d'architecture data, et êtes force de proposition, par exemple choix de mise en place de pipeline temps réels ou au contraire de flux de données en mode batch, ou bien encore stockage sur Big Query / Big Table en fonction des cas d'usage.\nVous définissez ces éléments structurants, en justifiant vos choix, et les\nmettez en œuvre.\nLes enjeux sont forts et les use cases nombreux à l'échelle du groupe : amélioration du pilotage de nos stocks en dimensionnant mieux nos quantités à produire et nos assortiments, par magasin, meilleure adéquation des achats matières premières vs objectifs de stocks, produits finis, personnalisation de nos sites e-commerce en temps réel en fonction de nos profils client, refonte de nos flux / Apisation, ...\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Goaheadspace",
        "location": "Pantin, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-x-at-goaheadspace-3915359112?position=5&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=l%2FLvWhMpcc6a1c7uxwyTjQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "MFG Labs est une société de conseil et réalisation experte en data, qui aide les entreprises à améliorer leurs prises de décision, à automatiser leurs processus et à créer de nouveaux services grâce à la data science, au design et à l'utilisation des dernières technologies.\nMFG Labs intervient à toutes les étapes de votre transformation data : de la création d'une feuille de route de projets data, à la découverte d'insights, à la modélisation de problématiques complexes, de la création d'un modèle prédictif à l'implémentation technique d'une solution data sur-mesure\nMFG Labs accompagne ses clients de différentes manières :\nStratégie\nSolutions\nFondations\nMFG Labs déploie une approche holistique pluridisciplinaire, en mêlant des data scientists, des designers, des data engineers et des consultants, afin d'apporter des solutions complètes de bout en bout à des problématiques complexes.\nDans le cadre du développement de l’équipe, nous recherchons un.e Data Engineer à\nPantin (magasins généraux).\nAu sein de l’équipe Data Technology, vous aurez pour mission de travailler sur des problématiques de collecte de la donnée sur tout type de support digital : web, mobile, application, voire IoT.\nVotre rôle au sein de l’équipe :\nFaire partie d’une équipe pluridisciplinaire avec des talents en Design de Service, Consulting et Data science.\nDévelopper des applications de production intégrant différents outils : des Mathématiques Appliqués, Machine (Deep) Learning, Recherche Opérationnelle, Statistiques.\nDévelopper des pipelines de traitement de données avec l’équipe de Data Science pour : ingérer, transformer et délivrer des données et mod��les à nos applications.\nDéployer des applications utilisant les derniers outils mis à disposition par les différents Clouds publics.\nÀ propos de vous :\nVous êtes titulaire d'un niveau Bac +4/Bac +5 d'une école d'ingénieur\nVous avez au minimum deux ans d'expérience hors stage ou alternance\nVous êtes rigoureux·se vis-à-vis de vous-même et des autres quant à la qualité du code.\nVous avez quelques connaissances et compétences solides en développement et en en Data Ingénierie au sens large.\nEn\ndéveloppement\nPython 3 et SQL\nFramework de traitement de données (Spark ou équivalent)\nDocker\nGIT\nEn +\nFramework permettant de déployer des APIs (Flask ou équivalent)\nCI/CD\nLa pratique d'au moins un cloud (AWS, GCP ou Azure) est appréciée\nEn Data Ingénierie\nDatawarehouse ou Datalake\nData Pipelines Batch et/ou Straming\nEn +\nOutils de BI (Tableau, Power BI…)\nOutils MLOps (Sagemaker, VertexAI, etc.)\nSi vous vous reconnaissez dans cette annonce, n'hésitez pas à postuler !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "Containers": [
                "Docker"
            ],
            "Other": [
                "Statistiques",
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ALTEN",
        "location": "Antibes, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-big-data-at-alten-3896177673?position=6&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=y%2FMsM8w7nqJAVlPH8FLpHg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Alten is one of the 3 main consulting societies in France. It is present in more than 30 countries in the word. Sophia-Antipolis is the first and largest technopole in Europe, located in the South of France between Nice and Cannes, near Antibes. It’s also called the European Silicon Valley.\nReporting to our consulting team, you will work in an IT environment as an IT Business Analyst, taking part in projects in an Agile environment.\nJob Description\nThe mission consists in taking part toprojectsthat are collecting, processing, and converting raw data into information (Flat Data + KPIs) that can be interpreted by data/business analysts.\nTheData accessibilityis the ultimate goal of this mission, by enabling the Digital data users to utilize data for their business decisions.\nThis role is more Pipeline-centric, for which you need in-depth knowledge of distributed systems and computer science.\nThe mission scope will encompass the design and delivery of some key components of the Digital Data platform(Spark environment, Scala language) including following activities:\nParticipate to specifications reviews, propose technical solutions and perform feasibility studies.\nAcquire datasets that align with business needs.\nDevelop algorithms to transform data into useful, actionable information.\nDevelop, construct, test, and maintain optimal data pipeline architectures.\nCreate new data validation methods and data analysis tools.\nEnsure compliance with data governance and security policies.\nIdentify ways to improve data reliability, efficiency, and quality.\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nPrepare data for predictive and prescriptive modeling.\nWork with data and analytics experts to strive for greater functionality in our data systems; which requires a closed collaboration with the other trains.\nDevelop software according to Amadeus Standards, including documentation\nPerform code reviews in line with Amadeus quality standards.\nConduct unit, package and performance tests of the software and ensure a level of quality in line with the Amadeus guidelines.\nParticipate in the validation / acceptance phase of the product cycle ensuring the fine-tuning necessary to finalize the product.\nProduce software documentation necessary for the application and issue it to the requesting departments.\nSupport the end user in the Production phase by debugging existing software solutions in response to Problem Tracking Records (PTR) and Change Requests (CR) issued from Product Management or Product Definition.\nAs part of the team, the consultant will, as well, work as DevOps, releasing the software load to production, monitoring jobs and being involved in maintenance activities. Quality analyst activities are also handled by the Devs.\nOur current data platform is a MapR architecture, and we are in the process to migrate the whole platform to the Azure cloud., which is one of the key focuses for the team in the upcoming months. This will trigger easier adoption of data and foster collaboration within Amadeus around data.\nQualifications\nTechnical skills:\nPrevious experience as a data engineer or in a similar role\nExperience building or optimizing “big data” data pipelines, architectures and data sets.\nHands-on experience with Scala (>2 years or a strong experience with Java/C++ with a good knowledge level of Scala)\nExperience with big data tool: Spark, Kafka, MapR , Hadoop\nUnderstanding extract, transform, and load ETL systems\nKnowledge of cloud services: MS Azure\nSoft skills:\nAgile Mindset: must be comfortable working with Agile values and artifacts\nFast learning: must be able to adapt quickly to the existing environment and new changes\nAnalytical thinking and problem-solving mindset: must be able to quickly identify, implement work-around to solve incidents and work on long term data solutions\nTeam spirit, knowledge sharing, Empathy: must be able to work in a team and to communicate clearly with the other team members and users\nPro-activity, Professionalism, Opennessand Innovative mindset\nVarious:\nEnglish: professional level\nKnowledge of Scrum framework and Agile methodologies.\nKnowledge of airline business is a plus\nAdditional Information\nALTEN places the career development of the Engineers at the heart of its model and allows you to quickly take on responsibilities and evolve in line with your professional objective. It's the promise of growing your skills on concrete subjects in a project team, with a permanent contract as an ALTEN consultant!\nDo you recognize yourself in this description? Then send us your CV.\nOur teams will be delighted to study your application and meet you!\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "2",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Empathy",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Wewyse",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-wewyse-3912830682?position=7&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=NaOlOQcNmnyGl8uU35Ujaw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Cette offre d’emploi est fournie par Pôle emploi\nDescription\nWewyse est un cabinet de conseil spécialisé en Data et en Intelligence Artificielle. C'est aussi et surtout une communauté de passionnés partageant l'ambition de grandir ensemble et d'ouvrir le champ des possibles dans leurs domaines. Si vous pensez que la Data et l'Intelligence Artificielle ont beaucoup à offrir au monde de demain, et si vous souhaitez apporter votre contribution à ce monde, avec humilité et enthousiasme, alors vous êtes un Wyser en puissance. Être Data Engineer chez Wewyse c'est : Intégrer une communauté d'experts Data passionnés. Recevoir et partager de la connaissance et des savoirs-faire lors de nombreux évènements. Intervenir chez des clients pour y porter l'expertise Wewyse dans des contextes et des secteurs variés. Participer à des projets innovants au sein de notre Datalab, avec des Wysers mais aussi avec des partenaires académiques et des start up. Viser l'excellence des développement en s'appuyant sur le Software craftsmanship Concevoir des architectures logicielles modernes. Penser DevOps pour l'automatisation des déploiements et la continuité des services. Être encouragé, conseillé et accompagné dans un parcours de formation adapté à vos ambitions professionnelles. Faire partie de la famille Wemanity avec ses évènements et ses multiples opportunités de carrière. Ce que nous aimons : Les personnalités ouvertes, curieuses, ambitieuses Les langages Scala, Python et Java Le cloud : AWS, GCP, Azure Les écosystèmes : Hadoop et Spark La conteneurisation : Docker et Kubernetes Les méthodes Agiles Le SQL et le NoSQL . L'approche DevOps : Jenkins, Ansible et Terraform Le versionning : Git L'anglais Vous vous reconnaissez ? Alors n'hésitez pas à postuler !\nPROFIL SOUHAITÉ\nExpérience\nDébutant accepté\nSource: Pole emploi (https://www.pole-emploi.fr)\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker",
                "Jenkins"
            ],
            "Automation": [
                "Kubernetes",
                "Ansible"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Dalkia",
        "location": "Angers, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-dalkia-3907349741?position=8&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=z4UpMnjSuKlynOh279nU7Q%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Descriptif du poste\nEt si vous faisiez équipe avec nous ? Rejoindre Dalkia, c'est plus de sens et d'implication contre le réchauffement climatique ; plus de relations humaines, avec un métier de service animé par l'esprit d'équipe ; plus de technicité, avec des projets ambitieux et innovants fondés sur nos expertises ; plus d'employabilité, avec des parcours diversifiés et individualisés. Rejoindre Dalkia, c'est rejoindre plus qu'une entreprise : un collectif de 20 000 collaborateurs engagés en faveur de la transition énergétique.\nDalkia Froid Solutions, acteur majeur de la réfrigération, spécialisé dans les services énergétiques pour les process industriels et tertiaires, recherche un(e) Data Engineer\n(H/F)\n. Rattaché (e) au Responsable Data au sein de la Direction des Systèmes D'Informations, vous êtes le garant du bon déroulement des développements de flux de données et de leur préparation pour leur analyse. Vous aurez l'opportunité de rejoindre une équipe en construction.\nCandidater chez Dalkia Froid Solutions, c’est avoir l’envie d’intégrer un grand groupe à l’esprit familial. L’humain est au cœur de nos métiers, nous donnons la chance à tous, afin de découvrir nos talents de demain. Venez renforcer notre Direction des Systèmes d'Informations et contribuez à l'optimisation énergétique à travers la data !\nVos Principales Missions\nDéfinir l'architecture ETL et développer les jobs d'intégration de données pour notre environnement Big Data.\nAssurer le monitoring quotidien des jobs et optimiser les performances de traitement.\nGarantir la qualité et l'intégrité des données en industrialisant leur nettoyage.\nAdapter les DataMarts pour le reporting en collaboration avec les équipes métiers : comprendre et analyser les besoins utilisateurs, et rédiger les spécifications fonctionnelles et techniques.\nVous serez également ammené à collaborer avec l'équipe Infrastructure pour définir les besoins techniques et planifier les investissements. En lien avec votre équipe vous conduirez des projets variés et participerez à la mise en oeuvre de rapports BI et de modèles de machine learning.\nLieu :\nSiège Social - Angers / Télétravail possible à raison de 2 jours par semaine après période d'essaie\nVotre profil\nDiplômé (e) d'un bac + 5 minimum spécialisé en Data Engineer ,vous avez de bonnes qualités relationnelles afin d'accompagner le déploiement des projets. Votre rigueur et votre logique sont incontestables. Vous aimez travailler en équipe pour accompagner l'entreprise vers l'excellence opérationnelle.\nCôté Outils ? Vous maitrisez les langages SQL et Pyhton et vous avez déjà pratiqué les outils DBT et GitLab. Une première expérience avec un outil de BI/Datavisualisation est souhaitée.\nLa connaissance des outils Qlik Sense ou Talend serait un plus!\nPrêt(e) à faire une différence avec nous ? Postulez dès maintenant !\nEnsemble, nous contribuons collectivement à la transition énergétique. C'est pourquoi chez Dalkia Froid Solutions, nous sommes convaincus que chacun peut participer à relever ce défi. De ce fait, chaque candidature recevra la même attention.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "Other": [
                "Machine Learning",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ramify",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-ramify-3896146641?position=9&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=yafYlDeSmYQN1tAFxdDJVA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ABOUT\nRamify’s mission is to help people achieve financial freedom, no matter who they are and how much they have. We are revolutionizing the private wealth management industry by making smart and affordable financial products for everyone.\nNo more standardized solutions, hidden fees and complicated words, Ramify enables everyone to become a smart investor. The team combines elements of its research with technology to design customized investment portfolios composed of world-class financial products.\nThe team comprises around 15 talented individuals. Ramify is looking for talented people in all sectors, who want to have a huge impact, move fast and deliver.\nJOB DESCRIPTION\nThe Quantitative Investment Solutions (QIS) Team is dedicated to designing innovative investment portfolio models and developing cutting-edge investment features within our product suite. Moreover, the QIS Team is at the forefront of driving AI-based solutions for Ramify. This involves conceptualizing and implementing transformative AI solutions tailored to meet the diverse needs of various teams within Ramify.\nAs a Data Engineer on our QIS Team, you will play a pivotal role in shaping the future of investment strategies through data-driven insights and AI-based solutions. Collaborating closely with our talented team of quantitative researchers, and investment experts, your responsibilities will encompass architecting and implementing robust data pipelines. These pipelines will facilitate the seamless integration of diverse data sources, empowering Ramify teams to make informed decision-making.\nKey Responsibilities:\nDesign, build and launch data pipelines at scale to move data across Ramify platform with SQL technologies.\nDesign and implement processes and tools for data onboarding and quality, helping to deliver an industry best-practice solution for managing the data lifecycle.\nProduce stand-alone tools that can be used by other teams to automate data quality and discover faults.\nBuild analytical tools that provide insight into business metrics across Ramify.\nArchitect and lead the implementation of AI based solutions within Ramify\nPREFERRED EXPERIENCE\nWe're looking for people who:\nWant to make a difference. We are a small team effectively reshaping how people look at the industry. We need people who 'get it' and want to play an integral part in helping us accomplish this mission and are persistent in getting the job done.\nSkills we're looking for:\nMaster’s or upper-year undergraduate-level coursework in either Computer Science, Management Information Systems, Business Information Systems, Mathematics or Finance related field.\n2+ years of professional experience in data engineering.\n2+ years of experience with one or more coding languages such as Python (is a must), Java.\nExperience with data modeling and ETL design, implementation and maintenance.\nDemonstrable mastery of industry best practices in the data lifecycle, including data quality automation and tooling.\nExcellent written and verbal communication skills with ability to communicate complex designs and solutions to non-technical and highly technical audiences alike.\nGood attention to detail.\nStrong analytics and strategic thinking skills\nNice-to-haves :\nUnderstanding of ML/ Generative AI technologies and their applications.\nPossess a passion, curiosity, and energy for finance + investing. You understand the ins and outs of the wealth management, trading, and more importantly - know how to explain these concepts simply\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Beelix",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-beelix-3865239426?position=10&pageNum=7&refId=c6HmMlA6dBcVTsy5H43rIw%3D%3D&trackingId=pX8nmXsW8q1tbsDo0F4T0A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants:\n🚗Automobile\n⚡Energie\n📡Médias & Télécoms\n👗Luxe & Retail\n💶 Banque, Finance & Assurance\n✈️Défense\nAujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.\nDans le cadre de notre développement, nous recherchons un Data Engineer en Île-de-France.\nQuelles missions au quotidien ?\nVous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :\nPasser de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le Datalake\nConsolider ces données au fur et à mesure de leur alimentation récurrente dans le Datalake\nLes exploiter pour atteindre la finalité business (exposition de Business View, réintégration des résultats dans le SI, service de scoring, …)\nDe mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)\nAccompagner les Data Engineers sur son périmètre pour garantir la qualité des livrables\nExpertise souhaitée\nExpertise en SPARK et PySpark\nExpertise sur Databricks\nUne expérience sur un cloud provider public comme Azure (idéalement), AWS, ou GCP\nConnaissances avancées d'outils de BI comme PowerBI (idéalement) ou Spotfire\nCapacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier\nEtre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de données relationnelles et NoSQL\nUne expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nA propos de vous ?\nDiplômé d'une école d'ingénieurs ou équivalent\nAu moins 3 ans d'expérience en tant que Data Engineer\nExpérience en mode de Delivery Agile (Scrum, Kanban, etc.…)\nVous avez un bon niveau d’anglais tant à l’écrit qu’à l’oral\nPourquoi nous rejoindre ?\nUn suivi et un accompagnement au quotidien\nUn organisme de formation certifié Qualiopi, un abonnement linkedin learning pour chaque salarié et des partenariats avec des spécialistes pour d’autres expertises\nDe nombreux événements : Afterworks, Communautés métiers, Happy talks…\nune Expérience personnalisée basée sur vos besoins grâce au Prédictive Index\nNotre package « unBeelievable » : 100% du titre de transport, Tickets restaurants, CSE, Prime de participation ...\nNombreux évènements (afterworks, sport, etc) et des communautés métiers dynamiques\nLe processus de recrutement ?\nÉchange téléphonique (15 min)\nEntretien 1 RH pour apprendre à vous connaître\nEntretien 2 avec votre futur N+1 pour appréhender la relation managériale\nEntretien 3 avec un Responsable commercial pour avoir la vision stratégique\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "100",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "FRG Technology Consulting",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-data-ops-at-frg-technology-consulting-3913842168?position=1&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=G1%2BA824Nb12%2FuKAKVjCsiA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous êtes un expert passionné par la Data et à la recherche de défis excitants ? Mon client recherche actuellement un\nData Engineer\n/ Data Ops\ntalentueux pour rejoindre une équipe dynamique et humaine.\nMissions principales :\nParticipation active au déploiement de la nouvelle plateforme sur Azure & Snowflake\nForte autonomie et gestion complète des projets data\nAnalyse des besoins actuels et futurs\nCréation de spécifications fonctionnelles et techniques\nModélisation de données\nDéveloppement de packages SSIS\nIntégration des données dans SnowFlake & Azure,\nCréation de rapports avec Power BI et Excel\nProfil recherché :\n3 à 4 ans d'expérience\nminimum\ndans la BI (SSIS, SQLServer, SSAS, SSRS) et/ou le cloud (Azure , Snowflake) ainsi qu'en SQL\nCompétences en\narchitecture sur Snowflake\nfortement appréciées\n1 à 2 ans d'expérience en tant que DevOps ( CI/CD ; GitLab)\nAutonome, rigoureux et anglais courant\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "3",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud",
                "DevOps",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ADVANCED Schema",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-advanced-schema-3539059697?position=2&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=xYdxyEx6HBpgfQXeuvfj1g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ADVANCED SCHEMA\nest une société de services informatiques\nspécialisée dans la donnée.\nDepuis 20 ans, nous créons des plateformes data sur mesure pour nos clients, orientées usages et alliant qualité, performance, sécurité et gouvernance.\nADVANCED SCHEMA\na développé de nouvelles activités pour réaliser l'ambition du groupe : devenir\nune entreprise end-to-end,\nen proposant une offre à 360° à nos clients pour les\naccompagner à chaque étape de leurs projets.\nÀ ce jour, nous sommes près de 220 passionnés répartis entre Paris, Lille, Nantes, Lyon mettant à profit leur expertises aussi bien dans le domaine du retail, de la finance/assurance, du luxe, des médias, de la santé et de l'industrie.\nAujourd’hui, nous souhaitons intégrer de nouveaux renforts dans nos équipes Lilloises.\nEn tant que Data Engineer, vous aurez les missions suivantes :\nConcevoir des modélisations physiques\nConstruire des mappings techniques et rédaction de spécifications d’alimentation.\nDévelopper des flux des données\nContribuer au pilotage de projets, de proof of concepts\nParticiper à des missions d’expertise\nCompétences professionnelles & niveau d'études requis :\nVous êtes titulaire d'un diplôme Bac +3 minimum dans le domaine de la data\nVous possédez minimum 2 ans d'expérience dans le métier\nPositif(ve), curieux(se), rigoureux(se) et doté(e) d'une bonne aisance relationnelle\nÊtre enthousiaste à l'idée d'apprendre de nouvelles technologies\nExpérience de la méthodologie Agile / Scrum\nCapacité à planifier et à prioriser les tâches et les activités confiées en autonomie\nMaîtrise de l’anglais oral et technique obligatoire\nExpérience avérée dans l'écriture de code propre avec 2 ou plusieurs des technologies suivantes : BASH, SQL, Java, Python, NoSQL\nNotre proposition :\nTemps plein en\nCDI\navec un\nsalaire attractif\n+ participation aux bénéfices + prime(s) sur investissement personnel\nMode de\ntravail hybride\n(agence, site, télétravail selon projets/clients)\nTicket restaurant (Sodexo)\nMutuelle financée à 50%\nPrévoyance\nComité entreprise\n5 jours d’onboarding plein temps via la\nADVANCED SCHEMA Academy\nNotre investissement :\nChez\nADVANCED SCHEMA\n, nous t’offrons un environnement de travail stimulant et collaboratif ainsi que des possibilités de croissance et de développement professionnel. Également un\naccompagnement/support au quotidien\npour te faire grandir et monter en compétences, sur des projets qui répondent à de\nvrais enjeux pour nos clients\n. Si tu es passionné(e) par les données et prêt(e) à relever de nouveaux défis, alors nous aussi nous aimerions te rencontrer\nProcess de recrutement :\nSi ta candidature retient notre attention, nous te proposons :\nUn premier échange téléphonique/visio\nUn entretien physique (+questionnaire d’évaluation) avec un senior manager\nUn entretien final à notre siège Parisien afin de rencontrer le DG\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Temps plein",
                "Senior"
            ],
            "TypeContract": "CDI",
            "Salary": "50",
            "Level": "",
            "Experience": "20 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Bash",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Mirakl",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-engineer-at-mirakl-3904071960?position=3&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=Q7xsnmeByL9zBjTSiMtetQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Mirakl, leader et pionnier de l’économie de plateforme, propose aux entreprises une suite unique de solutions leur permettant de transformer significativement leur e-commerce afin d'accélérer de façon durable et rentable leur croissance. Depuis 2012, Mirakl accompagne les entreprises B2C et B2B avec la technologie la plus avancée, sécurisée et évolutive leur permettant de digitaliser leur activité et d'élargir leur offre via la marketplace ou le dropship, faciliter la gestion des catalogues et des paiements de leurs fournisseurs pour plus d'efficacité, offrir une expérience d'achat personnalisée à leurs clients, et augmenter leurs profits grâce au retail media. Basée à Paris et Boston, Mirakl est certifiée Great Place to Work.\nA propos de Mirakl Labs\nNos équipes techniques et produits, nommées Mirakl Labs, sont principalement réparties entre nos 2 hubs situés à Paris et à Bordeaux. Elles collaborent au quotidien afin d'adresser les problématiques de nos clients et utilisateurs en répondant à différents challenges liés aux nouvelles fonctionnalités, à la scalabilité, la sécurité et l’ergonomie…\nElles opèrent en mode agile et s'organisent en Squads composées d'un Squad Lead, de 5 développeurs, d'un Product Manager et d'un QA. Chaque Squad est spécialisée sur un scope fonctionnel afin de concevoir et réaliser de nouvelles features, leurs évolutions et des APIs (avec un découpage en micro-services). Nos équipes Infrastructure, Architecture, Sécurité, Documentation, Product Design, Data et Support opèrent en transverse en apportant leur expertise et de la cohérence sur l’ensemble des produits.\nToutes les équipes sont responsables de leur périmètre et chacun des collaborateurs apporte son expérience et ses idées. Innovation, feedback et implication dans les prises de décision sont au cœur de notre philosophie.\nEt pour favoriser ce partage avec d’autres passionnés, nous sommes sponsors, speakers, et hôtes de différents événements, meetups, et associations de la scène Tech en France. Au cours des dernières années, nous avons participé à des événements tels que Devoxx, ReactEurope, ProductConf et Flupa UX Days.\nA propos du job\nLa solution SaaS Mirakl est le moteur des marketplaces des plus importants e-commerçants à travers le monde. Cette solution gère et produit de gros volumes de données qui présentent des challenges extrêmement intéressants pour les spécialistes de la donnée (produits, commandes, clients, niveaux de stock, prix, messages, appels API, données de navigation, séries temporelles, données géolocalisées etc.).\nEn tant que (Senior) Data Engineer au sein de l’équipe Data Mirakl, vos principales missions seront de :\ncontribuer à l'enrichissement de la Data Platform (ETL)\naméliorer la robustesse de nos pipelines de production pour nos applications Machine Learning (inférence real time etc.)\nIntégré(e) dans une équipe de spécialistes de la donnée (data engineers, machine learning engineers, data scientists, data analysts), vous êtes un des acteurs clés pour garantir la place de Mirakl comme solution dominante sur son marché.\nNotre stack et nos outils\nApache Spark, Kafka, AWS, Databricks, Python, Airflow, Mlflow, Tensorflow, Delta lake, Superset, Kubernetes, Redshift, SQL, Terraform, Ansible\nAu quotidien, vous allez :\nParticiper à la définition et à l’implémentation d’une architecture performante, robuste, scalable et aux coûts maîtrisés pour nos applications Spark ainsi que pour nos pipelines de production de Machine Learning (évaluation des feature stores, refactoring de DAG Airflow)\nAccompagner les Data Scientists lors de leur mise en production (relecture de code, pair programming) et mettre en place les best practices\nOptimiser et améliorer la CI/CD de l’équipe en collaboration avec l’équipe SRE\nAssurer la montée en compétence des membres de l’équipe sur les sujets de MLOps et Data Engineering\nRéfléchir à la meilleure façon d'intégrer les données Google Analytics dans la data platform\nPartager ses connaissances et présenter les travaux devant toutes les équipes Labs\nCe qu’on peut vous apporter :\nDes projets data driven, divers et variés (traitements massifs d’images, de textes, time series etc.) pour des produits différents de Mirakl\nUne culture orientée sur la veille technologique\nDes projets qui ont un vrai impact business devant être déployés sur des centaines de clients dans un contexte multilingue\nQuelques exemples de sujets en cours :\nEnrichissement des données produit à partir des images et des descriptions\nModération automatique des produits\nMapping automatique des données produit\nIdentification des produits à fort potentiels\nDétection de comportements frauduleux\nSentiment analysis sur les messages échangés entre clients et vendeurs et dans les évaluations\nDétermination de prix optimaux\nMonitoring de la qualité de service des vendeurs\nDes applications d’inférence en synchrone de nos modèles de ML\nVous aimerez ce job si :\nVous êtes passionné(e) par la data et les technologies modernes permettant d'en tirer partie\nVous vous intéressez à la data science et avez des connaissances générales sur les algorithmes de Machine Learning\nVous avez un background en développement et avez évolué dans un environnement Data\nVous avez a minima 4 ans d’expérience en environnement Machine Learning et/ou Data\nVous avez mis en production avec succès des applications Big Data faisant appel à du Machine Learning, du NLP, du traitement d’images dans des projets d'envergure, à fort volume de données\nVotre maîtrisez Python, êtes un pro des frameworks data de la fondation Apache et êtes à l'aise dans un environnement AWS\nVous maîtrisez au moins un outil d’orchestration (Airflow, Data Pipeline ou tout autre outil similaire)\nVous présentez vos travaux de manière simple et accessible\nVous faîtes preuve d'un bon relationnel et vous aimez mentorer des collaborateurs\nVous parlez couramment anglais et français\nLes plus pour le poste :\nVous avez une expérience significative dans le domaine du e-commerce\nVous avez déjà mis en place un Data Lake, Data Warehouse ou une Data Platform\nVous avez déployé des applicatifs en environnement Kubernetes\nVous avez mis en place des pipelines d'ingestion de données avec une approche CDC à l'aide de Debezium ou autre\nVous maîtrisez Java/Scala\nMirakl est engagée en faveur de la diversité, de l’égalité des chances et de l’inclusion. Nous célébrons nos différences car nous sommes convaincus que les qualités visibles et invisibles de chaque Mirakl Worker sont une source de force et d’innovation. Dans le cadre de cet engagement, nous étudions toutes les candidatures sans distinction de : genre, ethnicité, religion, orientation sexuelle, handicap, âge ou toute autre caractéristique protégée par la loi.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes",
                "Ansible"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "ML",
                "Machine Learning",
                "Big Data",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Scalian",
        "location": "Valbonne, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-scalian-3819563847?position=4&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=inh7goY5tExGCo%2B1yjcFcg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Ingénieur DATA / Data engineer (H/F)\nValbonne/Sophia-Antipolis\nType : CDI\nLieu : Locaux Scalian Sophia-Antipolis\nTélétravail : En fonction des possibilités\nDate de prise de poste : immédiatement ou en fonction de votre préavis\nSalaire : en fonction du profil - entre 40 et 48K Brut annuels (hors avantages Scalian)\nAvantages Scalian : Accord d'entreprise télétravail, Tickets restaurants, Mutuelle groupe, accord aménagement temps de travail, compte épargne temps, accord de participation et intéressement groupe, programme cooptation et apports d'affaires, accompagnement parentalité, avantages CSE\nVous êtes data engineer ou vous souhaitez le devenir !\nQuel sera votre rôle ?\nLa portée de la mission comprend (sans toutefois s'y limiter) :\nScience des données\nIngénierie des données\nAnalyse des données\nGénie logiciel\nCe que cette expérience va vous apporter\nVous êtes autonome, vous avez le sens du service et de l’analyse, vous êtes impliqué, nous vous offrons une ouverture sur des projets complexes et une rapide évolution de carrière. Vous rejoignez notre business unit à Sophia Antipolis composée d'environ 50 consultants, avec possibilité de télétravail en fonction des sujets.\nNous co-construisons votre trajectoire professionnelle et assurons votre montée en compétences.\nNous nous inscrivons ensemble dans la durée, nous assurons votre montée en compétences et disposons d'une variété de sujets passionnants.\nCe que nous recherchons chez vous\nDe formation supérieure (Bac+5, école ou université), vous possédez idéalement une première expérience réussie dans ce domaine (débutants acceptés), vous aimez le travail en équipe.\nCompétences requises\n:\nEtape d’analyse : Comprendre l’architecture technique, les sources de données, les objectifs fonctionnels.\nEtape de conception : Solution de conception avec un fort centrage sur les pipelines de données et les modèles ML et l’exposition des KPI via API\nMise en œuvre : Après les phases d’analyse et de conception, procéder à a mise en œuvre dans des technologies sélectionnées (Java,Scala,Python,Spark)\nCréer un code testé et documenté\nTechno : Linux, Shell, Hadoop, Scrum, Python, Spark, Scala\nPourquoi feriez-vous le grand saut ?\nParce que Scalian vous accompagne dans le développement de votre carrière :\nProgramme d'onboarding complet sur 1 an avec votre manager et votre RH\nProgramme de formation (Scalian Academy, e-learning, webinaires et formations externes)\nCommunautés techniques (Squads, Practices) afin de valoriser et développer votre expertise\nÉvénements internes (Afterworks, Awards Dinner, Kick Off, Live Event du COMEX, Stand Up) et externes (participation à des salons et forums spécialisés dans nos domaines d’activités…)\nDispositif d’accélération d’accès à la mobilité interne et à des échanges internationaux type Erasmus\nParce que Scalian favorise la Qualité de Vie au Travail :\nCertifications Great Place to Work® et Best Workplaces for Women®\nPrime de cooptation, prime vacances, prise en charge par l’employeur de 60% des titres-restaurant, Accord télétravail (jusqu’à 2,5 jours par semaine indemnisés), RTT (dont une partie monétisable), CSE (activités ludiques, chèques-cadeaux, chèques vacances)\nBerceaux en crèches inter-entreprises\nDon ou réception de jours de congés en cas de difficultés personnelles\nParce que Scalian développe une politique RSE concrète et ambitieuse :\nMobilité durable (indemnité kilométrique vélo, leasing de vélos à assistance électrique)\nActions environnementales (Fresque du Climat, Reforest'Action, Clean Up Day, mécénat ONF)\nPostes ouverts aux personnes en situation de Handicap\nDiverses politiques de diversité, d’inclusion et d’intégration mises en place\nScalian c’est aussi :\nUne entreprise en très forte croissance qui, créée en 1989, compte aujourd’hui plus de 5500 personnes\nDes références clients à forte valeur ajoutée auprès de grands industriels français (du CAC40) et internationaux\nUn terrain de jeu où l’expertise se conjugue avec audace, liberté d’entreprendre et convivialité\nSi vous aspirez à un environnement de travail qui valorise autant votre bien-être que votre développement professionnel,\nrejoignez-nous et exprimez pleinement votre talent !\nEnvie d'élargir le cadre ?\nJe suis Liza Djehel, Talent Acquisition Officer.\nSi votre CV est retenu, je vous contacte pour un premier échange téléphonique de 15 à 20 minutes.\nNous déterminons ensemble si ce poste est en adéquation avec vos compétences et surtout, avec vos attentes.\nL'échange est positif ? Nous convenons d'un entretien de 1h (en présentiel ou en visio) avec Lucas Daunar, Business Manager à Sophia-Antipolis. Cet échange permet de revenir en détail sur vos compétences, vos attentes, de vous présenter le poste plus en détail, et d'évoquer d'autres opportunités.\nNous prévoyons ensuite un rendez-vous technique de 1h (en présentiel ou en visio) avec un de nos responsable technique.\nEnfin, nous vous présentons notre proposition d'embauche.\nNotre processus de recrutement dure entre 15 et 30 jours\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "40",
            "Level": "Bac+5",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "OS": [
                "Linux"
            ],
            "Other": [
                "ML"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "BforBank",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-confirm%C3%A9-h-f-at-bforbank-3918327555?position=5&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=YLN7BIC2OVrtG%2B8DPUdn2Q%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Sur le modèle d'une\n\"Tech company\",\nBforBank place\nl'humain et le digital\nau cœur de sa transformation. Notre mission,\noffrir à nos clients une expérience bancaire incomparable\npour répondre à leurs besoins et usages mobile. 🌟 📱\nRejoindre BforBank c’est\nrejoindre une équipe engagée\ndans un\ngrand projet de développement stratégique en France et en Europe.\nNous sommes aujourd’hui 350 passionné(e)s et\nrecherchons nos talents pour construire la banque de demain\n. 🚀\nNous croyons en la force du collectif, chaque jour rassemblés autour de nos valeurs, de simplicité, d'optimisme et d’engagement, encourageant chacun à oser, essayer et accepter d’échouer.\n🎯 Au sein de la Direction Technologie, la Data Factory a pour objectifs de piloter, définir, déployer et opérer les meilleures solutions technologiques répondant aux cas d’usage data et d’automatisations de processus de la banque au travers de plateformes. Également, la Data Factory contribue au développement des produits, à la cristallisation et à la diffusion des pratiques au sein des Squads BforBank sur les usages data dans la banque.\nTu rejoindras une squad en charge de résoudre des problématiques métiers en créant des solutions applicatives utilisant les données, des data products, avec pour finalités la prise de décision via des moteurs de calcul ou des dashboards, la création de flux réglementaires, la création de data layer ou de reportings.\n🚀 Tes missions principales sont les suivantes :\n· Participer aux analyses, études d’impacts et cadrage techniques\n· Concevoir des solutions en respectant les bonnes pratiques d’architecture data et développement\n· Réaliser le développement de nouveaux data products et assurer la maintenance évolutive et corrective des data products existants\n· Rédiger la documentation technique des data products\n· Assurer un support aux testeurs\n· Reporter de ton activité à ta Squad et travailler dans une démarche d’efficacité collective\nConcrètement tu seras amené(e) à produire les livrables suivants :\n· Réaliser du code applicatif à l’état de l’art sur notre nouvelle Data Platform\n· Créer des data layer et des rapports sur notre outil de Data Visualisation\n· Rédiger les documentations techniques liées à ta solution, incluant le modèle de données, les procédures, l’ordonnancement\nCe que tu maîtrises :\n· Maitrise des services managés de GCP (BigQuery, dataproc, dataflow, CloudSQL …)\n· Maitrise du langage Python, Pandas, Spark\n· Maitrise de la modélisation de base de données et du langage SQL\n· Maitrise d’une chaine CI/CD (GitLab…)\n· Bonne connaissance de Kafka\n· Bonne connaissance d’un outil d’intégration de données type ETL (Informatica…)\n· Connaissance de l’infra as code (Terraform)\n· Connaissance d’un outil de reporting (Looker, BO…)\n🤝 Ce poste est fait pour toi si :\n· Tu es passionné(e) par la Data et leurs usages\n· Tu es orienté résolution de problème, est curieux(se) et force de proposition\n· Tu apprécies le travail en équipe\n· Tu as un bon relationnel et est rigoureux(se)\n· Tu as une bonne capacité d’analyse et rédactionnelle\n· Tu t’adaptes rapidement aux changements\n🎓\nFormation :\nTu es diplômé(e) d’un master en école de commerce, école d’ingénieur ou équivalent.\nChez BforBank nous recherchons avant tout des compétences. Tu ne disposes pas du diplôme requis mais as des expériences équivalentes ? N'hésite pas à postuler !\n💼\nExpérience :\nExpérience confirmée de 3 ans en tant que Data Engineer.\nEn rejoignant BforBank tu trouveras…\n· Un projet ambitieux de transformation digitale et culturelle à l’échelon européen, terrain d’innovation et d’ouverture d’esprit\n· Une organisation apprenante, proposant un large choix de formations toute l’année, et qui favorise l’échange avec les autres marques du Groupe\n· Une promo RSE multi-métiers qui fait évoluer en continu les actions de BforBank vers une banque plus responsable\n· Une organisation du travail en mode Agile, impliquant un degré élevé de collaboration et d'autonomie tout en travaillant avec un groupe de pairs diversifiés.\n· Une Direction Technologie en pleine expansion, porteuse de nombreux défis stratégiques\nMais aussi…\nDe 2 jours à 5 jours de télétravail modulables par semaine, dans la limite de 84 jours par an (frais de fonctionnement pris en charge)\n25 jours de congés + 16 jours de RTT\n80% du coût de la mutuelle d’entreprise pris en charge / couvert\nAvantages collaborateurs Crédit Agricole : taux et tarifs préférentiels\nDes frais de transports remboursés à 75%\nUn restaurant d’entreprise\nDes douches pour les sportifs et un tarif avantageux auprès d’une salle de sport toute proche\n📍 Le poste est basé à La Défense, dans des locaux flambant neufs !\nBforBank s'engage à garantir l'égalité des chances aux candidats car nous sommes convaincus de la richesse apportée par la diversité et l'inclusion dans nos équipes.\nRencontrons-nous !\nLe processus de recrutement se déroule en 4 étapes :\n🧑🏼‍💻\nCall de 30 minutes avec notre équipe Talent Acquisition\nEchange avec le Data Factory Manager et notre équipe Talent Acquisition (présentiel)\nEchange avec une personne de l’équipe avec qui tu seras amené à travailler (visio)\nEchange avec le CTO (visio ou présentiel)\nNotre processus de recrutement dure en moyenne 3 semaines et l’équipe Talent Acquisition se tiendra à ta disposition pour te donner un maximum de visibilité sur l’avancée du process.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "Pandas"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Other": [
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Action for Market Transformation - A4MT",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-action-for-market-transformation-a4mt-3910049004?position=6&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=a5VzP6Z29J%2BQGi4BQ1%2F3Dg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "A4MT – Action pour la Transformation des Marchés\nA4MT conçoit et implémente des programmes d’engagement et de « Market Transformation » qui visent à généraliser des pratiques vertueuses – au sens environnemental et sociétal – en modifiant la donne du marché, en reconfigurant le jeu d’acteurs, généralement via des actions collectives.\nCes programmes agissent sur la demande en suscitant de nouvelles pratiques individuelles et collectives. A4MT assure le rôle de pilote, orchestrant les plans d’action des parties prenantes grâce à une équipe de qualité à caractère international, un savoir-faire sur la mise en œuvre des programmes, une connaissance technico-économique experte des sujets traités, et une capacité à interpeller les décideurs à bon niveau.\nChampionnat de France des économies d’énergie\nA4MT avec ses partenaires opère l’ensemble des concours CUBE en France (Championnat de France des Economies d’Energies) et assure son développement international (Europe, Asie, etc.). CUBE est un concours original d’économies d’énergie et de CO2 pour les bâtiments tertiaires et résidentiels qui accélère fortement l’action de terrain grâce à une intelligence collective sur le terrain.\nLe concours est aujourd’hui présent dans 8 pays et se développe encore. Au-delà des économies les plus faciles, il s’agit de mettre en œuvre la trajectoire de gestion immobilière et d’investissement qui permettra, au-delà des avancées dans ce programme à faible investissement, de progresser sur la trajectoire de la neutralité carbone.\nhttps://championnatdefrancedeseconomiesdenergie.org/\nMISSION\nRendant compte au directeur d’A4MT et en étroite collaboration avec le directeur technique A4MT, vous êtes Data Engineer, vous serez responsable de la conception, du développement et de la maintenance des bases de données, et des outils de reporting. Vous travaillerez en étroite collaboration avec l'équipe de développement (prestataire externe) et vous participez à la structuration d’une équipe IT interne pour créer des solutions innovantes répondant aux besoins de l'entreprise.\nVotre mission s'articule autours des 3 axes ci-dessous:\n1/ Pilotage et et développement\ndévelopper et déployer des reporting robustes et évolutifs.\nle planning de développement et le budget alloué.\navec les équipes d’animation et le back office technique du programme et avec les prestataires externes pour comprendre les exigences et les spécifications du projet.\nà la conception de l'architecture des bases de données et à la prise de décisions techniques.\nla qualité des données en effectuant des contrôles qualité.\nles performances des applications pour garantir une expérience utilisateur fluide.\nla maintenance et les mises à jour régulières des applications existantes.\nà l'affût des tendances et des technologies émergentes.\nVous serez responsable du process, de la maîtrise d’ouvrage liée à la Data et garant(e) de la qualité de service.\n2/ Implication des équipes et de la sous-traitance\nVous serez impliqué dans une équipe informatique naissante et dans une équipe projet avec les différentes fonctions métiers. Vous devrez faire le suivi de votre implication avec le responsable de programme et le directeur technique d’ A4MT :\n3/ Gestion de projet\nVous tiendrez le tableau de bord des outils : budgets, engagements, planning, résultats, développements.\nPROFIL\nVous avez une expérience significative d’au moins 3 années dans l’écosystème de big data, des serveurs et bases de données dans des contextes de projets, d’exploitation de migration.\nCOMPETENCES\nBac +5 diplômé(e) d’une grande école d’ingénieur ou équivalent, vous êtes :\n+5 diplômé (e) d’une école d’ingénieurs ou équivalent, en Data science, Informatique, génie logiciel ou domaine connexe.\nprofessionnelle démontrée de 3 ans ou plus en tant que Data Engineer\ndes langages structurés (JavaScript, Scala, Python…),\navec les bases de données relationnelles (MySQL, PostgreSQL) et non relationnelles (MongoDB, Firebase).\nau moins un outil de reporting (Power BI, Tableau …)\ndes services de déploiement et d'hébergement cloud comme AWS, Azure ou Google Cloud Platform.\ncompétences en développement back-end avec des technologies comme Node.js, Python, Ruby on Rails, ou Java. et notamment en PHP sont recommandées\ndes langages de programmation front-end tels que HTML5, CSS3 et JavaScript (notamment frameworks: comme React, Angular ou Vue.js).\nà travailler en équipe, à communiquer efficacement et à résoudre les problèmes de manière autonome.\ndes principes de sécurité des applications web et des meilleures pratiques en matière de développement sécurisé ainsi que le respect du RGPD.\nDate d’entrée et conditions\nLe poste est à pourvoir immédiatement; il est basé au 54, rue de Clichy, Paris (IXème). Niveau de rémunération selon expérience.\nContact\nMerci d’adresser votre candidature complète (CV, lettre de motivation, présentation du cursus en cours de conclusions et références éventuelles) à l’attention de M. Adrien Brunella sur le mail elisabeth.clement@a4mt.com\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "MongoDB"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "CloudComputing": [
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "PostgreSQL",
                "MySQL"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Mirakl",
        "location": "France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-engineer-at-mirakl-3904076524?position=7&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=nk2bB5vqPzMQmCDY62Tvyg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Mirakl, leader et pionnier de l’économie de plateforme, propose aux entreprises une suite unique de solutions leur permettant de transformer significativement leur e-commerce afin d'accélérer de façon durable et rentable leur croissance. Depuis 2012, Mirakl accompagne les entreprises B2C et B2B avec la technologie la plus avancée, sécurisée et évolutive leur permettant de digitaliser leur activité et d'élargir leur offre via la marketplace ou le dropship, faciliter la gestion des catalogues et des paiements de leurs fournisseurs pour plus d'efficacité, offrir une expérience d'achat personnalisée à leurs clients, et augmenter leurs profits grâce au retail media. Basée à Paris et Boston, Mirakl est certifiée Great Place to Work.\nA propos de Mirakl Labs\nNos équipes techniques et produits, nommées Mirakl Labs, sont principalement réparties entre nos 2 hubs situés à Paris et à Bordeaux. Elles collaborent au quotidien afin d'adresser les problématiques de nos clients et utilisateurs en répondant à différents challenges liés aux nouvelles fonctionnalités, à la scalabilité, la sécurité et l’ergonomie…\nElles opèrent en mode agile et s'organisent en Squads composées d'un Squad Lead, de 5 développeurs, d'un Product Manager et d'un QA. Chaque Squad est spécialisée sur un scope fonctionnel afin de concevoir et réaliser de nouvelles features, leurs évolutions et des APIs (avec un découpage en micro-services). Nos équipes Infrastructure, Architecture, Sécurité, Documentation, Product Design, Data et Support opèrent en transverse en apportant leur expertise et de la cohérence sur l’ensemble des produits.\nToutes les équipes sont responsables de leur périmètre et chacun des collaborateurs apporte son expérience et ses idées. Innovation, feedback et implication dans les prises de décision sont au cœur de notre philosophie.\nEt pour favoriser ce partage avec d’autres passionnés, nous sommes sponsors, speakers, et hôtes de différents événements, meetups, et associations de la scène Tech en France. Au cours des dernières années, nous avons participé à des événements tels que Devoxx, ReactEurope, ProductConf et Flupa UX Days.\nA propos du job\nLa solution SaaS Mirakl est le moteur des marketplaces des plus importants e-commerçants à travers le monde. Cette solution gère et produit de gros volumes de données qui présentent des challenges extrêmement intéressants pour les spécialistes de la donnée (produits, commandes, clients, niveaux de stock, prix, messages, appels API, données de navigation, séries temporelles, données géolocalisées etc.).\nEn tant que (Senior) Data Engineer au sein de l’équipe Data Mirakl, vos principales missions seront de :\ncontribuer à l'enrichissement de la Data Platform (ETL)\naméliorer la robustesse de nos pipelines de production pour nos applications Machine Learning (inférence real time etc.)\nIntégré(e) dans une équipe de spécialistes de la donnée (data engineers, machine learning engineers, data scientists, data analysts), vous êtes un des acteurs clés pour garantir la place de Mirakl comme solution dominante sur son marché.\nNotre stack et nos outils\nApache Spark, Kafka, AWS, Databricks, Python, Airflow, Mlflow, Tensorflow, Delta lake, Superset, Kubernetes, Redshift, SQL, Terraform, Ansible\nAu quotidien, vous allez :\nParticiper à la définition et à l’implémentation d’une architecture performante, robuste, scalable et aux coûts maîtrisés pour nos applications Spark ainsi que pour nos pipelines de production de Machine Learning (évaluation des feature stores, refactoring de DAG Airflow)\nAccompagner les Data Scientists lors de leur mise en production (relecture de code, pair programming) et mettre en place les best practices\nOptimiser et améliorer la CI/CD de l’équipe en collaboration avec l’équipe SRE\nAssurer la montée en compétence des membres de l’équipe sur les sujets de MLOps et Data Engineering\nRéfléchir à la meilleure façon d'intégrer les données Google Analytics dans la data platform\nPartager ses connaissances et présenter les travaux devant toutes les équipes Labs\nCe qu’on peut vous apporter :\nDes projets data driven, divers et variés (traitements massifs d’images, de textes, time series etc.) pour des produits différents de Mirakl\nUne culture orientée sur la veille technologique\nDes projets qui ont un vrai impact business devant être déployés sur des centaines de clients dans un contexte multilingue\nQuelques exemples de sujets en cours :\nEnrichissement des données produit à partir des images et des descriptions\nModération automatique des produits\nMapping automatique des données produit\nIdentification des produits à fort potentiels\nDétection de comportements frauduleux\nSentiment analysis sur les messages échangés entre clients et vendeurs et dans les évaluations\nDétermination de prix optimaux\nMonitoring de la qualité de service des vendeurs\nDes applications d’inférence en synchrone de nos modèles de ML\nVous aimerez ce job si :\nVous êtes passionné(e) par la data et les technologies modernes permettant d'en tirer partie\nVous vous intéressez à la data science et avez des connaissances générales sur les algorithmes de Machine Learning\nVous avez un background en développement et avez évolué dans un environnement Data\nVous avez a minima 4 ans d’expérience en environnement Machine Learning et/ou Data\nVous avez mis en production avec succès des applications Big Data faisant appel à du Machine Learning, du NLP, du traitement d’images dans des projets d'envergure, à fort volume de données\nVotre maîtrisez Python, êtes un pro des frameworks data de la fondation Apache et êtes à l'aise dans un environnement AWS\nVous maîtrisez au moins un outil d’orchestration (Airflow, Data Pipeline ou tout autre outil similaire)\nVous présentez vos travaux de manière simple et accessible\nVous faîtes preuve d'un bon relationnel et vous aimez mentorer des collaborateurs\nVous parlez couramment anglais et français\nLes plus pour le poste :\nVous avez une expérience significative dans le domaine du e-commerce\nVous avez déjà mis en place un Data Lake, Data Warehouse ou une Data Platform\nVous avez déployé des applicatifs en environnement Kubernetes\nVous avez mis en place des pipelines d'ingestion de données avec une approche CDC à l'aide de Debezium ou autre\nVous maîtrisez Java/Scala\nMirakl est engagée en faveur de la diversité, de l’égalité des chances et de l’inclusion. Nous célébrons nos différences car nous sommes convaincus que les qualités visibles et invisibles de chaque Mirakl Worker sont une source de force et d’innovation. Dans le cadre de cet engagement, nous étudions toutes les candidatures sans distinction de : genre, ethnicité, religion, orientation sexuelle, handicap, âge ou toute autre caractéristique protégée par la loi.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes",
                "Ansible"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "ML",
                "Machine Learning",
                "Big Data",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "AXA en France",
        "location": "Hauts-de-Seine, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-senior-at-axa-en-france-3884386043?position=8&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=3U%2F2pMNXXaCLmgpNUaVdcg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Environnement\nEn tant que\nSenior Data Engineer F/H\n, vous allez contribuer directement aux projets des directions métier (ex : fraude santé, multiéquipements, pricing IARD, optimisation du lead management, fragilité auto, …) d’AXA France et à la construction du socle technique Big Data.\nVous allez intégrer une équipe d'une dizaine de personne composée de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus métier de la Direction Transformation Digital Tech et DATA (DT2).\nLa Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :\n- Une organisation agile en feature teams : tribus, guildes, squads\n- Des projets sur des applications innovantes à fort trafic (web, mobile…)\n- Des méthodologies craft (TDD, BDD, clean code, code review…) et DevOps\n- Une communauté de partage de bonnes pratiques (BBL, dojo, meetup, conf…)\nVotre rôle et vos missions\nVous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :\nPasser de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le datalake\nConsolider ces données au fur et à mesure de leur alimentation récurrente dans le data lake\nLes exploiter pour atteindre la finalité business (exposition de business view, réintégration des résultats dans le SI, service de scoring, …)\nDe travailler à la création du socle technique Big Data et industrialiser le cycle de développement de l'équipe\nDe mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)\nVotre profil\nVous justifiez de plusieurs expériences significatives (+ de 5 ans) sur du\ndéveloppement big data, en particulier sur du PySpark.\nCompétences techniques :\nConnaissances avancées en développement en\nPySpark\n(Spark avec le langage Python)\nMaitrise de l'environnement\nMicrosoft Azure\nConnaissances avancées d'outils de BI comme\nPowerBI\nCompétences transverses :\nCapacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier\nExpérience en mode de delivery Agile (Scrum, Kanban, etc...)\nDriver et accompagner des Data Engineer junior sur les aspects technique\nEt Idéalement :\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de données relationnelles et NoSQL\nUne expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nQui sommes nous ?\nAXA est un des leaders de l’assurance et de la gestion d’actifs dans le monde.\nNous aidons nos 108 millions de clients à traverser les petites et grandes difficultés de la vie.\nChaque jour, nous agissons ensemble pour inventer la meilleure manière de les protéger et voulons donner à chacun les moyens de vivre une vie meilleure.\nUn challenge qui donne le sourire et envie de se lever le matin !\nChez AXA, nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C’est pour cette raison que nous menons une politique RH engagée qui favorise la diversité, qui préserve l’équilibre vie privée-vie professionnelle et accélère le développement des compétences et des carrières.\nAinsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une véritable culture d’expertise, accélérant le développement des compétences de chacun et proposant une rémunération attractive.\nPourquoi nous rejoindre ?\nVous êtes porteur d’idées et d’initiatives innovantes ? Vous proposez des solutions et êtes au service du client ? Faites partie de notre grande famille en rejoignant\nUn leader mondial offrant des opportunités de carrières intéressantes\nUne entreprise qui donne une place de choix à l’innovation, à l’initiative et aux actions solidaires (notamment via l’association AXA Atout Cœur)\nUn environnement inclusif à tous les niveaux (mixité, handicap, initiatives pour favoriser l’insertion des jeunes, orientation sexuelle, etc.)\nUn accès à de multiples avantages (congés, temps partiel, télétravail, etc.)\nUn cadre stimulant, qui permet de rencontrer des collaborateurs performants et d’enrichir ses compétences\nVictime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Junior",
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Lyon, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3910994899?position=9&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=9jrBl%2BUf0qJsGQTz6OU5UQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ASTRELYA",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-astrelya-3910760230?position=10&pageNum=10&refId=Wt7FrOruL4%2BCplGL9tAn0A%3D%3D&trackingId=yV6o8FO4VtUJdgoHfgWavQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ASTRELYA est un groupe de conseil et d’expertise IT fondé en 2017, présent en France (Paris et régions) et en Suisse (Genève). Aujourd'hui plus de 280 collaborateurs accompagnent nos clients dans l’accélération et la transformation de leurs organisations.\nDans le cadre de notre développement, nous recherchons un\nData Engineeer F/H\n.\nVos rôles et responsabilités :\nDéveloppements Java Spark\nOptimisation et gestion des évolutions de l&#39;architecture pour intégrer des calculs sur des volumétries de plus en plus importantes\nSupport technique auprès des équipes de développement et du responsable applicatif\nConception des solutions applicatives cohérentes avec l&#39;ensemble du SI et avec les normes et standards\nDévelopper et garantir les pratiques de développement et de documentation associés (DevOps\nL’environnement technique dans lequel vous évoluerez :\nJava, Scala, Spark, écosystème Hadoop, environnement DevOps\nLes compétences recherchées :\nFormation : École d’ingénieur ou équivalent Bac+5\nExpériences : Minimum 5 ans d’expérience\nLangues : Anglais technique\nExcellent relationnel, force de proposition, autonome\nPourquoi rejoindre ASTRELYA ?\nUne gestion de carrière personnalisée et un management de proximité\nUne politique active de formations / certifications (technique, métier, leadership)\nUne offre variée de missions d’expertise\nUn engagement RSE fort : Ecovadis Gold, Signataire de la charte pour la diversité, du Pacte des Nations Unies et mise en place du Mécénat de compétences\nUn programme de cooptation attractif\nAfterworks, conférences techniques et activités sportives réguliers\nCette annonce vous correspond ? Postulez !\n🚀\nTous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "DevOps"
            ],
            "EnSoftSkils": [
                "Leadership"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Groupe INGENA",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-groupe-ingena-3883689479?position=1&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=PTJFmVLol0NZPIc8rRt%2BRw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Le groupe INGENA promeut la transition numérique en étant acteur d’un monde souhaitable.\nVotre mission :\nConcevoir, développer et tester des algorithmes de collecte et de traitement de gros volumes de données sous Scala, Python ou Java\nAutomatiser et optimiser les flux de données et leurs visualisations en dashboards\nIndustrialiser les traitements, la qualité et l’intégrité des données\nParticiper à la Modélisation et à la Gouvernance des données (process, normalisation, référentiel,…)\nContribuer à la scalabilité, la sécurité, la stabilité et la disponibilité des données de la plateforme\nAnalyser les données pour répondre aux questions métiers et participer à l’évolution de l’architecture Big Data\nConcevoir, Développer et Industrialiser des modèles de Machine Learning, Deep Learning, en collaboration avec les Data Scientists\nAppliquer une démarche CI/CD (Git, Jira, Jenkins)\nLes compétences techniques nécessaires sont :\nExpérience de 5 ans minimum en développements Scala, Python ou Java\nExpérience de 2 ans minimum sur SPARK et sur le traitement des flux en streaming\nExpertise sur Hadoop (Hive, HBase, HDFS) sous distributions MapR ou Hortonworks\nExpérience souhaitée sur ELK, Terraform, NoSQL,…\nFort background en Modélisation de données ou ETL\nMaîtrise des briques analytiques des clouds AWS, GCP ou Azure\nSensibilisation à la démarche CI/CD tools (Git, Jenkins)\nLa connaissance de Docker, Kubernetes et Ansible est un plus\nMise en œuvre des méthodes Agile (Scrum, Kanban,…)\nAnglais souhaité\nGroupe INGENA\n:\nLe Groupe INGENA est spécialisé en Conseil Métier et en Intégration pour les marchés de l’assurance, de la banque et de la Finance. INGENA intervient notamment sur les projets associés à la Data, aux Risques et à la Distribution.\nLe groupe comprend également la société DRiMS spécialisée en Finance de Marché.\nNos valeurs : Engagement, Intégrité et Bienveillance.\nLa mise en pratique du monde souhaitable, c’est pour nous une entreprise éco-responsable, éthique, inclusive, sociale, soucieuse du bien-être, de l’évolution et de l’épanouissement de ses équipes. Ce sont aussi des offres pour un monde durable comme la maîtrise des risques ou l’ESG.\nDans un esprit convivial et engagé, nous faisons en sorte que chacun puisse être acteur de l’INGENA souhaitable.\nBureau à Paris 9ème (Métro Le Peletier). Clients à Paris ou très proche banlieue.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "HBase",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker",
                "Jenkins"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Kubernetes",
                "Ansible"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Machine Learning",
                "Big Data",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Lincoln France",
        "location": "Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-lyon-h-f-at-lincoln-france-3851734549?position=2&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=VeypythEIv8Md9nnB3RwPA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Poste CDI : Data Engineer (Scala, Spark, GCP, etc.) -H/F - Lyon\nLincoln Pure Player Data\n💡: Réinventant l'analyse\ndepuis 30 ans\n. Experts en Modern BI, Big Data et Science des données 📊. Nous transformons vos données en solutions pour les grands comptes, des secteurs bancaire, retail, télécoms, industriel, santé, et plus encore 💼.\nDescription de poste\n🎯\nMissions\n:\nConcevoir et développer des pipelines de données robustes et évolutifs.\nIntégrer et transformer des données provenant de différentes sources.\nDévelopper et mettre en œuvre des algorithmes de traitement de données avancés.\nCollaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.\nAssurer la qualité et la fiabilité des solutions développées.\n🔍\nPrérequis\n:\nMaîtrise des langages de programmation (\nPython, Scala, Spark, etc\n.).\nConnaissance approfondie des bases de données et des technologies\nBig Data (Hadoop, Spark, Kafka, Talend,...) et Cloud (AWS, GCP, Azure,...)\n.\nExpérience avec\nMySQL, PostgreSQL, MongoDB.\nSolides compétences en conception et en optimisation de pipelines de données.\nExpérience de travail en\nméthode Agile\npour la gestion de projet et le développement de solutions.\nCapacité à travailler de manière autonome et en équipe.\nExcellentes compétences en communication et en résolution de problèmes.\n🌟\nAvantages :\nEnvironnement collaboratif et innovant\nFormations certifiantes et accompagnement individualisé\nTélétravail et horaires flexibles\nRémunération compétitive avec avantages sociaux attrayants\nPossibilité de mobilité à Lille, Paris ou Aix-en-Provence\n✨\nProcessus de recrutement\n: 2 entretiens (RH et technique)\nSi vous êtes passionné par les défis de la Data et que vous souhaitez rejoindre une équipe dynamique et innovante,\npostulez dès maintenant et contribuez à redéfinir l'avenir de l'analyse de données chez Lincoln! 😉\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "MongoDB"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "PostgreSQL",
                "MySQL"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Inetum",
        "location": "St.-Ouen, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/d%C3%A9veloppeur-big-data-h-f-at-inetum-3843965989?position=3&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=cF9s3MXmDqgW8F6HUp9rFw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Détail de l'offre\nInformations générales\nEntité de rattachement\nInetum est un leader européen des services numériques. Pour les entreprises, les acteurs publics et la société dans son ensemble, les 28 000 consultants et spécialistes du groupe visent chaque jour l'impact digital des solutions qui contribuent à la performance, à l'innovation et au bien commun.\nPrésent dans 19 pays au plus près des territoires, et avec ses grands partenaires éditeurs de logiciels, Inetum répond aux enjeux de la transformation digitale avec proximité et flexibilité.\nPorté par son ambition de croissance et d'industrialisation, Inetum a généré en 2023 un chiffre d'affaires de 2,5 milliards d'€.\nPour répondre à un marché en croissance continue depuis plus de 30ans, Inetum a fait le choix délibéré de se recentrer sur 4 métiers afin de gagner en puissance et proposer des solutions sur mesure, adaptées aux besoins spécifiques de ses clients le conseil (Inetum Consulting), la gestion des infrastructures et applications à façon (Inetum Technologies), l'implémentation de progiciels (Inetum Solutions) et sa propre activité d'éditeur de logiciels (Inetum Software). Inetum a conclu des partenariats stratégiques avec 4 grands éditeurs mondiaux - Salesforce, ServiceNow, Microsoft et SAP et poursuit une stratégie d'acquisitions dédiée afin d'entrer dans le top 5 européen sur ces technologies et proposer la meilleure expertise à ses clients.\nTous nos postes sont ouverts aux personnes en situation de handicap.\nDescription du poste\nMétier\nApplications Delivery - Software Development\nIntitulé du poste\nDéveloppeur Big Data H/F\nContrat\nCDI\nDescription De La Mission\nLe pôle\nBFA\nde la branche Application Services du groupe\nINETUM\n, recherche plusieurs développeurs\nBig Data\nafin d'intervenir auprès de clients grands comptes au sein\ndes marchés bancaires et de l'assurance.\nDirectement intégré(e) chez l'un de nos clients sur des sujets stratégiques et aux enjeux forts, vous serez amené(e) à\nParticiper à l'analyse détaillée à partir des besoins des utilisateurs et de l'analyse fonctionnelle\nConcevoir l'application et les tests à partir de l'analyse détaillée\nDérouler les tests et corriger les anomalies\nTravailler sur la mise en place d’infrastructures Big Data\nRéaliser les flux de données\nValider leur fonctionnement en sécurité et performance\nAssurer la pérennité de leurs évolutions\nParticiper quotidiennement aux réunions d’équipe (daily meeting) afin de contribuer à l’évaluation de l’effort de travail nécessaire\nProfil\nIssue d'une formation d'ingénieur / Bac+5 en Informatique\nDoté d'une expérience d’au moins 2 ans sur ce type poste\nMaîtrise des technologies Hadoop, Spark, Hive, Impala, ETL (Talend, Informatica, …), Java, Scala, Python, SQL, les bases de données SQL (oracle, …) et NoSQL (Cassandra, …)\nDes notions de Machine Learning et d’IA sont recommandées pour bien appréhender les besoins de nos Data Scientists.\nExpérience au sein d'un environnement agile (Scrum) indispensable\nAnglais obligatoire\nLocalisation du poste\nLocalisation du poste\nFrance, Ile-de-France\nVille\n145, Boulevard Victor Hugo 93400 Saint-Ouen\nCritères candidat\nNiveau d'expérience min. requis\nPlus de 2 ans\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "30 an(s)"
        },
        "title": "big data developer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "Cassandra",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Other": [
                "Machine Learning",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "L2C / Spécialiste du recrutement IT",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-d%C3%A9veloppeur-38-42k%E2%82%AC-nantes-h-f-at-l2c-sp%C3%A9cialiste-du-recrutement-it-3913996457?position=4&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=qmrHjOejaWqg6MEzmYhDuQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "L2C est un cabinet spécialisé dans le recrutement de profils informatiques, tech' et data en CDI pour des clients finaux.\nCLIENT\nPrécurseur, innovant et visionnaire, notre client a développé une plateforme basée sur l'IA prédictive, qui permet aux entreprises de prendre les meilleures décisions.\nRéférence reconnue et leader dans le domaine de la Business Intelligence, notre client développe des solutions d'intelligence artificielle commerciale BtoB pointues, complètes, avec un haut niveau de qualité : aide à la prospection, gestion des appels d'offres, prédiction financière\nSolidement établi, en croissance continue, notre client se réinvente constamment afin de rester à la pointe et de prendre soin de ses collaborateurs.\nDepuis plus de 20 ans, notre client a conquis près de 15 000 clients. (Paypal, Dassault, KPMG)\nC'est une société saine, bijou de la tech, précurseur en termes de stack technique et façon de fonctionner, avec un bon état d'esprit, offrant des conditions de travail agréables.\nNous recherchons un candidat polyvalent, à la croisée entre le traitement de données et le développement. (Proche Nantes)\nPoste à pourvoir en interne en CDI. (télétravail 3 jours/semaine)\nMissions\nVous participerez à l'amélioration des outils référentiels Big Data, en étroite collaboration avec les équipes de développeurs (10 personnes) et les data scientists (6 personnes)\nVous développerez des solutions de captation et d'intégration des données issues de l'Open Data et des partenaires, crawls, scraping, imports de fichiers\nVous modéliserez des bases de données\nVous orchestrerez des flux de données entre les applicatifs, référentiels, bases de données (SQL/NoSQL) avec des outils ETL.\nVous valoriserez les données avec des traitements complémentaires (géocodage, océrisation)\nVous développerez des APIs pour des usages internes et pour les clients\nVous assurerez la supervision et l'exploitation des outils (surveillance des performances et garantie de la disponibilité.\nVous assurerez la qualité des données (nettoyage, standardisation, valorisation pour garantir la fiabilité)\nVous rédigerez la documentation technique et accompagnerez les utilisateurs\nLes plus\nSociété précurseur, innovante et visionnaire, qui prend soin de ses collaborateurs\nRéférence reconnue et leader dans le domaine de la Business Intelligence (IA prédictive, sujets tech modernes et riches)\nGroupe solide en forte croissance\nSociété solidement implantée en France et à l'international depuis une vingtaine d'années\nTélétravail partiel\nEnvironnement de travail sain et agréable\nComplémentaire santé/Prévoyance, comité d'entreprise, prime vacances, tickets restaurant\nVous disposez d'un bon bagage technique sur Python.\nVous avez l'habitude de vous auto-former sur les outils et langages et vous savez maintenir une veille active sur les outils de traitement des données.\nCompétences / Connaissances\nData-Management, Audit de qualité des données et usage des ETL\nSystèmes Linux, gestion et administration des containers (Docker, Kubernetes)\nDéveloppement / Scripts : Python ++ (Avec Spark), JavaScript, C#\nBases De Données\nSQL Server ou autres SGBD Relationnels\nMongoDB\nElasticsearch\nBases de données orientées Graph (Neo4J)\nConnaissances CI/CD, Git\nConnaissance générale des technologies et framework big data usuels.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "20 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "MongoDB",
                "Elasticsearch",
                "SQL",
                "Neo4j",
                "NoSQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "OS": [
                "Linux"
            ],
            "DBMS": [
                "SQL Server"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Big Data",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Harry Hope.",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-harry-hope-3920545043?position=5&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=kGbVGmURBYVn0rXlEKV%2BSg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Harry Hope, cabinet de recrutement accompagne candidats et entreprises dans leurs recherches des meilleures opportunités en France et à l'international. Afin de mieux répondre à vos enjeux, tous nos consultants sont spécialisés par secteur d'activité et zone géographique.\nNotre client, société en pleine croissance, spécialisé dans le domaine de la Big Data, recherche des Consultants Data Engineer ! Participez à cette aventure et rejoignez une formidable équipe.\nVos missions principales seront diversifiées, comprenant notamment :\nParticipation aux processus d'avant-vente : Vous contribuerez à l'élaboration des propositions techniques, mettant en avant votre expertise pour répondre aux besoins des clients.\nQualification technique des prestataires : Vous participerez activement à l'évaluation et à la sélection des prestataires, garantissant un partenariat de qualité.\nDirection et coordination des projets : Vous dirigerez et coordonnerez la conception et la mise en oeuvre des projets, assurant leur réussite technique.\nDocumentation technique : Vous élaborerez, au besoin, des dossiers d'architecture, d'installation et d'exploitation, assurant une traçabilité et une compréhension optimale des solutions mises en place.\nParticipation active aux développements : Vous apporterez votre expertise en contribuant directement aux développements dans le cadre des projets.\nDe manière plus étendue, vous aurez l'opportunité de :\nEnrichir les bonnes pratiques : Vous contribuerez à l'évolution et à l'amélioration des bonnes pratiques d'architecture et de développement dans le domaine du Big Data.\nVeille technologique : Vous réaliserez une veille constante sur les avancées technologiques du secteur, assurant ainsi la pertinence des solutions proposées.\nFormation technique : Vous élaborerez des supports de formation technique pour nos clients et/ou nos consultants juniors, partageant ainsi votre savoir-faire.\nAnimation du pôle technique : Vous participerez activement à l'animation du pôle technique favorisant un environnement collaboratif et innovant.\nVous êtes détenteur d'un diplôme d'ingénieur (école ou université), et vous avez 5 ans d'expérience en tant que Data Engineer.\nEn tant que Consultant Data Engineer, nous recherchons des professionnels possédant des compétences solides et des convictions dans les domaines suivants :\nArchitectures Big Data : Kappa, Lambda, Réactive, SMACK, etc.\nSolutions technologiques : Hadoop, SGBD NoSQL, Kafka, Spark, etc.\nOutils de développement : Vous êtes à l'aise avec des outils tels que Hive, Pig, Python, Scala, etc.\nEnvironnements d'exploitation et de supervision : Vous avez une expérience pratique avec des outils tels qu'Ambari, Oozie, Zookeeper, etc. 20681288-55584\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Junior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Mobiskill | WEFY Group",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-mobiskill-wefy-group-3907391938?position=6&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=d9sK3EdR8JW0CN1NTR9hag%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La société ?\nCette startup a été créée en 2018 et vise à aider la prise de décision de ses clients qui sont principalement dans le secteur du retail ou de l'alimentaire.\nIls permettent d'enrichir la donnée afin d'améliorer la stratégie de vente et marketing d'une entreprise grâce à leur plateforme Saas basée sur des algorithmes d'IA.\nIls ont besoin de renforcer leur équipe en Data Engineering pour gérer au mieux leur volumétrie.\nLes missions ?\n- Editer le cahier des charges des données à collecter auprès de nos partenaires distributeurs\n- Prendre en main la gestion de la donnée dans le cloud de la société pour optimiser les coûts et l’efficacité des analyses effectuées par l’équipe Analytics\n- Anticiper les évolutions et participer aux choix structurants de la société liés à la gestion de la data\nLe profil recherché ?\n- Avoir 2/3 ans d'expérience en Data Engineering (hors stage et alternance)\n- Avoir pu travaillé en Python comme langage de programmation\nAvoir travaillé au moins deux ans et si possible sur des sujets d'optimisation avec Spark !\n- La maîtrise des outils tels Airflow, Kafka et Snowflake seraient un plus apprécié\n- Maîtriser un des cloud providers et si possible avoir une expérience sur Azure\nPourquoi les rejoindre ?\n- Une société stable financièrement (fonds propres uniquement)\n- Une startup en pleine croissance\n- Une rémunération en fonction de votre séniorité\n- Volumétrie de données incroyable, il y a de quoi s'amuser !\n- Faire parti de l'unique retail-tech qui a un impact écologique positif (fin des prospectus, éviter le gâchis alimentaire)\nHâte de vous en dire plus rapidement !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Lyon, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-mindpal-3896997013?position=7&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=GrAS148%2BSqUlVtQ2ePplOg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for\nData Engineer!\nResponsibilities\nDesigning, creating, and maintaining data processing systems\nAnalyzing and optimizing data processing workflows\nCollaborating with the team to ensure data quality and efficiency\nTesting and implementing new solutions\nRequirements\nAt least 2 years of experience in designing and creating data processing systems\nProficiency in tools and programming languages related to data engineering (e.g. Hadoop, Spark, Scala, Python)\nExcellent knowledge of databases and SQL language\nAbility to work in a team and communicate effectively with other departments\nCommunicative English skills\nExperience with AWS/AWS Glue is a plus\nWe Offer\nB2B contract\nFull-time job\nRemote work and flexible hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "AWS"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Wakam",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-alternant-e-at-wakam-3918901392?position=8&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=xnrf2vRnerqq3Y11vT3uZQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Who are we?\nWakam is a B2B2C insurance company that creates white-label insurance solutions via its Play&Plug® technology platform for more than 150 distribution partners. We provide most of our insurance products through API, and hosts white label insurance solutions via our Play&Plug technology platform.\nWith a footprint spanning 32 countries and revenue of more than €900 million in 2023, mostly generated outside France, Wakam is the European leader in digital and embedded insurance.\nStrongly committed to social responsibility,\nWakam is a mission-driven company\ndedicated to \"enabling transparent and impactful insurance\".\n✎\nMissions\nWakam assure la conception de produits d'assurances sur-mesure qui sont ensuite commercialisés par des partenaires distributeurs (courtiers, insurtech, retailers) sur un modèle B2B2C en marque blanche. Pour accompagner la forte croissance de l'entreprise, Wakam est à la recherche d'un(e) alternant(e) pour rejoindre notre Office Data.\nRattaché(e) à l'équipe Data Platform (DPF), vous contribuerez activement à l'élaboration de la nouvelle plateforme en y développant de nouveaux cas d'usages techniques et business.\nVos principales missions sont les suivantes :\nConstruire des pipelines ETL/ELT et reverse ETL sur les données des partenaires Wakam;\nComprendre l'architecture existante, incluant le portail d'ingestion des données, le framework de self-service et l'infrastructure technique;\nRéaliser des transformations de données à l'aide du framework DBT en SQL;\nContribuer au développement des produits de la Data Platform en utilisant Python et partager les connaissances acquises avec la communauté techniques de Wakam;\nCollecter des données aux formats variés provenant des différentes sources;\nParticiper activement au développement de la data platform sur Snowflake;\nAider et assister les utilisateurs métier dans l'utilisation des outils fournis par l'équipe DPF;\nDocumenter les différentes étapes de transformation et d'historisation des données;\n✯\nProfil recherché\nVous suivez actuellement des études dans l'un de ces domaines : école d'ingénieurs, master en informatique, data science, data engineering;\nVous avez de fortes capacités d'analyse et de réflexion et savez également être dans l'action et orienté.e résultats ;\nVous êtes curieux, autonome et agile;\nVous devez avoir de fortes capacités d'analyse et de réflexion, mais également être dans l'action et orienté.e résultats;\nExcellente maîtrise du français et de l'anglais, à l'oral comme à l'écrit;\nBonne connaissance de Python, SQL, entrepôts de données, systèmes distribués, Cloud;\nProcess de recrutement\nTo help you get a complete picture of our hiring process and Wakam's work culture, please visit our dedicated page: Interviews at Wakam\nÉchange avec Jade, Talent Acquisition\nÉchange avec Mariana Gherghina (Senior Data Engineer) et Simon Pichon (Engineering Manager)\n=> Welcome @Wakam\nPositive energy, agility, and team spirit are essential to support Wakam in its hyper-growth!\nYou have the Wakam mindset? Join us!\nMore about us\nOur culture?\nFree to impact\n. A culture where everything is possible, where all ideas are taken into consideration, where everyone has an impact on the transformation of insurance! Hungry for freedom? Thirsty for autonomy? If you are adventurous and like challenges, then the Wakam adventure might be made for you!\nDiscover on our website who we really are with the 11 cultural markers that so well describe us!\nWhat we are looking for ?\nMindset compatibility with our 'Free to Impact' culture:\nThink big\nBiased for action\nCurious and eager to learn\nCan say no and find solutions\nAims for the moon (but please don't stick on the moon)\nAnd above all: have fun working together 🤜🤛 !\nGood to know !\nWakam is not based on a hierarchy but on a methodology where everyone finds their role and knows their objectives.\nWith a flat hierarchical system and a highly collaborative operating model, Wakam is an extremely agile and transparent company.\nEvery last Friday of the month, it's Free.day @Wakam, a day without meetings to take a step aside and dedicate ourselves to skills sponsorship or other activities (because we are curious, I remind you).\nFull-remote is a reality at Wakam (there is even one Wakamee who works from his sailing boat ⛵) with our Wakam From Anywhere (WFA) program.\nLast but not least : we are nice and we have fun! (you'll find out by yourself 😉)\nAt Wakam, we are committed to fostering an inclusive environment where diversity is celebrated. If you require any reasonable adjustments during the recruitment process, please feel free to reach out to your recruiter.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full",
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Beelix",
        "location": "Mougins, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-cloud-h-f-at-beelix-3909193730?position=9&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=%2Bx0Epim5PeLAZ9pvSZREUQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants:\n🚗Automobile\n⚡Energie\n📡Médias & Télécoms\n👗Luxe & Retail\n💶 Banque, Finance & Assurance\n✈️Défense\nAujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.\nDans le cadre de notre développement, nous recherchons un Data Engineer Cloud (H/F) en région PACA.\nQuelles missions au quotidien ?\nConcevoir, développer et déployer des pipelines de données fiables et évolutifs sur GCP.\nCollaborer avec les équipes métier pour comprendre les besoins en données et fournir des solutions adaptées.\nOptimiser les performances et la disponibilité des solutions de données sur GCP.\nMettre en œuvre des pratiques de sécurité des données et assurer la conformité aux réglementations.\nTravailler en étroite collaboration avec les équipes de développement dans un environnement agile pour fournir des solutions dans des délais serrés.\nExpertise souhaitée\nExpérience significative dans le développement et la gestion de pipelines de données sur GCP ou autre plateforme cloud.\nMaîtrise des outils GCP tels que BigQuery, Dataflow, Pub/Sub, et Cloud Storage.\nSolide expérience en Python, Java ou Scala.\nCompréhension des principes de l'ingénierie des données, du traitement des données en continu et des entrepôts de données.\nCapacité à travailler de manière autonome et en équipe, avec d'excellentes compétences en communication.\nA propos de vous ?\nDiplômé d'une école d'ingénieurs ou équivalent\nAu moins 3 ans d'expérience en tant que Data Engineer\nExpérience en mode de Delivery Agile (Scrum, Kanban, etc.…)\nVous avez un bon niveau d’anglais tant à l’écrit qu’à l’oral\nPourquoi nous rejoindre ?\nUn suivi et un accompagnement au quotidien\nUn organisme de formation certifié Qualiopi, un abonnement linkedin learning pour chaque salarié et des partenariats avec des spécialistes pour d’autres expertises\nDe nombreux événements : Afterworks, Communautés métiers, Happy talks…\nune Expérience personnalisée basée sur vos besoins grâce au Prédictive Index\nNotre package « unBeelievable » : 100% du titre de transport, Tickets restaurants, CSE, Prime de participation...\nNombreux évènements (afterworks, sport, etc) et des communautés métiers dynamiques\nLe processus de recrutement ?\nÉchange téléphonique (15 min)\nEntretien 1 RH pour apprendre à vous connaître\nEntretien 2 avec votre futur N+1 pour appréhender la relation managériale\nEntretien 3 avec un Responsable commercial pour avoir la vision stratégique\nLocalisation : Mougins, format hybride\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride"
            ],
            "TypeContract": "",
            "Salary": "100",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Partoo",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-analyst-paris-f-h-x-cdi-at-partoo-3892387658?position=10&pageNum=12&refId=a575NEEXj%2F5ZYoAYMJyABw%3D%3D&trackingId=1AWLZFifjjDn5cL6tE%2Bgvw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Partoo, who are we? 👀\nPartoo est une scale-up saas B2B qui a à cœur d’aider les commerces locaux, grandes entreprises ou PME à se rapprocher de leurs clients. Pour cela, ils ont développé une plateforme tout-en-un et différentes solutions qui s’articulent autour de 3 propositions de valeur : Get found, Get chosen & Get clients.\nÀ travers ces 3 propositions, ils ont développé plusieurs produits qui s’adaptent aux évolutions du parcours d’achat des clients :\n🔎 Get found\nPresence: Synchroniser les informations des magasins sur les principales plateformes (Google, Facebook, Waze, etc.), annuaires et GPS\nStore Locator: Aider les clients à trouver le magasin qui leur convient grâce à des données locales actualisées et des filtres dédiés sur les sites web des enseignes\nRéseaux sociaux: Gérer les publications sur Facebook, Google, Instagram, etc\n🎯 Get chosen\nReview: Centraliser, répondre et analyser les avis clients reçus sur Google et Facebook\nBooster: Obtenir des avis positifs supplémentaires sur Google par le biais de SMS et de QR codes\n🤗 Get clients\nMessages: Centraliser et répondre à tous les messages de chat reçus via Google Business Messages, Messenger et bientôt aussi via Instagram, whatsapp, etc. (templates messages, conversations starter, appels manqués...)\nQuelques chiffres 🗝️\n> Un label Happy at Work et l'une des meilleures notes Glassdoor de l'écosystème avec 4.6/5 pour plus de 260 avis⭐️⭐️⭐️⭐️⭐️ ️️️️️️\n> 450+ employés heureux, 37 nationalités différentes, des bureaux à Paris et Barcelone 🚀\n> Ils gèrent 300 000 points de vente et travaillent de manière transversale avec +1000 chaînes (Carrefour, Generali, Toyota, Décathlon, Leroy Merlin etc.) et +6000 pme dans environ 150 pays\nNotre mémo 2024: le mot du CEO (https://www.partoo.co/fr/blog/memo-2024/)\nIMPACT 💥\nPartoo compte aujourd’hui pas moins de 400 collaborateurs, qui œuvrent au quotidien à maintenir une croissance saine, en phase avec les enjeux et challenges économiques du moment.\nUne des composantes clefs pour y parvenir réside en notre capacité à développer et maintenir un haut niveau d’efficacité opérationnelle. Dans cette logique, améliorer notre capacité à exploiter et utiliser la donnée présente dans nos systèmes est indispensable. Si nous avons déjà une équipe Data en place, celle-ci est aujourd’hui mobilisée presque exclusivement sur les thématiques data relatives au fonctionnement de notre application ainsi qu’à la construction d’éléments de visibilité pour nos clients.\nNous souhaitons donc recruter un Data Engineer & Analyst dont l’objectif principal sera de permettre aux équipes Opérations et client-facing de visibiliser et tirer le meilleur parti d’une donnée aujourd’hui difficile d’accès.\nManager : Adel Adman (cc. Clément Bouillaud, en charge de la team Operations)\nTEAM 💙\nMeetings récurrent avec les membres de Partoo :\nMembre à part entière de l’équipe Data (elle-même intégrée dans l’équipe Produit), tu seras néanmoins en contact régulier avec les équipes Opérations, qui seront tes principales interlocutrices.\nEn d’autres termes, tu seras le pilier central entre les équipes Ops et Data.\nDans un premier temps, tu auras un meeting hebdomadaire avec Adel (Lead Data) et avec Clément (COO), le temps de cadrer tes premières priorités et de trouver la bonne récurrence de rencontre avec les équipes Opérations.\nMISSIONS 🔥\nTon principal objectif consiste à faire en sorte que chaque personne, des équipes Opérations comme des équipes client-facing, ait accès à la donnée dont elle a besoin, au moment où elle en a besoin, sur le support le plus adéquat. Pour y parvenir, plusieurs missions seront tiennes :\nArchitecture\n:\nCréer des architectures de données robustes et évolutives pour collecter, stocker et analyser de grandes quantités de données provenant de diverses sources (Salesforce, Intercom, Chargebee, back office de Partoo, etc.)\nAnalyser et améliorer continuellement le modèle de données Salesforce (SF), en accompagnant l'équipe Ops dans le monitoring des anomalies et l'optimisation des performances\nIntégrations et flux\n:\nDévelopper et optimiser des pipelines de données, assurant l'intégration fluide des données dans notre Data Warehouse depuis différentes sources, et inversement\nTransformation & analyse\n:\nConcevoir et exécuter des requêtes SQL complexes pour l'analyse de données, permettant de soutenir les décisions business\nIdentifier et construire des KPI cruciaux, fournissant des insights précieux aux équipes business\nVisualisation\n:\nFournir aux équipes Ops et client-facing des outils de visualisation de données (Looker Studio, embedding, etc.), clés dans l'optimisation de notre gestion de clientèle.\nFormation\n:\nFormer les équipes Opérations sur l’exploitation des tables de notre Datawarehouse ainsi que sur l’usage de Looker Studio et propager les principales best practices associées. Tout ça, en collaboration au quotidien avec les équipes Ops !\nDESIRED PROFILE 🎯\nCompétences recherchées :\nUne très bonne connaissance du langage SQL, notamment PostgreSQL et BigQuery.\nMaîtrise du scripting Python et des notebooks pour l'analyse de données\nD’excellentes capacités d'analyse pour comprendre les besoins business, identifier les anomalies dans les données et proposer des améliorations pertinentes\nUne bonne aptitude à manipuler et analyser de grands ensembles de données et en extraire des insights actionnables\nUne très bonne maîtrise d'au moins un outil de business intelligence tel que Looker Studio, PowerBI ou Tableau\nProfils recherché :\nTu as plus de 3 ans d'expérience en Data Engineering /Advanced Data Analysis\nTu maîtrises les stacks de data les plus récentes (dbt, Airflow, Airbyte, etc.) et les meilleures pratiques en matière de données (ETL, reverse-ETL, etc.)\nTu es orienté(e) utilisateur et sais convertir les besoins commerciaux en solutions techniques\nTu sais communiquer avec les équipes et t'assurer que les meilleures pratiques sont adoptées\nTu es un team player !\nTu souhaites apprendre et grandir avec nous\nRECRUITMENT PROCESS 🛠️\nA first video call with Marine, Talent Acquisition Specialist, 45 min\nInterview with Adel, Lead Data Engineer, 1h\nCase Study\nInterview with Clément, Chief Operations Officer, 1h\nÀ compétences égales, ce poste est ouvert aux travailleurs et travailleuses en situation de handicap ou assimilés au sens de l’article L5212-13 du Code du travail. Partoo s’engage en faveur de la diversité, l’égalité professionnelle, l’emploi des travailleurs handicapés.\nWith equal skills, this position is open to disabled workers or those considered to be disabled within the meaning of Article L5212-13 of the French Labour Code. Partoo is committed to diversity, professional equality and the employment of disabled workers.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "DBMS": [
                "PostgreSQL",
                "BigQuery"
            ],
            "Automation": [
                "Airflow"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "fifty-five",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-cloud-engineer-h-f-at-fifty-five-3915044673?position=1&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=19LhPvryuXNgBpkjdc5ogw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Senior Data & Cloud Engineer\nfifty-five est une data-company d'un genre nouveau qui aide les marques à exploiter les données pour améliorer le marketing, les médias et l'expérience client grâce à une combinaison de services de conseil et de technologie spécialisés.\nEn tant que pilier data et marketing du Brandtech Group, nous offrons des services qui combinent le conseil en stratégie, les services de cloud, le conseil en média et l'expérience client.\nfifty-five, c'est plus de 400 experts du numérique. Des digital consultants, des spécialistes du tracking et du média, des ingénieurs et des data scientists, travaillent tous en étroite collaboration pour fournir des conseils marketing de haut niveau et une assistance technique aux marques, dans tout type d'industrie, partout dans le monde.\nPartenaire des annonceurs de la collecte à l'activation et l'exploitation des données, nous aidons les organisations à devenir de véritables entités omnicanales maîtrisant l'efficacité de leur écosystème digital et ses synergies avec le monde physique.\nBasé à Paris, nous opérons sur 3 fuseaux horaires depuis nos 10 bureaux, situés à Paris, Londres, Genève, Milan, Shanghai, Hong Kong, Shenzhen, Taipei, Singapour et New York. fifty-five attache une importance particulière au bien-être de ses collaborateurs, ce qui lui a permis de figurer dans le classement Best Workplaces France en 2018.\nContexte :\nL'équipe d'ingénierie développe et met en œuvre les solutions techniques permettant la réalisation de pipelines de données et l'implémentation de data platform pour nos clients : récupération de datas sur de multiples sources de données (APIs, files, etc.), data cleaning, data processing, automation et monitoring de l'ensemble. L'équipe s'appuie sur des technologies récentes (docker, kubernetes, terraform, notebooks, etc.) et met en place ses projets dans les différents clouds du marché (GCP, Azure, AWS...).\nMission :\nNous sommes à la recherche d'une personne capable de réaliser des projets techniques pour répondre aux besoins de nos clients (par exemple: système de recommandations de produits, détection d'anomalies, ranking). Les activités vont du chiffrage et du sizing technique à la mise en œuvre des architectures, en passant par la revue des spécifications fonctionnelles et la production de code. Le Data & Cloud Engineer sera épaulé par un Lead dans ses missions. Il sera également amené à participer à la R&D et à accompagner les équipes transverses dans la mise en place d'outils de travail internes (librairies pour les data scientists, environnement Notebooks pour les data analysts et data scientists, développement de frameworks sur différents cloud providers, etc.).\nNous souhaitons trouver la bonne personne pour faire évoluer ou créer de nouvelles solutions dans ce cadre. Les missions comprennent aussi bien du prototypage rapide pour des démonstrateurs, que de la production de code robuste qui tourne en production tous les jours.\nCompétences et expériences :\n4-5 ans d'expérience en tant que Data Engineer\nMaîtrise de Python, SQL\nMaîtrise des environnements Cloud. Idéalement certifié GCP, Azure ou AWS\nBonne connaissance de Docker/Kubernetes\nBonne connaissance d'au moins un data warehouse (BigQuery, Snowflake, etc)\nConnaissance autour des Notebooks (Jupyter)\nA l'aise avec des concepts liés aux APIs (OAuth, REST, etc.)\nA l'aise avec les notions d'Infrastructure as Code (Terraform)\nAu courant des pratiques GitOps et connaissances des concepts autour du CI/CD\nLa maîtrise d'un orchestrateur, comme Apache Airflow, est un plus\nEsprit d'équipe (collaborer aux tests unitaires, revue de code, partage de code, sprints)\nBon niveau en français et en anglais\nA déjà travaillé en mode projet avec des interlocuteurs variés (consultant, data analyst, data scientist)\nUne expérience en marketing digital est un plus\nNous proposons :\nun bureau au centre de Paris avec terrasse et jardin\nun environnement multiculturel avec des collaborateurs aux nationalités multiples (France, Royaume-Uni, Etats-Unis, Chine, Tunisie, Italie et plus)\ndes projets avec nos bureaux à Londres, Hong Kong, New York, Shanghai, Genève, Shenzhen et Taipei\ndes TGIF et supers soirées\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "NW",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-nw-3904072453?position=2&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=FFg227%2BWLr13f6Zrujr8iA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Type de contrat :\nCDI\nLocalisation :\nParis, 7ème (présentiel)\nDate de début :\ndès que possible\nExpérience requise :\n1 ou 2 ans d’expérience\nA propos de NW\nNW vise à rendre la transition énergétique accessible à tous. Depuis 2007, le groupe déploie des solutions innovantes pour augmenter la part des énergies décarbonées dans le mix électrique français, soutenir la stabilité du réseau électrique et contribuer au développement de la mobilité électrique.\nPremière licorne française de la transition énergétique, NW est le leader en France du stockage d'électricité avec sa solution JBox® et le précurseur de la recharge haute puissance grâce à ses bornes IECharge®. L'entreprise se développe également à l'international, en particulier en Europe et aux Etats-Unis. En février 2023, NW a rejoint la FrenchTech Next40.\nDans le cadre de notre forte croissance, nous recherchons un(e) Data Engineer pour rejoindre notre équipe tech. De formation Bac+5 Ingénieur ou similaire, vous souhaitez contribuer dans le domaine des énergies renouvelables, sur un poste à fort impact et au sein d’une jeune équipe et engagée.\nVos principales missions\nMettre en place des outils de traitement et stockage de données\nBenchmarking des solutions les plus adaptées aux besoins des équipes\nAssurer la sécurité de l’architecture de données\nParticiper à la sélection de la stack technique\nActualisation et adaptation des solutions utilisées selon l’évolution des technologies\nAssurer la gestion des coûts liés aux besoins data avec les équipes internes\nSupport technique aux utilisateurs internes\nImplémenter les processus de stockage et préparation des données\nConfigurer la connexion aux APIs internes et externes\nFormation des équipes internes sur les sujets data\nVos compétences\nVous êtes intéressé(e) par les énergies renouvelables\nVous avez une réelle aisance relationnelle et une bonne capacité rédactionnelle\nVous avez la volonté de rejoindre une entreprise en pleine croissance avec un projet de développement ambitieux\nVous assurez la conversion des données\nVous maîtrisez de l’automatisation de la CI/CD\nVous parlez l’anglais couramment\nVotre profil\nIdéalement, vous avez 1 ou 2 ans d’expérience dans un poste similaire\nVous avez un Bac +5 Ingénieur ou similaire\nVous êtes curieux(se), rigoureux(se), proactif(ve) et autonome\nTech stack\nPython\nDocker\nKubernetes\nApache Kafka\nApache Flink\nGithub Actions\nApache Iceberg\nPourquoi NW ?\nDécouvrir le secteur de la mobilité électrique, du stockage d'énergie et du développement de projet dans une entreprise\nRejoindre une équipe dynamique, positive, engagée\nParticipez activement aux défis majeurs de la transition énergétique et de la décarbonisation des énergies\nEntreprise en pleine croissance, possibilité d’avoir un impact important dans la valorisation de l’entreprise\nProcessus de recrutement :\nEntretien RH\nEntretien manager\nTest technique\nEntretien fit équipe\nNW Groupe est un employeur garantissant l'égalité des chances. NW Groupe célèbre la diversité et s'engage à fournir un environnement de respect mutuel où toutes les décisions de recrutement sont basées sur les qualifications, le mérite et les besoins de l'entreprise.\nOrganisation et méthodologies ✅\nNous travaillons en Squad sur le principe du roulement de projet avec pour but de participer activement à la réflexion et au développement de chacune des applications de l'entreprise.\nProjets et défis techniques 💻\nNos outils sont développés en interne et permettent de développer, installer et superviser plusieurs centaines de sites de stockage d'énergie. Chaque jour, ces outils permettent d'optimiser la gestion de notre activité et d'accélérer la transition énergétique visant la décarbonisation des énergies.\nRecherche et Développement 🔍\nNous travaillons activement sur l’amélioration continue des concepts et des solutions existantes autour de la transition énergétique. Nos équipes s'occupent non seulement de la conception et de la l'amélioration de ces systèmes mais aussi de l'exploration de futures opportunités dans le secteur.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Flink"
            ],
            "DevTools": [
                "Docker"
            ],
            "SoftBigDataProcessing": [
                "Apache Kafka",
                "Apache Flink"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "UpMan Consulting",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-syst%C3%A8me-r%C3%A9seaux-donn%C3%A9es-h-f-at-upman-consulting-3901572843?position=3&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=5F%2FT5Lsts3uu%2FuZVqqv4gQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Cette offre d’emploi est fournie par Pôle emploi\nDescription\nDescriptif du poste: C est maintenant l occasion pour toi de rejoindre UpMan Consulting Notre ambition est de trouver les meilleurs profils Data Engineer pour intervenir chez nos clients grands comptes de la métropole lilloise. On te propose une expérience professionnelle en adéquation avec ce que tu souhaites réellement. Tu découvriras une ambiance de travail saine & bienveillante, tu participeras activement au développement d une Happy StartUp, actuellement en forte croissance. Où convivialité rime avec efficacité & où ta performance individuelle contribue à notre réussite globale. Tes missions / compétences techniques Si tu l acceptes, ton rôle & tes missions seront les suivantes : * Réaliser le processus d intégration de nouvelles données (réflexion sur la solution, mise en place d ETL, règles de nettoyage, anonymisation ) * Être garant de l'accès aux sources de données. * Maîtrise de la donnée et être le garant de sa qualité (référencement, normalisation et qualification) afin d'en faciliter l'exploitation par les équipes (Data Analysts et Data Scientists). * Maîtrise de technologies Big Data et Cloud : Hadoop, Spark * Assurer la supervision et l'intégration de données structurées et non structurées venant de sources multiples, tout en veillant à garder des données de qualité. * Assurer le suivi, la cartographie et la documentation des données intégrées * Afin de garantir une bonne exécution de ta mission, nous recherchons les compétences techniques suivantes : Langages de programmation : * SQL * Python / Pyspark * Java/Scala (plus rare, mais important) Différents types d ETL & orchestrateurs : * Airflow * Dagster * Prefect * SSIS/informatica * Talend Plateformes cloud : * GCP * Microsoft Azure * AWS Base de données relationnelles & NoSQL : * postgreSQL, MySQL,... * Redis, graphDB * Data warehouse/data transform : * Snowflake, Bigquery (très important) * DBT Qualité & compétences nécessaires * Communiquant.e dans l âme * Avoir une bonne capacité de synthèse & l esprit critique * Travail d équipe * Curiosité aiguë * Comprendre les objectifs & les besoins Nice to have * Anglais courant * Connaissance de la méthodologie DevOps * Notions en Data-science The office Pas de full remote (pour l instant) mais de l hybride dans la plupart des missions. En moyenne, 2 jours de télétravail par semaine. Cependant, les portes de nos bureaux à Wambrechies sont toujours ouvertes pour accueillir nos collaborateurs pendant leurs journées de télétravail & passer une bonne journée tous ensemble ! Le salaire Junior : 30K à 36K Maîtrisant : 37K à 43K Expert : 44K à 50K & plus + notre package avantage Profil recherché: Ton Profil Tu es une personne passionnée & passionnante. Tu as envie d'évoluer, de partager, de participer à une mission collective & découvrir LA nouvelle façon de collaborer avec une ESN made in Lille. Tu peux justifier d'une expérience forte & significative en tant que Tech Lead Java, dans le développement Java ! Pas besoin d'avoir trop ou pas assez de diplômes, chez nous, ce sont les compétences qui priment  ! On se rencontre, on discute, on échange sur tes envies professionnelles & on laisse la magie opérer. L'envie de grandir & de monter en compétences est ton moteur au quotidien. Tu aimes les problématiques complexes et les défis technologiques. On dit de toi que tu es un.e agiliste dans l'âme, qui effectue une veille constante, à l'affût de tout ce qui évolue autour de toi... Ne réfléchis plus, saute le pas & découvre UpMan Consulting, tu ne seras pas déçu. Tu balances ta démission ?\nPROFIL SOUHAITÉ\nExpérience\nExpérience exigée de 1 An(s)\nSource: Pole emploi (https://www.pole-emploi.fr)\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Remote",
                "Full",
                "Junior"
            ],
            "TypeContract": "",
            "Salary": "30K, 1",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake",
                "PostgreSQL",
                "BigQuery",
                "MySQL"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Apside",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-gcp-f-h-at-apside-2859485219?position=4&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=wv7OLTbzzRJCbMwFBnb29w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre une entreprise apprenante ? Engagée pour t’accompagner dans ton évolution professionnelle et dans tes projets personnels ?\nRejoins Apside Paris pour travailler sur nos projets de demain !\nLe poste :\nTu seras amené à participer à la migration des données et le traitements Big Data depuis un cluster Hadoop interne vers l'infrastructure Google Cloud Platform.\nDans ce sens, tes missions seront les suivantes :\nParticipation aux chantiers de cadrage de la migration\nContribution à la mise en place des environnements et outils de déploiement automatisés\nAccompagnement et formation des équipes à l’outil GCP\n...\nEnvironnement technique :\nJira Big data\nCloud GCP\nHadoop\nKubernetes\nSpark, Kafka, Python\nToi ?\nTu as déjà participé à un projet de\nmigration Google Cloud Platform (GCP)\n?\nTu es\nrigoureux\n,\nbon communiquant\n?\nTu souhaites participer à un\nprojet d’envergure associant cloud et Big Data\n?\nAlors ce poste de\nData Engineer GCP\nest fait pour toi !\nEt la suite ?\nTu rencontres d’abord l’équipe RH pour parler de tes attentes, ton projet, ton futur !\nPuis les managers pour parler concret : missions, projets, parcours de carrière, et bien sûr salaire et avantages :)\nEt tu discutes avec un de nos Tech Leads, pour évaluer tes compétences et te challenger.\nTu souhaites donner un nouvel élan à ta carrière ? Rejoins la vie Apsidienne !\nPour en savoir plus à www.apside.com\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Salaire",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Sopra Steria",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-spark-scala-services-financiers-ile-de-france-at-sopra-steria-3913390665?position=5&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=jSrg1Q0BPIqKrW7h8N2KDQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Company Description\nSopra Steria\n, acteur majeur de la Tech en Europe, reconnu pour ses activités de conseil, de services numériques et d’édition de logiciels, aide ses clients à mener leur transformation digitale et à obtenir des bénéfices concrets et durables. Il apporte une réponse globale aux enjeux de compétitivité des grandes entreprises et organisations, combinant une connaissance approfondie des secteurs d’activité et des technologies innovantes à une approche résolument collaborative.\nSopra Steria place l’humain au centre de son action et s’engage auprès de ses clients à tirer le meilleur parti du digital pour construire un avenir positif.\nFort de 56 000 collaborateurs dans près de 30 pays, le Groupe a réalisé un chiffre d’affaires de 5,8 milliards d’euros en 2023.\nThe world is how we shape it\nJob Description\nVotre futur environnement de travail :\nChez notre client, grande banque commerciale française, vous intégrez nos squads pour intervenir sur des projets de grande envergure, des projets transformants et structurants de la banque de demain. Vous interviendrez sur la gestion d’applications sur des domaines tels que : la conformité, la fraude, la lutte anti-blanchiment, les risques de crédit, la trésorerie, les paiements, les financements structurés ou encore le réglementaire bancaire.\nAu sein de notre Data Factory, vous êtes pleinement impliqué(e) dans toutes les phases de nos projets pour le compte de grands clients, contribuant ainsi à leur succès. Vous avez l'occasion de développer vos compétences techniques et fonctionnelles de manière approfondie, tout en travaillant sur des projets exigeants et passionnants pour le compte de grands clients du secteur bancaire.\nVotre rôle et vos missions :\nVous avez pour rôle la mise en place de pipelines de données fiables, sécurisés et à l’échelle pour soutenir la mise à disposition des données aux cas d’usage métier qui en ont besoin.\nVos activités principales sont les suivantes :\nVous travaillez avec le client pour évaluer, concevoir, déployer, améliorer et maintenir les pipelines de données;\nVous vous assurez que les pipelines de données créés sont résilients, sécurisés et accessibles;\nVous définissez le modèle opérationnel pour monitorer et supporter les pipelines de données\nVous fournissez une expertise à nos clients sur leurs données pour assurer leur optimisation et leur sécurité par rapport à leurs besoins;\nVous apportez un savoir en gestion de la qualité et la gouvernance de la donnée pour assurer le suivi de la conformité à la gouvernance de la donnée\nVous faites de la veille technologique dans le domaine afin d’enrichir les roadmaps technologiques et fournir des solutions modernes à nos clients.\nQualifications\nVotre profil :\nDe formation Master 2 Ecole d'Ingénieurs ou Informatique, ou équivalent, vous justifiez d'une expérience technique de 3 ans minimum et souhaitez évoluer rapidement dans un contexte motivant. Vous avez ces compétences requises :\nMaîtrise des technologies de bases de données Relationnelles et NoSQL\nMaîtrise d’au moins un outil d’ETL/ELT (Informatica, datastage, etc.)\nMaîtrise des technologies de traitement distribué de données (spark, scala, Hadoop)\nMaîtrise d’au moins un framework de streaming de données (Kafka, RabbitMQ, etc.)\nMaîtrise de chaines CI/CD et de des bonnes pratiques de DataOps\nMaîtrise de solution de Vitrtualisation de données (Denodo, Dremio, etc.)\nMéthodologie Agile Scrum\nAnglais professionnel\nVous êtes attiré(e) par le monde du numérique, le Cloud (maitrise d'un environnement public ou privé est un plus) et des technologies innovantes.\nVous avez un bon esprit d'analyse, êtes curieux(se) et passionné(e) et vous avez le sens du travail en équipe.\nAdditional Information\nCe que nous vous proposons :\nUn accord télétravail pour télétravailler jusqu'à 2 jours par semaine selon vos missions.\nUn package avantages intéressants : une mutuelle, un CSE, des titres restaurants, un accord d'intéressement, des primes vacances et cooptation.\nUn accompagnement individualisé avec un mentor.\nDes opportunités de carrières multiples : plus de 50 métiers, autant de passerelles à imaginer ensemble.\nPlusieurs centaines de formations accessibles en toute autonomie depuis l'app mobile avec Sopra Steria Academy.\n.La possibilité de s'engager auprès de notre fondation ou de notre partenaire « Vendredi ».\nL'opportunité de rejoindre le collectif Tech'Me UP (formations, conférences, veille, et bien plus encore).\nEmployeur inclusif et engagé, notre société œuvre chaque jour pour lutter contre toute forme de discrimination et favoriser un environnement de travail respectueux. C’est pourquoi, attachés à la mixité et à la diversité, nous encourageons toutes les candidatures et tous les profils.\nhttps://www.soprasteria.fr/nous-connaitre/nos-engagements\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "50",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala"
            ],
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "scient",
        "location": "Aix-en-Provence, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-spark-hadoop-aix-en-provence-f-h-syst%C3%A8me-r%C3%A9seaux-donn%C3%A9es-h-f-at-scient-3904578388?position=6&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=J%2B0%2BLvS4df0oncADPyQXRg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Cette offre d’emploi est fournie par Pôle emploi\nDescription\nDescriptif du poste: Votre travail quotidien consistera à : * Concevoir des modèles efficaces pour stocker et analyser des téraoctets de données. * Mettre en œuvre des flux d'acquisition et de transformation complexes * Construire des modèles de données intelligents pour servir nos équipes produits et nos équipes BI, Insights et data science tout en minimisant les coûts. * Développer des outils pour aider nos data scientists à industrialiser les projets d'apprentissage automatique. * Travailler sur la qualité et la fiabilité des données pour garantir que nous fournissons des mesures fiables à l'ensemble de l'entreprise. * Développer des outils pour aider nos scientifiques à industrialiser les projets d'apprentissage automatique. * Développer notre plateforme de science des données * Industrialiser les projets d'apprentissage automatique avec les scientifiques spécialisés dans les données. Les enjeux * Data powerBI sur un data center -> doit être migré dans 1 autre serveur * Travailler sur une infra hadoop / migration de MariaDB à techno GPAS. * Aujourd'hui reçoit des flux de fichiers avec un ETL et met dans sa base MariaDB. L'environnement va être remplacé en plus du changement de serveur. L'ETL fait dans MariaDB va devoir être recréé dans GPAS Profil recherché: Votre profil : * Avec un minimum de 3 ans d'expérience, vous avez une parfaite connaissance des Data Engineering, des technologies et vous maîtrisez python (idéalement avec plusieurs autres langages backend et vous connaissez bien les meilleures pratiques de développement logiciel, telles que CI/CD, tests unitaires, QA et création de mocks...). * Excellent esprit d'équipe et à l'aise pour interagir avec les parties prenantes techniques et commerciales. * Habitué à travailler dans un environnement agile et à accepter les changements de priorités. * Curieux, humble et faisant preuve d'un équilibre entre créativité et pragmatisme. * Grande volonté d'apprendre et d'enseigner aux autres * Maîtrise de l'anglais (écrit et parlé) Compétences techniques : * Maîtrise de Python, Scala, Spark et PostgreSQL * Maitrise Pyspark, Hive et Hadoop * Vous êtes curieux, autonome, rigoureux et proactif, vous souhaitez rejoindre une équipe passionnée d'informatique, d'IA et d'innovations et que vous maîtrisez les compétences nécessaires.\nPROFIL SOUHAITÉ\nExpérience\nExpérience exigée de 1 An(s)\nSource: Pole emploi (https://www.pole-emploi.fr)\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DBMS": [
                "PostgreSQL"
            ],
            "Other": [
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Pictarine",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-pictarine-3911913926?position=7&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=U7AF5ggHYB4XAUH2FvxmZQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Mission and challenges 🎯\nSi tu es enthousiaste à embarquer dans la nouvelle équipe data de Pictarine pour la faire rayonner avec tout ton savoir-faire, alors c’est l’aventure qu’il te faut! 🏔️\nAvec plus de 1K tables, 2M de clients et 4M de commandes en 2022, les équipes de Pictarine ne sont jamais à court d’idées pour explorer de nouveaux horizons. 🚀\nEn tant que Data Engineer chez Pictarine tu vas pouvoir utiliser toutes tes compétences SQL pour garantir la qualité de la data sur GCP, accompagner et challenger les besoins data.\nTu évolueras au sein de l’équipe Engineering, composée des pôles dev & data.\nTon rôle comprendra les aspects suivants 👇🏻\nTu es garant de la qualité de la data !\nEn simplifiant la structure de la data et réduisant le nombre de tables\nEn transformant les données pour les rendre facilement utilisables\nEn orchestrant le flux des données de manière continue et automatique\nTu accompagnes et challenges les équipes de Pictarine !\nEn co-construisant des solutions data appropriées\nEn élevant le niveau de jeu des méthodes data existantes\nEn faisant rayonner la data autour de bonnes pratiques et d’outillages adéquates\nProfil Recherché\nAbout you 💎\nTu as au moins 5 ans d’expérience sur un poste similaire\nTu maîtrises le data warehouse BigQuery et son langage SQL\nTu es à l'aise avec les services GCP\nTu as de bonnes connaissances dans la conception de modèles de données et les stratégies d'optimisation des requêtes SQL\nTu as des compétences en DevOps pour le déploiement et la gestion efficace des pipelines de données\nTu as une bonne maîtrise de Python & Github\nTu es organisé, rigoureux et portes une grande attention aux détails\nTu es doté d’excellentes qualités relationnelles, de communication et de vulgarisation\nTu as une passion pour résoudre des problèmes business avec la programmation\nTu es curieux de tester des nouvelles technologies\nTu es un team player et toujours à l'affût de nouvelles idées\nWork @ Pictarine✨\nUn environnement de travail agile, collaboratif, international et multiculturel\nDes perspectives d’évolution rapides\nDes locaux tout beaux à Labège avec du matériel dernier cri (mais aussi des snacks à profusion et un frigo à boissons toujours bien rempli)\nUn apprentissage permanent : conférence, meet-up, Pictarine Academy, cours d’anglais.\nDes events tous les mois : massage, pilates, TGIF, team building .\nUn environnement de travail flexible : horaires, politique de remote hybride.\nUn package de rémunération attractif : salaire compétitif, RTT, mutuelle & prévoyance 100% prise en charge, intéressement.\nDes petits + : Développement de photos gratuit, subvention sport, 3 jours “entraide familiale”, jours de congés en plus avec l'ancienneté... 🤫 on ne te dévoile pas tout !\nRecruitment process ⚙️\nTu souhaites nous rejoindre ? Viens rencontrer les gens avec qui tu vas bosser :\n1er échange pour apprendre à se connaître avec Marie - Engineering Manager Data (15’)\nEntretien Manager avec Marie (60-90’)\nTest pratique afin de nous montrer tes talents 🙂 (3 heures)\nEntretien final avec 2 membres du Codir (90’)\nWelcome aboard !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "100, 100",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Other": [
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "NEXTON",
        "location": "Lyon, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/d%C3%A9veloppeur-big-data-spark-at-nexton-3911787310?position=8&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=gspnV%2B2uQbImXFxdlEEzMQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Mission (fiche métier)\nNEXTON recrute un\nDéveloppeur Big Data - Spark\n, en CDI, à\nLyon\n!\nQui sommes-nous ?\nNEXTON c’est avant tout une entreprise qui accompagne ses clients dans leur transformation digitale. Tous les jours, nous travaillons avec des grands comptes et des pures players (SNCF, Orange, BNP PARIBAS…).\nNous sommes experts du digital aussi bien sur de l’accompagnement stratégique qu’opérationnel.\nFort du succès, Nexton connaît aujourd’hui un développement significatif, autour de ses valeurs piliers : cohésion, confiance et performance.\nEt pour toi ? Notre politique de développement des compétences dynamique saura te séduire avec un programme de suivi de carrière sur-mesure.\nLe contexte :\nPour l'un de nos clients, dans le secteur de l'énergie, nous sommes à la recherche d'un développeur Big Data.\nLes missions :\nApporter une expertise\nBig Data\npour faciliter la manipulation des données.\nDéfinir les solutions techniques permettant le traitement massif des données.\nMettre en place des\nsolutions de stockage de données\n(SQL, NoSQL etc.)\nVeiller la sécurisation et la clarté des pipelines de données pour faciliter\nl'analyse\net la\ntransformation\n.\nAssurer la création, la maintenance, l'optimisation et la sécurité des bases de données.\nAssurer le support aux équipes de\ndéveloppement\nafin d'identifier et proposer des solutions performantes.\nProfil (fiche métier)\nDe formation supérieure, tu justifies d'une expérience d'au moins\n4 ans\ndans le domaine.\nTu maitrises\nSpark\n,\nPython\net\nSQL\n.\nTu es\nautonome\n,\nrigoureux\net\nforce de proposition\n.\nDe plus, tu as acquis une\ncapacité d'analyse\net de\nsynthèse\ngrâce à tes différentes expérience.\nTu maitrises également les fondamentaux de\nl'agilité\n.\nEnfin, ton\nesprit d'équipe\nte permet de communiquer et de travailler dans les meilleures conditions.\nNEXTON c’est aussi et surtout de nombreux moments de rencontres tout au long de l’année :\n- Des communautés : 2 Meet Up par mois pour partager et échanger avec des experts\n- De nombreux moments de rencontres professionnels et extra professionnels tout au long de l’année\n- Des moments privilégiés avec ton manager\nPrêts à nous rejoindre ? Rencontrons-nous !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "big data developer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Sibylone",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-talend-spark-scala-msbi-at-sibylone-3918822674?position=9&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=GQ2byteWKyn3cPr%2FE6GnrA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "SIBYLONE\n, société de conseil spécialisée dans les systèmes d’information de synthèse et de pilotage, aide ses clients à tirer toute la valeur de leur patrimoine de données, levier stratégique majeur de développement et de rentabilité.\nNotre ambition : rendre les différents acteurs de l’entreprise autonomes dans l’exploitation des données, libérer les usages Métier, pour qu’ils soient en mesure de relever les défis de performance, de couverture de risque, de financement, de conquête client, de RSE… qui s’imposent à eux.\nSpécialistes reconnus, nos consultants s’appuient pour cela sur une connaissance approfondie de l’activité business de nos clients, en lien avec nos trois piliers que sont le Métier, la Data et le Projet.\nSIBYLONE emploie 250 salariés et réalise un CA de 30m€ dans la prestation de services auprès de grandes entreprises (8 grands comptes représentant 80% du CA). SIBYLONE est une filiale du Groupe Smart 4 Engineering créé en 2020. Le groupe s’est constitué en procédant à l’acquisition de 12 sociétés en France, en Italie, en Espagne et au Portugal dans le domaine de l’ingénierie. Avec nos 3,000 ingénieurs / consultants hautement qualifiés, le Groupe offre ses services dans les domaines très porteurs du Digital, de la Data, de l’Intelligence Artificielle, de la Cybersécurité, du Cloud.\nNous recherchons pour l’un de nos clients du domaine bancaire :\nUn.e Data Engineer\nLe Data Engineer intégrera une équipe projet Big Data dont l’objectif premier est de conduire des projets ayant traits à des problématiques d’architecture et de conception.\nLe Data Engineer sera en charge de la maintenance, du support et de l’évolution d’un outil de pilotage financier déployé au sein des directions centrales du groupe et de la facturation interne. Il participera notamment à la conception, la construction, le déploiement et le maintien en production d’architectures Big Data, ces dernières ayant pour objectif de permettre tant l’évolution que l’optimisation du système d’information décisionnel existant.\nMissions\nAnalyser, comprendre et cadrer une architecture permettant de répondre aux besoins métiers des clients\nConcevoir et mettre en place des plateformes Data en tenant compte des contraintes tant techniques que fonctionnelles\nIntervenir sur la conception et le déploiement d’environnements\nDéveloppement de pipelines d’ingestion et de préparation\nGestion du stockage de données (systèmes de fichiers comme HDFS, bases SQL ou NoSQL)\nAlimentation d’entrepôts de données (Hive, Impala, Hbase, Snowflake, BigQuery, …)\nDévelopper des applications d’exploration et de manipulation de données (SPARK / pySpark, Scala) afin d’alimenter les flux sortants, les reporting et d’exposer les données\nEvoluer sur l’ordonnancement des traitements de données (Oozie, Bash / Shell)\nAssurer le maintien en conditions opérationnelles des plateformes produites\nEtablir, formaliser, et promouvoir les best practices\nPourquoi pas vous ?\nProfil recherché :\nDe formation supérieure ingénieur en Informatique, vous justifiez d’une première expérience réussie en data engineering acquise dans un contexte projet au sein d’une start-up, d’un pure player, ou d’une ESN.\nVous disposez d’une bonne maitrise des langages propres aux environnements Big Data tels que :\nHadoop\nTalend (Data Integration, Big Data)\nLes solutions Cloud (Azure, AWS, GPC)\nSpark, Scala, Python, Unix, SQL\nMicrosoft Power BI\nUne connaissance de : Docker, ELK, Kubernetes, Cassandra, Kafka, … serait un plus, de même que des fondamentaux DevOps (CI / CD).\nVous avez déjà évolué dans un contexte projet agile ou scrum et faites preuve de flexibilité, d’adaptabilité et savez être force de proposition.\nAu-delà de vos compétences techniques, vous êtes curieux, autonome, organisé, doté d’un bon sens relationnel et d’un esprit de synthèse.\nVous vous reconnaissez dans la description du poste ?\nVous souhaitez travailler dans un environnement stimulant et dynamique ?\nVous souhaitez rejoindre une société ambitieuse ?\nVous souhaitez comprendre l’origine de Sibylone ?\nVenez-nous rencontrer :\nLa Team Talent Acquisition sera ravie d'échanger avec vous !\nCe poste est ouvert aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Bash",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "Cassandra",
                "HBase",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "SoftBigDataProcessing": [
                "Cassandra",
                "HBase"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "CI / CD",
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Amiltone",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-amiltone-3846492584?position=10&pageNum=15&refId=3Lir5Tz0viOiyHjhQOdm%2FQ%3D%3D&trackingId=nJmMAJWbmBew3htigNQ6mA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nNous sommes passionnés par les nouvelles technologies, et vous ?\nRejoindre Amiltone, c’est intégrer des équipes dynamiques et soudées dans le cadre de projets novateurs et ambitieux. Nous relevons les challenges techniques de nos clients et les accompagnons dans leur transformation digitale.\nPourquoi choisir Amiltone ?\nAmiltone, plus qu’une entreprise, un état d’esprit !\nNotre objectif ? Votre épanouissement professionnel !\nNous Avons à Cœur De\nVous accompagner au mieux au travers d’un suivi personnalisé\nVous faire monter en compétences en vous proposant des formations tout au long de votre carrière\nComprendre vos besoins et respecter nos engagements\nVous proposer des missions de qualité avec des technologies innovantes\nCultiver votre potentiel grâce à notre programme de développement personnel Addvise\nVotre bien-être passe aussi par des activités extraprofessionnelles, c’est pourquoi nous vous proposons des séances sportives animées par nos coachs, soirées pour se retrouver et animations (à l'agence ou en visio), Gaming nights…\nLes Missions D'un Amiltonien\nEn tant que Data Engineer\n(H/F)\n, vous serez en charge des missions suivantes :\n– Concevoir et développer les futures fonctionnalités de la plateforme Big Data sous Google Cloud Platform.\n– Concevoir les flux d'alimentation et les tables (structure de donnée).\n– Automatiser et industrialiser les flux.\n– Assurer le run applicatif, le cas échéant.\nLa Stack Technique\nMaîtrise des langages suivants : SQL, Talend, BigQuery\nConnaissances de Google (GCP)\nNotion de programmation fonctionnelle\nLe Profil D’un Amiltonien\nDiplômé Bac+4/5 (Ecole d'ingénieur/Master), vous disposez de 2 années d'expérience dans le développement de data.\nToujours sur le qui-vive des nouveautés technologiques, vous êtes force de proposition sur des technos, des outils ou des process qui permettent d'améliorer la qualité du code et la stabilité de nos applications.\nOutre vos compétences techniques, nous nous intéressons également à votre potentiel et votre motivation.\nNos postes sont ouverts aux personnes en situation de handicap.\nPostuler\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    }
]