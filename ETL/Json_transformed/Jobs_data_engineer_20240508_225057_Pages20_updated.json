[
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Boulogne-Billancourt, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-analyst-junior-h-f-at-akademija-oxford-3917871112?position=2&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=4%2Fa7pNvAsIgWJtMpHVaPXA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Notre entreprise partenaire, spécialiste de la vente de médicaments et produits de santé, recherche un apprenti Data Analyst (H/F) préparant un bac +4/+5 en filière Big Data pour la rentrée de Septembre 2021.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data analyst",
        "skills": {
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Ille-et-Vilaine, Brittany, France",
        "link": "https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-at-akademija-oxford-3912800588?position=3&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=oQd1rnnUf9GBIkM2KvjqXw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous souhaitez intégrer une école à taille humaine ? Vous êtes intéressé.es par la donnée ? Sup de Vinci propose une formation complète, en Mastère Big Data & Intelligence Artificielle (2 années en alternance).\nL’école Sup de Vinci, Rennes, accompagne l’une de ses entreprises partenaires dans son projet de recrutement d’un profil ingénieur data, en alternance pour la rentrée 2022.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Boulogne-Billancourt, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-s%C3%A9curit%C3%A9-h-f-at-akademija-oxford-3917866657?position=4&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=LZWNDjODSr2WKASECOYsmw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Une entreprise du secteur de la protection de l’environnement et basée dans les Hauts-De-Seine recherche un.e Data Engineer Sécurité dans le cadre d’un contrat d’apprentissage.\nAu sein de son Data Service, votre rôle sera d’établir la stratégie des architectures data sur les aspects sécurité en lien avec la stratégie globale métier et contribuer à la déclinaison des principes du modèle de sécurité globale.\nVous devrez également élaborer des modèles de référence pour les architectures data, mais aussi contribuer à la déclinaison des politiques de sécurité en standards de sécurité opérationnels.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {}
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Boulogne-Billancourt, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-analyst-at-akademija-oxford-3917868480?position=5&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=vHT%2FnfuS2czWJA7syxxhGQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Nos collaborateurs travaillent au quotidien avec tous les métiers et avec notre réseau international. Et pour être au top de l’innovation, on a ce qui se fait de mieux en matière d’environnement de travail et d’outils technos : datalab, delta room, intelligence artificielle, machine learning…\nRésultats ? Une activité multipliée par 2 en 3 ans (et ce n’est pas fini), une expertise reconnue et une position de leader en France.\nBienvenue au cœur du big bang !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data analyst",
        "skills": {
            "Other": [
                "Machine Learning"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Ille-et-Vilaine, Brittany, France",
        "link": "https://fr.linkedin.com/jobs/view/data-analyst-at-akademija-oxford-3917868488?position=6&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=Hn7b1Ugq%2FuoI3OMUVld89Q%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Diplômé(e) d’un bac+2 dans le secteur informatique et plus précisément dans le domaine du développement.\nDans le cadre d’un contrat d’apprentissage de 12 mois, nous recherchons pour l’un de nos partenaires, un Data Analyst qui souhaite apporter ses compétences techniques tout en faisant preuve d’un excellent savoir-être.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data analyst",
        "skills": {}
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Ille-et-Vilaine, Brittany, France",
        "link": "https://fr.linkedin.com/jobs/view/data-analyst-at-akademija-oxford-3917872056?position=7&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=p0A9v%2BlDKjsXyUwA1W0smA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "L’école Sup de Vinci, Rennes, accompagne l’une de ses entreprises partenaires dans son projet de recrutement d’un profil Big Data en alternance pour la rentrée de Septembre 2021.\nA noter : Sup de Vinci présentera à cette entreprise partenaire uniquement des candidats ayant passé les tests d’admission et qui seront admissibles dans son établissement.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data analyst",
        "skills": {
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bordeaux-at-capgemini-3889788624?position=8&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=4fssEcWN4xK2SLQ4KEHn8w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une\ncommunauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la\nvaleur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.\nVos missions :\nVous êtes passionné par le domaine de la Data, vous souhaitez prendre part à des projets d'envergure, concevoir des solutions, les implémenter et les faire évoluer?\nAlors rejoignez notre équipe Data Engineering Services au sein de Capgemini Cloud Infrastructure Services en tant que Data Engineer.\nVous avez acquis une expérience solide dans le développement, la mise en œuvre et l’optimisation de solutions pour le traitement d'un grand volume de données, vous êtes capable de créer des solutions qui répondent aux besoins métiers et IT, alors rejoignez notre équipe d’experts.\nEn qualité de Data engineer, vos missions sont les suivantes :\n▪ Concevoir et développer des solutions Data/IA.\n▪ Accompagner les Métier dans la compréhension et la mise en œuvre de solution orientées données.\n▪ Collaborer avec les Dev, les Ops, les experts infrastructures dans la construction de solutions et d’infrastructures axées sur les données.\n▪ Gérer un écosystème de partenaires data et assurer un haut niveau d'expertise\n▪ Assurer un rôle de veille technologique sur tous les outils autours de la data, de l’IA et de la BI.\nVotre profil :\nVous êtes issu d’une formation ingénieur ou équivalent bac+5 informatique spécialisée en DATA et vous justifiez d’une expérience de 3 à 5 ans dans un rôle similaire. Expert dans une technologie de base de données relationnelle (PostgreSQL, Oracle...)\nExpert dans une technologie de base NoSQL (MongoDB, Cassandra...)\nVous maitrisez un framework de manipulation de données (Hadoop, Spark, Kafka...)\nVous maitrisez les concepts DevOps et avez de bonnes notions en scripting et développement\nVous avez une expérience des outils BI et de data visualisation (Kibana, PowerBI...)\nLa maitrise de l'anglais est nécessaire.\n3 raisons de nous rejoindre :\nQualité de vie au travail :\naccord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu :\ncertifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, tarif préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culture\nNos engagements et priorités :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nÀ propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "MongoDB",
                "Cassandra",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DBMS": [
                "PostgreSQL"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "SFR",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-sfr-3879318123?position=9&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=9OjeHMY3jduvNS6nVrWxVg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "En tant que Data Ingénieur expérimenté, vous occuperez un rôle essentiel dans notre équipe Data Science.\nVous serez responsable de la conception, du développement et de la maintenance des pipelines de données ainsi que de l'intégration de sources de données multiples.\nVotre expertise sera cruciale pour garantir une gestion efficace des flux de données, ainsi que pour faciliter l'analyse et la visualisation des données en plus du support aux data scientists vos missions seront les suivantes :\nArchitecture projet des données\n: Concevoir et développer des architectures projet de données robustes, évolutives et performantes pour intégrer et gérer de grandes quantités de données provenant de sources multiples. Assurer la fiabilité, l'évolutivité et la sécurité des flux de données entrant d’un projet Data Science.\nIntégration des données\n: Élaborer des pipelines de données efficaces pour l'extraction, la transformation et le chargement des données (via notre Framework ELT/ETL interne) provenant de différentes sources. Mettre en place des processus d'intégration automatisés et veiller à la qualité des données.\nGestion des bases de données\n: Concevoir et optimiser des bases de données pour répondre aux besoins analytiques et de reporting. Assurer la performance, la disponibilité et la sécurité des bases de données, ainsi que la gestion efficace des requêtes.\nCollaboration interfonctionnelle\n: Support des Data Scientists, vous travaillerez avec les équipes business pour comprendre leurs besoins et fournir des conseils et des recommandations basés sur les données.\nOptimisation des performances\n: Surveiller et optimiser les performances des pipelines de données, des bases de données et des requêtes. Identifier les goulots d'étranglement et les points d'optimisation, et proposer des améliorations pour garantir des performances optimales.\nSécurité et conformité\n: Veiller à ce que les données soient traitées et stockées conformément aux normes de sécurité et de confidentialité. Mettre en place des mécanismes de sécurité pour protéger les données sensibles et garantir la conformité aux réglementations en vigueur.\nVotre profil :\nVous avez un\nDiplôme universitaire en informatique, en génie logiciel, en science des données ou dans un domaine connexe et vous avez à minima 5 ans d'expérience en tant que Data Ingénieur.\nVous possédez également une solide maîtrise des technologies et des outils suivants :\nHadoop, Spark, SQL, Kafka, GCP BigQuery,\nDe plus vous avez une bonne compréhension des architectures, des modèles et des concepts de base de donnés avec une expérience avancée dans la mise en œuvre de pipelines ETL et dans la gestion de bases de données.\nVos connaissances en matière de sécurité des données, de conformité aux réglementations ainsi que vos compétences en programmation scripting et en développement logiciel seront un plus.\nVos excellentes compétences en communication seront des qualités appréciées et\nun niveau d'anglais (appliquée au domaine technique) est un plus.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-lille-at-capgemini-3914228495?position=10&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=PkwyrVao7fUjVVda5pf%2BSg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini en quelques mots\nChoisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.\nVos missions\nVous maitrisez au minimum un langage de programmation appliqué à l’analyse de données\n(Java, Python, Scala et les environnements Spark et / ou Hadoop).\nVous êtes passionné par le Big Data et le Machine Learning et l’analyse de données\nVous concevez et mettez en œuvre des stratégies sécurisées d'acquisition et d'intégration de données\nVous configurez des référentiels de données à la pointe de la technologie dans des environnements distribués\nVous construisez des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée\nVotre profil\nDiplômé(e) de Bac+5 en informatique\n4 ans d’expérience\n(au sein d’une ESN ou chez un intégrateur) en conseil clientèle\nUne solide culture technologique\nUn bon niveau d’anglais\n3 raisons de nous rejoindre\nQualité de vie au travail :\naccord de télétravail en France et à l’international, accord sur l’égalité\nprofessionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu :\ncertifications et formations en libre accès, accompagnement sur mesure avec\nvotre carreer manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, activités à tarifs préférentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel\nNos engagements et priorités\nLe groupe Capgemini encourage une\nculture inclusive dans un cadre multiculturel et handi-accueillant.\nEn nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des\ninitiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nCapgemini\nest un\nleader mondial\n, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de\n55 ans d’expérience,\nnous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le\ncloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/d%C3%A9veloppeur-euse-data-at-akademija-oxford-3917871099?position=11&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=H45gptKDFqpuRTlD%2BOGEbA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Une entreprise du secteur de la prévoyance située à Paris recherche un.e Développeur.euse Data, rattaché au Responsable Data – BI dans le cadre d’un contrat d’apprentissage débutant en Septembre 2021.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "Other",
        "skills": {}
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Vélizy-Villacoublay, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-thales-3908228180?position=12&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=QoUqtZ4lBaasx8PwNxxUag%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.\nThales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.\nQUI ETES-VOUS ?\nDiplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.\nVOS MISSIONS :\n• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Hadoop.\n• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.\n• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.\n• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.\n• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.\n• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.\n• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.\nInnovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "BigData": [
                "Hadoop"
            ],
            "Other": [
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LVMH",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=13&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=88V2zvA7VgiUnRocByb4DA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.\nWe are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.\nIf you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,\nIf you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,\nIf you like building tech solutions having direct impacts on billion-dollar-valued businesses,\nIf you have good communication skills and like sharing your knowledge,\nApply now, and join us!\nThe mission\nThe Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.\nOur team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.\nYou will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).\nMain responsibilities\nYou will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).\nYou will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.\nYou will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.\nApplying the quality and security standards. Making them evolve if necessary.\nProducing realistic, understandable and documented solutions following the group guidelines.\nSharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.\nTaking responsibility as member of the team on the product performances (delivery and long term usage)\nRequired expertise and knowledge\nAbility to build technical solutions answering concrete usage (User Stories) and communicate them to the team.\nDimension and evaluate complexity for technical solution productions.\nExtensive knowledge and experience with good learning and sharing abilities.\nEvaluate quickly risks and opportunities about technical choices.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProblem-solving skills on Data and AI, coding and software development\nTech lover\nFeedback taker and giver\nTeam player\nKey benefits to join our team\nAttractive packages\nOffices in the 8th arrondissement near the Champs Elysées\nFlexibility on the working hours\nRemote work possible (~40%)\n7 weeks of holidays (congés payés + RTT)\nLVMH brands exclusive private sales\nGreat employee committee and health insurance (CE, mutuelle)\nLast generation MacBooks\nPart of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!\nYou’re eligible if\nYou have a strong experience (3+ years) in cloud data architecting or consultancy.\nYou graduated from an engineering (or equivalent) with a master’s degree. Computer Science knowledge is mandatory.\nExperience on data stacks and/or Google Cloud (built in components) is a huge plus.\nFrench and English both written and oral (Maisons are all over the world)\nYou’re thrilled to support the #1 luxury group to get even better.\nHiring Process\nCall with our HR partner dedicated to the Tech Team\nTechnical interview with the Solution Engineering Manager\nTechnical test\nInterview with the Head of Engineering\nStill here? Apply now!!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "40",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Flexibility",
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Grenoble, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-grenoble-at-capgemini-3905836212?position=14&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=kCCidaEslhnyuykxU40ubg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations. Avec le soutien et l'inspiration d'une communauté d’experts dans le monde entier, vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participer à la construction d’un monde plus durable et inclusif.\nVos missions :\nEn tant que Data Engineer au sein d'une équipe multidisciplinaire, vos responsabilités principales seront les suivantes :\nIntervenir sur les différentes phases d'un projet dans un environnement Cloud et Agile.\nContribuer à la gestion de la qualité des données et extraction et analyse de celle-ci, ainsi qu’à la présentation des données dans leur forme raffinée.\nProposer des nouvelles lectures de données via un travail de fouille sur les gisements d’information, notamment client.\nAdopter une posture de consultant : proposer de nouvelles solutions et accompagner le client dans ses choix.\nVotre profil :\nTitulaire d'un Bac+5 en école d’ingénieur ou en université.\nConnaissances approfondies des ETL (Talend, Informatica ou SSIS), du traitement de données (Spark, Python, Scala) ainsi que des bases de données (Oracle, SQL Server, Postgres).\nFaculté pour se montrer curieux, autonome et proactif dans la réalisation de ses tâches.\nCapacité à faire preuve de rigueur et à travailler en équipe.\nBon niveau d’anglais (B2 minimum).\n3 raisons de nous rejoindre :\nQualité de vie au travail\n: accord de télétravail en France et à l’international, accord sur l’égalité professionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu\n: certifications et formations en libre accès, accompagnement sur mesure avec votre career manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE\n: plan actionnariat, tarif préférentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorités\n:\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nÀ propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "55 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DBMS": [
                "SQL Server"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Lyon, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-thales-3903089036?position=15&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=QCrBY3k8aK8YeRYaEeexyg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "📢 Nous recherchons un(e) Data Engineer, basé(e) à Lyon\n👉Quelques mots sur les activités numériques de Thales Lyon :\nLes activités numériques représentent une entité rattachée au groupe Thales, spécialisée dans l’IT et présente au national.\nL’agence de Lyon adresse divers sujets d’expertise : ingénierie logiciels, cybersécurité, infogérance des infrastructures et transformation digitale.\n🎯\nVotre rôle et missions\nEn nous rejoignant, vous intégrerez le centre de compétences\nAugmented data\n,\nspécialisé dans la conception, le développement et l’évolution d’applications data centrées. Vous y boosterez votre carrière en travaillant sur des technologies telles que\nSpark, Elasticsearch, Kube ...\nle plus souvent dans un environnement\nAgile\n.\nDans le cadre des projets que nous opérons aujourd’hui :\n- Vous contribuerez à la conception, au maintien, à la scalabilité des plateformes d’analyse de données au travers de votre expertise sur les sujets data (base de données, gestion de flux, ETL …)\n- Vous contribuerez à la conception et à la mise en production des pipelines d’analyses et de transformations de données en veillant à leur bonne adaptation aux besoins métiers et aux contraintes techniques du client\n- Vous pourrez intervenir sur des sujets de visualisations, dans le but de notamment accompagnées nos clients sur la conception de Dashboard métier intelligent …\n- Vous serez également amenées à échanger directement avec des DevOps/Datascientist pour la mise en place, l’intégration des pipelines et l’élaboration des algorithmes de traitements de données.\n- A l’échelle du département, Vous serez un acteur majeur du développement de notre activité et du lancement de nouveaux projets de valorisation de données.\n🙋‍♀️ 🙋‍♂️\nVotre profil\nDe formation Bac +5 en informatique (école d’ingénieur, Master ou équivalent), vous justifiez d’une première expérience réussie sur un projet data ? Vous souhaitez participer à la conception et intervenir sur des solutions de récupération et d’exploitation de données métiers dans des contextes critiques et hautement sécurisés ?\nAutonome, dynamique, organisé(e) et proactif(ve), vous souhaitez évoluer au sein d’équipes passionnées par l’exploration et l’intégration des technologies nouvelles au service des métiers de nos clients ?\nVous avez des compétences qui couvrent les domaines suivants :\nMise en place et gestion de base de données (SQL, Elasticsearch, Clickhouse ...)\nLangages de programmations (Java, Python)\nGestion de flux (Kafka, flink, logstash …)\nEnvironnements big Data (Spark/hadoop )\nPrincipes et outils de type ETL\nVous êtes de plus intéressé(e):\nPar les environnements containerisés (docker, kubernetes, helm ...)\nLes concepts DevOps (Ansible, CI/CD...)\nLes sujets de Datavisualisation (Vega, Kibana, python librairies...)\nVous aimez travailler en équipe ? Vous êtes reconnu(e) pour vos qualités relationnelles et vos capacités de vulgarisation ?\nAlors notre poste d’Ingénieur(e) Data(H/F) est fait pour vous !\n🙌\nVotre carrière chez Thales\nDifférentes opportunités vous permettront de découvrir d'autres domaines ou sites. Vous pourrez évoluer et développer vos compétences dans différents domaines.\nExplorez un espace attentif au développement personnel.\nDéveloppez vos talents dans un autre domaine du groupe Thales, en découvrant de nouveaux produits, de nouveaux clients, un nouveau pays ou en vous orientant vers une solution plus complexe.\nChoisissez entre une expertise technique ou un parcours de leadership.\nVous travaillerez dans une entreprise résolument humaine avec des valeurs fortes comme la sécurité au travail, l’égalité Homme/Femme et l’équilibre vie personnelle/professionnelle (Accord Télétravail).\nRattaché(e) à la Convention métallurgie, vous bénéficierez aussi de ses multiples avantages (…)\nVous souhaitez en savoir plus ?\nN’hésitez pas à contacter notre équipe de recrutement ou nos équipes directement.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Flink",
                "Spark"
            ],
            "DevTools": [
                "Docker"
            ],
            "Automation": [
                "Kubernetes",
                "Ansible"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Leadership"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=16&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=wZbQb5KuBsOLycS5Aft7kg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini\nChoisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.\nVos missions :\nIntégré(e) au sein d'une équipe projets intervenant pour des clients dans des secteurs d'activités variées, vous serez notamment en charge des missions suivantes :\nConcevoir et mettre en oeuvre des stratégies sécurisées d'acquisition et d'intégration de données,\nConfigurer des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),\nConstruire des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée.\nVotre profil :\nDiplôme d’ingénieur ou équivalent universitaire\nMinimum 3 ans d'expérience\nAnglais courant\nMaîtrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.\nExpertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks…\n3 raisons de nous rejoindre :\nQualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité\nprofessionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorités :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nCapgemini\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akademija Oxford",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/apprenti-e-ing%C3%A9nieur-data-at-akademija-oxford-3912806043?position=17&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=4jIJa%2B5MYicKzQ2041q3Og%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous souhaitez intégrer une école à taille humaine ? Vous êtes intéressé par le secteur du développement informatique ? Sup de Vinci propose une formation complète en Mastère Big Data.\nL’école Sup de Vinci à Bordeaux, accompagne l’une de ses entreprises partenaires dans son projet de recrutement d’un profil Ingénieur Data, en alternance pour la rentrée 2022.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/big-data-engineer-spark-python-f-h-at-orange-business-3916552415?position=18&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=gVbBfCnKp17Fh%2FK7yMriLQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?\nNous l’avons fait ! Notre alchimie nous positionne comme un\nacteur unique\nintervenant sur toutes les étapes du\nvoyage de la donnée.\nDepuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’\nESN d’Orange Business\nalliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …\nDescription du poste\nOrange Business, recherche pour son site de Lille, son futur Ingénieur Big Data pour rejoindre sa team Data.\nVotre quotidien ?\nEn intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :\nAu démarrage du projet :\nRecueillir et analyser les besoins du client\nRédiger les spécifications fonctionnelles et techniques\nEstimer les charges\nPendant la phase de réalisation :\nModéliser des datawarehouses et datamart\nDévelopper les procédures d’alimentation (ETL)\nDéveloppement SPARK\nen batch et streaming\nDévelopper les visualisations de données (DataViz)\nRéaliser la recette et les tests\nSuivre et mettre en production\nEn fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !\nQualifications\nVous possédez 5 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels.\nVous avez de solides compétences\nSpark\n(job, scripting, déploiement) ainsi que sur\nPython.\nAvoir des connaissances Kafka sera un plus également.\nEnvie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?\nOutre l’aspect technique, c’est une personnalité qui est recherchée !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Technology & Strategy",
        "location": "Lyon, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-technology-strategy-3881556102?position=19&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=rfE5EC9w%2FL0oU8z85LCfuA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Découvrez Novencia\n:\nExpert en Data et Intelligence Artificielle, nous aidons nos clients à exploiter et à valoriser leurs données sous toutes ses formes en les accompagnant sur des projets de Data Analyse, Data Gourvernance, Data Architecture, Data Science, et Data Engineering…\nVous avez une solide expérience de minimum 2 ans dans l'ingénierie des données et vous êtes à la recherche de nouveaux défis ? Bouclez votre ceinture, la suite est pour vous !\nType de contrat : CDI\nLieu : Lyon\nEn qualité de Data Engineer (H/F), votre rôle sera :\nConcevoir et proposer les solutions de développement répondant aux besoins fonctionnels et techniques des projets big data.\nTu participes à la conception de solutions permettant le traitement de volumes importants de pipelines données.\nRéaliser ces solutions par l’écriture de code, en respectant les méthodes et procédures qualités définies au sein du département Technique.\nMise à disposition sécurisé et lisible de la data.\nS’assurer de la conformité fonctionnelle et technique de ces réalisations en effectuant les tests automatisés nécessaire et la mise en place de monitoring (système et qualité).\nAssurer la maintenance des applicatifs / plateforme data science\nAssurer une veille technologique\nVous disposez des compétences suivantes :\nMaitrise des plateformes Cloud (AWS, GCP ou Azure), de Scala et de SQL.\nUn.e touche à tout : possédant des compétences en langage Python/Spark, de bonnes capacités de modélisation, une forte appétence pour le Big Data\nFin.e connaisseur.euse : Data Engineer convaincu, très peu de secrets pour les clusters et pour les calculs parallèles\nExplorateur.trice : découvre de nouvelles technos grâce à une veille régulière\nDébrouillard.e : relève de nouveaux défis\nNotre objectif commun est de co-construire votre carrière en fonction de vos aspirations et de vos compétences.\nContactez-moi en message privé ou par mail à s.ziki@technologyandstrategy.com !\nLet's make it possible #together\n*Nos postes sont ouverts aux personnes bénéficiant d’une Reconnaissance de la Qualité de Travailleur Handicapé (RQTH). T&S Groupe encourage la diversité et l’égalité sur le lieu de travail. Tous les candidats qualifiés H/F/* sont pris en considération pour un emploi sur un même pied d'égalité.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "digiRocks recrute ✅",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-digirocks-recrute-%E2%9C%85-3903481080?position=20&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=TlAWC5PhE0jt5w68KPQDTg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "😎 Envie d'accompagner des organisations dans leurs stratégies, Fan de data?\nRejoins un jeune cabinet de conseil en stratégie spécialisé en data. Le cabinet a été créé il y a 4 ans pas des anciens de grands cabinets de conseil en stratégie qui ne se reconnaissaient plus dans ce qu'est devenu le \"consulting\". Cependant ils n'ont pas perdu espoir de pouvoir apporter du conseil à haute valeur ajoutée dans une ambiance friendly, façon start-up, sans sacrifier l'excellence.\nJean-Patrick recrute un(e) Consultant Data Engineer à Paris en CDI\n✅ MISSION :\nVous serez responsable de la mise en œuvre de bout en bout de la pile de données, de la collecte au reporting, avec un accent sur l'infrastructure et les processus techniques. Vous travaillerez avec des Consultants en Stratégie & Data et les soutiendrez dans la résolution des défis liés aux données de leurs clients. Vous contribuerez à la définition des stratégies de données, à la mise en œuvre des systèmes de données et vous soutiendrez l'exploitation des données dans des projets transformationnels. En général, vous serez responsable de comprendre intimement les problèmes, de concevoir une stratégie technique pour les adresser et de faciliter une exécution technique de haute qualité.\n✅ RÉSULTATS ATTENDUS :\n🚀 Résultat 1: Unificateur de Données : Architecturer, assembler, assimiler, nettoyer et conformer de grands ensembles de données complexes pour livrer des insights commerciaux et alimenter les expériences de produits de données.\n🚀 Résultat 2: Agent de Sécurité des Données : Concevoir et construire une infrastructure de données fiable et évolutive avec les techniques de confidentialité et de sécurité de pointe pour protéger les données.\n🚀 Résultat 3: DataOps : Posséder la pile de données de bout en bout, y compris la collecte d'événements, la gouvernance des données, les intégrations de données et la modélisation.\n🚀 Résultat 4: Gardien des Données : Assurer la cohérence et la qualité de l'environnement technique et de la structure des données à travers des métriques, de la documentation, des processus, des tests de données et de la formation.\nRequirements\n✅ PROFIL RECHERCHÉ :\nDiplômé d'une Grande Ecole de Commerce ou d'ingénieur, avec une première expérience réussie comme Data Engineer, idéalement dans un contexte similaire au Conseil,\nConnaissance des services de Data Warehouses Cloud. Expérience avec Google BigQuery, Snowflake, AWS Redshift/Athena, Looker, Azure SQL DWH, ou Azure Databricks est très souhaitable.\nConnaissance des architectures de données relationnelles et de grandes données, de l'entreposage de données, de l'intégration de données, de la modélisation de données, de l'optimisation de données et des techniques d'analyse de données.\nExpérience dans la construction de pipelines de données de bout en bout en utilisant des plateformes de données sur site ou basées sur le cloud.\nExpérience pratique dans la livraison de solutions comprenant des bases de données, SQL avancé et développement logiciel dans des langues telles que Python.\nIntéressé et connaissant les technologies Big Data et les technologies de l'écosystème Apache telles que Beam, Spark, Kafka, Airflow, bases de données, intégration, gestion des données de référence, assurance qualité, manipulation de données et technologies de gouvernance des données.\nExpérience avec les plateformes cloud publiques et l'infrastructure cloud qui est essentielle.\nExposé aux outils ETL/ELT et de gouvernance.\nIntéressé par les technologies et principes IA et ML.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "ML",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-sql-gcp-f-h-at-orange-business-3916557264?position=21&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=KZtYfL29GoYlm476j4MI9A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?\nNous l’avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les étapes du voyage de la donnée.\nDepuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’ESN d’Orange Business alliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …\nDescription du poste\nOrange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.\nVotre quotidien ?\nEn intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :\nAu démarrage du projet :\nRecueillir et analyser les besoins du client\nRédiger les spécifications fonctionnelles et techniques\nEstimre les charges\nPendant la phase de réalisation :\nModéliser des datawarehouses et datamart (intégration de flux et consolidation des données)\nDévelopper les procédures d’alimentation (ETL)\nDévelopper en SQL\n/ PLSQL / Shell\nGarantir la qualité des données et leur disponibilité\nConcevoir et développer des solutions frontend BI à des fins analytics & dashboarding\nRéaliser la recette et les tests\nSuivre et mettre en production\nEn fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !\nQualifications\nVous possédez 3 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels et ingénierie ou analyse data.\nVous avez de\nsolides compétences en développement SQL\n(job, scripting, déploiement), vous avez l’habitude de travailler dans un\nenvironnement Google Cloud Plateform\nainsi qu’avec\nPower BI\n.\nEnvie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?\nOutre l’aspect technique, c’est une personnalité qui est recherchée !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Chantelle",
        "location": "Cachan, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-chantelle-3909775663?position=22&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=WesNidpJXmnR0vuCvjZZUA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La Direction des Systèmes d'Information et du Digital du groupe Chantelle recherche son/sa futur.e Data Engineer H/F, pour le lancement du grand chantier de rénovation de l'architecture Data : la bascule de l'intégralité de son Data Warehouse vers Google Big Query.\nNous souhaitons recruter un Data Engineer H/F confirmé.e, chargé.e de contribuer à la définition de la feuille de route de la Chantelle Data Plaform. En tant que Data Engineer vous travaillerez au sein de l'équipe Data Intégration en charge de la Chantelle Data Platform.\nVos Missions :\n- Mettre en œuvre une infrastructure autour de Google Cloud Platform permettant de collecter (airbyte, API, ...) , transformer (dataform, Bigquery ...), exposer (dataviz, API, applications, ...) et historiser les données générées par l'entreprise.\n- Travailler en étroite proximité avec les responsables des différents domaines fonctionnels (Référentiels, Supply Chain, Manufacturing, B2B, Retail & e-commerce, Finance, ...), avec notre équipe de Data Analysts ainsi qu'avec l'équipe technique en charge des infrastructures transverses\n- Être force de proposition sur tous les sujets d'architecture et de modélisation (choix de mise en place de pipeline temps réels ou au contraire de flux de données en mode batch, ou bien encore stockage sur Big Query / Big Table en fonction des cas d'usage).\n- Définir les éléments structurants, en justifiant vos choix, et les mettre en œuvre.\n- Rationaliser et moderniser notre architecture d'intégration inter-applicative; se projeter sur la création d'un modèle de données de type Datamesh.\n- Faire la refonte de la BI de nombreux use cases tels que le pilotage de nos stocks, personnalisation de nos sites e-commerce en temps réel en fonction de nos profils client, etc…\nStack technique : Google Cloud Platform, BigQuery, DataForm, DataFlow, PubSub, Airbyte, Github ...\nBonne maîtrise des langages Python et SQL\nPourquoi travailler chez Chantelle ?\nUne flexibilité dans votre lieu de travail, selon la politique de télétravail de l'entreprise.\n11 jours de RTT/an ainsi qu'un 13ème mois.\nUne culture d'entreprise familiale basée sur des valeurs de respect, de créativité, de durabilité et de transparence\nUne aventure dans laquelle vous pourrez vous épanouir, apprendre et entreprendre, avec une grande variété de missions et beaucoup d'autonomie\nDes équipes ressources humaines et des managers à votre écoute pour vous accompagner dans votre parcours professionnel\nDes réductions sur nos produits et des ventes au personnel\nDes avantages dans votre qualité de vie au travail : une conciergerie complète proposant un large panel de services, des activités en interne, un CSE.\nVous souhaitez rejoindre un Groupe familial, innovant, engagé et leader dans son secteur en France comme à l'international et vous souhaitez apporter votre expertise et authenticité pour guider votre équipe vers le succès : postulez et rejoignez le Groupe Chantelle !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DBMS": [
                "Big Query",
                "BigQuery"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-mod%C3%A9lisation-sql-f-h-at-orange-business-3916551577?position=23&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=8Y5dwKvhVb6ZluKzoHL6Ag%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l’un des leaders européens de la Data transformation ?\nNous l’avons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les étapes du voyage de la donnée.\nDepuis 30 ans, Business & Decision, entité de Digital Services s'est imposée comme un partenaire stratégique pour la transformation Data de nombreux clients, dans des secteurs très variés. Digital Services est aujourd’hui l’ESN d’Orange Business alliant les expertises historiques Cloud et Digital d’Orange ainsi que le cœur de métier Data/IA de Business & Decision. Son but est d’accompagner les entreprises et les acteurs publics dans leur transformation grâce aux 4000 experts présents dans plusieurs grandes villes françaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse …\nDescription du poste\nOrange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.\nVotre quotidien ?\nEn intégrant Orange Business, vous pouvez participer à une grande diversité d’activités dans la Data. En voici un aperçu :\nAu démarrage du projet :\nRecueillir et analyser les besoins du client\nRédiger les spécifications fonctionnelles et techniques\nEstimer les charges\nPendant la phase de réalisation :\nModéliser des datawarehouses et datamart (intégration de flux et consolidation des données)\nDévelopper les procédures d’alimentation (ETL)\nDévelopper en SQL / PLSQL / Shell\nGarantir la qualité des données et leur disponibilité\nRéaliser la recette et les tests\nSuivre et mettre en production\nEn fonction de votre évolution et de nos enjeux, vous pouvez aussi évoluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d’initiative est toujours la bienvenue !\nQualifications\nVous possédez 5 ans d'expérience ou plus dans la mise en œuvre de projets décisionnels et en modélisation.\nVous avez de s\nolides compétences en développement SQL\n(job, scripting, déploiement) ainsi que sur Python.\nEnvie d’apprendre de nouvelles technos ? Vous souhaitez partager vos compétences et bénéficier des expertises de la Team Orange Business ?\nOutre l’aspect technique, c’est une personnalité qui est recherchée !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "eXalt Value",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=24&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=%2BCqn5%2Bror35%2Fyn7g5I0gCQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "eXalt\nest un cabinet de conseil IT\nPure player Data\n& IA basé à Paris.\nNotre offre s’articule autour de 4 piliers réunis au sein d’une même communauté pour un accompagnement à 360° alliant une expertise technique et méthodologique à une approche conseil métier:\nData Gouvernance & Project\nData Engineering & Big Data\nData Performance & Analytics\nData Science & IA\nFiliale du groupe eXalt, créé en 2018,\nregroupant plus de\n950 collaborateurs en France\n(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)\net à l’international\n(Colombie, Etats-Unis, Espagne, Belgique),\neXalt Value\napporte une\nexpertise approfondie\ndans le domaine de la Data & IA et conseille les entreprises dans le déploiement de leurs stratégies data-driven.\nBénéficiant du support du groupe eXalt\n(1er dans la catégorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value\nest en pleine croissance et regroupe aujourd’hui une communauté d’expertise de plus de 60 collaborateurs en région parisienne.\nNos consultants interviennent sur d\nes projets d’envergure\ndans divers secteurs d’activité,\nBanque & Assurance, Médias, Transports, Retail, Tourisme, etc.\nNous recherchons un\nData Engineer Confirmé H/F (minimum 4 ans d'expérience dans la fonction)\npour rejoindre notre communauté sur le\npilier Data Engineering & Big Data.\nVos missions:\nConcevoir et développer des pipelines et des flux de données.\nIntégrer et transformer des données provenant de différentes sources.\nDévelopper et mettre en œuvre des algorithmes de traitement de données avancés.\nCollaborer étroitement avec les équipes clients pour comprendre leurs besoins et fournir des solutions adaptées.\nAssurer la qualité et la fiabilité des solutions développées.\nConseiller les équipes clients sur les solutions à mettre en place.\nLes Prérequis :\nTitulaire d'un Bac+5, Ecole d'Ingénieur\nMaîtrise d'un ou plusieurs langages de programmation (\nPython, Scala, Spark, etc\n.).\nExpérience approfondie des technologies\nBig Data (Hadoop, Spark, Kafka, Talend, etc.)\nExpérience avérée\nen\nenvironnement Cloud (AWS, GCP, ou Azure)\n.\nSolides compétences en conception et en optimisation de pipelines de données.\nExpérience de travail en\nméthode Agile\nCapacité à travailler de manière autonome et en équipe.\nExcellentes compétences en communication et en résolution de problèmes.\nMaîtrise de l’anglais (oral & écrit dans un contexte international professionnel).\nVotre environnement eXalté:\nUn environnement de travail Collaboratif\nfavorisant les initiatives et projets transverses à la Practice Data & IA (Lab IA, Data Hub, etc.).\nUn collectif de consultants passionnés,\ns’intéressant aux tendances innovantes du secteur.\nUne Practice de proximité,\nprivilégiant la montée en compétence de ses collaborateurs (formations, coachings, mentorats, etc.)\nUn suivi individualisé et de proximité\npar un.e Data Sales Manager référent du compte client, un.e Chargé.e RH et un.e Practice Manager\nUne équipe ouverte et dynamique,\nqui privilégie les moments de partage et de convivialité (séminaires, eXaltemps, meet-up, déjeuners d’équipe, etc.)\nNotre processus de recrutement :\nUn entretien RH avec Estelle,\nà la suite duquel vous saurez tout (ou presque) d’eXalt Value,\nUn entretien technique avec un Manager assorti d’un test technique,\nlors duquel vous aurez l’occasion de démontrer vos talents mais aussi d’apprendre avant même de dire oui,\nUn entretien final avec la Directrice Associée ou le Directeur Opérationnel,\npour finir de vous convaincre de nous rejoindre 😊\nNous avons hâte de recevoir vos CV, et de faire votre connaissance!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-mindpal-3919800161?position=25&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=qCmZh4kBZiW%2FKGAELVKS7Q%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for\nData Engineer!\nResponsibilities\nDesigning, creating, and maintaining data processing systems\nAnalyzing and optimizing data processing workflows\nCollaborating with the team to ensure data quality and efficiency\nTesting and implementing new solutions\nRequirements\nAt least 2 years of experience in designing and creating data processing systems\nProficiency in tools and programming languages related to data engineering (e.g. Hadoop, Spark, Scala, Python)\nExcellent knowledge of databases and SQL language\nAbility to work in a team and communicate effectively with other departments\nCommunicative English skills\nExperience with AWS/AWS Glue is a plus\nWe Offer\nB2B contract\nFull-time job\nRemote work and flexible hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "AWS"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "EarthDaily Agro",
        "location": "Balma, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-earthdaily-agro-3883708013?position=26&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=JIXRz5%2F3ejQTYD%2BCJbgc1w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "About Us\nEarthDaily Agro provides space age data and analytics to the organizations and people who feed the planet!\nWith 35 years of industry experience, EarthDaily Agro provides customers with the data, analysis and knowledge they need to make more efficient and effective decisions. B2B services range from global risk management and monitoring of agricultural commodities to the marketing of inputs and precision agriculture consulting, using the latest research in agronomy, information technologies and remote sensing.\nEarthDaily Agro also develops highly customized business solutions for agricultural lenders, insurers, input suppliers and food companies, with easy-to-use analytics, that help reduce the daily risks of agriculture.\nEarthDaily Agro is headquartered in Minneapolis, MN, USA, with offices in France, Brazil, Australia and Switzerland and is a division of EarthDaily Analytics Corp.\nEarthDaily Analytics Corp., a vertically-integrated data processing and analytics company, is launching a new constellation of earth observation satellites. The EarthDaily satellite constellation will significantly enhance geospatial analytics capabilities in agriculture, forestry, environment, financial services, and intelligence, among many other verticals.\nMain Job Tasks And Responsibilities\nAs a EarthDailyAgro Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and infrastructure specialized for geospatial and remote sensing applications. You will work closely with data scientists, geospatial analysts, remote sensing experts, software engineers, and DevOps teams to ensure the successful deployment and scaling of data pipeline to feed geospatial data machine learning models. Your role will be crucial in optimizing the geospatial machine learning ecosystem and ensuring the seamless integration of AI-driven geospatial solutions into real-world applications.\nYour Responsibilities Include\nCloud-based data pipeline Conceptualization, Development and Scaling: Build up pipeline to ingest large volumes of geospatial data, pre-process them and meet data scientists’ requirements, in terms of accessibility, speed, format, quality.\nAutomation and CI/CD: Industrialization of pipeline deployment, orchestration, workflows, and versioning.\nCost & Speed Optimization: Collaborate with infrastructure team to develop, optimize, and fine-tune pipeline.\nCloud and Containerization: Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization tools.\nInfrastructure Management: Utilize containerization technologies and cloud-based services to set up and manage infrastructure, enabling seamless deployment and scalability.\nMonitoring and Anomaly Detection: Implement monitoring systems to track pipeline performance and identify anomalies.\nVersion Control and Data Version Control: Proficient with version control systems like Git and DVC.\nSecurity and Compliance: Ensure the security and privacy of geospatial data, adhering to relevant data protection regulations and industry best practices.\nCollaboration and Communication: Collaborate with interdisciplinary teams to integrate data pipeline into existing applications or develop new geospatial products.\nIssue Resolution and Troubleshooting: Identify and resolve promptly technical issues related to geospatial data processing, performance, or infrastructure.\nEducation, Knowledge And Abilities\nRequirements\nEducation: Master's degree in Computer Science, specialisation in Geomatics and/or Remote sensing would be a plus.\nExperience: 3+ years experiences with data pipeline processes and deployment is a must-have. Proven hands-on experience in setting up pipelines and data processes with opensource tools (e.g., MLFlow, Argo, Kubeflow) is desirable.\nProgramming Skills: Proficiency in Python and with data manipulation frameworks (e.g., dataframe, numpy, pandas, xarray, rasterio) and librairies (e.g., Dask).\nProblem-Solving Skills: Autonomous, and strong analytical and problem-solving abilities to address complex geospatial data and analysis challenges.\nCommunication Skills: Excellent communication and interpersonal skills to collaborate effectively with cross-functional teams and stakeholders.\nFrench mandatory (job based in France). Fluent in English (oral and written): meetings with internal are mostly in English.\nPreferred Additional Skills\nExperience with Earth Observation (EO) data analysis and processing.\nExperience with geospatial data formats (e.g., GeoTIFF, Shapefile, NetCDF).\nSpatial Analysis Techniques: Understanding of spatial analysis techniques and algorithms commonly used in geospatial data manipulation.\nRemote Sensing Integration: Knowledge of remote sensing data sources (e.g., STAC catalog, satellite imagery, LiDAR, SAR) integration into data pipelines for accurate and up-to-date geospatial analysis.\nCONDITIONS\nFull time job based in Balma, near Toulouse, France.\nFixed + Bonuses\nTR / \"Family\" insurance / CSE\nPowered by JazzHR\nWrfSXQ5YJg\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataAnalytics": [
                "NumPy",
                "Pandas"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Interpersonal Skills",
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Aubay",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=27&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=%2BDhsy3rEKRHdvXmK%2BbFIcQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Passionné par la Data, tu souhaites rejoindre une communauté d’experts dans le domaine afin de développer tes compétences en Data Engineering. Aubay renforce ses équipes Data et recherche des Data Engineers pour intégrer des dispositifs de projets pointus et variés.\nTon quotidien en tant que Data Engineer chez Aubay, :\nDéfinition de la stratégie de stockage et mise en œuvre des technologie appropriées (base de données SQL, NoSQL, stockage distribué,…)\nIngestion des données (structurées, semi-structurées ou non-structurées) selon différentes fréquences : batch, micro-batch ou temps réel\nConception et mise en œuvre de pipelines de données afin de fournir des données prêtes à l’emploi aux consommateurs : uniformisation, mise en qualité, enrichissement, calcul d’indicateurs,…\nConception et développement d’API pour exposer les données auprès d’applications tierces\nAppui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning\nPréparation et animation d’ateliers de travail avec des interlocuteurs variés : recueil/approfondissement des besoins métiers, avancement/restitution des travaux, transfert de compétences,…\nTon profil :\nTu dispose d’une formation niveau BAC+5 (Master 2 ou école d’ingénieur) spécialisée en informatique\nTu as déjà une première expérience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data\nLes technologies telles que Hadoop, Spark ou Kafka sont tes technologies de prédilection\nLa programmation n’a plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python\nTu maîtrises les tenants et aboutissants de la philosophie DevOps et des outils orientés CI/CD\nTu es soucieux de la qualité et de la performance de tes développements et tu t'intéresse à l’innovation frugale\nTu es un expert technique dans ton domaine sans pour autant oublier l’importance d’une communication orale et écrite de qualité et adaptée à chacun de tes interlocuteurs\nTu travaille au quotidien en mode agile et tu en maitrise les fondements\nCe qui nous caractérise :\nDes missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs variés (Banque, Assurance, Telecom, Industrie,…) qui permettent à nos collaborateurs de monter en compétences et de devenir des experts Data reconnus\nDe l’apprentissage en continu avec des formations et des certifications sur les technologies Data d’aujourd’hui et de demain\nDes experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets\nDes communautés de savoir-faire Data proposant de manière régulière aux collaborateurs d’Aubay du contenu et des évènements de partage (webinar, meetup/afterwork, BBL,…) sur les thématiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,…\nAubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.\nTa carrière chez Aubay :\nTu auras la possibilité de développer et certifier tes compétences sur les dernières technologies Data avec un focus fort sur les plateformes Data Cloud telles qu’Azure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks\nTu pourras rejoindre la BU d’excellence Data et évoluer au sein d’un environnement humain et professionnel de haut niveau. Tu profiteras d’un management sur-mesure pour t'accompagner dans ta trajectoire de carrière\nAu sein de la BU d’excellence, de multiples perspectives s’offriront à toi :\nRôle de « Lead » : Vous pourrez gagner en responsabilité sur le plan technologique et devenir un référent auprès de nos clients et des collaborateurs de la communauté Data Engineering\nRôle de « Champion » : Vous représenterez Aubay auprès d’un ou plusieurs de nos partenaires éditeurs stratégiques et vous participerez activement à l’animation de la relation sur le plan technologique\nRôle de « Head » : Vous pourrez prendre la responsabilité du savoir-faire Data Engineering et de ses offres et en assurer le développement au sens large (développement business, recrutement, management de collaborateurs, définition de la stratégie et animation de la communauté au sein du groupe Aubay,…)\nBesoin d’en savoir plus sur le processus de recrutement ?\nUn échange macro au niveau RH avec Doriane\nUn entretien technique avec Marius ou Peter, deux de nos référents techniques\nUn échange managérial avec le Directeur de la BU Modern BI & Data\nA savoir que l’ordre des étapes peut varier selon tes envies (ex : échange managérial avec l’échange technique)\nAubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "CI/CD",
                "Big Data",
                "Machine Learning",
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Air France",
        "location": "Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-d%C3%A9veloppeur-big-data-%23-h-f-at-air-france-3900080172?position=28&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=dmp2wF79Mu2ZqoKLf6rgVA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Description du poste\nIntitulé du poste\nData Engineer / Développeur Big Data # H/F\nMétier\nSystèmes d'informations - Développement\nCatégorie socio-professionnelle\nCadre\nPrésentation du contexte\nVous avez peut-être déjà voyagé avec nous, mais que connaissez-vous de nos métiers et de la richesse des données qu’ils génèrent au quotidien ? Comment le traitement et l’exploitation de ces données peut contribuer à notre stratégie de Revenue Management, ou encore aux multiples opérations à réaliser pour permettre à un vol de partir à l’heure ?\nAir France-KLM fait rêver 104 millions de passagers par an, en les emmenant vers plus de 250 destinations, grâce à une flotte de plus de 500 appareils. Le Groupe emploie 80 000 collaborateurs partout dans le monde :les opportunités sont vastes pour mettre à profit ses compétences, apprendre et se développer !\nLe département de développement DATA, OR & AI d’Air France, au sein de la direction des Systèmes d’Information, intervient dans toute la chaîne de captation et de traitement des données du groupe pour délivrer à nos métiers des solutions applicatives clés en main.\nLe département est également en charge de l’ensemble des outils techniques (ETL, DataLakes, DataWarehouses, Data visualisation) et du développement des talents et compétences de Data Engineering.\nNotre mission ? Transformer la donnée brute en décision intelligente, pour mieux optimiser les métiers d’Air France – KLM !\nPour cela, nous avons chacun un rôle essentiel à jouer, pourquoi le vôtre ne serait pas celui de Data Engineer et de développeur Big Data ?\nDescription de la mission\nAu sein de notre département, vous travaillerez main dans la main avec d’autres Data Engineers et développeurs Big Data ainsi qu’avec des spécialistes des métiers.\nIntégré au sein d’une product team agile passionnée et dynamique :\nVous participez à l’analyse des besoins métiers du commercial, des opérations aériennes, de l’exploitation sol en aéroport, de la maintenance aéronautique ou encore du Cargo.\nVous contribuez à la définition, au développement, à l’industrialisation et à la maintenance d’applications Big Data ou en Business Intelligence\nVous présentez la restitution de vos travaux et accompagnez les utilisateurs d’un point de vue fonctionnel ou méthodologique\nVous serez en contact avec les directions métier du groupe Air France KLM.\nNous attachons beaucoup d'importance au développement des compétences de nos collaborateurs ainsi qu’à leur offrir des conditions de travail favorables à l’autonomie et aux missions à forte valeur ajoutée. L'ouverture, le respect, la bienveillance et le partage sont des valeurs humaines portées par l'entreprise.\nProfil recherché\nVous êtes diplômé de niveau Master ou Ingénieur dans les domaines informatiques, vous avez acquis une expérience professionnelle dans le développement d’applications.\nVous disposez d’une expérience du développement indispensable en Backend / Java\nVous maîtrisez les bases de données relationnelles et le langage SQL\nEn Complément, Vous Avez Une Connaissance Ou Une Expérience Dans Tout Ou Partie Des Concepts Ou Outils Suivants\nEnvironnement Big Data (Spark, Hadoop, Elasticsearch, Kafka, ...)\nBase de données noSQL (MongoDB, HBase, REDIS) ou Data Warehouse Teradata\nOutil de Datavisualisation (Spotfire, PowerBI, Qlik ou Kibana)\nSolutions de Cloud (GCP) et hybride (GCP / AZURE)\n(Ces compétences complémentaires ou manquantes pouvant aussi s'acquérir à travers un parcours de reskilling et de formations aux outils du data engineering dispensé en interne).\nVous avez participé à des projets organisés en Scrum ou Kanban, et avez peut-être même œuvré comme Scrum-Master, ce qui vous permettra de vous intégrer aisément au sein d’une Product Team. Votre esprit de synthèse, votre force de conviction et votre maîtrise de la communication facilitent les décisions avec l’ensemble des collaborateurs de l’équipe, éventuellement en langue anglaise, à l’écrit comme à l’oral.\nVous êtes autonome, rigoureux(se), responsable et curieux(se), vous aimez travailler en équipe. Vous possédez de bonnes capacités d'écoute, d'analyse, de synthèse et de communication.\nEt bien sûr, vous êtes passionné(e), enthousiaste et ingénieux(se)\nCe que nous vous offrons\nDe la création de valeur pour l’ensemble des métiers d’Air France KLM\nDes challenges et problématiques complexes à résoudre\nL’opportunité de déployer des solutions Data industrielles à l’échelle !\nUne grande part de responsabilité dans une structure hiérarchique horizontale\nUn important degré de liberté pour apprendre et développer son expertise au sein de l’équipe\nOn vous attend le plus rapidement possible ! Et pour une durée indéterminée ;)\nType de contrat\nCDI\nTemps partiel possible\nNon\nType d'horaires\nAdministratif\nProfil candidat\nNiveau d'études min. requis\nBac + 5 et plus\nLangue\nAnglais (4 - Confirmé / C1)\nLocalisation du poste\nLocalisation du poste\nFrance, Provence-Alpes-Côte d'Azur, Alpes Maritimes (06)\nSite\nValbonne\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Confirmé"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java"
            ],
            "DataBase": [
                "MongoDB",
                "HBase",
                "Elasticsearch",
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "Azure"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "WA.Technology",
        "location": "Crouseilles, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/junior-data-engineer-at-wa-technology-3908458326?position=29&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=pTTf1RlEgSzwnv0Px26QCw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "WA.Technology\nis a B2B supplier of iGaming solutions with rapidly growing clients and partnerships in emerging markets. We offer a state-of-the-art iGaming platform, turnkey solutions, and standalone products that empower operators to enter or expand into emerging markets quickly and easily.\nThe WA.Platform is a fully scalable and customisable solution, featuring over 75 game providers, 6,400+ games, and support for multiple currencies, along with access to over 80 payment methods. WA. Technology enables operators to build their own casino, sportsbook, lottery, fantasy, or poker business precisely as they envision it.\nAbout The Role,\nWe are seeking a highly skilled and motivated\nData Engineer\nto design, implement, and maintain efficient and scalable data pipelines on the Google Cloud Platform (GCP). In this role, you will be responsible for managing data from MariaDB and Kafka sources, ensuring seamless integration into BigQuery, our primary data destination. Collaboration with cross-functional teams is crucial to understanding and meeting diverse data needs.\nIn this role, you will need to:\nDesign, implement, and maintain robust data pipelines on Google Cloud Platform.\nIntegrate MariaDB and Kafka as primary data sources for seamless data flow into BigQuery.\nCollaborate across departments to address unique data requirements aligned with organizational goals.\nUtilize Dataflow and Dataform for efficient data processing and transformation.\nEnsure data integrity through rigorous validation and cleansing processes.\nOptimize cloud-based infrastructure for speed and scalability.\nImplement monitoring tools for proactive system performance tracking and issue resolution.\nProvide ongoing support for data integrity and availability.\nMaintain comprehensive documentation of data architecture, updating regularly.\nStay informed about the latest data technology trends.\nEvaluate and recommend new technologies/methodologies to enhance processing and analysis capabilities.\nWhat are the key experience and personal attribute requirements?\nBachelor's degree in Computer Science, Information Technology, or a related field.\n2+ Hands-on experience relational database\nProven experience in developing data pipelines and ETL processes.\nStrong SQL skills.\nKnowledge of data modeling and database design.\nExcellent collaboration and communication skills.\nStrong problem-solving and troubleshooting abilities.\nAbility to work independently and as part of a team.\nContinuous learner, keeping up with emerging trends in data engineering.\nWhat are some of the benefits of working at WA Technology?\n100% remote opportunity\nFlexible work environment\nAttractive remuneration package\nOpportunity to work with well-connected industry leaders.\nA leadership approach that fosters innovation, creativity, and trust.\nOpportunity to experience the buzz of highly driven and motivated work colleagues.\nExperience a start-up feel in a fast-paced growth-driven environment.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Creativity",
                "Leadership",
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "iPepperGroup",
        "location": "Valbonne, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ipeppergroup-3894091360?position=30&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=ha0JPYWhIadsVqbabNBPCg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "iPepper recrute pour l'un de ses clients une PME éditeur de logiciel dans le domaine du voyage, un\nData Engineer (H/F)\npassionné(e) et expérimenté(e) pour rejoindre une équipe dynamique.\nEn tant qu'Ingénieur(e) Data, vous serez en charge d'extraire et de transformer des données, de construire et d'optimiser des pipelines de données, ainsi que de concevoir des visualisations de données intuitives et informatives.\nResponsabilités :\nConcevoir, construire et maintenir des pipelines de données évolutifs et efficaces pour transférer des données entre des bases de données SQL et NoSQL.\nDévelopper et mettre en œuvre des processus ETL pour extraire, transformer et charger des données à partir de différentes sources dans notre entrepôt de données.\nCollaborer avec des équipes pluridisciplinaires pour comprendre les besoins en données et garantir la fourniture réussie de solutions de données.\nOptimiser et ajuster les pipelines de données existants pour la performance et la fiabilité.\nConcevoir et développer des visualisations de données et des tableaux de bord pour fournir des insights exploitables aux parties prenantes.\nSurveiller et résoudre les problèmes de pipelines de données, en veillant à la qualité et à l'intégrité des données.\nProfil recherché :\nDiplôme universitaire en informatique, en ingénierie ou dans un domaine connexe.\nExpérience avérée en tant que Data Engineer ou dans un rôle similaire, avec un accent particulier sur la construction de pipelines de données et de processus ETL.\nCompréhension solide des bases de données\nSQL\net\nNoSQL\n, y compris la modélisation des données et la conception de schémas.\nMaîtrise des langages de programmation tels que\nPython, Java ou Scala.\nExpérience avec des outils de visualisation de données tels que\nTableau, Power BI.\nSolides compétences en analyse et en résolution de problèmes, avec la capacité de traduire des données complexes en insights exploitables.\nExcellentes compétences en communication et en collaboration, avec la capacité de travailler efficacement dans un environnement d'équipe pluridisciplinaire.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LCL",
        "location": "Villejuif, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-profils-exp%C3%A9riment%C3%A9s-h-f-at-lcl-3888403052?position=31&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=1669O%2BXVTXi8nbyJkeSoXA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "🏦 LCL, c’est LA banque urbaine du Groupe Crédit Agricole - avec nous, accompagnez la transformation, le développement et le maintien technologiques de nos outils avec une vision business et de satisfaction de nos 6 millions de clients.\nEn tant qu’acteur majeur de la banque de détail, nous nous adaptons chaque jour aux nouveaux modes de consommation et les projets de nos de clients internes et externes tout en garantissant le besoin de sécurité et de développement technologique qu’impliquent nos activités.\n💡Organisées en mode Agile, les 8 squads de la tribu DATA (6 squads Métier et 2 squads transverses) œuvrent au quotidien pour répondre à un enjeu majeur pour la banque : la collecte, le stockage, la gestion et l’usage de la donnée. En interaction permanente avec les autres tribus IT et les métiers, elles étudient et proposent les solutions et architectures à déployer pour répondre au mieux aux stratégies de développement et de pilotage de l’ensemble des métiers de la banque.\nRejoignez-nous si vous souhaitez participer aux réflexions et au développement de la trajectoire technique et DataCentric du SI LCL et plus largement du Groupe CA. Vous côtoierez et serez au cœur de l’implémentation de technologies variées telles que les plateformes Teradata, les solutions d’architecture applicative des technologies BigData ou IA, des environnements analytiques ou encore des solutions de datavisualisation. Vous assurerez le traitement de données en temps réel ou en batch et exposerez les données sous différentes formes.\nQue vous souhaitiez devenir expert sur les socles technologiques ou relever le challenge de la gestion de projets Métier, nous vous aiderons à atteindre vos propres objectifs.\nVous rejoindrez une équipe pluridisciplinaire, clairement orientée vers le développement de ses collaborateurs à de nouvelles technologies !\n🎯 En tant que Data Engineer :\n· Vous aimez analyser les besoins avec les métiers, challenger, identifier les sources de données dans les différents univers technologiques, industrialiser des algorithmes, concevoir et développer des Datalab ou des Datamart sur les plateformes ? Vous saurez relever les challenges proposés par les squads métier !\n· Vous préférez travailler à l’architecture et au déploiement de nouvelles plateformes, à la levée de la dette technologique ou encore réaliser de la veille au service de notre trajectoire ? La squad Socles Data est faite pour vous !\n· Au-delà des projets que vous gérerez, garant du bon fonctionnement de votre parc applicatif, vous attacherez une grande attention à la mise en œuvre de solutions optimisées.\n· La rigueur, la communication, l’esprit d’équipe mais aussi la curiosité et la créativité font partie de vos soft skills ! ils vous permettront de répondre aux enjeux de sécurité, de qualité, de transmission de la connaissance et contribueront à l’atteinte des objectifs de l’IT et plus largement de LCL, au service de ses clients.\n💻 Voici les principales technologies utilisées au sein de la tribu, si certaines vous sont familières, nous vous aiderons à monter en compétence sur d’autres !\nLangages utilisés : SQL, Python, Scala\nSGBD : Teradata et utilitaires (TPT, BTEQ, …)\nStreaming : Kafka\nSearch : ElasticSearch, SolR\nEnvironnement : Unix\nSolutions Big Data : Hadoop Cloudera, DataIku, HDFS, Hive, Impala,\nDevops : GitLab, Jenkins, Nexus\nOutils de visualisation : MS BI (SSIS, SSAS, SSRS) Qlik Sens, BO\nModélisation : MEGA\nOutils collaboratifs : GIT, Jira, Confluence, Teams\n⚡Si les nouveaux enjeux bancaires vous intéressent, que vous souhaitez intégrer une équipe Agile au service des métiers dans laquelle vous serez force de proposition et que vous aimez travailler dans un environnement motivant et dynamique, rejoignez-nous, cette offre est faite pour vous !\n🔥 Les + de notre entreprise :\nAccès au Plan d’épargne Groupe, intéressement et participation aux bénéfices de l’entreprise + abondement\nPrix préférentiels bancaires et avantages CSE\nParcours évolutif dans l’entreprise et/ou dans le Groupe CA.S.A\nTélétravail (jusqu'à 2 jours de télétravail par semaine)\nDe multiples commodités sur le campus (restaurants d'entreprise, salle de sport, crèche, centre médical, médiathèque...)\nForfait et avantages pratiques « mobilité durable » pour les velotafeurs\nDes équipes aussi diversifiées que structurées dans une dynamique de transformation\nLCL s’engage en faveur de la diversité et nous encourageons tout(e) candidat(e) ayant l’expérience requise à postuler à nos offres. Tous nos postes sont ouverts aux personnes en situation de handicap.\nNous avons encore de nombreuses raisons à vous présenter pour vous convaincre de nous rejoindre mais pour cela, il faudra postuler ici !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL"
            ],
            "BigData": [
                "Hadoop"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Collaboration": [
                "Confluence",
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ramify",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-ramify-3896146641?position=32&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=e4XI0cdZ19g8lYZB0g9RxQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ABOUT\nRamify’s mission is to help people achieve financial freedom, no matter who they are and how much they have. We are revolutionizing the private wealth management industry by making smart and affordable financial products for everyone.\nNo more standardized solutions, hidden fees and complicated words, Ramify enables everyone to become a smart investor. The team combines elements of its research with technology to design customized investment portfolios composed of world-class financial products.\nThe team comprises around 15 talented individuals. Ramify is looking for talented people in all sectors, who want to have a huge impact, move fast and deliver.\nJOB DESCRIPTION\nThe Quantitative Investment Solutions (QIS) Team is dedicated to designing innovative investment portfolio models and developing cutting-edge investment features within our product suite. Moreover, the QIS Team is at the forefront of driving AI-based solutions for Ramify. This involves conceptualizing and implementing transformative AI solutions tailored to meet the diverse needs of various teams within Ramify.\nAs a Data Engineer on our QIS Team, you will play a pivotal role in shaping the future of investment strategies through data-driven insights and AI-based solutions. Collaborating closely with our talented team of quantitative researchers, and investment experts, your responsibilities will encompass architecting and implementing robust data pipelines. These pipelines will facilitate the seamless integration of diverse data sources, empowering Ramify teams to make informed decision-making.\nKey Responsibilities:\nDesign, build and launch data pipelines at scale to move data across Ramify platform with SQL technologies.\nDesign and implement processes and tools for data onboarding and quality, helping to deliver an industry best-practice solution for managing the data lifecycle.\nProduce stand-alone tools that can be used by other teams to automate data quality and discover faults.\nBuild analytical tools that provide insight into business metrics across Ramify.\nArchitect and lead the implementation of AI based solutions within Ramify\nPREFERRED EXPERIENCE\nWe're looking for people who:\nWant to make a difference. We are a small team effectively reshaping how people look at the industry. We need people who 'get it' and want to play an integral part in helping us accomplish this mission and are persistent in getting the job done.\nSkills we're looking for:\nMaster’s or upper-year undergraduate-level coursework in either Computer Science, Management Information Systems, Business Information Systems, Mathematics or Finance related field.\n2+ years of professional experience in data engineering.\n2+ years of experience with one or more coding languages such as Python (is a must), Java.\nExperience with data modeling and ETL design, implementation and maintenance.\nDemonstrable mastery of industry best practices in the data lifecycle, including data quality automation and tooling.\nExcellent written and verbal communication skills with ability to communicate complex designs and solutions to non-technical and highly technical audiences alike.\nGood attention to detail.\nStrong analytics and strategic thinking skills\nNice-to-haves :\nUnderstanding of ML/ Generative AI technologies and their applications.\nPossess a passion, curiosity, and energy for finance + investing. You understand the ins and outs of the wealth management, trading, and more importantly - know how to explain these concepts simply\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Renault Digital",
        "location": "Boulogne-Billancourt, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-renault-digital-3911202728?position=33&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=0bxX79baJOSj%2BbQi4KG%2BzA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Contexte :\nDans le cadre de son programme Industrie 4.0, Renault développe depuis 2017 sa propre plateforme pour connecter et agréger les données industrielles des 22 sites du Groupe et de plus de 2500 machines.\nFort de partenariats stratégiques signés avec Google Cloud (stack data full GCP), Renault Digital est à la recherche d’un(e) Data Engineer au sein du Pôle Architecture et Data pour mettre en place des chaînes de traitement de données répondant à de nouveaux besoins métiers.\nVous collaborerez au jour le jour avec les équipes métiers ainsi qu’avec les autres fonctions du Pôle Architecture & Data (Data Analysts et Scientists, architectes, …), exploitant des téraoctets de données (événements en mode streaming, traitements en batch et temps réels et les appels aux APIs) afin entre autres d’alimenter des modèles de machine learning (segmentation clients, détection automatiquement des pannes des véhicules, …).\nResponsabilités principales :\nVous participez aux phases de framing, MVP et release des produits, services et APIs orientés data ;\nVous argumentez les choix d’architecture des projets et de la plateforme datalake sur GCP ;\nVous contribuez à la valeur métier des produits orientés Data s’appuyant sur le Datalake, en mettant en place des chaînes bout en bout de traitement de la data, de l’ingestion à l’exposition d’APIs et à la visualisation des données et des solutions ML/DS ;\nVous êtes garant de la qualité des données transformées dans le Datalake, du bon fonctionnement des chaînes de traitement et de l’optimisation de l’utilisation des ressources des ressources cloud ;\nVous proposez des standards d’architecture et de développement ;\nVous êtes force de proposition, innovant(e) et bienveillant(e).\nEnvironement technique :\nSpark, Scala, Python, Java, Airflow, SQL, Google Cloud Platform (BigQuery, Cloud Storage, PubSub, Beam, Dataflow, Cloud ML, TensorFlow, Kubernetes), Git, Docker, JSON, Bash, Spotfire\nProfil recherché :\nVous avez minimum 5 ans d’expérience en tant que Data Engineer ;\nVous disposez d’une expérience en développement Spark, Scala, Python et requêtage SQL sur des gros volumes de données ;\nVous avez une appétence pour la data : validation, transformation, analyse, valorisation ;\nVous possédez une expérience de développement et orchestration de chaines ETL complexes via Airflow ou équivalent ;\nVous pratiquez la méthodologie agile (Agile Scrum et/ou Kanban) ;\nVous utilisez les services cloud (préférablement GCP) ;\nVous êtes capable d’échanger en anglais technique écrit et oral.\nInformations complémentaires :\nVotre poste sera basé à Boulogne-Billancourt (France) en CDI (temps plein)\nVous bénéficiez de 2 à 3 jours de télétravail par semaine\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Temps plein",
                "Full"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Bash",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "DataSerialization": [
                "Json"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "ML"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Inetum",
        "location": "St.-Ouen, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-inetum-3843966639?position=34&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=TdwMuSB3Y6fg6jshb%2BTZRg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Détail de l'offre\nInformations générales\nEntité de rattachement\nInetum est un leader européen des services numériques. Pour les entreprises, les acteurs publics et la société dans son ensemble, les 28 000 consultants et spécialistes du groupe visent chaque jour l'impact digital des solutions qui contribuent à la performance, à l'innovation et au bien commun.\nPrésent dans 19 pays au plus près des territoires, et avec ses grands partenaires éditeurs de logiciels, Inetum répond aux enjeux de la transformation digitale avec proximité et flexibilité.\nPorté par son ambition de croissance et d'industrialisation, Inetum a généré en 2023 un chiffre d'affaires de 2,5 milliards d'€.\nPour répondre à un marché en croissance continue depuis plus de 30ans, Inetum a fait le choix délibéré de se recentrer sur 4 métiers afin de gagner en puissance et proposer des solutions sur mesure, adaptées aux besoins spécifiques de ses clients le conseil (Inetum Consulting), la gestion des infrastructures et applications à façon (Inetum Technologies), l'implémentation de progiciels (Inetum Solutions) et sa propre activité d'éditeur de logiciels (Inetum Software). Inetum a conclu des partenariats stratégiques avec 4 grands éditeurs mondiaux - Salesforce, ServiceNow, Microsoft et SAP et poursuit une stratégie d'acquisitions dédiée afin d'entrer dans le top 5 européen sur ces technologies et proposer la meilleure expertise à ses clients.\nTous nos postes sont ouverts aux personnes en situation de handicap.\nDescription du poste\nMétier\nApplications Delivery - Software Development\nIntitulé du poste\nData Engineer H/F\nContrat\nCDI\nDescription De La Mission\nLe pôle BFA de la branche Application Services du groupe INETUM, recherche plusieurs Data Engineers afin d'intervenir auprès de clients grands comptes au sein des marchés bancaires et de l'assurance.\nAu sein de l'équipe Data, en tant que Data Engineer, vous participez à la réalisation de divers projets et vos missions sont\nApporter votre connaissance en Big Data permettant la manipulation des données\nConcevoir les plateformes permettant de traiter des volumes de données importants\nMettre en place des bases de données\nPréparer le pipeline de données pour que les données déployées soient sécurisées et claires afin d'être analysées et transformées.\nProfil\nDe formation ingénieure en informatique Bac + 5 informatique ou scientifique\nBonne communication orale et écrite en français et niveau d’anglais professionnel\nSavoir- être Bon esprit d'analyse et de synthèse, sens de l'organisation et de la qualité, force de proposition, rigueur, travail en équipe, adaptabilité.\nSi vous vous reconnaissez, n'hésitez pas à postuler !\nLocalisation du poste\nLocalisation du poste\nFrance\nVille\nSaint-Ouen\nCritères candidat\nNiveau d'études min. requis\nBac+5\nNiveau d'expérience min. requis\nPlus de 2 ans\nCompétences\nSQL\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "Other": [
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bi-nantes-at-capgemini-3803963477?position=35&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=0Pfx7yf7BS4uAZp8IATuug%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini\nChoisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.\nVos missions :\nIntégré(e) au sein d’une équipe projet BI, Big Data ou Data Gouvernance pour des clients intervenant dans des secteurs d'activités divers, vous serez notamment en charge des missions suivantes :\nMener les analyses fonctionnelles destinées à traduire les besoins du client,\nMener les travaux de conception et de modélisation,\nDiriger le développement de la solution / des traitements d'alimentation du DataWareHouse,\nOrganiser et préparer les travaux de recette utilisateurs,\nMettre en place les processus d'industrialisation et mener cette dernière.\nVotre profil :\nDiplôme d’ingénieur ou équivalent universitaire\nMinimum 3 ans d'expérience\nAnglais courant\nCompétences en BI sur SAP BI (Hana, BW, BODS, BI 4), Microsoft BI (SQL Server, SSIS, SSAS, SSRS), Oracle (ODI, OBIEE), Teradata, Informatica (Powercenter), IBM (Datastage, Cognos, TM1), Talend, AB Initio\nMaîtrise d'un ou de plusieurs outils de Dataviz : Microsoft Power BI, Tableau, Qlikview\nConnaissances en Big Data (Ecosystème Hadoop (HIVE, PIG, Mahout…), Cloudera, Pivotal, Spark, HNX) ou en analytics (R, SAS, IBM SPSS)\n3 raisons de nous rejoindre :\nQualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité\nprofessionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorités :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nCapgemini\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les\nplateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "DBMS": [
                "SQL Server"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Ollioules, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-thales-3902424527?position=36&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=Tln2wh6ZyG6g2nuwAtwA0A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.\nThales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.\nQUI ETES-VOUS ?\nDiplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez a\nu moins 3 ans d'expérience\ndans les technologies Big Data.\nPassionné par le\nsecteur de la Défense et du Naval.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que\nData Engineer,\nvous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.\nVOS MISSIONS :\n• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies\nHadoop, Spark, Scala\n.\n• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.\n• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.\n• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.\n• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.\n• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.\nInnovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Beelix",
        "location": "Antibes, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-beelix-3838611420?position=37&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=broPHyuEihx3QbpwB5jySg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants :\n🚗Automobile\n⚡Energie\n📡Médias & Télécoms\n👗Luxe & Retail\n💶 Banque, Finance & Assurance\n✈️Défense\nAujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.\nDans le cadre de notre développement, nous recherchons un Data Engineer (H/F) pour l'un de nos clients.\nQuelles missions au quotidien ?\nÊtre le leader de la brique Datalakehouse\nDévelopper les scripts de transformations de données et les pipelines d’alimentation\nProposer des évolutions architecturales ou de fonctionnalités pour améliorer le socle technique\nÊtre le back-up du leader technique sur la partie reporting (Power BI)\nOrientation satisfaction client et résultat final forte mais également sensibilité au « comment »\nInnovation et proposition de nouvelles pratiques pour améliorer l’environnement et les conditions de travail des équipes\nA propos de vous ?\n5 + années d'expérience en tant que Data Engineer\nMaîtrise des technologies suivantes : Microsoft Azure, Microsoft Azure Synapse Analytics (Spark / Python / Pipeline / Serverless), fichiers parquet / delta, Microsoft Power BI, Microsoft SQL Server, langage SQL, Datawarehousing / Modélisation de données\nAnalyses et export de données\nConnaissance de l’ensemble du processus depuis la collecte jusqu’à la mise à disposition des données en ayant comme point fort la maitrise de sa transformation et mise en forme\nVous avez un bon niveau d’anglais\nLocalisation : Biot et/ou Carros\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DBMS": [
                "SQL Server"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Digital Waffle",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-digital-waffle-3913824888?position=38&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=0AvzzJrWcc2DQ043UYCc1A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Digital Waffle is proud to have partnered with an innovative tech startup in Paris, who are looking for a talented Data Engineer to join their growing team!\nThey are made up of a powerhouse of experts, combining\ndata engineers, business process gurus, and Project managers\nwho leverage the most advanced solutions available; utilising process mining, automation tools, and smart execution systems.\nLooking for an experienced Data Engineer (3-5 years)\nWhat You'll Do:\nThis is a full-time,\nhybrid role (Paris-based)\nwhere you'll wear many hats: data exploration, system integration, data prep, data modeling, and implementing data solutions.\nExperience:\nExpertise in data engineering, data modeling, and ETL (Extract, Transform, Load) processes\nData warehousing and data analytics skills\nExperience handling large, complex datasets\nProficiency in SQL and programming languages like Python or Java\nStellar problem-solving and analytical skills\nTop-notch communication and collaboration abilities\nBachelor's or Master's in Computer Science, Information Systems, or a similar field (a plus for process mining or intelligent process automation experience)\nIf you are an experienced and driven Data Engineer, please apply here!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ippon Technologies",
        "location": "Nantes",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=39&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=%2BfCnAFVBGALLgH%2BE4pEmKQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre la communauté DATA la plus dynamique de France ?\nNotre spécialité est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.\nMembre de la Practice Data, le/la futur(e) Data Engineer sera intégré(e) à nos équipes de conseil et sera suivi(e) par un(e) mentor qui l’aidera à monter en compétences.\nVotre champs d’expertise :\nIntervenir sur les data platforms de nos clients pour développer de nouveaux pipelines de données (ingestion, traitement, exposition).\nTravailler en collaboration avec les métiers et les data scientists pour leur fournir un support à l’industrialisation de leurs travaux (tests, intégration continue, scalabilité des modèles, craftsmanship etc…)\nDéployer des infrastructures cloud full\ninfra-as-code\n(Terraform, CloudFormation).\nParticiper aux évènements internes à la communauté data (BBL, webinar, datapéro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).\nCapitaliser sur les missions et les différents évènements de la communauté au travers d’articles de blogs, REX, BBL interne.\nVos connaissances :\nUn framework de calcul distribué tel que Spark, Storm, Flink.\nUn ou plusieurs langages de programmation (Python, Scala, Java...)\nDifférents systèmes de stockage de données (SQL ou NoSQL) et bien sûr le langage SQL.\nLa connaissance de Snowflake est bienvenue ;-)\nUn framework de streaming de données tel que Kafka ou Amazon Kinesis.\nUne expérience sur les technologies Cloud : AWS, GCP, Azure\nLe delivery et les projets en production faisant partie de notre ADN, vous êtes capable de livrer du code de qualité dans des environnements agiles.\nDe plus en plus de nos projets se font en remote avec des clients du monde entier, il devient nécessaire d’être à l’aise en Anglais.\nIppon technologies c’est aussi :\n👍 Bénéficier d'un suivi de proximité réalisé par votre manager technique : points réguliers pour votre suivi en mission, votre formation et votre évolution de carrière\n✌️ Rejoindre une entreprise où les valeurs du sport sont nos leitmotiv : dépassement de soi, travail en équipe, bienveillance.\n🗒️ Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL\n😁 Travailler en pair programming ou avec un.e mentor pour gravir les échelons !\n💪 Pouvoir participer à une aventure humaine au sein de notre Fondation Ippon pour réduire la fracture numérique dans le monde !\n🤝 Participer à nos apéros et divers évènements internes pour consolider la cohésion d’équipe\nEt après ?\nEt oui alors ? Que se passe-t-il une fois que vous êtes convaincu d’avoir lu l’offre d’emploi qui vous correspond bien ?\nNous vous proposons de prendre contact et de nous rencontrer !\nLes Next Steps :\n1 call RH\n1 échange RH\n1 échange Technique\nSi le match est bon des deux côtés : Hadjimé ! Vous vous lancerez sur le tatami Ippon !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Flink",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "InfrastructureAsCode": [
                "CloudFormation",
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Shippeo",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-shippeo-3908268236?position=40&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=JjADy0kCP97kvy2bKYZWgw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Founded in 2014, Shippeo is a French based SaaS company providing supply chain visibility. Shippeo has grown from 70 to 220 employees during the last two years and is continuing to rapidly scale after an additional $40M fundraising round in October 2022.\nShippeo is an exceptionally diverse company with colleagues from 27 different nationalities and speaking 29 languages. With offices throughout Europe, North America and recently Asia, Shippeo provides global coverage to all of our clients.\nOur product is composed of a mission critical SaaS web platform (API everywhere), with high traffic inbound/outbound integrations.\nOur mission is to anticipate problems and proactively alert end-customers so they can efficiently manage exceptions. We achieve this by collecting and matching millions of theoretical and real data from different stakeholders.\nThe Data Intelligence Tribe is responsible for leveraging Shippeo’s data from our large shipper and carrier base, to build data products that help our users (shippers and carriers alike) and ML models to provide predictive insights. This tribe’s typical responsibilities are to:\nget accurately alerted in advance of any potential delays on their multimodal flows or anomalies so that they can proactively anticipate any resulting disruptions\nextract the data they need, get direct access to it or analyze it directly on the platform to gain actionable insights that can help them increase their operational performance and the quality and compliance of their tracking\nprovide best-in-class data quality by implementing advanced cleansing & enhancement rules\nAs a Data Engineer at Shippeo, your objective is to ensure that data is available and exploitable by our Data Scientists and Analysts on our different data platforms. You will contribute to the construction and maintenance of Shippeo’s modern data stack that’s composed of different technology blocks:\nData Acquisition (Kafka, KafkaConnect, RabbitMQ),\nBatch data transformation (Airflow, DBT),\nCloud Data Warehousing (Snowflake, BigQuery),\nStream/event data processing (Python, docker, Kubernetes) and all the underlying infrastructure that support these use cases.\nQualifications\nRequired:\nYou have a degree (MSc or equivalent) in Computer Science.\n3+ years of experience as a Data Engineer.\nExperience building, maintaining, testing and optimizing data pipelines and architectures\nProgramming skills in Python and experience with asynchronous event processing (asyncio).\nAdvanced working knowledge of SQL, experience working with relational databases and familiarity with a variety of databases.\nWorking knowledge of message queuing and stream processing.\nKnowledge of Docker and Kubernetes.\nKnowledge of a cloud platform (preferably GCP).\nExperience working with workflow management systems such as Airflow.\nDesired:\nExperience with cloud based data warehouse solutions (BigQuery, Snowflake).\nExperience with Kafka and KafkaConnect (Debezium).\nExperience with Infrastructure as code (Terraform/Terragrunt).\nExperience building and evolving CI/CD pipelines with Github Actions.\nMonitoring and alerting on Grafana / Prometheus.\nExperience working on Apache Nifi.\nInformations supplémentaires\nWe are looking for talents who share our values:\n🚀 Ambition\n💙 Care\n🎯 Deliver\n🤝 Collaboration\nFind out more about our values in\nOur Culture Book\nIf you identify with our values and enjoy working in a fast-paced and international environment, Shippeo is just the place for you!\nWe are committed to fostering diversity and inclusion within our workplace as we value the unique perspectives and experiences that individuals from all backgrounds bring to our team. We are dedicated to providing equal employment opportunities to all candidates, regardless of their background or abilities, and our commitment to inclusion is reflected in our policies, practices, and workplace culture.\nWe understand that candidates may have unique needs or questions related to disability inclusion. To facilitate this, you can reach our dedicated Disability Advisor at\ninclusion@shippeo.com\nwith any inquiries or requests for accommodations during the application process.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "ML",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "United Robotics Group",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=41&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=FazbdV6EK0j0iUsCsme7kQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Bienvenue chez\nAldebaran\n, leader européen de la robotique\nau sein du groupe\nUnited Robotics Group\n.\nNous concevons et industrialisons des robots innovants avec une vision sociétale ambitieuse pour façonner un monde plus humain. Depuis 2005, nous sommes à l'avant-garde de l'interaction homme-robot avec des produits emblématiques tels que NAO et Pepper.\nNotre dernier-né,\nPlato\n,\nincarne notre engagement envers la technologie de pointe et la sécurité,\nfabriqué en France avec des composants européens.\nRejoignez nos équipes multiculturelles et dynamiques pour être au cœur de la révolution de la robotique.\nSi vous êtes passionné.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer à façonner l'avenir, nous vous offrons une expérience enrichissante et stimulante.\nEn tant que membre de notre équipe, vous bénéficierez d'une culture d'entreprise axée sur le sens de ce que nous faisons et valorisant la responsabilité sociale et environnementale.\nChez Aldebaran, nous valorisons l'innovation, la diversité et l'égalité et encourageons chacun.e à être ouvert.e, authentique, courageux.se, responsable et engagé.e.\nFinalité du poste\nAu sein de l'équipe Cloud-Online Services, le Data engineer intégrera l'équipe Data, responsable du développement des produits destinés à la collecte, aux process et à l'exploitation des données de nos robots.\nIl aura pour rôle de définir et d'implémenter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui gèrent les robots du groupe.\nMissions principales\nLe Data engineer aura pour responsabilités de :\névaluer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,\nconcevoir et développer des services Data en respectant la spécification fonctionnelle et la méthodologie agile,\nagréger et stocker de grandes quantités de données,\nmettre en place des solutions de data processing,\nintégrer/développer des outils de visualisation de données et analyser les KPI,\ndévelopper, tester, sélectionner et mettre en production des algorithmes qui permettent de répondre aux besoins,\nréaliser des analyses de données,\nmettre en place des tests de charge et fonctionnels pour les solutions Data,\ninvestiguer et corriger les bugs remontés par les utilisateurs,\ncontribuer à la mise en place de l'infrastructure et outil de déploiement (CI/CD)\nRejoignez-nous pour faire partie d'une aventure passionnante où Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !\nRequirements\nPour la bonne exécution des missions confiées, vous témoignez d'au moins 6 ans d'expérience en tant que développeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.\nCompétences demandées :\nBonne compréhension des technologies d'infrastructure et de déploiement,\nCompétences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,\nBonne compréhension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,\nBonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)\nBonne connaissance et une expérience pratique de Scrum\\Scrumban et des méthodes agiles,\nUne certification AWS sera appréciée,\nUn niveau de français et d'anglais courant est indispensable,\nDes expériences dans des environnements fortement internationaux sont un plus\nBenefits\nNos principaux avantages :\nUne culture du bien-être en entreprise qui a fait ses preuves (budget célébration et moments de convivialité par équipes et directions, restauration collective de qualité, environnement de travail agréable)\nUn engagement fort en matière de responsabilité sociale et environnementale (promotion de l'égalité professionnelle, performance de notre plan diversité et inclusion, référent handicap, fresque du numérique)\nUne culture du télétravail encadrée de manière appropriée !\nTous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "6 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Neosoft",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-neosoft-3877878521?position=42&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=EeSkZArOsu8330eBfIJXDw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Tous nos postes sont ouverts au télétravail\nGroupe indépendant de conseil en transformation digitale de près de 1800 collaborateurs, Néosoft s’est construit, depuis 2005, sur un modèle qui place l’excellence, le dépassement de soi et la RSE au cœur de sa stratégie.\nEn nous rejoignant, vous intégrez des communautés d’experts et de talents qui vous permettent de développer vos compétences et d’offrir à nos clients le meilleur accompagnement possible.\nNotre savoir-faire s’articule autour de nos 6 domaines d’expertise :\nConseil & Agilité\nCybersécurité\nData\nDevOps\nInfrastructures & Cloud\nSoftware Engineering\nNous recherchons pour intégrer notre\nagence lilloise\nun(e)\nData Engineer confirmé(e)\n.\nNous aimerions vous voir rayonner au sein de notre communauté DATA (+100 collaborateurs) animée par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients à consolider un patrimoine Data responsable.\n🎯\nVos missions :\nAprès une période d’intégration, en tant que\nData Engineer\n, voici à quoi ressembleront vos activités dans des contextes clients Retail ou Banque / Assurance / Finance :\nAnalyser et s'approprier les cas d'usages\nAnalyser et valoriser les données du patrimoine\nMettre en place des flux de transformation de données\nRéaliser les tests permettant de s'assurer la qualité du delivery\nContinuer la mise au point de frameworks data\nCréer et développer des modules de déploiement des solutions\nAssurer l'industrialisation de moteurs basés sur l'IA\nAssurer le niveau de performance des pipelines\nImplémenter les outils de monitoring du socles de données\n📝\nVotre profil :\nNous vous imaginons avec au moins 4 ans d’expériences sur des projets autour de la\nData\n, une maîtrise des\nbases de données (SQL)\n, des outils de transformation de la donnée\n(Talend, BigQuery, Airflow)\n, et un socle de compétences solides autours des langages\nPython, Spark, Scala, Hadoop, Java.\n👉\nVotre carrière chez Néosoft\nDepuis sa création, Néosoft place ses collaborateurs au cœur de sa stratégie. Notre culture pourrait se résumer en un mot : le collectif.\nNos communautés d’experts vous donnent la possibilité d’apprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.\nNous veillons à ce que chacun bénéficie d’un accompagnement de proximité et d’un suivi de carrière personnalisé auprès de votre manager dédié :\n1 bilan d’activité trimestriel pour suivre le développement de vos compétences\n1 entretien d’évaluation qui a lieu chaque année pour évaluer votre performance et déterminer vos nouveaux objectifs\n1 entretien annuel auprès de votre RH dans le but de cartographier vos nouvelles compétences pour échanger sur vos projets professionnels et souhaits de formations\n👉\nVos avantages\nFormations et développement de l’expertise :\nVous disposez de temps alloué et rémunéré en contribuant au développement de votre expertise technique et de celle du groupe (Participations à des Tech days, animation d’une conférence à l’interne ou à l’externe, rédaction d’articles, rencontres avec nos candidats en processus de recrutement…)\nUn abonnement illimité LinkedIn Learning offert\nBien-être au travail :\nUn accord de télétravail flexible jusqu’à 100% de télétravail et personnalisable\nUn partenariat avec Gymlib qui favorise le sport en entreprise\nDes initiatives locales (afterworks, défis sportifs, team buildings, …)\nEt bien plus encore :\nParce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive rémunérée dès l’arrivée du collaborateur\nEn plus de votre salaire : participation, compte épargne temps, actionnariat...\n👉\nVotre parcours candidat\nNotre processus de recrutement se compose de deux étapes clés :\nUn entretien de recrutement RH avec un Talent Acquisition Spécialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carrière possibles au sein de notre groupe\nUn entretien d’évaluation technique pour réaliser un diagnostic de vos compétences techniques et identifier les compétences sur lesquels poursuivre votre évolution\nVous aurez également la possibilité de rencontrer pour compléter votre processus un acteur de notre pôle Business ou un pair de votre métier pour échanger sur son expérience collaborateur.\nNous avons hâte de vous rencontrer !\nA bientôt,\nL’équipe Néosoft 🖐\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "",
            "Salary": "Salaire",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "fifty-five",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-cloud-engineer-h-f-at-fifty-five-3910028674?position=43&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=%2BsCqFzOn7VuceeF9BVnI4w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data & Cloud Engineer\nfifty-five est une data-company d'un genre nouveau qui aide les marques à exploiter les données pour améliorer le marketing, les médias et l'expérience client grâce à une combinaison de services de conseil et de technologie spécialisés.\nEn tant que pilier data et marketing du Brandtech Group, nous offrons des services qui combinent le conseil en stratégie, les services de cloud, le conseil en média et l'expérience client.\nfifty-five, c'est plus de 400 experts du numérique. Des digital consultants, des spécialistes du tracking et du média, des ingénieurs et des data scientists, travaillent tous en étroite collaboration pour fournir des conseils marketing de haut niveau et une assistance technique aux marques, dans tout type d'industrie, partout dans le monde.\nPartenaire des annonceurs de la collecte à l'activation et l'exploitation des données, nous aidons les organisations à devenir de véritables entités omnicanales maîtrisant l'efficacité de leur écosystème digital et ses synergies avec le monde physique.\nBasé à Paris, nous opérons sur 3 fuseaux horaires depuis nos 10 bureaux, situés à Paris, Londres, Genève, Milan, Shanghai, Hong Kong, Shenzhen, Taipei, Singapour et New York. fifty-five attache une importance particulière au bien-être de ses collaborateurs, ce qui lui a permis de figurer dans le classement Best Workplaces France en 2018.\nContexte :\nL'équipe d'ingénierie développe et met en œuvre les solutions techniques permettant la réalisation de pipelines de données et l'implémentation de data platform pour nos clients : récupération de datas sur de multiples sources de données (APIs, files, etc.), data cleaning, data processing, automation et monitoring de l'ensemble. L'équipe s'appuie sur des technologies récentes (docker, kubernetes, terraform, notebooks, etc.) et met en place ses projets dans les différents clouds du marché (GCP, Azure, AWS...).\nMission :\nNous sommes à la recherche d'une personne capable de réaliser des projets techniques pour répondre aux besoins de nos clients (par exemple: système de recommandations de produits, détection d'anomalies, ranking). Les activités vont du chiffrage et du sizing technique à la mise en œuvre des architectures, en passant par la revue des spécifications fonctionnelles et la production de code. Le Data & Cloud Engineer sera épaulé par un Lead dans ses missions. Il sera également amené à participer à la R&D et à accompagner les équipes transverses dans la mise en place d'outils de travail internes (librairies pour les data scientists, environnement Notebooks pour les data analysts et data scientists, développement de frameworks sur différents cloud providers, etc.).\nNous souhaitons trouver la bonne personne pour faire évoluer ou créer de nouvelles solutions dans ce cadre. Les missions comprennent aussi bien du prototypage rapide pour des démonstrateurs, que de la production de code robuste qui tourne en production tous les jours.\nCompétences et expériences :\n2 ans d'expérience en tant que Data Engineer\nMaîtrise de Python, SQL\nMaîtrise des environnements Cloud. Idéalement certifié GCP, Azure ou AWS\nBonne connaissance de Docker/Kubernetes\nBonne connaissance d'au moins un data warehouse (BigQuery, Snowflake, etc)\nConnaissance autour des Notebooks (Jupyter)\nA l'aise avec des concepts liés aux APIs (OAuth, REST, etc.)\nA l'aise avec les notions d'Infrastructure as Code (Terraform)\nAu courant des pratiques GitOps et connaissances des concepts autour du CI/CD\nLa maîtrise d'un orchestrateur, comme Apache Airflow, est un plus\nEsprit d'équipe (collaborer aux tests unitaires, revue de code, partage de code, sprints)\nBon niveau en français et en anglais\nA déjà travaillé en mode projet avec des interlocuteurs variés (consultant, data analyst, data scientist)\nUne expérience en marketing digital est un plus\nNous proposons :\nun bureau au centre de Paris avec terrasse et jardin\nun environnement multiculturel avec des collaborateurs aux nationalités multiples (France, Royaume-Uni, Etats-Unis, Chine, Tunisie, Italie et plus)\ndes projets avec nos bureaux à Londres, Hong Kong, New York, Shanghai, Genève, Shenzhen et Taipei\ndes TGIF et supers soirées\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "AFD Technologies",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-afd-technologies-3899625641?position=44&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=orGpiUT6ZkfNr5fLDMxvXw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "AFD.TECH part of Accenture\nest le spécialiste du conseil en transformation digitale des grandes entreprises 🚀.\nA ce jour, le Groupe est composé de 2.000 talents répartis dans 3 pays (France, Belgique & Maroc) 🌎 pour un chiffre d’affaires annuel de 125M€ !\nNos Talents d’abord 😎:\nLes Talents d’AFD.TECH part of Accenture sont au cœur de la stratégie du Groupe et il est primordial pour nous que chaque collaborateur trouve du sens dans son travail.\nAu-delà de proposer une carrière ambitieuse et personnalisée à nos Talents, nous avons à cœur de leur offrir un environnement de travail flexible (remote), inclusif et épanouissant dans tous nos bureaux (Paris, Bruxelles, Rabat, Lyon, Strasbourg, Lille, Nantes, Toulouse, Marseille, Bordeaux et Rennes)🌍.\nAvec 20% de croissance par an et plus de 20 ans d’existence, AFD.TECH part of Accenture est devenu l’acteur incontournable du marché des infrastructures informatiques, réseaux et télécoms.\nNotre proposition de valeur ? Intervenir sur l’ensemble du cycle de vie de projets complexes, pour des clients grands comptes, venant de secteurs tels que la banque, le ferroviaire, les médias, télécoms, etc (comme la Société Générale, Bouygues Telecom, Orange, Thales et bien d’autres encore !)👩🏻‍💻.\nNous rejoindre est une formidable aventure humaine : nous vous proposons un poste un poste de\nData Engineer en CDI\n, au sein de notre agence Lilloise.\nVos missions ✅:\nEn tant que Data Engineer pour l'un de nos clients grands comptes, votre rôle s’articulera autour de différents axes :\nAppréhender le contexte et les enjeux métier du client.\nCollaborer avec les équipes métier pour comprendre les exigences en matière de données.\nDéfinir des architectures data.\nConcevoir et mettre en place des pipelines de données.\nConstruire des flux de données complexes.\nVous travaillerez dans une mission à forte valeur ajoutée et de longue durée (minimum 1 an et demi).\nVotre profil✅:\nVous maîtrisez le langage SQL, les ETL et les ELT.\nVous aimez automatiser, mettre en place vos data pipelines et maîtriser les technologies: CI/CD, Terraform, Github, Python, Kafka.\nVous possédez des compétences en data visualisation : Business Objects, Qlikview, Qlik Sense, PowerBI ou Data Studio.\nVous connaissez Google Cloud Platform (GCS, BigQuery).\nVous êtes diplômé(e) d’une formation BAC + 5.\nVous avez une première expérience significative dans la data engineering (\nminimum 3 ans\n).\nVous projetez votre carrière dans un cabinet de conseil exigent et successful, qui vous permettra de développer votre esprit entrepreneurial et de répondre à vos ambitions.\nCe que nous offrons chez AFD.TECH part of Accenture 🤗:\nUne politique de flexibilité dans votre organisation et un bon équilibre de vie 🏃‍♂️.\nDes avantages plus que compétitifs 💰.\nUn accompagnement et un suivi régulier durant tout votre parcours chez AFD.TECH (Launchpad, Linkers, rookies, etc…).\nUn état d’esprit familial et de la proximité entre tous 👨‍👩‍👧‍👦.\nDes moments de convivialité toute l’année 🍾 (évent en équipe, séminaire annuel, sports collectifs etc.).\nUn parcours d’évolution sur mesure 🔼.\nA très bientôt chez AFD.TECH part of Accenture!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "20 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Extia",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-extia-3599188121?position=45&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=idnvST7Wgm%2FYg5eqGhW2tQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous souhaitez rejoindre une entreprise qui place l’humain au cœur de ses préoccupations ? On vous attend chez\nExtia\n!\nSociété de conseil spécialisée dans les métiers de l’IT, de l’ingénierie et du digital, Extia privilégie depuis sa création en 2007 une approche qui allie performance et bien-être au travail. Une vision de l’entreprise partagée aujourd’hui par plus de 2 500 Extiens en France et à l'international et récompensée par le label Great Place to Work® depuis 13 ans, notamment en\n2024 où les Extiens se hissent à la première place du palmarès Best Workplaces France\n!\nChez Extia, c’est « D’abord qui, ensuite quoi » alors, allons-y !\nD'abord qui\nVous êtes habitué à travailler aussi bien avec des méta-données qu’avec des données non-structurées. A cet effet vous maitrisez un ou plusieurs des concepts comme l’ETL, le Data mining le Machine learning, les Big data ou encore la Théorie des graphes par exemple,\nVous maitrisez les bases de l’analyse statistique,\nVous êtes apte à rédiger des scripts en Python et/ou R, et une connaissance d'autres langages de programmation comme Java, Scala ou SAS est un plus,\nVous maitrisez Spark et Hadoop\nVous êtes familiarisé avec l’environnement Linux,\nUne expérience avec les outils de Stockage de fichiers volumineux (HDFS, Data Lake, S3, stockage Blob), la connaissance des infrastructures cloud AWS ou GCP et des bases en streaming temps réel seront aussi de réels atouts.\nEnsuite quoi\nVous aurez le rôle de support technique aux équipes d’analyse : structurer les données, réaliser des analyses « statistiques » ou « techniques » sur les données, développer des outils d’analyse…\nVous mènerez des études afin d’évaluer les nouvelles technologies dans le domaine du Big Data, Data Mining ou Machine Learning afin d’identifier les solutions les plus pertinentes.\nVous serez en charge de :\nParticiper à la définition des besoins et à la rédaction des User Stories,\nCollaborer avec les Data Scientists au développement des modules d’analyse de donnée,\nConcevoir et construire des architectures de données,\nIntégrer des sources de données,\nVous assurez que les données sont facilement accessibles et que leur exploitation fonctionne comme demandé, même dans des circonstances hautement évolutives,\nExécuter des processus ETL (extraire / transformer / charger) à partir d'ensembles de données complexes et / ou volumineux\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "13 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala",
                "Python"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS"
            ],
            "OS": [
                "Linux"
            ],
            "Other": [
                "Statistiques",
                "Machine Learning",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ternair",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ternair-3915757963?position=46&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=k70htbH6tNUd3%2BJHjI90jA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "👨‍🚀 MISSION : 👩‍🚀\nEn cohérence avec la stratégie d’entreprise et la roadmap data, vous aurez pour principales missions de :\nEn lien avec l’équipe DevOps, construire, maintenir et faire évoluer la plateforme de données;\nDéfinir et piloter la cohérence de la collecte, la gestion et l’alimentation des données internes et externes, en différents modes : batch, streaming, API (architecture micro-services);\nPréparer et mettre en qualité les données pour les rendre disponibles dans les différents environnements de travail (datalake, datawarehouse, datamart);\nVérifier la qualité des données, de leur bonne et régulière exécution ainsi que de leur utilisation adéquate (gestion des coûts);\nTravailler en étroite collaboration avec les data analysts, scientists et data stewards et business de l’entreprise ;\nEn lien avec l’IT et la sécurité, veiller aux règles d'intégrité et de sécurité des données;\nVeille technologique.\n🧮 Les outils :\nPlateforme data : Google Cloud Platform (Big Query, Airflow)\nDéveloppement : Github/GitLab, Docker, Terraform, Python\nAnalytiques : Qlik\nGestion de projet s: Jira, Confluence, Miro, Drive, Docs, Sheets, Slides\n🤩 Profil recherché : 🤩\nExpérience d'au moins 4-5 ans (après études) en data ingénierie (flux, modélisation, run)\nA l’aise avec l’environnement Cloud et les infrastructures digitales\nCommuniquant, pédagogue et fortes capacités relationnelles\nAnglais (à l’écrit)\nRémunération : 42-60 k€ en package selon expérience\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Automation": [
                "Airflow"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker"
            ],
            "Collaboration": [
                "Confluence",
                "JIRA"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Hermès",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/alternance-data-engineer-h-f-at-herm%C3%A8s-3889716412?position=47&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=ugM4PYXtnIt7owFtQtvTFQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Eléments de contexte\nHermès Digital Ventes et Services recherche pour sa direction Data & Performance :\nUn Alternant Data Engineer (H/F)\nContrat d'alternance de 12 mois\nA partir de Septembre 2024\nBasé à Paris\nPrincipales activités\nVous êtes rattaché au Data manager.\nVous avez pour principale mission d’accompagner l’équipe Data dans les tâches quotidiennes :\nReporting et statistiques de ventes et trafic (notamment via l’outil Google Analytics et Google BigQuery)\nAnalyse des leviers d’acquisition de traffic SEA/SEO/Referral\nCréation de Dashboard via l’outil Google Data Studio\nParticipation aux travaux de CRO (Conversion Rate Optimization) et d’AB testing\nMise en place d’étude prédictive sur les données des sites Ecommerce\nProfil\nEtudiant en école d’ingénieur possédant une forte culture Internet et une sensibilité aux problématiques digitales e-commerce, vous avez une première expérience en entreprise\nProfil technique ou aisé avec la technique, une spécialisation en digital est en plus\nOrganisé, rigoureux, curieux, autonome, bonne expression écrite et aisance relationnelle\nMaîtrise du Pack Office indispensable, ayant déjà utilisé Google Analytics\nLa connaissance d’outils de BI / Datavisualisation serait appréciée (Google Data Studio, Tableau Software, Bime ou QlikView/QlikSense, PowerBI), de Base de Données (SQL, MySQL, BigQuery)\nUne appétence pour la Data, ses languages (Python, R) et ses technologies (Notebooks, modélisation statistique, Machine learning) est fortement appréciée.\nAnglais courant souhaité\nSensible aux produits hauts de gamme, vous souhaitez vous investir dans un stage riche et formateur\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "DBMS": [
                "BigQuery",
                "MySQL"
            ],
            "Other": [
                "Statistiques",
                "Machine Learning"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Vélizy-Villacoublay, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-spark-f-h-at-thales-3890949531?position=48&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=dcxA0hGPUFkhVxFUbO46aw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs présents sur tous les continents. Le Groupe investit dans les innovations du numérique et de la « deep tech » – big data, intelligence artificielle, connectivité, cybersécurité et quantique – pour construire un avenir de confiance, essentiel au développement de nos sociétés, en plaçant l’humain au cœur des décisions.\nThales propose des solutions, services et produits qui aident ses clients – entreprises, organisations, Etats – dans cinq grands marchés vitaux pour le fonctionnement de nos sociétés : identité et sécurité numériques, défense, aéronautique, espace, et transport.\nQUI ETES-VOUS ?\nDiplômé d’un Bac+5 en école d’ingénieur ou équivalent universitaire avec une spécialisation en informatique, vous avez au moins 3 ans d'expérience dans les technologies Big Data.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que Data Engineer, vous jouerez un rôle clé dans la conception, le développement et la maintenance de notre infrastructure de données, ainsi que dans la transformation et la gestion des flux de données.\nVOS MISSIONS :\n• Concevoir, développer et déployer des solutions Big Data en utilisant les technologies Spark.\n• Mettre en place des pipelines de données performants pour l'ingestion, le traitement et le stockage des données massives.\n• Collaborer étroitement avec les équipes métier pour comprendre leurs besoins en matière d'analyse de données et proposer des solutions adaptées.\n• Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des données.\n• Assurer la qualité et la fiabilité des données traitées, en mettant en place des processus de validation et de nettoyage.\n• Identifier et résoudre les problèmes liés à l'infrastructure Big Data et proposer des améliorations.\n• Travailler en étroite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents à partir des données.\nInnovation, passion, ambition : rejoignez Thales et créez le monde de demain, dès aujourd’hui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Coders Connect",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-coders-connect-3870419202?position=49&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=9QJTVineCdvNmTlkNlCzGg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Coders Connect and Sanofi are joining forces to bring an electrifying twist to the biopharmaceutical world!\nWork with a rhythm that suits your style (2 days remote and 3 days onsite magic).\nLanguage\n: Proficiency in English is required for this role to ensure effective communication within our diverse, global team.\nAbout Sanofi:\nWe're not just a company; we're a global movement, focused on human health and making a real difference. Our mission? To battle pain, ease suffering, and sprinkle a little bit of magic in the process by developing life-changing medicines and vaccines through breakthrough science and wizard-level technology.\nDigital & Data: The Pulse of Our Mission\nAt the heart of our quest lies our digital and data powerhouse. Think of us as the digital healthcare platform of your dreams, where innovation meets speed, and technology shakes hands with medicine. With our scale, deep-rooted connections in health ecosystems worldwide, and a knack for pushing boundaries, we're here to revolutionise medicine, one digital solution at a time.\nThe Role: Data Engineering Virtuoso\nAs our Data Engineering Virtuoso, you're tasked with designing and orchestrating the data pipelines that power our ambitious data analytics initiatives. You'll ensure our data's integrity and accessibility, laying the groundwork for groundbreaking insights and innovations.\nRequirements\nCloud Platforms: Proficient in AWS services, with Azure & GCP knowledge a plus. Your work involves leveraging cloud solutions for scalable data processing and storage.\nData Modeling & Query Performance: Expertise in crafting data models and optimizing SQL queries to enhance performance. Experience with Snowflake or similar data warehousing solutions is highly valued.\nIntegration Services: Skilled in utilizing Integration Services like IICS and Tibco, you facilitate seamless data flow and integration across various platforms.\nScripting & Development: Proficient in scripting languages such as Python and R, enabling you to automate tasks and manipulate data efficiently. Familiarity with GitHub for source code management underscores your commitment to collaborative development and version control.\nVisualization & Reporting: Knowledgeable in creating insightful data visualizations using tools like PowerBI, Tableau, or similar, turning complex data into actionable insights.\nData Governance & Compliance: A keen understanding of data quality, security, and governance standards, especially in healthcare environments subject to regulations like GxP, SOX, and data privacy laws.\nReal World Data & Standard Models: Experience with Real World Data (e.g., EHR, Claims) and familiarity with standard data models (e.g., OMOP, FHIR) enhance your ability to contribute to our healthcare objectives.\nPipeline Frameworks: Proficiency in using frameworks like Apache Airflow or Kedro for crafting efficient, reliable data pipelines that support our AI and ML initiatives.\nThe Reward:\nA chance to play a crucial role in a collaboration that's redefining healthcare through digital transformation.\nA seat at the round table of diversity and inclusion, where different backgrounds and experiences conjure the richness of our culture.\nAn endless horizon of professional growth, learning opportunities, and the chance to contribute to a future where better health is a global reality.\nThe Call to Adventure:\nIf you're ready to join a quest for better – better treatments, better outcomes, and better science – and believe in the magic of bringing diverse talents together to make miracles happen, we want you. Let's embark on this journey together and transform the future of healthcare.\nBetter is out there. Are you ready to find it with us?\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Airswift",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-airswift-3909165766?position=50&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=q7y9K5yzxgMnbFX9%2Fd5j6g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data Engineer\nLocation\n: Paris (Hybrid)\nContract type\n: 12 months +\nYears of Experience\n: 4+\nRecruitment Partner:\nAirswift\nKey Words:\nProject Management | Jira | Digiboard | Banking | Stakeholder Management | Architecture | Cloud | Payments/Credit |ServiceNow | PPM |\nResponsibilities\nDesign, develop, and implement data pipelines to collect, process, and store structured and unstructured data from various sources.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and translate them into technical solutions.\nOptimize and tune data pipelines for performance, scalability, and reliability.\nEnsure data quality and integrity throughout the data lifecycle, implementing data validation and monitoring processes.\nEvaluate and implement new tools and technologies to enhance our data infrastructure and capabilities.\nRequirements\n:\nExtensive experience in Python.\nStrong experience with data processing frameworks and tools such as Apache Spark.\nExperience with cloud platforms such as AWS, Azure, or Google Cloud Platform.\nSolid understanding of data modelling, database design, and SQL\nFrench and English speaking\nFreelancing opportunity\nThe next step\nWe have an exceptional team in place, and we are pleased to be able to appoint a further person to our growing business. We are aware that you may not ‘tick all the boxes’, but if you believe you can genuinely offer some valuable skills and experience to our business, please in the first instance contact our recruitment partner Airswift.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Antibes, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-antibes-france-h-f-at-astek-3909192086?position=51&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=%2Fx2IpOfjS03kulxGTh%2FqRg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nAntibes - France\nPubliée il y a 2 semaines\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nIntervenir dans la conception, le développement, les tests unitaires, la qualification, l’intégration continue et la mise en production d’évolutions sur les projets du pôle produits scoring (un pôle visant à développer des solutions permettant de générer des scores ou des segments d’information pertinents dans divers domaines, notamment : profiling TV, PUB, SAB, MMDM, Voscastview) chez l’un de nos partenaires spécialisé dans le secteur des télécoms.\nVotre Mission, Si Vous L’acceptez :\nEn collaboration avec les autres membres de l’équipe, vous devrez prendre en charge le RUN des applications du pôle produit scoring.\nConception d’une solution se basant sur les développements existants et les besoins métiers remontés par le Product Owner.\nRéalisation et développement de nouvelles fonctionnalités sur les composants des applications du pôle produits scoring et environnement CGP.\nVotre Future Équipe :\nAu sein d’un environnement riche et complexe, vous évoluerez avec des experts passionnés à la fois techniques et fonctionnels (Ingénieurs spécialisés, chef de projet, scrum master, product owner, analystes …).\nVotre stack de jeu\nD ans un environnement SAFE sous cloud GCP, Big Query, OnPrime, Grafana, Python et Ansible.\nVous ?\nDe formation Ingénieur, vous justifiez d’une première expérience sur un poste de Data engineer. Vous possédez des compétences d’autonomie et d’adaptabilité et vous avez une capacité à communiquer efficacement au sein d’une équipe.\nLe Groupe Astek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies, présent sur les 5 continents. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de ses 7800 collaborateurs qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde et ayant réalisé un chiffre d’affaires hors\nacquisitions de 600M€ en 2023.\nTous les détails sur le Groupe sur le site\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Data engineer – Big Data\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Data engineer – Big Data\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Automation": [
                "Chef",
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Web Transition",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=52&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=BIIB%2Bl%2B%2Fq%2BoLTD44AOhf6A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Web Transition, c’est qui ?\nFondée en 2011,\nWeb transition\nest une entreprise de services numériques opérant sur le marché de l’IT/Digital !\nConstituant une part essentielle de\nMoOngy Digital Lab\n, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et également en Data !\nNotre objectif : nous implanter comme un acteur principal sur le marché de la Transformation Digitale en accompagnant et valorisant les compétences de nos collaborateurs !\nNous sommes convaincus que le succès de MoOngy Digital Lab réside dans la somme des potentiels de nos équipes 🤝\nTon équipe : La tribu Data\nParce qu’il est indispensable que tu puisses partager tes connaissances mais aussi en acquérir de nouvelles, tu feras partie de l’une de nos tribus : celle de la Data. De plus, cela te permettra d’être acteur dans le développement et la stratégie de Web Transition. Ce système de co-réflexion et co-construction est un fondement essentiel chez nous !\nDans cette aventure, tu :\nT’assures\nde la maîtrise de la donnée et est garant de la qualité de son utilisation (référencement, normalisation, et qualification)\nTravailles\nà la compréhension et l'intégration des données en provenance des différents formats\ndes interfaces de flux\négalement à la définition de la politique de la donnée et à la structuration de son cycle de vie dans le respect des réglementations en vigueur\nla supervision et l'intégration des données de diverse nature qui proviennent de ces sources multiples et vérifie la qualité des données qui entrent dans le Data Lake\nGarantis\nl'accès qualitatif aux sources de données\nFacilites\nl’accès aux données pour tes collègues (data scientists, data analysts…)\nAssistes\nles autres équipes dans l'accès et la compréhension des données des socles.\nRejoins-nous si tu as :\nExpérience d’au-moins 4 ans dans la Data\nAppétence à la qualité des données.\nConnaissance familière des Datawarehouses.\nMaitrise de Python, Oracle SQL, GCP/Power BI\nAisance avec les indicateurs, tu as une bonne capacité d'analyse et de rédaction.\nTon savoir-être :\nOuvert d’esprit\nRigoureux\nAutonome\nRespectueux des différences de chacun\nCurieux\nProactif\nAgile\nPar où on commence ?\nUn premier entretien RH d’1h pour comprendre ton parcours et tes aspirations\nUn second entretien de 45 minutes avec l’un de nos Business Manager afin de valider tes compétences et qu’il se projette sur l’une des missions qu’il pourrait te proposer\nUn troisième entretien de quelques minutes avec notre responsable d’agence pour te proposer d’intégrer notre superbe Team Web !\n3 entretiens en peu de temps, si ton profil correspond tu intègreras très vite nos équipes 😉\nPrêt pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant à cette offre ! Voici les avantages qui t’attendent en tant que Weber :\n🤩 Des collègues incroyables\n🏆 Certifiée Great Place to Work\n🎮 Des bureaux sympas (où vous serez toujours les bienvenus)\n🎉 Des teambuilding et évents tous les mois\n💻 Des tributs métiers pour échanger entre Weber du même métier\nDes missions chez le client qui sont accompagnées et coachées par ton manager\nUn accompagnement dans ton plan de carrière et tes envies de re skilling\n🤓 Un catalogue de formations certifiantes ouvert à tous les salariés\n🍽️ Une carte tickets restaurant MyEdenred\n❤️ Une mutuelle GrasSavoye\n🚎 Une prise en charge des frais de transport à 100%\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "Oracle SQL"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Mobilize Financial Services – France",
        "location": "Noisy-le-Grand, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-mobilize-financial-services-%E2%80%93-france-3869637982?position=53&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=CKz25Qo0Q47Y0UW4EgySmA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "🚗 En route vers Mobilize !\nA l’écoute de tous nos clients, nous créons des services financiers innovants pour construire une mobilité durable pour tous.\nRejoindre Mobilize Financial Services,\nc’est d’abord choisir d’intégrer un groupe international\n, filiale de Renault Group, une banque de financement solide, partenaire des constructeurs Renault–Nissan–Mitsubishi. Nos 4 000 collaborateurs présents dans 35 pays, agissent ensemble au service de nos clients.\nNous proposons à nos clients - particuliers comme professionnels - les financements et les services les plus adaptés pour les véhicules neufs et d'occasion.\nNous finançons également l'activité des réseaux de concessionnaires des marques de l'Alliance Renault-Nissan-Mitsubishi et nous veillons à faciliter leur gestion au quotidien pour leur permettre de développer leurs ventes et assurer leur pérennité financière.\nNotre entreprise se \"MOBILIZE\" en faveur de la diversité culturelle, l'égalité hommes-femmes et l'intégration de personnes en situation de Handicap. Nous favorisons un environnement de travail où les différences individuelles sont reconnues, appréciées, respectées et valorisées, de façon à mettre à profit les talents et les forces de chacun.\n🚘Prenez le volant ! Pas de routine, tous nos itinéraires sont différents !\nAu sein de la DSI\n,\nvotre\nfutur métier consistera à :\nAccompagner l’équipe dans la transformation du domaine décisionnel construit sur une architecture type DWH et le porter sur la solution GCP (Google Cloud Platform) de Mobilize FS\nParticiper à la construction du projet de transformation vers GCP\nParticiper aux projets d’évolution de notre plateforme Suite Elastic (ELK - Kibana)\nPiloter des projets en étroite collaboration avec les directions métier et en accord avec le TBA (Tableau de Bord des Actions).\nAssurer la gestion du budget, du planning, de la tenue des jalons et du respect des engagements sur les projets en responsabilité\nAssurer la qualité et le bon fonctionnement du chargement des données.\nAssurer la mise à disposition des données et des outils de reportings à toutes les directions clientes dans le respect des contrats de service\nVéritable tout-terrain, vous nous intéressez !\nL’esprit d’équipe et le sens du service client pour atteindre ensemble les différents objectifs ambitieux et satisfaire les différentes parties avec un haut niveau de qualité.\nVous avez un bon relationnel, de l’écoute et une excellente communication afin d’interagir avec des interlocuteurs de différents niveaux (direction technique et métier) et de travailler en transverse.\nLe sens de l’analyse et de bonnes capacités d’anticipation pour déceler les problèmes avant la naissance de ces derniers.\nForce de proposition : avec vous il n'y a pas de problèmes, que des solutions\nVous avez un niveau d’anglais vous permettant de lire et de comprendre de la documentation technique\n💻🖱 Environnement technique :\nMaitrise des langages Python - SQL / NoSQL\nExpérience significative sur Python\nExpérience avec Git\nUne expérience avec les outils Nifi, Airflow et GCP (BigQuery / Cloud Function / Cloud Storage …) serait un plus\nGestion de projet, maintenance, évolution, support\nAppétence pour les sujets techniques et fonctionnels : outils de modélisation, exploration de données, IA, machine learning\nPourquoi nous rejoindre ?\nVotre Pack confort\nest composé de nombreux avantages 😀 :\nRejoindre Mobilize Financial Services c’est intégrer un grand groupe international qui offre des opportunités de carrière\n.\nUn environnement de travail moderne et convivial\n: locaux agréables, salle de sport, terrasse, restaurant d’entreprise, parking avec un CSE dynamique avec de nombreuses offres voyages, sport, famille,\nNous sommes mobilisés pour développer la qualité de vie au travail de nos collaborateurs en faisant évoluer nos façons de travailler (méthodes, outils, organisation du travail…) et nous sommes fiers d’être certifiés ⭐\nGreat Place To Work ⭐\nPossibilité de télétravailler 2 jours par semaine\nNous proposons une\nrémunération selon profil + Participation + Intéressement\nLocaux situés au pied du RER A – Noisy le Grand Mont d’Est\n❗ Mobilize Financial Services déménage ❗ Les postes à pourvoir en région parisienne seront basés à Boulogne Billancourt à horizon 2026\nPour en savoir plus sur notre entreprise,\nsuivez-nous sur LinkedIn !\nLa route du recrutement ?\n📞 Un rapide entretien téléphonique,\n🛑\nun premier échange\navec Marie DE CARLI, Responsable du département DATA\n↪ et un dernier virage avec Agathe GROSBOIS, Responsale des Ressources Humaines\nL’équipe Mobilize FS a hâte de vous recevoir !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Machine Learning",
                "Cloud"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MP DATA",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-mp-data-3908719610?position=54&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=6BLg7dCoO3iP33cOMKL%2B9g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "MP DATA est une société spécialisée dans l’acquisition, le traitement, et la valorisation des données.\nDepuis sa création en 2015, MP DATA accompagne ses clients, majoritairement industriels, dans le management de leur performance et l’exploitation de leurs données.\nLes collaborateurs, tous issus de grandes écoles, incarnent au quotidien les valeurs d’Excellence, de Partage et d’Engagement.\nIls associent savoir-faire technique, méthodologie et passion et mettent leurs compétences au service de missions et projets au sein de grands groupes français.\nMP DATA accompagne ses clients sur toute la chaine au travers de 3 pôles d’expertise : Conseil et Stratégie, Infrastructure & CloudOPS, Data Science.\nChez MP DATA, les équipes commerciales cherchent des missions en fonction des envies des collaborateurs et non pas l’inverse. Les consultants sont accompagnés dans tous leurs projets, de la mobilité géographique, au changement de secteur d’activité en passant par le développement de nouvelles compétences.\nRejoindre MP DATA, c’est la garantie de travailler sur des sujets passionnants avec un cadre technique fort.\nDescriptif du poste :\nNous recherchons un Data Engineer expérimenté pour rejoindre notre équipe.\nEn tant que Data Engineer, vous serez responsable de la conception, du développement et de la mise en œuvre de pipelines de traitement de données en temps réel à grande échelle.\nVous travaillerez avec des technologies telles que Kafka, Flink, Kinesis et vous utiliserez les services du cloud AWS pour stocker et traiter les données.\nVos responsabilités :\nUtiliser Kafka pour le traitement de flux de données en temps réel à grande échelle, en travaillant avec les producteurs, les consommateurs et les topics.\nMettre en œuvre des pipelines de traitement de données en streaming avec Flink, en appliquant des transformations complexes et en gérant les états.\nÉcrire du code efficace et maintenable en Java / Python pour manipuler et analyser les données en temps réel.\nUtiliser Kubernetes pour déployer et gérer des applications conteneurisées à grande échelle, en assurant la résilience et l’évolutivité des services.\nUtiliser les services AWS tels que Amazon S3, AWS Lambda, Elastic Kubernetes Service (EKS), Elastic Container Service (ECS) et Elastic Compute Cloud (EC2) pour le stockage, le traitement et le calcul des données en temps réel.\nSuivre les meilleures pratiques pour une utilisation efficace du cloud, en assurant la gestion des coûts, la sécurité des données et la disponibilité des services.\nCollaborer avec l’équipe de développement logiciel et la gestion de projets pour assurer un flux de développement fluide et une livraison efficace des fonctionnalités.\nBon à savoir :\nCDI / ASAP / Toulouse\nProfil recherché:\nNous recherchons un candidat diplômé d'une grande école d'Ingénieur avec une première expérience.\nCompétences nécessaires :\nExpérience significative dans un environnement industriel en mode DevOps, avec des outils tels que CICD, gitlab, Jenkins, Sonar, Nexus, XLdeploy, Camunda, etc.\nMaîtrise des langages de programmation tels que Python, Java et expertise dans l’écriture et l’optimisation du code SQL\nMaîtrise du français et bonne maitrise de l’anglais.\nCapacité à travailler en équipe et esprit d’équipe.\nLe processus de recrutement se déroule en 3 entretiens :\nPrise de contact\n1er entretien : Présentation et projet du candidat + présentation MP DATA\n2ème entretien : Entretien de qualification technique\n3ème entretien : Rencontre avec les équipes dans les locaux MP DATA + Proposition de collaboration\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Flink"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "SEVETYS",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-sevetys-3905649273?position=55&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=Lf0jM8Rc3OE5oWaxmOH4DQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Sevetys, premier groupe français de cliniques vétérinaires, est présent dans toute la France avec plus de 150 établissements. Créé en 2016, le groupe souhaite moderniser le métier et mettre la qualité des soins et la satisfaction client au cœur de son projet.\nLe projet d’entreprise se caractérise par son hyper croissance et une culture de type start-up axée sur le collectif, la cohésion, et l’engagement.\nFort de son succès, Sevetys poursuit sa structuration et recrute un / une :\nData Engineer\n​Le\nData Engineer\ntravaille en étroite collaboration avec une équipe Agile pluridisciplinaire pour construire des pipelines de données de haute qualité permettant de mettre en œuvre des solutions analytiques. Ces solutions génèreront des informations à partir de nos données collectées, permettant de faire progresser les capacités de prise de décision du management de l’entreprise. Ce rôle nécessite une compréhension approfondie de l'architecture des données, de l'ingénierie des données, de l'analyse des données, du reporting. Le candidat idéal est un ingénieur en données/logiciel ayant au moins une première expérience dans la création de produits de données soutenant des solutions analytiques.\nMissions :\nConçoit, développe, optimise et maintient une architecture de données et des pipelines qui respectent les objectifs de l'entreprise ;\nRésout des problèmes de données afin de fournir des informations qui aident notre entreprise à atteindre ses objectifs ;\nCrée des jeux de données pour les membres de l'équipe d'analyse afin d'améliorer leur productivité ;\nFavorise une culture du partage, de la réutilisation, de la stabilité de la conception à l'échelle et de l'efficacité opérationnelle des données et des solutions analytiques ;\nContribue à l'évaluation, la mise en œuvre et le déploiement d'outils et de processus émergents pour l'ingénierie des données analytiques afin d'améliorer notre productivité en tant qu'équipe ;\nÉlabore et met en œuvre des plans de communication/éducation sur les capacités, les normes et les processus d'ingénierie des données analytiques ;\nTravaille en partenariat avec des analystes business et des architectes de solutions pour développer des architectures techniques pour les projets et initiatives stratégiques de l'entreprise.\nExpertises techniques :\nExpérience du développement de bases de données et d'une variété de technologies de bases de données relationnelles ;\nExpérience des entrepôts de données ;\nExpertise en SQL et en analyse de données ; maîtrise Python ;\nIdéalement certifié des technologies BI du Cloud Azure (Synapse, Azure Data Factory, PurView) ;\nConnaissance de l'intelligence artificielle, des statistiques et/ou des mathématiques appliquées ;\nExpérience dans le développement de solutions sur des services et infrastructures de cloud computing dans le domaine des données et de l'analyse ;\nExpérience du déploiement de Power BI ;\nExpérience conceptuelle des données et de l'analyse, par exemple ETL, modélisation dimensionnelle, outils de reporting, gouvernance des données, entreposage des données, données structurées et non structurées, qualité de données ;\nConnaissance CI/CD et GitLab fortement apprécié.\nExpérience agile / Digitale / gouvernance :\nPassionné(e) le développement basé sur les données, la fiabilité et l'expérimentation ;\nExpérience souhaitée de travail au sein d'une équipe produit Agile collaborative ;\nConnaissance de la gouvernance de la donnée.\nSkills Individuels :\nMotivé(e) et doté(e) de solides compétences en matière de résolution de problèmes et d'apprentissage ;\nFlexibilité face aux changements d'orientation du travail au fur et à mesure de l'évolution du projet ;\nExcellentes capacités de communication, d'écoute et de persuasion.\nAttitude attendue :\nSens aigu des chiffres, curiosité intellectuelle et volonté d'adapter sa position sur la base d'informations complémentaires ;\nForte éthique de travail ; capacité à travailler à un niveau abstrait et à obtenir un consensus.\nInformations supplémentaires :\nPoste à pourvoir dès que possible ;\nRemboursement des frais de transports + Mutuelle ;\nPossibilité de télétravail jusqu'à un jour par semaine.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "Other": [
                "Statistiques",
                "Cloud",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akkodis",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-akkodis-3890779946?position=56&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=utcB03oN0vsGUS9G8IIHTw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La ligne de service Consulting & Solutions d’Akkodis France renforce ses équipes en région Hauts-de-France et recrute un\nData engineer H/F\nen\nCDI\nsur la\nmétropole lilloise\n:\nDescription de la mission :\nConcevoir, mettre en oeuvre et maintenir des pipelines de données efficaces et évolutifs dans un environnement cloud (comme AWS, Azure, Google Cloud Platform…)\nAssurer la qualité des données et des modèles\nDéfinir les bonnes pratiques de développement en implémentant des outils de CI/CD\nAssurer une veille technologique sur les technologies Cloud\nCapacité à interagir avec des parties prenantes diverses : business analyst, architecte, métier…\nVeiller au bon fonctionnement des pipelines en production\nProfil :\nDe formation\nBac +4/5 en informatique\nou issu d'une\nécole d'ingénieur\n, vous possédez une expérience de\n3 ans\nminimum en tant que data engineer ainsi que les compétences suivantes :\nUne bonne connaissance des écosystèmes liés à la data (Kafka, ETL, base de données…)\nUne première expérience sur un cloud provider (AWS, Azure, GCP)\nUne bonne maitrise de langages de programmation tels que SQL, Python, Scala\nAkkodis accompagne ses clients dans la mondialisation de leurs projets, aussi un anglais courant est requis pour l’ensemble de nos collaborateurs.\nProcessus de recrutement :\nUne chargée de recrutement vous contacte pour échanger sur votre projet professionnel\nVous échangez ensuite avec un.e manager sur les aspects techniques, les projets\nChez Akkodis nous sommes convaincus que de l’intelligence collective naît le succès. Il n’existe pas qu’un modèle, nous valorisons l’agilité et l’excellence, l’audace et la créativité.\nEt si nous parlions ensemble de vos ambitions pour les prochaines années ?\nAkkodis est une entreprise handi-engagée et inclusive. Tous nos postes sont ouverts aux handicaps et à la diversité. Tous différents, tous compétents !\nAkkodis, est un acteur mondial de l’ingénierie et de l’IT et un leader dans la smart industrie. Nous accompagnons nos clients dans leurs projets de transformation digitale via 4 lignes de service : Consulting, Solutions, Talent et Academy. Akkodis est un partenaire technologique de confiance pour ses clients à l’échelle internationale. Nous co-créons et nous imaginons des solutions de pointe pour répondre aux défis majeurs de notre société, qu'il s'agisse d'accélérer la transition énergétique et de développer la mobilité verte, ou encore de construire des approches centrées sur les utilisateurs.\nDotés d’une forte culture de l’inclusion et de la diversité, nos 50 000 experts en IT et en ingénierie, présents dans 30 pays, allient les meilleures compétences technologiques à une connaissance transverse de toutes les industries pour façonner un futur plus durable. Nous sommes passionnés par l’idée d’inventer ensemble un avenir meilleur.\nAkkodis en France, ce sont près de 9.000 experts en IT et en ingénierie répartis sur l'ensemble du territoire, des collaborateurs partageant des valeurs fortes d'honnêteté, de respect, d'équité et d'inclusion. Notre engagement : leur permettre au quotidien d'être eux-mêmes au travail, et acteurs de leur vie et de leur développement au sein d'Akkodis.\n*Akkodis est une marque commerciale sous laquelle les entités AKKA et Modis opèrent\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "FINAXYS",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-at-finaxys-3887107285?position=57&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=8PYxM2n53IXQxAe8yjEkSw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LE CONTEXTE\nLeader en\nIT\n, dans les domaines Banque\n,\nFinance\net\nAssurance\n,\nFinaxys\nest un cabinet de\nconseil\ncréé en 2008. Nous accompagnons au quotidien les plus grandes banques du pays sur leur\ntransformation digitale\n(BNP Paribas, Société Générale, Crédit Agricole, Natixis, etc.)\nNos clients bancaires travaillent également dans des contextes Big Data sur des applications centrales rattachées aux Datalakes.\nLES MISSIONS\nDéveloppement et traitements sur des applications Big Data (Python)\nÊtre force de proposition sur les choix techniques les plus pertinents\nMaintenir la qualité des solutions, mesure de cette qualité, alerte sur les non-conformités et validation des solutions définitives.\nAnalyser des risques liés aux solutions envisagées et proposition des actions de remédiation.\nApporter des solutions IT répondant au mieux aux besoins du business porté par la/le Product Owner (Métiers/Fonctions) en cherchant toujours la maximisation de la valeur générée\nAccompagner les équipes dans les migrations Cloud\nENVIRONNEMENT TECHNIQUE\nPython\nPandas\nScirpting Big Data\nCulture DevOps (Jenkins, Maven, Ansible)\nPROFIL\nCompétences Techniques et Fonctionnelles requises\nMaitrise obligatoire de l’anglais\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataAnalytics": [
                "Pandas"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Automation": [
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Apside",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-apside-3909772916?position=58&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=njzYBPoccfJwy9kiqugRwg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Offre d'Emploi : DATA ENGINEER H/F chez Apside\nDescription du poste :\nNous sommes à la recherche d'un Data Engineer passionné pour rejoindre notre équipe dynamique. Si vous avez une expertise dans le Big Data, la Data Science, l'analyse de données et l'architecture de données, cette opportunité est faite pour vous. Intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.\nVos missions :\nDéveloppement des jobs Spark pour la collecte et la transformation des données comptables disponibles dans les bucket S3.\nOptimisation des jobs Spark.\nDéveloppement des batchs Java et écriture des données au formats comptables.\nÉcriture et ordonnancement des DAGs Airflow.\nSupport du développement Spark Scala.\nMaintenance applicative.\nProduction des événements dédiés à la plateforme de données.\n.\nVotre rôle, vos compétences :\nVous maîtrisez au minimum un langage de programmation appliqué à l’analyse de données (SQL, Scala, Python, Java).\nVous êtes passionné par le Big Data et le Machine Learning.\nVous concevez et mettez en œuvre des stratégies sécurisées d'acquisition et d'intégration de données.\nVous configurez des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks).\nEnvironnement technique :\nSQL\nPython/Spark\nCloud AWS: AWS Glue, AWS Lambda (possibilité de vous former sur AWS)\nStockage objet (AWS S3)\nOrchestration et scheduling de tâches (Apache Airflow)\nBases analytiques et bases NoSQL (ElasticSearch, AWS Athena)\nVotre profil :\nFort de 4 années d’expérience en Data Engineer/ DATA ANALYST\nTitulaire d’une formation supérieure IT.\nCapacité à s’intégrer dans un cadre technique client tout en étant à même de proposer des pistes d’améliorations pertinentes.\nAutonome dans la gestion des projets.\nCurieux et impliqué, vous êtes bon communicant avec les clients et les acteurs de culture technique différente.\nDe bonnes raisons de rejoindre Apside ?\nUn esprit start-up avec la stabilité d’un grand groupe, qui favorise l’agilité, le travail d’équipe et la proximité. Alors qu’Apside ne cesse d’agrandir sa famille déjà forte de plus de 3000 consultants, nous sommes à la recherche de nos nouveaux talents !\nCDI + package salarial avantageux (Mutuelle offerte, RTT, Tickets Restaurant, Intéressement ...)\nParticipez et animez nos soirées techniques (Project Lab, Test Lab…),\nDevenez speaker (Devoxx, DevFest, NCraft…),\nFormez vous avec l’Academy By Apside (e-learning, formation, certification).\nDéveloppez votre réseau (Soirées trimestrielles, Afterwork, Soirées d’intégration…),\nIntégrez notre Communautés d’Experts et testez les dernières innovations techniques sur notre Bac à Sable !\nApside s’engage en faveur de l’emploi des personnes en situation de handicap avec sa filiale Apsid’EA : 1ère entreprise adaptée totalement intégrée à une ESN !\nPour aller plus loin avec APSIDE !\nhttps://www.apside.com/fr/nos-offres-emploi/?_joboffer-agency=17833&_paged=2\nCe poste de DATA ENGINEER est fait pour vous !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "17833",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Apache Airflow",
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "AXA en France",
        "location": "Hauts-de-Seine, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-senior-at-axa-en-france-3884386043?position=59&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=jYXbX2rDlEmATeFHwBpbng%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Environnement\nEn tant que\nSenior Data Engineer F/H\n, vous allez contribuer directement aux projets des directions métier (ex : fraude santé, multiéquipements, pricing IARD, optimisation du lead management, fragilité auto, …) d’AXA France et à la construction du socle technique Big Data.\nVous allez intégrer une équipe d'une dizaine de personne composée de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus métier de la Direction Transformation Digital Tech et DATA (DT2).\nLa Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :\n- Une organisation agile en feature teams : tribus, guildes, squads\n- Des projets sur des applications innovantes à fort trafic (web, mobile…)\n- Des méthodologies craft (TDD, BDD, clean code, code review…) et DevOps\n- Une communauté de partage de bonnes pratiques (BBL, dojo, meetup, conf…)\nVotre rôle et vos missions\nVous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :\nPasser de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le datalake\nConsolider ces données au fur et à mesure de leur alimentation récurrente dans le data lake\nLes exploiter pour atteindre la finalité business (exposition de business view, réintégration des résultats dans le SI, service de scoring, …)\nDe travailler à la création du socle technique Big Data et industrialiser le cycle de développement de l'équipe\nDe mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)\nVotre profil\nVous justifiez de plusieurs expériences significatives (+ de 5 ans) sur du\ndéveloppement big data, en particulier sur du PySpark.\nCompétences techniques :\nConnaissances avancées en développement en\nPySpark\n(Spark avec le langage Python)\nMaitrise de l'environnement\nMicrosoft Azure\nConnaissances avancées d'outils de BI comme\nPowerBI\nCompétences transverses :\nCapacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier\nExpérience en mode de delivery Agile (Scrum, Kanban, etc...)\nDriver et accompagner des Data Engineer junior sur les aspects technique\nEt Idéalement :\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de données relationnelles et NoSQL\nUne expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nQui sommes nous ?\nAXA est un des leaders de l’assurance et de la gestion d’actifs dans le monde.\nNous aidons nos 108 millions de clients à traverser les petites et grandes difficultés de la vie.\nChaque jour, nous agissons ensemble pour inventer la meilleure manière de les protéger et voulons donner à chacun les moyens de vivre une vie meilleure.\nUn challenge qui donne le sourire et envie de se lever le matin !\nChez AXA, nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C’est pour cette raison que nous menons une politique RH engagée qui favorise la diversité, qui préserve l’équilibre vie privée-vie professionnelle et accélère le développement des compétences et des carrières.\nAinsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une véritable culture d’expertise, accélérant le développement des compétences de chacun et proposant une rémunération attractive.\nPourquoi nous rejoindre ?\nVous êtes porteur d’idées et d’initiatives innovantes ? Vous proposez des solutions et êtes au service du client ? Faites partie de notre grande famille en rejoignant\nUn leader mondial offrant des opportunités de carrières intéressantes\nUne entreprise qui donne une place de choix à l’innovation, à l’initiative et aux actions solidaires (notamment via l’association AXA Atout Cœur)\nUn environnement inclusif à tous les niveaux (mixité, handicap, initiatives pour favoriser l’insertion des jeunes, orientation sexuelle, etc.)\nUn accès à de multiples avantages (congés, temps partiel, télétravail, etc.)\nUn cadre stimulant, qui permet de rencontrer des collaborateurs performants et d’enrichir ses compétences\nVictime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Junior",
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Beelix",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-beelix-3865239426?position=60&pageNum=0&refId=3%2BsKCnzKBomYeuJjjsOexw%3D%3D&trackingId=fIAQGZ8QGGFUOJnyjmSu2g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants:\n🚗Automobile\n⚡Energie\n📡Médias & Télécoms\n👗Luxe & Retail\n💶 Banque, Finance & Assurance\n✈️Défense\nAujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.\nDans le cadre de notre développement, nous recherchons un Data Engineer en Île-de-France.\nQuelles missions au quotidien ?\nVous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :\nPasser de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le Datalake\nConsolider ces données au fur et à mesure de leur alimentation récurrente dans le Datalake\nLes exploiter pour atteindre la finalité business (exposition de Business View, réintégration des résultats dans le SI, service de scoring, …)\nDe mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)\nAccompagner les Data Engineers sur son périmètre pour garantir la qualité des livrables\nExpertise souhaitée\nExpertise en SPARK et PySpark\nExpertise sur Databricks\nUne expérience sur un cloud provider public comme Azure (idéalement), AWS, ou GCP\nConnaissances avancées d'outils de BI comme PowerBI (idéalement) ou Spotfire\nCapacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier\nEtre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de données relationnelles et NoSQL\nUne expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nA propos de vous ?\nDiplômé d'une école d'ingénieurs ou équivalent\nAu moins 3 ans d'expérience en tant que Data Engineer\nExpérience en mode de Delivery Agile (Scrum, Kanban, etc.…)\nVous avez un bon niveau d’anglais tant à l’écrit qu’à l’oral\nPourquoi nous rejoindre ?\nUn suivi et un accompagnement au quotidien\nUn organisme de formation certifié Qualiopi, un abonnement linkedin learning pour chaque salarié et des partenariats avec des spécialistes pour d’autres expertises\nDe nombreux événements : Afterworks, Communautés métiers, Happy talks…\nune Expérience personnalisée basée sur vos besoins grâce au Prédictive Index\nNotre package « unBeelievable » : 100% du titre de transport, Tickets restaurants, CSE, Prime de participation ...\nNombreux évènements (afterworks, sport, etc) et des communautés métiers dynamiques\nLe processus de recrutement ?\nÉchange téléphonique (15 min)\nEntretien 1 RH pour apprendre à vous connaître\nEntretien 2 avec votre futur N+1 pour appréhender la relation managériale\nEntretien 3 avec un Responsable commercial pour avoir la vision stratégique\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "100",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "EarthDaily Agro",
        "location": "Balma, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-earthdaily-agro-3883708013?position=1&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=Ytwd%2FdNgDJIktwGUm2oPgA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "About Us\nEarthDaily Agro provides space age data and analytics to the organizations and people who feed the planet!\nWith 35 years of industry experience, EarthDaily Agro provides customers with the data, analysis and knowledge they need to make more efficient and effective decisions. B2B services range from global risk management and monitoring of agricultural commodities to the marketing of inputs and precision agriculture consulting, using the latest research in agronomy, information technologies and remote sensing.\nEarthDaily Agro also develops highly customized business solutions for agricultural lenders, insurers, input suppliers and food companies, with easy-to-use analytics, that help reduce the daily risks of agriculture.\nEarthDaily Agro is headquartered in Minneapolis, MN, USA, with offices in France, Brazil, Australia and Switzerland and is a division of EarthDaily Analytics Corp.\nEarthDaily Analytics Corp., a vertically-integrated data processing and analytics company, is launching a new constellation of earth observation satellites. The EarthDaily satellite constellation will significantly enhance geospatial analytics capabilities in agriculture, forestry, environment, financial services, and intelligence, among many other verticals.\nMain Job Tasks And Responsibilities\nAs a EarthDailyAgro Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and infrastructure specialized for geospatial and remote sensing applications. You will work closely with data scientists, geospatial analysts, remote sensing experts, software engineers, and DevOps teams to ensure the successful deployment and scaling of data pipeline to feed geospatial data machine learning models. Your role will be crucial in optimizing the geospatial machine learning ecosystem and ensuring the seamless integration of AI-driven geospatial solutions into real-world applications.\nYour Responsibilities Include\nCloud-based data pipeline Conceptualization, Development and Scaling: Build up pipeline to ingest large volumes of geospatial data, pre-process them and meet data scientists’ requirements, in terms of accessibility, speed, format, quality.\nAutomation and CI/CD: Industrialization of pipeline deployment, orchestration, workflows, and versioning.\nCost & Speed Optimization: Collaborate with infrastructure team to develop, optimize, and fine-tune pipeline.\nCloud and Containerization: Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization tools.\nInfrastructure Management: Utilize containerization technologies and cloud-based services to set up and manage infrastructure, enabling seamless deployment and scalability.\nMonitoring and Anomaly Detection: Implement monitoring systems to track pipeline performance and identify anomalies.\nVersion Control and Data Version Control: Proficient with version control systems like Git and DVC.\nSecurity and Compliance: Ensure the security and privacy of geospatial data, adhering to relevant data protection regulations and industry best practices.\nCollaboration and Communication: Collaborate with interdisciplinary teams to integrate data pipeline into existing applications or develop new geospatial products.\nIssue Resolution and Troubleshooting: Identify and resolve promptly technical issues related to geospatial data processing, performance, or infrastructure.\nEducation, Knowledge And Abilities\nRequirements\nEducation: Master's degree in Computer Science, specialisation in Geomatics and/or Remote sensing would be a plus.\nExperience: 3+ years experiences with data pipeline processes and deployment is a must-have. Proven hands-on experience in setting up pipelines and data processes with opensource tools (e.g., MLFlow, Argo, Kubeflow) is desirable.\nProgramming Skills: Proficiency in Python and with data manipulation frameworks (e.g., dataframe, numpy, pandas, xarray, rasterio) and librairies (e.g., Dask).\nProblem-Solving Skills: Autonomous, and strong analytical and problem-solving abilities to address complex geospatial data and analysis challenges.\nCommunication Skills: Excellent communication and interpersonal skills to collaborate effectively with cross-functional teams and stakeholders.\nFrench mandatory (job based in France). Fluent in English (oral and written): meetings with internal are mostly in English.\nPreferred Additional Skills\nExperience with Earth Observation (EO) data analysis and processing.\nExperience with geospatial data formats (e.g., GeoTIFF, Shapefile, NetCDF).\nSpatial Analysis Techniques: Understanding of spatial analysis techniques and algorithms commonly used in geospatial data manipulation.\nRemote Sensing Integration: Knowledge of remote sensing data sources (e.g., STAC catalog, satellite imagery, LiDAR, SAR) integration into data pipelines for accurate and up-to-date geospatial analysis.\nCONDITIONS\nFull time job based in Balma, near Toulouse, France.\nFixed + Bonuses\nTR / \"Family\" insurance / CSE\nPowered by JazzHR\nWrfSXQ5YJg\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataAnalytics": [
                "NumPy",
                "Pandas"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Interpersonal Skills",
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Aubay",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=2&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=Z5Mewzr%2FSGPKfaqPEhLUvw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Passionné par la Data, tu souhaites rejoindre une communauté d’experts dans le domaine afin de développer tes compétences en Data Engineering. Aubay renforce ses équipes Data et recherche des Data Engineers pour intégrer des dispositifs de projets pointus et variés.\nTon quotidien en tant que Data Engineer chez Aubay, :\nDéfinition de la stratégie de stockage et mise en œuvre des technologie appropriées (base de données SQL, NoSQL, stockage distribué,…)\nIngestion des données (structurées, semi-structurées ou non-structurées) selon différentes fréquences : batch, micro-batch ou temps réel\nConception et mise en œuvre de pipelines de données afin de fournir des données prêtes à l’emploi aux consommateurs : uniformisation, mise en qualité, enrichissement, calcul d’indicateurs,…\nConception et développement d’API pour exposer les données auprès d’applications tierces\nAppui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning\nPréparation et animation d’ateliers de travail avec des interlocuteurs variés : recueil/approfondissement des besoins métiers, avancement/restitution des travaux, transfert de compétences,…\nTon profil :\nTu dispose d’une formation niveau BAC+5 (Master 2 ou école d’ingénieur) spécialisée en informatique\nTu as déjà une première expérience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data\nLes technologies telles que Hadoop, Spark ou Kafka sont tes technologies de prédilection\nLa programmation n’a plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python\nTu maîtrises les tenants et aboutissants de la philosophie DevOps et des outils orientés CI/CD\nTu es soucieux de la qualité et de la performance de tes développements et tu t'intéresse à l’innovation frugale\nTu es un expert technique dans ton domaine sans pour autant oublier l’importance d’une communication orale et écrite de qualité et adaptée à chacun de tes interlocuteurs\nTu travaille au quotidien en mode agile et tu en maitrise les fondements\nCe qui nous caractérise :\nDes missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs variés (Banque, Assurance, Telecom, Industrie,…) qui permettent à nos collaborateurs de monter en compétences et de devenir des experts Data reconnus\nDe l’apprentissage en continu avec des formations et des certifications sur les technologies Data d’aujourd’hui et de demain\nDes experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets\nDes communautés de savoir-faire Data proposant de manière régulière aux collaborateurs d’Aubay du contenu et des évènements de partage (webinar, meetup/afterwork, BBL,…) sur les thématiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,…\nAubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.\nTa carrière chez Aubay :\nTu auras la possibilité de développer et certifier tes compétences sur les dernières technologies Data avec un focus fort sur les plateformes Data Cloud telles qu’Azure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks\nTu pourras rejoindre la BU d’excellence Data et évoluer au sein d’un environnement humain et professionnel de haut niveau. Tu profiteras d’un management sur-mesure pour t'accompagner dans ta trajectoire de carrière\nAu sein de la BU d’excellence, de multiples perspectives s’offriront à toi :\nRôle de « Lead » : Vous pourrez gagner en responsabilité sur le plan technologique et devenir un référent auprès de nos clients et des collaborateurs de la communauté Data Engineering\nRôle de « Champion » : Vous représenterez Aubay auprès d’un ou plusieurs de nos partenaires éditeurs stratégiques et vous participerez activement à l’animation de la relation sur le plan technologique\nRôle de « Head » : Vous pourrez prendre la responsabilité du savoir-faire Data Engineering et de ses offres et en assurer le développement au sens large (développement business, recrutement, management de collaborateurs, définition de la stratégie et animation de la communauté au sein du groupe Aubay,…)\nBesoin d’en savoir plus sur le processus de recrutement ?\nUn échange macro au niveau RH avec Doriane\nUn entretien technique avec Marius ou Peter, deux de nos référents techniques\nUn échange managérial avec le Directeur de la BU Modern BI & Data\nA savoir que l’ordre des étapes peut varier selon tes envies (ex : échange managérial avec l’échange technique)\nAubay encourage la diversité sous toutes ses formes et garantit l'égalité des chances à tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les aménagements nécessaires.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "CI/CD",
                "Big Data",
                "Machine Learning",
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Air France",
        "location": "Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-d%C3%A9veloppeur-big-data-%23-h-f-at-air-france-3900080172?position=3&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=U19TyprNBrvh%2BroLfsfJ7g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Description du poste\nIntitulé du poste\nData Engineer / Développeur Big Data # H/F\nMétier\nSystèmes d'informations - Développement\nCatégorie socio-professionnelle\nCadre\nPrésentation du contexte\nVous avez peut-être déjà voyagé avec nous, mais que connaissez-vous de nos métiers et de la richesse des données qu’ils génèrent au quotidien ? Comment le traitement et l’exploitation de ces données peut contribuer à notre stratégie de Revenue Management, ou encore aux multiples opérations à réaliser pour permettre à un vol de partir à l’heure ?\nAir France-KLM fait rêver 104 millions de passagers par an, en les emmenant vers plus de 250 destinations, grâce à une flotte de plus de 500 appareils. Le Groupe emploie 80 000 collaborateurs partout dans le monde :les opportunités sont vastes pour mettre à profit ses compétences, apprendre et se développer !\nLe département de développement DATA, OR & AI d’Air France, au sein de la direction des Systèmes d’Information, intervient dans toute la chaîne de captation et de traitement des données du groupe pour délivrer à nos métiers des solutions applicatives clés en main.\nLe département est également en charge de l’ensemble des outils techniques (ETL, DataLakes, DataWarehouses, Data visualisation) et du développement des talents et compétences de Data Engineering.\nNotre mission ? Transformer la donnée brute en décision intelligente, pour mieux optimiser les métiers d’Air France – KLM !\nPour cela, nous avons chacun un rôle essentiel à jouer, pourquoi le vôtre ne serait pas celui de Data Engineer et de développeur Big Data ?\nDescription de la mission\nAu sein de notre département, vous travaillerez main dans la main avec d’autres Data Engineers et développeurs Big Data ainsi qu’avec des spécialistes des métiers.\nIntégré au sein d’une product team agile passionnée et dynamique :\nVous participez à l’analyse des besoins métiers du commercial, des opérations aériennes, de l’exploitation sol en aéroport, de la maintenance aéronautique ou encore du Cargo.\nVous contribuez à la définition, au développement, à l’industrialisation et à la maintenance d’applications Big Data ou en Business Intelligence\nVous présentez la restitution de vos travaux et accompagnez les utilisateurs d’un point de vue fonctionnel ou méthodologique\nVous serez en contact avec les directions métier du groupe Air France KLM.\nNous attachons beaucoup d'importance au développement des compétences de nos collaborateurs ainsi qu’à leur offrir des conditions de travail favorables à l’autonomie et aux missions à forte valeur ajoutée. L'ouverture, le respect, la bienveillance et le partage sont des valeurs humaines portées par l'entreprise.\nProfil recherché\nVous êtes diplômé de niveau Master ou Ingénieur dans les domaines informatiques, vous avez acquis une expérience professionnelle dans le développement d’applications.\nVous disposez d’une expérience du développement indispensable en Backend / Java\nVous maîtrisez les bases de données relationnelles et le langage SQL\nEn Complément, Vous Avez Une Connaissance Ou Une Expérience Dans Tout Ou Partie Des Concepts Ou Outils Suivants\nEnvironnement Big Data (Spark, Hadoop, Elasticsearch, Kafka, ...)\nBase de données noSQL (MongoDB, HBase, REDIS) ou Data Warehouse Teradata\nOutil de Datavisualisation (Spotfire, PowerBI, Qlik ou Kibana)\nSolutions de Cloud (GCP) et hybride (GCP / AZURE)\n(Ces compétences complémentaires ou manquantes pouvant aussi s'acquérir à travers un parcours de reskilling et de formations aux outils du data engineering dispensé en interne).\nVous avez participé à des projets organisés en Scrum ou Kanban, et avez peut-être même œuvré comme Scrum-Master, ce qui vous permettra de vous intégrer aisément au sein d’une Product Team. Votre esprit de synthèse, votre force de conviction et votre maîtrise de la communication facilitent les décisions avec l’ensemble des collaborateurs de l’équipe, éventuellement en langue anglaise, à l’écrit comme à l’oral.\nVous êtes autonome, rigoureux(se), responsable et curieux(se), vous aimez travailler en équipe. Vous possédez de bonnes capacités d'écoute, d'analyse, de synthèse et de communication.\nEt bien sûr, vous êtes passionné(e), enthousiaste et ingénieux(se)\nCe que nous vous offrons\nDe la création de valeur pour l’ensemble des métiers d’Air France KLM\nDes challenges et problématiques complexes à résoudre\nL’opportunité de déployer des solutions Data industrielles à l’échelle !\nUne grande part de responsabilité dans une structure hiérarchique horizontale\nUn important degré de liberté pour apprendre et développer son expertise au sein de l’équipe\nOn vous attend le plus rapidement possible ! Et pour une durée indéterminée ;)\nType de contrat\nCDI\nTemps partiel possible\nNon\nType d'horaires\nAdministratif\nProfil candidat\nNiveau d'études min. requis\nBac + 5 et plus\nLangue\nAnglais (4 - Confirmé / C1)\nLocalisation du poste\nLocalisation du poste\nFrance, Provence-Alpes-Côte d'Azur, Alpes Maritimes (06)\nSite\nValbonne\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Confirmé"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java"
            ],
            "DataBase": [
                "MongoDB",
                "HBase",
                "Elasticsearch",
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "Azure"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "WA.Technology",
        "location": "Crouseilles, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/junior-data-engineer-at-wa-technology-3908458326?position=4&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=i6ZV0xp3%2B13c2YxX5xinEQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "WA.Technology\nis a B2B supplier of iGaming solutions with rapidly growing clients and partnerships in emerging markets. We offer a state-of-the-art iGaming platform, turnkey solutions, and standalone products that empower operators to enter or expand into emerging markets quickly and easily.\nThe WA.Platform is a fully scalable and customisable solution, featuring over 75 game providers, 6,400+ games, and support for multiple currencies, along with access to over 80 payment methods. WA. Technology enables operators to build their own casino, sportsbook, lottery, fantasy, or poker business precisely as they envision it.\nAbout The Role,\nWe are seeking a highly skilled and motivated\nData Engineer\nto design, implement, and maintain efficient and scalable data pipelines on the Google Cloud Platform (GCP). In this role, you will be responsible for managing data from MariaDB and Kafka sources, ensuring seamless integration into BigQuery, our primary data destination. Collaboration with cross-functional teams is crucial to understanding and meeting diverse data needs.\nIn this role, you will need to:\nDesign, implement, and maintain robust data pipelines on Google Cloud Platform.\nIntegrate MariaDB and Kafka as primary data sources for seamless data flow into BigQuery.\nCollaborate across departments to address unique data requirements aligned with organizational goals.\nUtilize Dataflow and Dataform for efficient data processing and transformation.\nEnsure data integrity through rigorous validation and cleansing processes.\nOptimize cloud-based infrastructure for speed and scalability.\nImplement monitoring tools for proactive system performance tracking and issue resolution.\nProvide ongoing support for data integrity and availability.\nMaintain comprehensive documentation of data architecture, updating regularly.\nStay informed about the latest data technology trends.\nEvaluate and recommend new technologies/methodologies to enhance processing and analysis capabilities.\nWhat are the key experience and personal attribute requirements?\nBachelor's degree in Computer Science, Information Technology, or a related field.\n2+ Hands-on experience relational database\nProven experience in developing data pipelines and ETL processes.\nStrong SQL skills.\nKnowledge of data modeling and database design.\nExcellent collaboration and communication skills.\nStrong problem-solving and troubleshooting abilities.\nAbility to work independently and as part of a team.\nContinuous learner, keeping up with emerging trends in data engineering.\nWhat are some of the benefits of working at WA Technology?\n100% remote opportunity\nFlexible work environment\nAttractive remuneration package\nOpportunity to work with well-connected industry leaders.\nA leadership approach that fosters innovation, creativity, and trust.\nOpportunity to experience the buzz of highly driven and motivated work colleagues.\nExperience a start-up feel in a fast-paced growth-driven environment.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Creativity",
                "Leadership",
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "iPepperGroup",
        "location": "Valbonne, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ipeppergroup-3894091360?position=5&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=dD9us48XNutKivZNI9mbOQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "iPepper recrute pour l'un de ses clients une PME éditeur de logiciel dans le domaine du voyage, un\nData Engineer (H/F)\npassionné(e) et expérimenté(e) pour rejoindre une équipe dynamique.\nEn tant qu'Ingénieur(e) Data, vous serez en charge d'extraire et de transformer des données, de construire et d'optimiser des pipelines de données, ainsi que de concevoir des visualisations de données intuitives et informatives.\nResponsabilités :\nConcevoir, construire et maintenir des pipelines de données évolutifs et efficaces pour transférer des données entre des bases de données SQL et NoSQL.\nDévelopper et mettre en œuvre des processus ETL pour extraire, transformer et charger des données à partir de différentes sources dans notre entrepôt de données.\nCollaborer avec des équipes pluridisciplinaires pour comprendre les besoins en données et garantir la fourniture réussie de solutions de données.\nOptimiser et ajuster les pipelines de données existants pour la performance et la fiabilité.\nConcevoir et développer des visualisations de données et des tableaux de bord pour fournir des insights exploitables aux parties prenantes.\nSurveiller et résoudre les problèmes de pipelines de données, en veillant à la qualité et à l'intégrité des données.\nProfil recherché :\nDiplôme universitaire en informatique, en ingénierie ou dans un domaine connexe.\nExpérience avérée en tant que Data Engineer ou dans un rôle similaire, avec un accent particulier sur la construction de pipelines de données et de processus ETL.\nCompréhension solide des bases de données\nSQL\net\nNoSQL\n, y compris la modélisation des données et la conception de schémas.\nMaîtrise des langages de programmation tels que\nPython, Java ou Scala.\nExpérience avec des outils de visualisation de données tels que\nTableau, Power BI.\nSolides compétences en analyse et en résolution de problèmes, avec la capacité de traduire des données complexes en insights exploitables.\nExcellentes compétences en communication et en collaboration, avec la capacité de travailler efficacement dans un environnement d'équipe pluridisciplinaire.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LCL",
        "location": "Villejuif, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-profils-exp%C3%A9riment%C3%A9s-h-f-at-lcl-3888403052?position=6&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=IJXsroGkVIWK02iuomr4Xg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "🏦 LCL, c’est LA banque urbaine du Groupe Crédit Agricole - avec nous, accompagnez la transformation, le développement et le maintien technologiques de nos outils avec une vision business et de satisfaction de nos 6 millions de clients.\nEn tant qu’acteur majeur de la banque de détail, nous nous adaptons chaque jour aux nouveaux modes de consommation et les projets de nos de clients internes et externes tout en garantissant le besoin de sécurité et de développement technologique qu’impliquent nos activités.\n💡Organisées en mode Agile, les 8 squads de la tribu DATA (6 squads Métier et 2 squads transverses) œuvrent au quotidien pour répondre à un enjeu majeur pour la banque : la collecte, le stockage, la gestion et l’usage de la donnée. En interaction permanente avec les autres tribus IT et les métiers, elles étudient et proposent les solutions et architectures à déployer pour répondre au mieux aux stratégies de développement et de pilotage de l’ensemble des métiers de la banque.\nRejoignez-nous si vous souhaitez participer aux réflexions et au développement de la trajectoire technique et DataCentric du SI LCL et plus largement du Groupe CA. Vous côtoierez et serez au cœur de l’implémentation de technologies variées telles que les plateformes Teradata, les solutions d’architecture applicative des technologies BigData ou IA, des environnements analytiques ou encore des solutions de datavisualisation. Vous assurerez le traitement de données en temps réel ou en batch et exposerez les données sous différentes formes.\nQue vous souhaitiez devenir expert sur les socles technologiques ou relever le challenge de la gestion de projets Métier, nous vous aiderons à atteindre vos propres objectifs.\nVous rejoindrez une équipe pluridisciplinaire, clairement orientée vers le développement de ses collaborateurs à de nouvelles technologies !\n🎯 En tant que Data Engineer :\n· Vous aimez analyser les besoins avec les métiers, challenger, identifier les sources de données dans les différents univers technologiques, industrialiser des algorithmes, concevoir et développer des Datalab ou des Datamart sur les plateformes ? Vous saurez relever les challenges proposés par les squads métier !\n· Vous préférez travailler à l’architecture et au déploiement de nouvelles plateformes, à la levée de la dette technologique ou encore réaliser de la veille au service de notre trajectoire ? La squad Socles Data est faite pour vous !\n· Au-delà des projets que vous gérerez, garant du bon fonctionnement de votre parc applicatif, vous attacherez une grande attention à la mise en œuvre de solutions optimisées.\n· La rigueur, la communication, l’esprit d’équipe mais aussi la curiosité et la créativité font partie de vos soft skills ! ils vous permettront de répondre aux enjeux de sécurité, de qualité, de transmission de la connaissance et contribueront à l’atteinte des objectifs de l’IT et plus largement de LCL, au service de ses clients.\n💻 Voici les principales technologies utilisées au sein de la tribu, si certaines vous sont familières, nous vous aiderons à monter en compétence sur d’autres !\nLangages utilisés : SQL, Python, Scala\nSGBD : Teradata et utilitaires (TPT, BTEQ, …)\nStreaming : Kafka\nSearch : ElasticSearch, SolR\nEnvironnement : Unix\nSolutions Big Data : Hadoop Cloudera, DataIku, HDFS, Hive, Impala,\nDevops : GitLab, Jenkins, Nexus\nOutils de visualisation : MS BI (SSIS, SSAS, SSRS) Qlik Sens, BO\nModélisation : MEGA\nOutils collaboratifs : GIT, Jira, Confluence, Teams\n⚡Si les nouveaux enjeux bancaires vous intéressent, que vous souhaitez intégrer une équipe Agile au service des métiers dans laquelle vous serez force de proposition et que vous aimez travailler dans un environnement motivant et dynamique, rejoignez-nous, cette offre est faite pour vous !\n🔥 Les + de notre entreprise :\nAccès au Plan d’épargne Groupe, intéressement et participation aux bénéfices de l’entreprise + abondement\nPrix préférentiels bancaires et avantages CSE\nParcours évolutif dans l’entreprise et/ou dans le Groupe CA.S.A\nTélétravail (jusqu'à 2 jours de télétravail par semaine)\nDe multiples commodités sur le campus (restaurants d'entreprise, salle de sport, crèche, centre médical, médiathèque...)\nForfait et avantages pratiques « mobilité durable » pour les velotafeurs\nDes équipes aussi diversifiées que structurées dans une dynamique de transformation\nLCL s’engage en faveur de la diversité et nous encourageons tout(e) candidat(e) ayant l’expérience requise à postuler à nos offres. Tous nos postes sont ouverts aux personnes en situation de handicap.\nNous avons encore de nombreuses raisons à vous présenter pour vous convaincre de nous rejoindre mais pour cela, il faudra postuler ici !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL"
            ],
            "BigData": [
                "Hadoop"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Collaboration": [
                "Confluence",
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ramify",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-ramify-3896146641?position=7&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=sd79m7aEJdZqm5M%2BxmhIsA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ABOUT\nRamify’s mission is to help people achieve financial freedom, no matter who they are and how much they have. We are revolutionizing the private wealth management industry by making smart and affordable financial products for everyone.\nNo more standardized solutions, hidden fees and complicated words, Ramify enables everyone to become a smart investor. The team combines elements of its research with technology to design customized investment portfolios composed of world-class financial products.\nThe team comprises around 15 talented individuals. Ramify is looking for talented people in all sectors, who want to have a huge impact, move fast and deliver.\nJOB DESCRIPTION\nThe Quantitative Investment Solutions (QIS) Team is dedicated to designing innovative investment portfolio models and developing cutting-edge investment features within our product suite. Moreover, the QIS Team is at the forefront of driving AI-based solutions for Ramify. This involves conceptualizing and implementing transformative AI solutions tailored to meet the diverse needs of various teams within Ramify.\nAs a Data Engineer on our QIS Team, you will play a pivotal role in shaping the future of investment strategies through data-driven insights and AI-based solutions. Collaborating closely with our talented team of quantitative researchers, and investment experts, your responsibilities will encompass architecting and implementing robust data pipelines. These pipelines will facilitate the seamless integration of diverse data sources, empowering Ramify teams to make informed decision-making.\nKey Responsibilities:\nDesign, build and launch data pipelines at scale to move data across Ramify platform with SQL technologies.\nDesign and implement processes and tools for data onboarding and quality, helping to deliver an industry best-practice solution for managing the data lifecycle.\nProduce stand-alone tools that can be used by other teams to automate data quality and discover faults.\nBuild analytical tools that provide insight into business metrics across Ramify.\nArchitect and lead the implementation of AI based solutions within Ramify\nPREFERRED EXPERIENCE\nWe're looking for people who:\nWant to make a difference. We are a small team effectively reshaping how people look at the industry. We need people who 'get it' and want to play an integral part in helping us accomplish this mission and are persistent in getting the job done.\nSkills we're looking for:\nMaster’s or upper-year undergraduate-level coursework in either Computer Science, Management Information Systems, Business Information Systems, Mathematics or Finance related field.\n2+ years of professional experience in data engineering.\n2+ years of experience with one or more coding languages such as Python (is a must), Java.\nExperience with data modeling and ETL design, implementation and maintenance.\nDemonstrable mastery of industry best practices in the data lifecycle, including data quality automation and tooling.\nExcellent written and verbal communication skills with ability to communicate complex designs and solutions to non-technical and highly technical audiences alike.\nGood attention to detail.\nStrong analytics and strategic thinking skills\nNice-to-haves :\nUnderstanding of ML/ Generative AI technologies and their applications.\nPossess a passion, curiosity, and energy for finance + investing. You understand the ins and outs of the wealth management, trading, and more importantly - know how to explain these concepts simply\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Renault Digital",
        "location": "Boulogne-Billancourt, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-renault-digital-3911202728?position=8&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=u6M0icZBx1jNW9jmHGT6PQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Contexte :\nDans le cadre de son programme Industrie 4.0, Renault développe depuis 2017 sa propre plateforme pour connecter et agréger les données industrielles des 22 sites du Groupe et de plus de 2500 machines.\nFort de partenariats stratégiques signés avec Google Cloud (stack data full GCP), Renault Digital est à la recherche d’un(e) Data Engineer au sein du Pôle Architecture et Data pour mettre en place des chaînes de traitement de données répondant à de nouveaux besoins métiers.\nVous collaborerez au jour le jour avec les équipes métiers ainsi qu’avec les autres fonctions du Pôle Architecture & Data (Data Analysts et Scientists, architectes, …), exploitant des téraoctets de données (événements en mode streaming, traitements en batch et temps réels et les appels aux APIs) afin entre autres d’alimenter des modèles de machine learning (segmentation clients, détection automatiquement des pannes des véhicules, …).\nResponsabilités principales :\nVous participez aux phases de framing, MVP et release des produits, services et APIs orientés data ;\nVous argumentez les choix d’architecture des projets et de la plateforme datalake sur GCP ;\nVous contribuez à la valeur métier des produits orientés Data s’appuyant sur le Datalake, en mettant en place des chaînes bout en bout de traitement de la data, de l’ingestion à l’exposition d’APIs et à la visualisation des données et des solutions ML/DS ;\nVous êtes garant de la qualité des données transformées dans le Datalake, du bon fonctionnement des chaînes de traitement et de l’optimisation de l’utilisation des ressources des ressources cloud ;\nVous proposez des standards d’architecture et de développement ;\nVous êtes force de proposition, innovant(e) et bienveillant(e).\nEnvironement technique :\nSpark, Scala, Python, Java, Airflow, SQL, Google Cloud Platform (BigQuery, Cloud Storage, PubSub, Beam, Dataflow, Cloud ML, TensorFlow, Kubernetes), Git, Docker, JSON, Bash, Spotfire\nProfil recherché :\nVous avez minimum 5 ans d’expérience en tant que Data Engineer ;\nVous disposez d’une expérience en développement Spark, Scala, Python et requêtage SQL sur des gros volumes de données ;\nVous avez une appétence pour la data : validation, transformation, analyse, valorisation ;\nVous possédez une expérience de développement et orchestration de chaines ETL complexes via Airflow ou équivalent ;\nVous pratiquez la méthodologie agile (Agile Scrum et/ou Kanban) ;\nVous utilisez les services cloud (préférablement GCP) ;\nVous êtes capable d’échanger en anglais technique écrit et oral.\nInformations complémentaires :\nVotre poste sera basé à Boulogne-Billancourt (France) en CDI (temps plein)\nVous bénéficiez de 2 à 3 jours de télétravail par semaine\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Temps plein",
                "Full"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Bash",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "DataSerialization": [
                "Json"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "ML"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Inetum",
        "location": "St.-Ouen, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-inetum-3843966639?position=9&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=Gwv1jWjhHHw8djXLLXWCHA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Détail de l'offre\nInformations générales\nEntité de rattachement\nInetum est un leader européen des services numériques. Pour les entreprises, les acteurs publics et la société dans son ensemble, les 28 000 consultants et spécialistes du groupe visent chaque jour l'impact digital des solutions qui contribuent à la performance, à l'innovation et au bien commun.\nPrésent dans 19 pays au plus près des territoires, et avec ses grands partenaires éditeurs de logiciels, Inetum répond aux enjeux de la transformation digitale avec proximité et flexibilité.\nPorté par son ambition de croissance et d'industrialisation, Inetum a généré en 2023 un chiffre d'affaires de 2,5 milliards d'€.\nPour répondre à un marché en croissance continue depuis plus de 30ans, Inetum a fait le choix délibéré de se recentrer sur 4 métiers afin de gagner en puissance et proposer des solutions sur mesure, adaptées aux besoins spécifiques de ses clients le conseil (Inetum Consulting), la gestion des infrastructures et applications à façon (Inetum Technologies), l'implémentation de progiciels (Inetum Solutions) et sa propre activité d'éditeur de logiciels (Inetum Software). Inetum a conclu des partenariats stratégiques avec 4 grands éditeurs mondiaux - Salesforce, ServiceNow, Microsoft et SAP et poursuit une stratégie d'acquisitions dédiée afin d'entrer dans le top 5 européen sur ces technologies et proposer la meilleure expertise à ses clients.\nTous nos postes sont ouverts aux personnes en situation de handicap.\nDescription du poste\nMétier\nApplications Delivery - Software Development\nIntitulé du poste\nData Engineer H/F\nContrat\nCDI\nDescription De La Mission\nLe pôle BFA de la branche Application Services du groupe INETUM, recherche plusieurs Data Engineers afin d'intervenir auprès de clients grands comptes au sein des marchés bancaires et de l'assurance.\nAu sein de l'équipe Data, en tant que Data Engineer, vous participez à la réalisation de divers projets et vos missions sont\nApporter votre connaissance en Big Data permettant la manipulation des données\nConcevoir les plateformes permettant de traiter des volumes de données importants\nMettre en place des bases de données\nPréparer le pipeline de données pour que les données déployées soient sécurisées et claires afin d'être analysées et transformées.\nProfil\nDe formation ingénieure en informatique Bac + 5 informatique ou scientifique\nBonne communication orale et écrite en français et niveau d’anglais professionnel\nSavoir- être Bon esprit d'analyse et de synthèse, sens de l'organisation et de la qualité, force de proposition, rigueur, travail en équipe, adaptabilité.\nSi vous vous reconnaissez, n'hésitez pas à postuler !\nLocalisation du poste\nLocalisation du poste\nFrance\nVille\nSaint-Ouen\nCritères candidat\nNiveau d'études min. requis\nBac+5\nNiveau d'expérience min. requis\nPlus de 2 ans\nCompétences\nSQL\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "Other": [
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bi-nantes-at-capgemini-3803963477?position=10&pageNum=2&refId=ScAHuRnPnFA7sPESHrUwtg%3D%3D&trackingId=NR3ocFz4q9uIIduh74kWZg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini\nChoisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.\nVos missions :\nIntégré(e) au sein d’une équipe projet BI, Big Data ou Data Gouvernance pour des clients intervenant dans des secteurs d'activités divers, vous serez notamment en charge des missions suivantes :\nMener les analyses fonctionnelles destinées à traduire les besoins du client,\nMener les travaux de conception et de modélisation,\nDiriger le développement de la solution / des traitements d'alimentation du DataWareHouse,\nOrganiser et préparer les travaux de recette utilisateurs,\nMettre en place les processus d'industrialisation et mener cette dernière.\nVotre profil :\nDiplôme d’ingénieur ou équivalent universitaire\nMinimum 3 ans d'expérience\nAnglais courant\nCompétences en BI sur SAP BI (Hana, BW, BODS, BI 4), Microsoft BI (SQL Server, SSIS, SSAS, SSRS), Oracle (ODI, OBIEE), Teradata, Informatica (Powercenter), IBM (Datastage, Cognos, TM1), Talend, AB Initio\nMaîtrise d'un ou de plusieurs outils de Dataviz : Microsoft Power BI, Tableau, Qlikview\nConnaissances en Big Data (Ecosystème Hadoop (HIVE, PIG, Mahout…), Cloudera, Pivotal, Spark, HNX) ou en analytics (R, SAS, IBM SPSS)\n3 raisons de nous rejoindre :\nQualité de vie au travail : accord de télétravail en France et à l’international, accord sur l’égalité\nprofessionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu : certifications et formations en libre accès, accompagnement sur mesure avec votre carreer manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE : plan actionnariat, activités à tarifs préférentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorités :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nCapgemini\nCapgemini est un leader mondial, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d’expérience, nous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le cloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les\nplateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "DBMS": [
                "SQL Server"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Astek",
        "location": "Antibes, Provence-Alpes-Côte d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-antibes-france-h-f-at-astek-3909192086?position=1&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=zTGOsE%2F7O3zniEvcuaWrOw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "CDI\nAntibes - France\nPubliée il y a 2 semaines\nLe Groupe Astek\nCe Que Nous Allons Accomplir Ensemble :\nIntervenir dans la conception, le développement, les tests unitaires, la qualification, l’intégration continue et la mise en production d’évolutions sur les projets du pôle produits scoring (un pôle visant à développer des solutions permettant de générer des scores ou des segments d’information pertinents dans divers domaines, notamment : profiling TV, PUB, SAB, MMDM, Voscastview) chez l’un de nos partenaires spécialisé dans le secteur des télécoms.\nVotre Mission, Si Vous L’acceptez :\nEn collaboration avec les autres membres de l’équipe, vous devrez prendre en charge le RUN des applications du pôle produit scoring.\nConception d’une solution se basant sur les développements existants et les besoins métiers remontés par le Product Owner.\nRéalisation et développement de nouvelles fonctionnalités sur les composants des applications du pôle produits scoring et environnement CGP.\nVotre Future Équipe :\nAu sein d’un environnement riche et complexe, vous évoluerez avec des experts passionnés à la fois techniques et fonctionnels (Ingénieurs spécialisés, chef de projet, scrum master, product owner, analystes …).\nVotre stack de jeu\nD ans un environnement SAFE sous cloud GCP, Big Query, OnPrime, Grafana, Python et Ansible.\nVous ?\nDe formation Ingénieur, vous justifiez d’une première expérience sur un poste de Data engineer. Vous possédez des compétences d’autonomie et d’adaptabilité et vous avez une capacité à communiquer efficacement au sein d’une équipe.\nLe Groupe Astek\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de\nses 7800 collaborateurs\nqui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde ayant réalisé un chiffre d’affaires de 600 M€ en 2023.\nTous les détails sur le Groupe sur le site https://astekgroup.fr. Et vous pouvez aussi nous suivre sur notre blog : https://blog.groupeastek.com.\nRencontrons-nous\nCréé en France en 1988, Astek est un acteur mondial de l’ingénierie et du conseil en technologies, présent sur les 5 continents. Fort de son expertise dans de nombreux secteurs industriels et tertiaires, Astek accompagne ses clients internationaux dans le déploiement intelligent de leurs produits et de leurs services, et dans la mise en œuvre de leur transformation digitale.\nDepuis sa création, le Groupe a fondé son développement sur une forte culture d’entrepreneuriat et d’innovation, et sur l’accompagnement et la montée en compétence de ses 7800 collaborateurs qui s’engagent chaque jour à promouvoir la complémentarité entre les technologies numériques et l’ingénierie des systèmes complexes.\nRejoignez un Groupe en fort développement en France et à travers le monde et ayant réalisé un chiffre d’affaires hors\nacquisitions de 600M€ en 2023.\nTous les détails sur le Groupe sur le site\nNos Plus\nAstek est green et fait bénéficier ses salariés d’une indemnité kilométrique vélo\nUne politique CARE sur-mesure déployée par nos équipes RH pour nos collaborateurs (https://astekgroup.fr/engagements)\nNotre charte de la Diversité\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Data engineer – Big Data\nCaractéristiques de l'emploi\nCatégorie Ingénieur\nJob Industry Télécom / Média\nPostuler en ligne\nNom *\nPrénom *\nEmail *\nUn email valide est requis.\nTéléphone *\nUn numéro de téléphone valide est requis.\nJoindre un CV *\nMots-clés :\ningénieur – ingénieure – consultant – consultante – Data engineer – Big Data\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Automation": [
                "Chef",
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Web Transition",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=2&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=J8wiaPcFaiEqxgjJ7EJDjw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Web Transition, c’est qui ?\nFondée en 2011,\nWeb transition\nest une entreprise de services numériques opérant sur le marché de l’IT/Digital !\nConstituant une part essentielle de\nMoOngy Digital Lab\n, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et également en Data !\nNotre objectif : nous implanter comme un acteur principal sur le marché de la Transformation Digitale en accompagnant et valorisant les compétences de nos collaborateurs !\nNous sommes convaincus que le succès de MoOngy Digital Lab réside dans la somme des potentiels de nos équipes 🤝\nTon équipe : La tribu Data\nParce qu’il est indispensable que tu puisses partager tes connaissances mais aussi en acquérir de nouvelles, tu feras partie de l’une de nos tribus : celle de la Data. De plus, cela te permettra d’être acteur dans le développement et la stratégie de Web Transition. Ce système de co-réflexion et co-construction est un fondement essentiel chez nous !\nDans cette aventure, tu :\nT’assures\nde la maîtrise de la donnée et est garant de la qualité de son utilisation (référencement, normalisation, et qualification)\nTravailles\nà la compréhension et l'intégration des données en provenance des différents formats\ndes interfaces de flux\négalement à la définition de la politique de la donnée et à la structuration de son cycle de vie dans le respect des réglementations en vigueur\nla supervision et l'intégration des données de diverse nature qui proviennent de ces sources multiples et vérifie la qualité des données qui entrent dans le Data Lake\nGarantis\nl'accès qualitatif aux sources de données\nFacilites\nl’accès aux données pour tes collègues (data scientists, data analysts…)\nAssistes\nles autres équipes dans l'accès et la compréhension des données des socles.\nRejoins-nous si tu as :\nExpérience d’au-moins 4 ans dans la Data\nAppétence à la qualité des données.\nConnaissance familière des Datawarehouses.\nMaitrise de Python, Oracle SQL, GCP/Power BI\nAisance avec les indicateurs, tu as une bonne capacité d'analyse et de rédaction.\nTon savoir-être :\nOuvert d’esprit\nRigoureux\nAutonome\nRespectueux des différences de chacun\nCurieux\nProactif\nAgile\nPar où on commence ?\nUn premier entretien RH d’1h pour comprendre ton parcours et tes aspirations\nUn second entretien de 45 minutes avec l’un de nos Business Manager afin de valider tes compétences et qu’il se projette sur l’une des missions qu’il pourrait te proposer\nUn troisième entretien de quelques minutes avec notre responsable d’agence pour te proposer d’intégrer notre superbe Team Web !\n3 entretiens en peu de temps, si ton profil correspond tu intègreras très vite nos équipes 😉\nPrêt pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant à cette offre ! Voici les avantages qui t’attendent en tant que Weber :\n🤩 Des collègues incroyables\n🏆 Certifiée Great Place to Work\n🎮 Des bureaux sympas (où vous serez toujours les bienvenus)\n🎉 Des teambuilding et évents tous les mois\n💻 Des tributs métiers pour échanger entre Weber du même métier\nDes missions chez le client qui sont accompagnées et coachées par ton manager\nUn accompagnement dans ton plan de carrière et tes envies de re skilling\n🤓 Un catalogue de formations certifiantes ouvert à tous les salariés\n🍽️ Une carte tickets restaurant MyEdenred\n❤️ Une mutuelle GrasSavoye\n🚎 Une prise en charge des frais de transport à 100%\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "Oracle SQL"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Mobilize Financial Services – France",
        "location": "Noisy-le-Grand, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-mobilize-financial-services-%E2%80%93-france-3869637982?position=3&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=nMUVu2xWNyaFQacbVrv%2B9Q%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "🚗 En route vers Mobilize !\nA l’écoute de tous nos clients, nous créons des services financiers innovants pour construire une mobilité durable pour tous.\nRejoindre Mobilize Financial Services,\nc’est d’abord choisir d’intégrer un groupe international\n, filiale de Renault Group, une banque de financement solide, partenaire des constructeurs Renault–Nissan–Mitsubishi. Nos 4 000 collaborateurs présents dans 35 pays, agissent ensemble au service de nos clients.\nNous proposons à nos clients - particuliers comme professionnels - les financements et les services les plus adaptés pour les véhicules neufs et d'occasion.\nNous finançons également l'activité des réseaux de concessionnaires des marques de l'Alliance Renault-Nissan-Mitsubishi et nous veillons à faciliter leur gestion au quotidien pour leur permettre de développer leurs ventes et assurer leur pérennité financière.\nNotre entreprise se \"MOBILIZE\" en faveur de la diversité culturelle, l'égalité hommes-femmes et l'intégration de personnes en situation de Handicap. Nous favorisons un environnement de travail où les différences individuelles sont reconnues, appréciées, respectées et valorisées, de façon à mettre à profit les talents et les forces de chacun.\n🚘Prenez le volant ! Pas de routine, tous nos itinéraires sont différents !\nAu sein de la DSI\n,\nvotre\nfutur métier consistera à :\nAccompagner l’équipe dans la transformation du domaine décisionnel construit sur une architecture type DWH et le porter sur la solution GCP (Google Cloud Platform) de Mobilize FS\nParticiper à la construction du projet de transformation vers GCP\nParticiper aux projets d’évolution de notre plateforme Suite Elastic (ELK - Kibana)\nPiloter des projets en étroite collaboration avec les directions métier et en accord avec le TBA (Tableau de Bord des Actions).\nAssurer la gestion du budget, du planning, de la tenue des jalons et du respect des engagements sur les projets en responsabilité\nAssurer la qualité et le bon fonctionnement du chargement des données.\nAssurer la mise à disposition des données et des outils de reportings à toutes les directions clientes dans le respect des contrats de service\nVéritable tout-terrain, vous nous intéressez !\nL’esprit d’équipe et le sens du service client pour atteindre ensemble les différents objectifs ambitieux et satisfaire les différentes parties avec un haut niveau de qualité.\nVous avez un bon relationnel, de l’écoute et une excellente communication afin d’interagir avec des interlocuteurs de différents niveaux (direction technique et métier) et de travailler en transverse.\nLe sens de l’analyse et de bonnes capacités d’anticipation pour déceler les problèmes avant la naissance de ces derniers.\nForce de proposition : avec vous il n'y a pas de problèmes, que des solutions\nVous avez un niveau d’anglais vous permettant de lire et de comprendre de la documentation technique\n💻🖱 Environnement technique :\nMaitrise des langages Python - SQL / NoSQL\nExpérience significative sur Python\nExpérience avec Git\nUne expérience avec les outils Nifi, Airflow et GCP (BigQuery / Cloud Function / Cloud Storage …) serait un plus\nGestion de projet, maintenance, évolution, support\nAppétence pour les sujets techniques et fonctionnels : outils de modélisation, exploration de données, IA, machine learning\nPourquoi nous rejoindre ?\nVotre Pack confort\nest composé de nombreux avantages 😀 :\nRejoindre Mobilize Financial Services c’est intégrer un grand groupe international qui offre des opportunités de carrière\n.\nUn environnement de travail moderne et convivial\n: locaux agréables, salle de sport, terrasse, restaurant d’entreprise, parking avec un CSE dynamique avec de nombreuses offres voyages, sport, famille,\nNous sommes mobilisés pour développer la qualité de vie au travail de nos collaborateurs en faisant évoluer nos façons de travailler (méthodes, outils, organisation du travail…) et nous sommes fiers d’être certifiés ⭐\nGreat Place To Work ⭐\nPossibilité de télétravailler 2 jours par semaine\nNous proposons une\nrémunération selon profil + Participation + Intéressement\nLocaux situés au pied du RER A – Noisy le Grand Mont d’Est\n❗ Mobilize Financial Services déménage ❗ Les postes à pourvoir en région parisienne seront basés à Boulogne Billancourt à horizon 2026\nPour en savoir plus sur notre entreprise,\nsuivez-nous sur LinkedIn !\nLa route du recrutement ?\n📞 Un rapide entretien téléphonique,\n🛑\nun premier échange\navec Marie DE CARLI, Responsable du département DATA\n↪ et un dernier virage avec Agathe GROSBOIS, Responsale des Ressources Humaines\nL’équipe Mobilize FS a hâte de vous recevoir !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Machine Learning",
                "Cloud"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MP DATA",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-mp-data-3908719610?position=4&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=QC7vcJ028LquS7NZmeHEFA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "MP DATA est une société spécialisée dans l’acquisition, le traitement, et la valorisation des données.\nDepuis sa création en 2015, MP DATA accompagne ses clients, majoritairement industriels, dans le management de leur performance et l’exploitation de leurs données.\nLes collaborateurs, tous issus de grandes écoles, incarnent au quotidien les valeurs d’Excellence, de Partage et d’Engagement.\nIls associent savoir-faire technique, méthodologie et passion et mettent leurs compétences au service de missions et projets au sein de grands groupes français.\nMP DATA accompagne ses clients sur toute la chaine au travers de 3 pôles d’expertise : Conseil et Stratégie, Infrastructure & CloudOPS, Data Science.\nChez MP DATA, les équipes commerciales cherchent des missions en fonction des envies des collaborateurs et non pas l’inverse. Les consultants sont accompagnés dans tous leurs projets, de la mobilité géographique, au changement de secteur d’activité en passant par le développement de nouvelles compétences.\nRejoindre MP DATA, c’est la garantie de travailler sur des sujets passionnants avec un cadre technique fort.\nDescriptif du poste :\nNous recherchons un Data Engineer expérimenté pour rejoindre notre équipe.\nEn tant que Data Engineer, vous serez responsable de la conception, du développement et de la mise en œuvre de pipelines de traitement de données en temps réel à grande échelle.\nVous travaillerez avec des technologies telles que Kafka, Flink, Kinesis et vous utiliserez les services du cloud AWS pour stocker et traiter les données.\nVos responsabilités :\nUtiliser Kafka pour le traitement de flux de données en temps réel à grande échelle, en travaillant avec les producteurs, les consommateurs et les topics.\nMettre en œuvre des pipelines de traitement de données en streaming avec Flink, en appliquant des transformations complexes et en gérant les états.\nÉcrire du code efficace et maintenable en Java / Python pour manipuler et analyser les données en temps réel.\nUtiliser Kubernetes pour déployer et gérer des applications conteneurisées à grande échelle, en assurant la résilience et l’évolutivité des services.\nUtiliser les services AWS tels que Amazon S3, AWS Lambda, Elastic Kubernetes Service (EKS), Elastic Container Service (ECS) et Elastic Compute Cloud (EC2) pour le stockage, le traitement et le calcul des données en temps réel.\nSuivre les meilleures pratiques pour une utilisation efficace du cloud, en assurant la gestion des coûts, la sécurité des données et la disponibilité des services.\nCollaborer avec l’équipe de développement logiciel et la gestion de projets pour assurer un flux de développement fluide et une livraison efficace des fonctionnalités.\nBon à savoir :\nCDI / ASAP / Toulouse\nProfil recherché:\nNous recherchons un candidat diplômé d'une grande école d'Ingénieur avec une première expérience.\nCompétences nécessaires :\nExpérience significative dans un environnement industriel en mode DevOps, avec des outils tels que CICD, gitlab, Jenkins, Sonar, Nexus, XLdeploy, Camunda, etc.\nMaîtrise des langages de programmation tels que Python, Java et expertise dans l’écriture et l’optimisation du code SQL\nMaîtrise du français et bonne maitrise de l’anglais.\nCapacité à travailler en équipe et esprit d’équipe.\nLe processus de recrutement se déroule en 3 entretiens :\nPrise de contact\n1er entretien : Présentation et projet du candidat + présentation MP DATA\n2ème entretien : Entretien de qualification technique\n3ème entretien : Rencontre avec les équipes dans les locaux MP DATA + Proposition de collaboration\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Flink"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "SEVETYS",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-sevetys-3905649273?position=5&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=Pr6uHm3LOJ7s%2BqiuHJTBTA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Sevetys, premier groupe français de cliniques vétérinaires, est présent dans toute la France avec plus de 150 établissements. Créé en 2016, le groupe souhaite moderniser le métier et mettre la qualité des soins et la satisfaction client au cœur de son projet.\nLe projet d’entreprise se caractérise par son hyper croissance et une culture de type start-up axée sur le collectif, la cohésion, et l’engagement.\nFort de son succès, Sevetys poursuit sa structuration et recrute un / une :\nData Engineer\n​Le\nData Engineer\ntravaille en étroite collaboration avec une équipe Agile pluridisciplinaire pour construire des pipelines de données de haute qualité permettant de mettre en œuvre des solutions analytiques. Ces solutions génèreront des informations à partir de nos données collectées, permettant de faire progresser les capacités de prise de décision du management de l’entreprise. Ce rôle nécessite une compréhension approfondie de l'architecture des données, de l'ingénierie des données, de l'analyse des données, du reporting. Le candidat idéal est un ingénieur en données/logiciel ayant au moins une première expérience dans la création de produits de données soutenant des solutions analytiques.\nMissions :\nConçoit, développe, optimise et maintient une architecture de données et des pipelines qui respectent les objectifs de l'entreprise ;\nRésout des problèmes de données afin de fournir des informations qui aident notre entreprise à atteindre ses objectifs ;\nCrée des jeux de données pour les membres de l'équipe d'analyse afin d'améliorer leur productivité ;\nFavorise une culture du partage, de la réutilisation, de la stabilité de la conception à l'échelle et de l'efficacité opérationnelle des données et des solutions analytiques ;\nContribue à l'évaluation, la mise en œuvre et le déploiement d'outils et de processus émergents pour l'ingénierie des données analytiques afin d'améliorer notre productivité en tant qu'équipe ;\nÉlabore et met en œuvre des plans de communication/éducation sur les capacités, les normes et les processus d'ingénierie des données analytiques ;\nTravaille en partenariat avec des analystes business et des architectes de solutions pour développer des architectures techniques pour les projets et initiatives stratégiques de l'entreprise.\nExpertises techniques :\nExpérience du développement de bases de données et d'une variété de technologies de bases de données relationnelles ;\nExpérience des entrepôts de données ;\nExpertise en SQL et en analyse de données ; maîtrise Python ;\nIdéalement certifié des technologies BI du Cloud Azure (Synapse, Azure Data Factory, PurView) ;\nConnaissance de l'intelligence artificielle, des statistiques et/ou des mathématiques appliquées ;\nExpérience dans le développement de solutions sur des services et infrastructures de cloud computing dans le domaine des données et de l'analyse ;\nExpérience du déploiement de Power BI ;\nExpérience conceptuelle des données et de l'analyse, par exemple ETL, modélisation dimensionnelle, outils de reporting, gouvernance des données, entreposage des données, données structurées et non structurées, qualité de données ;\nConnaissance CI/CD et GitLab fortement apprécié.\nExpérience agile / Digitale / gouvernance :\nPassionné(e) le développement basé sur les données, la fiabilité et l'expérimentation ;\nExpérience souhaitée de travail au sein d'une équipe produit Agile collaborative ;\nConnaissance de la gouvernance de la donnée.\nSkills Individuels :\nMotivé(e) et doté(e) de solides compétences en matière de résolution de problèmes et d'apprentissage ;\nFlexibilité face aux changements d'orientation du travail au fur et à mesure de l'évolution du projet ;\nExcellentes capacités de communication, d'écoute et de persuasion.\nAttitude attendue :\nSens aigu des chiffres, curiosité intellectuelle et volonté d'adapter sa position sur la base d'informations complémentaires ;\nForte éthique de travail ; capacité à travailler à un niveau abstrait et à obtenir un consensus.\nInformations supplémentaires :\nPoste à pourvoir dès que possible ;\nRemboursement des frais de transports + Mutuelle ;\nPossibilité de télétravail jusqu'à un jour par semaine.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "Other": [
                "Statistiques",
                "Cloud",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akkodis",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-akkodis-3890779946?position=6&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=51eLIaLagCCvbVYrGyhL0A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La ligne de service Consulting & Solutions d’Akkodis France renforce ses équipes en région Hauts-de-France et recrute un\nData engineer H/F\nen\nCDI\nsur la\nmétropole lilloise\n:\nDescription de la mission :\nConcevoir, mettre en oeuvre et maintenir des pipelines de données efficaces et évolutifs dans un environnement cloud (comme AWS, Azure, Google Cloud Platform…)\nAssurer la qualité des données et des modèles\nDéfinir les bonnes pratiques de développement en implémentant des outils de CI/CD\nAssurer une veille technologique sur les technologies Cloud\nCapacité à interagir avec des parties prenantes diverses : business analyst, architecte, métier…\nVeiller au bon fonctionnement des pipelines en production\nProfil :\nDe formation\nBac +4/5 en informatique\nou issu d'une\nécole d'ingénieur\n, vous possédez une expérience de\n3 ans\nminimum en tant que data engineer ainsi que les compétences suivantes :\nUne bonne connaissance des écosystèmes liés à la data (Kafka, ETL, base de données…)\nUne première expérience sur un cloud provider (AWS, Azure, GCP)\nUne bonne maitrise de langages de programmation tels que SQL, Python, Scala\nAkkodis accompagne ses clients dans la mondialisation de leurs projets, aussi un anglais courant est requis pour l’ensemble de nos collaborateurs.\nProcessus de recrutement :\nUne chargée de recrutement vous contacte pour échanger sur votre projet professionnel\nVous échangez ensuite avec un.e manager sur les aspects techniques, les projets\nChez Akkodis nous sommes convaincus que de l’intelligence collective naît le succès. Il n’existe pas qu’un modèle, nous valorisons l’agilité et l’excellence, l’audace et la créativité.\nEt si nous parlions ensemble de vos ambitions pour les prochaines années ?\nAkkodis est une entreprise handi-engagée et inclusive. Tous nos postes sont ouverts aux handicaps et à la diversité. Tous différents, tous compétents !\nAkkodis, est un acteur mondial de l’ingénierie et de l’IT et un leader dans la smart industrie. Nous accompagnons nos clients dans leurs projets de transformation digitale via 4 lignes de service : Consulting, Solutions, Talent et Academy. Akkodis est un partenaire technologique de confiance pour ses clients à l’échelle internationale. Nous co-créons et nous imaginons des solutions de pointe pour répondre aux défis majeurs de notre société, qu'il s'agisse d'accélérer la transition énergétique et de développer la mobilité verte, ou encore de construire des approches centrées sur les utilisateurs.\nDotés d’une forte culture de l’inclusion et de la diversité, nos 50 000 experts en IT et en ingénierie, présents dans 30 pays, allient les meilleures compétences technologiques à une connaissance transverse de toutes les industries pour façonner un futur plus durable. Nous sommes passionnés par l’idée d’inventer ensemble un avenir meilleur.\nAkkodis en France, ce sont près de 9.000 experts en IT et en ingénierie répartis sur l'ensemble du territoire, des collaborateurs partageant des valeurs fortes d'honnêteté, de respect, d'équité et d'inclusion. Notre engagement : leur permettre au quotidien d'être eux-mêmes au travail, et acteurs de leur vie et de leur développement au sein d'Akkodis.\n*Akkodis est une marque commerciale sous laquelle les entités AKKA et Modis opèrent\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "FINAXYS",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-at-finaxys-3887107285?position=7&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=LAW%2BLRyFCy23JG8FyMIylQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LE CONTEXTE\nLeader en\nIT\n, dans les domaines Banque\n,\nFinance\net\nAssurance\n,\nFinaxys\nest un cabinet de\nconseil\ncréé en 2008. Nous accompagnons au quotidien les plus grandes banques du pays sur leur\ntransformation digitale\n(BNP Paribas, Société Générale, Crédit Agricole, Natixis, etc.)\nNos clients bancaires travaillent également dans des contextes Big Data sur des applications centrales rattachées aux Datalakes.\nLES MISSIONS\nDéveloppement et traitements sur des applications Big Data (Python)\nÊtre force de proposition sur les choix techniques les plus pertinents\nMaintenir la qualité des solutions, mesure de cette qualité, alerte sur les non-conformités et validation des solutions définitives.\nAnalyser des risques liés aux solutions envisagées et proposition des actions de remédiation.\nApporter des solutions IT répondant au mieux aux besoins du business porté par la/le Product Owner (Métiers/Fonctions) en cherchant toujours la maximisation de la valeur générée\nAccompagner les équipes dans les migrations Cloud\nENVIRONNEMENT TECHNIQUE\nPython\nPandas\nScirpting Big Data\nCulture DevOps (Jenkins, Maven, Ansible)\nPROFIL\nCompétences Techniques et Fonctionnelles requises\nMaitrise obligatoire de l’anglais\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataAnalytics": [
                "Pandas"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Automation": [
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Apside",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-apside-3909772916?position=8&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=KiWiQcocdp%2B9zcfFjDOmIw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Offre d'Emploi : DATA ENGINEER H/F chez Apside\nDescription du poste :\nNous sommes à la recherche d'un Data Engineer passionné pour rejoindre notre équipe dynamique. Si vous avez une expertise dans le Big Data, la Data Science, l'analyse de données et l'architecture de données, cette opportunité est faite pour vous. Intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.\nVos missions :\nDéveloppement des jobs Spark pour la collecte et la transformation des données comptables disponibles dans les bucket S3.\nOptimisation des jobs Spark.\nDéveloppement des batchs Java et écriture des données au formats comptables.\nÉcriture et ordonnancement des DAGs Airflow.\nSupport du développement Spark Scala.\nMaintenance applicative.\nProduction des événements dédiés à la plateforme de données.\n.\nVotre rôle, vos compétences :\nVous maîtrisez au minimum un langage de programmation appliqué à l’analyse de données (SQL, Scala, Python, Java).\nVous êtes passionné par le Big Data et le Machine Learning.\nVous concevez et mettez en œuvre des stratégies sécurisées d'acquisition et d'intégration de données.\nVous configurez des référentiels de données à la pointe de la technologie dans des environnements distribués, majoritairement dans le cloud (Google Cloud Platform, Azure, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks).\nEnvironnement technique :\nSQL\nPython/Spark\nCloud AWS: AWS Glue, AWS Lambda (possibilité de vous former sur AWS)\nStockage objet (AWS S3)\nOrchestration et scheduling de tâches (Apache Airflow)\nBases analytiques et bases NoSQL (ElasticSearch, AWS Athena)\nVotre profil :\nFort de 4 années d’expérience en Data Engineer/ DATA ANALYST\nTitulaire d’une formation supérieure IT.\nCapacité à s’intégrer dans un cadre technique client tout en étant à même de proposer des pistes d’améliorations pertinentes.\nAutonome dans la gestion des projets.\nCurieux et impliqué, vous êtes bon communicant avec les clients et les acteurs de culture technique différente.\nDe bonnes raisons de rejoindre Apside ?\nUn esprit start-up avec la stabilité d’un grand groupe, qui favorise l’agilité, le travail d’équipe et la proximité. Alors qu’Apside ne cesse d’agrandir sa famille déjà forte de plus de 3000 consultants, nous sommes à la recherche de nos nouveaux talents !\nCDI + package salarial avantageux (Mutuelle offerte, RTT, Tickets Restaurant, Intéressement ...)\nParticipez et animez nos soirées techniques (Project Lab, Test Lab…),\nDevenez speaker (Devoxx, DevFest, NCraft…),\nFormez vous avec l’Academy By Apside (e-learning, formation, certification).\nDéveloppez votre réseau (Soirées trimestrielles, Afterwork, Soirées d’intégration…),\nIntégrez notre Communautés d’Experts et testez les dernières innovations techniques sur notre Bac à Sable !\nApside s’engage en faveur de l’emploi des personnes en situation de handicap avec sa filiale Apsid’EA : 1ère entreprise adaptée totalement intégrée à une ESN !\nPour aller plus loin avec APSIDE !\nhttps://www.apside.com/fr/nos-offres-emploi/?_joboffer-agency=17833&_paged=2\nCe poste de DATA ENGINEER est fait pour vous !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "17833",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Apache Airflow",
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "AXA en France",
        "location": "Hauts-de-Seine, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-senior-at-axa-en-france-3884386043?position=9&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=JA%2FYt%2BTcyAXjKeciB27Q5g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Environnement\nEn tant que\nSenior Data Engineer F/H\n, vous allez contribuer directement aux projets des directions métier (ex : fraude santé, multiéquipements, pricing IARD, optimisation du lead management, fragilité auto, …) d’AXA France et à la construction du socle technique Big Data.\nVous allez intégrer une équipe d'une dizaine de personne composée de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus métier de la Direction Transformation Digital Tech et DATA (DT2).\nLa Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :\n- Une organisation agile en feature teams : tribus, guildes, squads\n- Des projets sur des applications innovantes à fort trafic (web, mobile…)\n- Des méthodologies craft (TDD, BDD, clean code, code review…) et DevOps\n- Une communauté de partage de bonnes pratiques (BBL, dojo, meetup, conf…)\nVotre rôle et vos missions\nVous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :\nPasser de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le datalake\nConsolider ces données au fur et à mesure de leur alimentation récurrente dans le data lake\nLes exploiter pour atteindre la finalité business (exposition de business view, réintégration des résultats dans le SI, service de scoring, …)\nDe travailler à la création du socle technique Big Data et industrialiser le cycle de développement de l'équipe\nDe mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)\nVotre profil\nVous justifiez de plusieurs expériences significatives (+ de 5 ans) sur du\ndéveloppement big data, en particulier sur du PySpark.\nCompétences techniques :\nConnaissances avancées en développement en\nPySpark\n(Spark avec le langage Python)\nMaitrise de l'environnement\nMicrosoft Azure\nConnaissances avancées d'outils de BI comme\nPowerBI\nCompétences transverses :\nCapacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier\nExpérience en mode de delivery Agile (Scrum, Kanban, etc...)\nDriver et accompagner des Data Engineer junior sur les aspects technique\nEt Idéalement :\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de données relationnelles et NoSQL\nUne expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nQui sommes nous ?\nAXA est un des leaders de l’assurance et de la gestion d’actifs dans le monde.\nNous aidons nos 108 millions de clients à traverser les petites et grandes difficultés de la vie.\nChaque jour, nous agissons ensemble pour inventer la meilleure manière de les protéger et voulons donner à chacun les moyens de vivre une vie meilleure.\nUn challenge qui donne le sourire et envie de se lever le matin !\nChez AXA, nous sommes persuadés que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C’est pour cette raison que nous menons une politique RH engagée qui favorise la diversité, qui préserve l’équilibre vie privée-vie professionnelle et accélère le développement des compétences et des carrières.\nAinsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une véritable culture d’expertise, accélérant le développement des compétences de chacun et proposant une rémunération attractive.\nPourquoi nous rejoindre ?\nVous êtes porteur d’idées et d’initiatives innovantes ? Vous proposez des solutions et êtes au service du client ? Faites partie de notre grande famille en rejoignant\nUn leader mondial offrant des opportunités de carrières intéressantes\nUne entreprise qui donne une place de choix à l’innovation, à l’initiative et aux actions solidaires (notamment via l’association AXA Atout Cœur)\nUn environnement inclusif à tous les niveaux (mixité, handicap, initiatives pour favoriser l’insertion des jeunes, orientation sexuelle, etc.)\nUn accès à de multiples avantages (congés, temps partiel, télétravail, etc.)\nUn cadre stimulant, qui permet de rencontrer des collaborateurs performants et d’enrichir ses compétences\nVictime ou témoin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination à alerte.discrimination.harcelement@axa.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Junior",
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Beelix",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-beelix-3865239426?position=10&pageNum=5&refId=ta7dp2AfGVWmvLdIA1X0jA%3D%3D&trackingId=xgTZ8s7KT%2BRRnLHwJ5y8rA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des problématiques de Product Management, Data et Design Thinking. Beelix contribue à façonner le monde de demain en participant aux grandes avancées des secteurs suivants:\n🚗Automobile\n⚡Energie\n📡Médias & Télécoms\n👗Luxe & Retail\n💶 Banque, Finance & Assurance\n✈️Défense\nAujourd’hui, Beelix compte plus de 200 collaborateurs motivés et dynamiques. Labélisée Great Place To work en 2023, Beelix est aussi une entreprise engagée où il fait bon vivre.\nDans le cadre de notre développement, nous recherchons un Data Engineer en Île-de-France.\nQuelles missions au quotidien ?\nVous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :\nPasser de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le Datalake\nConsolider ces données au fur et à mesure de leur alimentation récurrente dans le Datalake\nLes exploiter pour atteindre la finalité business (exposition de Business View, réintégration des résultats dans le SI, service de scoring, …)\nDe mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)\nAccompagner les Data Engineers sur son périmètre pour garantir la qualité des livrables\nExpertise souhaitée\nExpertise en SPARK et PySpark\nExpertise sur Databricks\nUne expérience sur un cloud provider public comme Azure (idéalement), AWS, ou GCP\nConnaissances avancées d'outils de BI comme PowerBI (idéalement) ou Spotfire\nCapacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier\nEtre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de données relationnelles et NoSQL\nUne expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nA propos de vous ?\nDiplômé d'une école d'ingénieurs ou équivalent\nAu moins 3 ans d'expérience en tant que Data Engineer\nExpérience en mode de Delivery Agile (Scrum, Kanban, etc.…)\nVous avez un bon niveau d’anglais tant à l’écrit qu’à l’oral\nPourquoi nous rejoindre ?\nUn suivi et un accompagnement au quotidien\nUn organisme de formation certifié Qualiopi, un abonnement linkedin learning pour chaque salarié et des partenariats avec des spécialistes pour d’autres expertises\nDe nombreux événements : Afterworks, Communautés métiers, Happy talks…\nune Expérience personnalisée basée sur vos besoins grâce au Prédictive Index\nNotre package « unBeelievable » : 100% du titre de transport, Tickets restaurants, CSE, Prime de participation ...\nNombreux évènements (afterworks, sport, etc) et des communautés métiers dynamiques\nLe processus de recrutement ?\nÉchange téléphonique (15 min)\nEntretien 1 RH pour apprendre à vous connaître\nEntretien 2 avec votre futur N+1 pour appréhender la relation managériale\nEntretien 3 avec un Responsable commercial pour avoir la vision stratégique\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "100",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Renault Group",
        "location": "Le Plessis-Robinson, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-renault-group-3904830231?position=1&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=HMXluzzoE9T1Sim4D7X%2BAg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Votre environnement\nLe métier Procurement de Renault est en pleine transformation et nécessite des outils performants pour gérer son réseau de fournisseurs, simuler des scénarios et anticiper les risques associés (logistiques, qualité, managériaux, sociaux-économiques, écologiques).\nEn s’appuyant sur les partenariats stratégiques de Renault avec\nGoogle Cloud\n(utilisant la pile complète de données GCP), la fonction Procurement a décidé de créer une nouvelle\nplateforme DATA et IA\nqui répondent aux défis actuels du secteur de la mobilité.\nC’est pourquoi, nous sommes donc à la recherche d’un(e)\nData Engineer\nau sein de la\nDirection IS Procurement (Domaine Supplier Quotation & ESG)\n.\nCe rôle consistera à mettre en place une plateforme pour la qualification de nos fournisseurs et la gestion des risques d’approvisionnement futurs.\nCette plateforme inclura également les nouveaux enjeux de réglementation ESG à l’échelle mondiale.\nLe poste est basé au\nPlessis-Robinson (France) en CDI.\nVous bénéficiez de\n2 à 3\njours\nde\ntélétravail par semaine.\nVos missions\nEn qualité de\nData Engineer\nvous êtes\ngarant.e de l’accès qualitatif aux sources de données\n. Vous\ndéveloppez\ndes outils permettant d’automatiser les traitements autour de la data (ingestion, stockage, transformation) afin d’en faciliter son exploitation par les équipes Data ainsi que par le reste des équipes informatiques. Vos principales missions sont :\nConcevoir des solutions robustes permettant le traitement de volumes importants de flux de données\nAssurer l’intégration de données provenant de sources multiples (applications ou extérieur à l’entreprise)\nDéfinir et faire respecter les bonnes pratiques et règles à suivre autour des sujets d’ingestion, stockage et de transformation de la donnée\nConstruire l’architecture relative aux plateformes data ainsi qu’à ses composants\nEffectuer une veille technologique importante afin d’assurer que les plateformes data soient les plus efficientes possibles\nVous collaborerez au jour le jour avec les équipes métiers ainsi qu’avec les autres fonctions du\nPôle Architecture & Data\n(Data Analysts et Scientists, architectes, …), exploitant des téraoctets de données (événements en mode streaming, traitements en batch et temps réels et les appels aux APIs) afin entre autres d’alimenter à terme des modèles de machine learning (segmentation fournisseurs, détection de risques, Tecno watch & market search…).\nVotre profil\nVous êtes diplômé(e) d’un BAC+5 en université ou en école d’ingénieur avec\nun parcours significatif dans\nl’informatique\n(Domaine DATA) avec\nau moins 5 années d’expérience.\nVous avez une appétence pour la\nDATA\n: validation, transformation, analyse, valorisation, et vous disposez d’une\nsolide expérience en développement Java\n,\nScala\net\nPython\net\nrequêtage SQL\nsur des gros volumes de données.\nVous avez une expérience de développement et orchestration de chaines\nETL\ncomplexes via Composer (GCP), Airflow ou équivalent.\nVous savez utiliser des services cloud\nGoogle Cloud Platform\n(BigQuery, Cloud Storage, PubSub, Beam, Dataflow, Kubernetes, Composer, Big Table, Firestore), Git, JSON, Bash, Docker.\nVous avez fait de l’\nadministration\net\nconfiguration de systèmes\n.\nVous avez une forte capacité de\ntravailler dans un contexte multiculturel,\nvous avez un esprit orienté «\nutilisateur final\n» et vous faites preuve\nd’autonomie\n. La pratique du\nframework Agile Scrum/Kanban est acquise\n.\nVous avez une grande\ncapacité d’analyse\net savez prendre du recul. Vous faites preuve de\nrigueur\net d’\nesprit de synthèse\n.\nVous avez un\nbon niveau d'anglais écrit et oral\n, facteur clé de réussite dans le poste (équipes internationales basées en Inde, Roumanie...).\nVous vous reconnaissez dans cette annonce ? Alors ce poste est fait pour vous !\nRejoignez-nous ! #CarMakerCareMakers\nDécouvrez notre processus de recrutement\n: https://www.renaultgroup.com/talents/nos-offres-dans-le-monde/\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Bash",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataSerialization": [
                "Json"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Machine Learning",
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Devoteam Innovative Tech",
        "location": "Lyon, Auvergne-Rhône-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-devoteam-innovative-tech-3889051125?position=2&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=IEJ4ZmL2g0XiUrCZk88fDg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Company Description\nAvec près de 10 000 collaborateurs à travers le monde, nous accompagnons les entreprises dans leur transformation numérique. Nous imaginons et concrétisons leurs ambitions grâce aux possibilités infinies des plateformes digitales, pour faire évoluer leur culture et leur mode de travail, et créer de la valeur dans leurs organisations.\nPrésent dans 18 pays d’Europe et du Moyen-Orient et fort de 25 ans d’expérience, nous mettons la \"Technologie au service de l’Homme\" afin de construire un monde plus humain et plus durable.\nTravailler chez Devoteam, c’est :\ntravailler aux côtés de partenaires comme Google, Microsoft, AWS ou Salesforce dont nous implémentons les solutions chez nos clients.\névoluer dans un groupe international qui vous accompagne dans le développement de votre carrière avec des parcours de formation et de certification adaptés.\nrejoindre une équipe spécialisée, accompagné par un manager de proximité qui saura vous guider dans vos choix et favoriser les échanges avec vos pairs, que ce soit lors d'événements techniques ou conviviaux.\ngrandir dans une entreprise qui challenge ses équipes en étant agile et ambitieuse, s’adaptant pour permettre les succès individuels et collectifs.\nLa mission\nEn pleine transformation digitale, le client conçoit et fournit des produits et services digitaux pour accompagner la transformation digitale de ses entités dans le monde entier.\nLe client souhaite renforcer les capacités de développements d’une des équipes\nsur la technologie de développement Databricks (Databricks, Python, Spark SQL) dans un\nenvironnement 100% Cloud Azure. Une expérience Azure passée serait\nappréciée.\nLes principales activités sont :\n• Développements de pipelines Data permettant la construction et l’évolution de leur\nDatalake\n• Participation aux déploiements sur les environnements de test et de production\n• Maintien en condition opérationnelle des éléments logiciels en production\nEn fonction des autres compétences du candidats, d’autres activités pourraient être :\n• Participation à des réunions de conceptions et de chiffrages\n• Participation à des réunions de prises de décisions techniques\n• Conseils techniques au Tech Lead / Architecte de l’équipe « Data » et de la\nplateforme\nLe candidat devra pouvoir s’intégrer dans un projet dynamique en méthodologie agile et\ndans une équipe de grande taille.\nCompétences techniques attendues:\nDatabricks\nPython\nTravailler en SCRUM\nEnvironnement Azure\nAdditional information\nLe Groupe Devoteam œuvre pour l'égalité des chances, pour la promotion de ses collaboratrices et de ses collaborateurs au mérite et lutte activement contre toute forme de discrimination. Nous sommes persuadés que la diversité contribue à la créativité, au dynamisme et à l'excellence de notre organisation. Tous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "25 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "FRG Technology Consulting",
        "location": "Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-data-ops-at-frg-technology-consulting-3913842168?position=3&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=fueQLWECQUoFhtWNsICVBQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous êtes un expert passionné par la Data et à la recherche de défis excitants ? Mon client recherche actuellement un\nData Engineer\n/ Data Ops\ntalentueux pour rejoindre une équipe dynamique et humaine.\nMissions principales :\nParticipation active au déploiement de la nouvelle plateforme sur Azure & Snowflake\nForte autonomie et gestion complète des projets data\nAnalyse des besoins actuels et futurs\nCréation de spécifications fonctionnelles et techniques\nModélisation de données\nDéveloppement de packages SSIS\nIntégration des données dans SnowFlake & Azure,\nCréation de rapports avec Power BI et Excel\nProfil recherché :\n3 à 4 ans d'expérience\nminimum\ndans la BI (SSIS, SQLServer, SSAS, SSRS) et/ou le cloud (Azure , Snowflake) ainsi qu'en SQL\nCompétences en\narchitecture sur Snowflake\nfortement appréciées\n1 à 2 ans d'expérience en tant que DevOps ( CI/CD ; GitLab)\nAutonome, rigoureux et anglais courant\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "3",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud",
                "DevOps",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Mobiskill | WEFY Group",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-mobiskill-wefy-group-3907393935?position=4&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=orRfmeGxiNXVgrUdTGjoxQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La société :\nLe produit repose sur la data, leur solution basée sur de l’intelligence artificielle permet de personnaliser le monde de la promotion et de la fidélisation. Ils viennent d'être rachetés et ont une forte ambition pour leur expansion internationale.\nLes missions :\n- Travailler avec les data scientists pour apporter des solutions\n- Industrialiser les modèles\n- Optimiser la performance du produit\n- Développer des outils big data pour scaler\n- Mentorer des profils plus juniors\nStack :\n- Scala\n- Spark / Spark Streaming\n- Kafka\n- GCP\n- Cassandra\n- Docker\nProfil recherché :\n- Entre 3 et 5 ans d'expérience dans le Data Engineering\n- Expérience en Scala/Spark\n- Expérience sur cloud (très idéalement GCP)\n- Prêt à faire des missions polyvalentes\n- Ouvert à d'autres technos (ils ont pour objectif d'implémenter prochainement des outils en Python)\nPourquoi les rejoindre :\n- Expansion internationale : USA, Brésil, Russie, Espagne…\n- Une stack à la pointe et un champs d’action pour POCer de nouvelles technos si il y a un intérêt business\n- Un encadrement bienveillant : les 2 leads techniques sont deux excellents techs ET d’excellents mentors avec qui échanger sur comment faire avancer la société (tu serais le troisième maillons de la chaine).\n- Politique remote hybride\n- Des bureaux dans Paris intra-Muros\n- Une rémunération pouvant dépasser 70k (avec package)\n- Une entreprise très tech, particulièrement orientée Data\nHâte de vous en dire plus rapidement !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Remote",
                "Junior"
            ],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Cassandra"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DevTools": [
                "Docker"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Containers": [
                "Docker"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Equativ",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-equativ-3814251519?position=5&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=K3xNAWACEsa%2BL%2Fe8C6y7wQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "👫 About the team\nAt Equativ, we’re on a mission to develop advertising technologies that empower our customers to reach their digital business goals. This means that we rely on massively scalable, widely distributed, highly available, and efficient software systems; the platform deals with over 100 billions requests per day and above 40 Gbps of network traffic.\nOur innovation team based in Paris, Nantes, Limoges, Kraków and Berlin is composed of 90 straightforward and energetic engineers working in an Agile environment and ready to tackle the most complex technical challenges.\nOur data engineering team is composed of 10 skilled engineers and is based in Paris. We are part of the R&D department which is composed of 140+ engineers spread across Paris, Nantes, Limoges, Kraków and Berlin all working in an Agile environment and ready to tackle the most complex technical challenges.\nOur mission 👇\nData Engineering team is central to Equativ’s data centric business and is responsible to ingest, transform, model and redistribute all data coming from our adtech platform.\nWe aim at building scalable and robust Big Data platforms from ingestion to business actionable consumption. Our Big Data ecosystem must handle massive log ingestion (tens of billions per day), short & long term data storage, complex data modelling, real-time and batch ELT as well as providing external access through dedicated APIs.\nData Engineers serve Equativ data directly to our customers and throughout the company whether it is for BI analysis, data science algorithms (clustering and optimization), customer reporting, invoicing and more.\nEquativ Data Engineering team is engaged in an ambitious migration of its main data stack (Hadoop on-premise) to GCP with the objective to increase reporting features, lower maintenance time, improve performances and simplify the access to our raw data.\nWhat you'll do ✏️\nAs a Big Data Engineer, you’ll primarily focus on maintaining and enhancing the operationality of our on-premise and cloud data pipelines which feed our warehouses and APIs\n-Design, develop, test, promote and industrialize all data components from data ingestion to datawarehouse delivery (ClickHouse, BigQuery):\nPropose and develop innovative solutions to achieve the best levels of scalability and performance for our Big Data engines\nAutomate and streamline our real-time and batch data pipelines (on-premise and in the cloud) in order to simplify access to our data by other teams and lower amount of work spent by other teams on ETL processes\nPerform end-to-end monitoring to ensure high availability of production data processing, data quality and reliability\nApply best in class Devops guidelines and secure deployments\n-Brainstorm with other team members working on our data backend (datawarehouse modelling and data exposure through our reporting APIs) on optimizing our architecture and support them in the use of our pipelines\n-Contribute to data roadmap definition in coordination with other R&D and product teams in order to build a best in class data infrastructure that will generate insights for Equativ’s analytics\n-Take part in improving and deploy data engineering standards, procedures, processes and operational guidelines around target data components at Equativ\n💪 About you\nMaster degree in Computer Science or similar technical field of study\n3+ years of software development with open source technologies\nFluent in Java and/or in Scala. SQL mastery\nVery good understanding of Devops principles (Gitlab, Docker, Kubernetes, Gradle, ci/cd)\nExperience with large-scale data engineering technologies (ClickHouse, Flink, Kafka, Hadoop, Spark, Hbase)\nExperience on building data pipelines on Google Cloud Platform (BigQuery, Dataflow, GCS, Cloud Run, Airflow …) would be a big plus\nExperience in working with high QPS Rest APIs is a plus\nEntrepreneurial spirit and know-how to identify opportunities of improvement\nWorking proficiency and communication skills in verbal and written English\nPassion for playing with large volume of data\n🚀 How you'll grow\nWithin 1 month:\nYou'll be just finishing your onboarding.\nYou'll probably have tackled a few small tasks in peer-coding\nWithin 4 months:\nYou'll have an overview of 50% of the stack, CI/CD and team’s main processes. You’ll be able to work on more complex developments\nYou'll now have enough knowledge to participate to deployments of chosen applications\nWithin 9 months:\nYou'll be autonomous on most of our stack and will have participated to major projects\nYou’ll be helping the team on production matters\n👋 About us\nEquativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus — four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.\nHeadquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.\nThe company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times’ FT 1000: Europe’s Fastest-Growing Companies.\nEquativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.\nCome and lead the charge with us in building a transparent ecosystem based on quality!\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala"
            ],
            "DataBase": [
                "SQL",
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Flink",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "CGI",
        "location": "Niort, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-cgi-3902411140?position=6&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=JPIoTGBzLJTzxy%2BVAMsOUQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Big Data, Data Science, Data analyse, Data architecture ... Ça n’a pas de secret pour vous ?\nQue vous commenciez votre carrière professionnelle ou que vous soyez spécialiste de l’une de ces disciplines, intégrer notre communauté Data, c’est l’assurance de progresser, innover, partager, vous certifier et rendre service à nos clients.\nSi vous souhaitez intégrer nos équipes à Niort et accompagner les plus grands acteurs du secteur des Assurances, cette annonce est susceptible de vous intéresser.\nEn tant que Data Engineer, vous serez responsable de la conception, du développement, de la gestion et de l'intégration des systèmes basés sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop. Ce rôle implique la mise en place d'architectures évolutives et hautement disponibles pour répondre aux besoins de traitement et de stockage de données de l'entreprise.\nFonctions et responsabilités\nVos responsabilités seront les suivantes:\n-Maintenir et développer des solutions basées sur les services AWS pour le stockage, le traitement et l'analyse de données\n-Utiliser les services AWS appropriés tels que Amazon EC2, S3, RDS, Lambda, etc., pour répondre aux exigences du projet.\n-Créer et maintenir les configurations Terraform pour la gestion de l'infrastructure en tant que code (IaC) sur AWS\n-Participer à la maintenance et à la mise en place d'environnements OpenShift pour l'hébergement d'applications et de services\n-Gérer et administrer les clusters Kafka pour garantir la disponibilité, la performance et la sécurité du système de messagerie\nParticiper à l’assistance utilisateurs sur les briques de la plateforme Hadoop Cloudera Data\n-Travailler avec les projets et les devOps pour assurer un traitement efficace des données\nEn rejoignant CGI, vous bénéficiez notamment d’une offre complète de formations (techniques, métiers, développement personnel,…), de flexibilité grâce à notre accord télétravail (jusqu’à 3 jours de télétravail par semaine), d’une politique de congés avantageuse (27 jours de congés payés, RTT, congés ancienneté et enfant malade,…) et d’un package d’avantages intéressant (régime d’achats d’actions, participation, CSE,...).\nQualités requises pour réussir dans ce rôle\nAyant une première expérience en tant que Data Engineer, vous avez une première expérience relative aux points suivants:\n-Développement et intégration sur les technologies AWS, Terraform, OpenShift, Kafka et Hadoop\n-Connaissance avancée de l'administration Kafka, y compris la configuration, la gestion et la résolution des problèmes\n-Mise en œuvre de l'infrastructure en tant que code à l'aide de Terraform\n-Bonne compréhension des bonnes pratiques de sécurité pour les systèmes cloud, les clusters Kafka et les plateformes Hadoop\nCGI est un employeur inclusif et attentif aux candidatures des personnes en situation de handicap, à l’évolution de carrières des hommes et des femmes et au bien-être de nos salariés LGBT+. Dans un souci d’accessibilité et de clarté, le point médian n’est pas utilisé dans cette annonce. Tous les termes employés se réfèrent aussi bien au genre féminin que masculin.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "BigData": [
                "Hadoop"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "OpenShift"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Groupe INGENA",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-groupe-ingena-3883689479?position=7&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=yIgRfMAvvjQ%2BhgrmzkPCRg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Le groupe INGENA promeut la transition numérique en étant acteur d’un monde souhaitable.\nVotre mission :\nConcevoir, développer et tester des algorithmes de collecte et de traitement de gros volumes de données sous Scala, Python ou Java\nAutomatiser et optimiser les flux de données et leurs visualisations en dashboards\nIndustrialiser les traitements, la qualité et l’intégrité des données\nParticiper à la Modélisation et à la Gouvernance des données (process, normalisation, référentiel,…)\nContribuer à la scalabilité, la sécurité, la stabilité et la disponibilité des données de la plateforme\nAnalyser les données pour répondre aux questions métiers et participer à l’évolution de l’architecture Big Data\nConcevoir, Développer et Industrialiser des modèles de Machine Learning, Deep Learning, en collaboration avec les Data Scientists\nAppliquer une démarche CI/CD (Git, Jira, Jenkins)\nLes compétences techniques nécessaires sont :\nExpérience de 5 ans minimum en développements Scala, Python ou Java\nExpérience de 2 ans minimum sur SPARK et sur le traitement des flux en streaming\nExpertise sur Hadoop (Hive, HBase, HDFS) sous distributions MapR ou Hortonworks\nExpérience souhaitée sur ELK, Terraform, NoSQL,…\nFort background en Modélisation de données ou ETL\nMaîtrise des briques analytiques des clouds AWS, GCP ou Azure\nSensibilisation à la démarche CI/CD tools (Git, Jenkins)\nLa connaissance de Docker, Kubernetes et Ansible est un plus\nMise en œuvre des méthodes Agile (Scrum, Kanban,…)\nAnglais souhaité\nGroupe INGENA\n:\nLe Groupe INGENA est spécialisé en Conseil Métier et en Intégration pour les marchés de l’assurance, de la banque et de la Finance. INGENA intervient notamment sur les projets associés à la Data, aux Risques et à la Distribution.\nLe groupe comprend également la société DRiMS spécialisée en Finance de Marché.\nNos valeurs : Engagement, Intégrité et Bienveillance.\nLa mise en pratique du monde souhaitable, c’est pour nous une entreprise éco-responsable, éthique, inclusive, sociale, soucieuse du bien-être, de l’évolution et de l’épanouissement de ses équipes. Ce sont aussi des offres pour un monde durable comme la maîtrise des risques ou l’ESG.\nDans un esprit convivial et engagé, nous faisons en sorte que chacun puisse être acteur de l’INGENA souhaitable.\nBureau à Paris 9ème (Métro Le Peletier). Clients à Paris ou très proche banlieue.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "HBase",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker",
                "Jenkins"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Kubernetes",
                "Ansible"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Machine Learning",
                "Big Data",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Mirakl",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-engineer-at-mirakl-3904071960?position=8&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=SigrjGWkAlndnlAkvWWyaA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Mirakl, leader et pionnier de l’économie de plateforme, propose aux entreprises une suite unique de solutions leur permettant de transformer significativement leur e-commerce afin d'accélérer de façon durable et rentable leur croissance. Depuis 2012, Mirakl accompagne les entreprises B2C et B2B avec la technologie la plus avancée, sécurisée et évolutive leur permettant de digitaliser leur activité et d'élargir leur offre via la marketplace ou le dropship, faciliter la gestion des catalogues et des paiements de leurs fournisseurs pour plus d'efficacité, offrir une expérience d'achat personnalisée à leurs clients, et augmenter leurs profits grâce au retail media. Basée à Paris et Boston, Mirakl est certifiée Great Place to Work.\nA propos de Mirakl Labs\nNos équipes techniques et produits, nommées Mirakl Labs, sont principalement réparties entre nos 2 hubs situés à Paris et à Bordeaux. Elles collaborent au quotidien afin d'adresser les problématiques de nos clients et utilisateurs en répondant à différents challenges liés aux nouvelles fonctionnalités, à la scalabilité, la sécurité et l’ergonomie…\nElles opèrent en mode agile et s'organisent en Squads composées d'un Squad Lead, de 5 développeurs, d'un Product Manager et d'un QA. Chaque Squad est spécialisée sur un scope fonctionnel afin de concevoir et réaliser de nouvelles features, leurs évolutions et des APIs (avec un découpage en micro-services). Nos équipes Infrastructure, Architecture, Sécurité, Documentation, Product Design, Data et Support opèrent en transverse en apportant leur expertise et de la cohérence sur l’ensemble des produits.\nToutes les équipes sont responsables de leur périmètre et chacun des collaborateurs apporte son expérience et ses idées. Innovation, feedback et implication dans les prises de décision sont au cœur de notre philosophie.\nEt pour favoriser ce partage avec d’autres passionnés, nous sommes sponsors, speakers, et hôtes de différents événements, meetups, et associations de la scène Tech en France. Au cours des dernières années, nous avons participé à des événements tels que Devoxx, ReactEurope, ProductConf et Flupa UX Days.\nA propos du job\nLa solution SaaS Mirakl est le moteur des marketplaces des plus importants e-commerçants à travers le monde. Cette solution gère et produit de gros volumes de données qui présentent des challenges extrêmement intéressants pour les spécialistes de la donnée (produits, commandes, clients, niveaux de stock, prix, messages, appels API, données de navigation, séries temporelles, données géolocalisées etc.).\nEn tant que (Senior) Data Engineer au sein de l’équipe Data Mirakl, vos principales missions seront de :\ncontribuer à l'enrichissement de la Data Platform (ETL)\naméliorer la robustesse de nos pipelines de production pour nos applications Machine Learning (inférence real time etc.)\nIntégré(e) dans une équipe de spécialistes de la donnée (data engineers, machine learning engineers, data scientists, data analysts), vous êtes un des acteurs clés pour garantir la place de Mirakl comme solution dominante sur son marché.\nNotre stack et nos outils\nApache Spark, Kafka, AWS, Databricks, Python, Airflow, Mlflow, Tensorflow, Delta lake, Superset, Kubernetes, Redshift, SQL, Terraform, Ansible\nAu quotidien, vous allez :\nParticiper à la définition et à l’implémentation d’une architecture performante, robuste, scalable et aux coûts maîtrisés pour nos applications Spark ainsi que pour nos pipelines de production de Machine Learning (évaluation des feature stores, refactoring de DAG Airflow)\nAccompagner les Data Scientists lors de leur mise en production (relecture de code, pair programming) et mettre en place les best practices\nOptimiser et améliorer la CI/CD de l’équipe en collaboration avec l’équipe SRE\nAssurer la montée en compétence des membres de l’équipe sur les sujets de MLOps et Data Engineering\nRéfléchir à la meilleure façon d'intégrer les données Google Analytics dans la data platform\nPartager ses connaissances et présenter les travaux devant toutes les équipes Labs\nCe qu’on peut vous apporter :\nDes projets data driven, divers et variés (traitements massifs d’images, de textes, time series etc.) pour des produits différents de Mirakl\nUne culture orientée sur la veille technologique\nDes projets qui ont un vrai impact business devant être déployés sur des centaines de clients dans un contexte multilingue\nQuelques exemples de sujets en cours :\nEnrichissement des données produit à partir des images et des descriptions\nModération automatique des produits\nMapping automatique des données produit\nIdentification des produits à fort potentiels\nDétection de comportements frauduleux\nSentiment analysis sur les messages échangés entre clients et vendeurs et dans les évaluations\nDétermination de prix optimaux\nMonitoring de la qualité de service des vendeurs\nDes applications d’inférence en synchrone de nos modèles de ML\nVous aimerez ce job si :\nVous êtes passionné(e) par la data et les technologies modernes permettant d'en tirer partie\nVous vous intéressez à la data science et avez des connaissances générales sur les algorithmes de Machine Learning\nVous avez un background en développement et avez évolué dans un environnement Data\nVous avez a minima 4 ans d’expérience en environnement Machine Learning et/ou Data\nVous avez mis en production avec succès des applications Big Data faisant appel à du Machine Learning, du NLP, du traitement d’images dans des projets d'envergure, à fort volume de données\nVotre maîtrisez Python, êtes un pro des frameworks data de la fondation Apache et êtes à l'aise dans un environnement AWS\nVous maîtrisez au moins un outil d’orchestration (Airflow, Data Pipeline ou tout autre outil similaire)\nVous présentez vos travaux de manière simple et accessible\nVous faîtes preuve d'un bon relationnel et vous aimez mentorer des collaborateurs\nVous parlez couramment anglais et français\nLes plus pour le poste :\nVous avez une expérience significative dans le domaine du e-commerce\nVous avez déjà mis en place un Data Lake, Data Warehouse ou une Data Platform\nVous avez déployé des applicatifs en environnement Kubernetes\nVous avez mis en place des pipelines d'ingestion de données avec une approche CDC à l'aide de Debezium ou autre\nVous maîtrisez Java/Scala\nMirakl est engagée en faveur de la diversité, de l’égalité des chances et de l’inclusion. Nous célébrons nos différences car nous sommes convaincus que les qualités visibles et invisibles de chaque Mirakl Worker sont une source de force et d’innovation. Dans le cadre de cet engagement, nous étudions toutes les candidatures sans distinction de : genre, ethnicité, religion, orientation sexuelle, handicap, âge ou toute autre caractéristique protégée par la loi.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes",
                "Ansible"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "ML",
                "Machine Learning",
                "Big Data",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ALFI : Financial Markets Consultancy Services",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-alfi-financial-markets-consultancy-services-3916552424?position=9&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=Upxx%2BSq7mrkfa3H4edVMzw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Le Data Engineer intervient au sein de l’équipe Engineering Open Big Data du Département Guilde Data, qui regroupe l’ensemble des expertises technologiques liées à l’ingénierie de la donnée, de l’automatisation et à l’exploitation des modèles de Machine Learning.\nVotre rôle et vos missions :\nVous aurez pour missions principales de développer les projets Big Data demandés par le métier, et notamment :\nPasser de la donnée brute à de la donnée exploitable, exposée sous forme de tables requêtables dans le Datalake\nConsolider ces données au fur et à mesure de leur alimentation récurrente dans le Datalake\nLes exploiter pour atteindre la finalité business (exposition de Business View, réintégration des résultats dans le SI, service de scoring, …)\nDe mettre en place et de garantir le respect dans la durée d'un processus qualité sur l'ensemble du cycle de DEV (documents, tests unitaires / intégration / fonctionnels, commentaires, versionning, etc.)\nAccompagner les Data Engineers sur son périmètre pour garantier la qualité des livrables\nExpertise souhaitée\nCompétences techniques :\nExpertise en SPARK et PySpark\nExpertise sur Databricks\nUne expérience sur un cloud provider public comme Azure (idéalement), AWS, ou GCP\nConnaissances avancées d'outils de BI comme PowerBI (idéalement) ou Spotfire\nCompétences transverses :\nCapacité à interagir avec des parties prenantes diverses : Business analyst, Architectes, Métier\nExpérience en mode de Delivery Agile (Scrum, Kanban, etc.…)\nEtre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)\nEt :\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de données relationnelles et NoSQL\nUne expérience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nConformément à la règlementation, et à notre politique d’égalité professionnelle, tous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "BforBank",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-confirm%C3%A9-h-f-at-bforbank-3918327555?position=10&pageNum=7&refId=m3%2FMjbsrRC0%2FAGJwy5bQNw%3D%3D&trackingId=%2Fu5Xq4NFwA%2B3skuO1G0f5Q%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Sur le modèle d'une\n\"Tech company\",\nBforBank place\nl'humain et le digital\nau cœur de sa transformation. Notre mission,\noffrir à nos clients une expérience bancaire incomparable\npour répondre à leurs besoins et usages mobile. 🌟 📱\nRejoindre BforBank c’est\nrejoindre une équipe engagée\ndans un\ngrand projet de développement stratégique en France et en Europe.\nNous sommes aujourd’hui 350 passionné(e)s et\nrecherchons nos talents pour construire la banque de demain\n. 🚀\nNous croyons en la force du collectif, chaque jour rassemblés autour de nos valeurs, de simplicité, d'optimisme et d’engagement, encourageant chacun à oser, essayer et accepter d’échouer.\n🎯 Au sein de la Direction Technologie, la Data Factory a pour objectifs de piloter, définir, déployer et opérer les meilleures solutions technologiques répondant aux cas d’usage data et d’automatisations de processus de la banque au travers de plateformes. Également, la Data Factory contribue au développement des produits, à la cristallisation et à la diffusion des pratiques au sein des Squads BforBank sur les usages data dans la banque.\nTu rejoindras une squad en charge de résoudre des problématiques métiers en créant des solutions applicatives utilisant les données, des data products, avec pour finalités la prise de décision via des moteurs de calcul ou des dashboards, la création de flux réglementaires, la création de data layer ou de reportings.\n🚀 Tes missions principales sont les suivantes :\n· Participer aux analyses, études d’impacts et cadrage techniques\n· Concevoir des solutions en respectant les bonnes pratiques d’architecture data et développement\n· Réaliser le développement de nouveaux data products et assurer la maintenance évolutive et corrective des data products existants\n· Rédiger la documentation technique des data products\n· Assurer un support aux testeurs\n· Reporter de ton activité à ta Squad et travailler dans une démarche d’efficacité collective\nConcrètement tu seras amené(e) à produire les livrables suivants :\n· Réaliser du code applicatif à l’état de l’art sur notre nouvelle Data Platform\n· Créer des data layer et des rapports sur notre outil de Data Visualisation\n· Rédiger les documentations techniques liées à ta solution, incluant le modèle de données, les procédures, l’ordonnancement\nCe que tu maîtrises :\n· Maitrise des services managés de GCP (BigQuery, dataproc, dataflow, CloudSQL …)\n· Maitrise du langage Python, Pandas, Spark\n· Maitrise de la modélisation de base de données et du langage SQL\n· Maitrise d’une chaine CI/CD (GitLab…)\n· Bonne connaissance de Kafka\n· Bonne connaissance d’un outil d’intégration de données type ETL (Informatica…)\n· Connaissance de l’infra as code (Terraform)\n· Connaissance d’un outil de reporting (Looker, BO…)\n🤝 Ce poste est fait pour toi si :\n· Tu es passionné(e) par la Data et leurs usages\n· Tu es orienté résolution de problème, est curieux(se) et force de proposition\n· Tu apprécies le travail en équipe\n· Tu as un bon relationnel et est rigoureux(se)\n· Tu as une bonne capacité d’analyse et rédactionnelle\n· Tu t’adaptes rapidement aux changements\n🎓\nFormation :\nTu es diplômé(e) d’un master en école de commerce, école d’ingénieur ou équivalent.\nChez BforBank nous recherchons avant tout des compétences. Tu ne disposes pas du diplôme requis mais as des expériences équivalentes ? N'hésite pas à postuler !\n💼\nExpérience :\nExpérience confirmée de 3 ans en tant que Data Engineer.\nEn rejoignant BforBank tu trouveras…\n· Un projet ambitieux de transformation digitale et culturelle à l’échelon européen, terrain d’innovation et d’ouverture d’esprit\n· Une organisation apprenante, proposant un large choix de formations toute l’année, et qui favorise l’échange avec les autres marques du Groupe\n· Une promo RSE multi-métiers qui fait évoluer en continu les actions de BforBank vers une banque plus responsable\n· Une organisation du travail en mode Agile, impliquant un degré élevé de collaboration et d'autonomie tout en travaillant avec un groupe de pairs diversifiés.\n· Une Direction Technologie en pleine expansion, porteuse de nombreux défis stratégiques\nMais aussi…\nDe 2 jours à 5 jours de télétravail modulables par semaine, dans la limite de 84 jours par an (frais de fonctionnement pris en charge)\n25 jours de congés + 16 jours de RTT\n80% du coût de la mutuelle d’entreprise pris en charge / couvert\nAvantages collaborateurs Crédit Agricole : taux et tarifs préférentiels\nDes frais de transports remboursés à 75%\nUn restaurant d’entreprise\nDes douches pour les sportifs et un tarif avantageux auprès d’une salle de sport toute proche\n📍 Le poste est basé à La Défense, dans des locaux flambant neufs !\nBforBank s'engage à garantir l'égalité des chances aux candidats car nous sommes convaincus de la richesse apportée par la diversité et l'inclusion dans nos équipes.\nRencontrons-nous !\nLe processus de recrutement se déroule en 4 étapes :\n🧑🏼‍💻\nCall de 30 minutes avec notre équipe Talent Acquisition\nEchange avec le Data Factory Manager et notre équipe Talent Acquisition (présentiel)\nEchange avec une personne de l’équipe avec qui tu seras amené à travailler (visio)\nEchange avec le CTO (visio ou présentiel)\nNotre processus de recrutement dure en moyenne 3 semaines et l’équipe Talent Acquisition se tiendra à ta disposition pour te donner un maximum de visibilité sur l’avancée du process.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirmé"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "Pandas"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Other": [
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "fifty-five",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-cloud-engineer-h-f-at-fifty-five-3915044673?position=1&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=TI0d5l99iWJiD%2B5Azqy3CA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Senior Data & Cloud Engineer\nfifty-five est une data-company d'un genre nouveau qui aide les marques à exploiter les données pour améliorer le marketing, les médias et l'expérience client grâce à une combinaison de services de conseil et de technologie spécialisés.\nEn tant que pilier data et marketing du Brandtech Group, nous offrons des services qui combinent le conseil en stratégie, les services de cloud, le conseil en média et l'expérience client.\nfifty-five, c'est plus de 400 experts du numérique. Des digital consultants, des spécialistes du tracking et du média, des ingénieurs et des data scientists, travaillent tous en étroite collaboration pour fournir des conseils marketing de haut niveau et une assistance technique aux marques, dans tout type d'industrie, partout dans le monde.\nPartenaire des annonceurs de la collecte à l'activation et l'exploitation des données, nous aidons les organisations à devenir de véritables entités omnicanales maîtrisant l'efficacité de leur écosystème digital et ses synergies avec le monde physique.\nBasé à Paris, nous opérons sur 3 fuseaux horaires depuis nos 10 bureaux, situés à Paris, Londres, Genève, Milan, Shanghai, Hong Kong, Shenzhen, Taipei, Singapour et New York. fifty-five attache une importance particulière au bien-être de ses collaborateurs, ce qui lui a permis de figurer dans le classement Best Workplaces France en 2018.\nContexte :\nL'équipe d'ingénierie développe et met en œuvre les solutions techniques permettant la réalisation de pipelines de données et l'implémentation de data platform pour nos clients : récupération de datas sur de multiples sources de données (APIs, files, etc.), data cleaning, data processing, automation et monitoring de l'ensemble. L'équipe s'appuie sur des technologies récentes (docker, kubernetes, terraform, notebooks, etc.) et met en place ses projets dans les différents clouds du marché (GCP, Azure, AWS...).\nMission :\nNous sommes à la recherche d'une personne capable de réaliser des projets techniques pour répondre aux besoins de nos clients (par exemple: système de recommandations de produits, détection d'anomalies, ranking). Les activités vont du chiffrage et du sizing technique à la mise en œuvre des architectures, en passant par la revue des spécifications fonctionnelles et la production de code. Le Data & Cloud Engineer sera épaulé par un Lead dans ses missions. Il sera également amené à participer à la R&D et à accompagner les équipes transverses dans la mise en place d'outils de travail internes (librairies pour les data scientists, environnement Notebooks pour les data analysts et data scientists, développement de frameworks sur différents cloud providers, etc.).\nNous souhaitons trouver la bonne personne pour faire évoluer ou créer de nouvelles solutions dans ce cadre. Les missions comprennent aussi bien du prototypage rapide pour des démonstrateurs, que de la production de code robuste qui tourne en production tous les jours.\nCompétences et expériences :\n4-5 ans d'expérience en tant que Data Engineer\nMaîtrise de Python, SQL\nMaîtrise des environnements Cloud. Idéalement certifié GCP, Azure ou AWS\nBonne connaissance de Docker/Kubernetes\nBonne connaissance d'au moins un data warehouse (BigQuery, Snowflake, etc)\nConnaissance autour des Notebooks (Jupyter)\nA l'aise avec des concepts liés aux APIs (OAuth, REST, etc.)\nA l'aise avec les notions d'Infrastructure as Code (Terraform)\nAu courant des pratiques GitOps et connaissances des concepts autour du CI/CD\nLa maîtrise d'un orchestrateur, comme Apache Airflow, est un plus\nEsprit d'équipe (collaborer aux tests unitaires, revue de code, partage de code, sprints)\nBon niveau en français et en anglais\nA déjà travaillé en mode projet avec des interlocuteurs variés (consultant, data analyst, data scientist)\nUne expérience en marketing digital est un plus\nNous proposons :\nun bureau au centre de Paris avec terrasse et jardin\nun environnement multiculturel avec des collaborateurs aux nationalités multiples (France, Royaume-Uni, Etats-Unis, Chine, Tunisie, Italie et plus)\ndes projets avec nos bureaux à Londres, Hong Kong, New York, Shanghai, Genève, Shenzhen et Taipei\ndes TGIF et supers soirées\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ATAWIZ",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-cdi-ou-freelance-at-atawiz-3913839795?position=2&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=AURvP5OGbeKbtJbRzCZeqA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Pour la petite histoire, Atawiz a été créé en 2016, pour devenir le cabinet d’expertise que l’on connaît aujourd’hui. Nous sommes Microsoft Solutions Partner dans plusieurs domaines de compétences et partenaire Databricks. Les domaines d’expertise d’Atawiz s’adaptent aux nouveaux enjeux des entreprises : Time to Market, agilité, data, digitalisation des processus, …\nNos consultant·e·s combinent ainsi agilité, approche DevOps pour soutenir nos clients dans la réalisation de leurs objectifs, et ce, depuis 2016 !\nAu-delà d’un groupe d’expert·e·s, nous sommes des passionné·e·s qui collaborent dans différents domaines d’expertise :\nLe Cloud, spécialement Azure\nDevOps\nLe Big Data\nLe développement d’application mobile et web\nLes architectures micro-services\nAlimenté·e par une forte culture d’entreprise, notre objectif est d’assurer la satisfaction de nos consultant·e·s autant que de celle de nos clients. Pour cela, nous sélectionnons avec attention nos projets pour que chacun de nos collaborateur·rice·s se sentent stimulé(e)s et progressent rapidement.\n📇 Descriptif du poste\nNous avons de fortes demandes client et par conséquent besoin de renforcer notre équipe.\nL’aventure Atawiz, c’est aussi intégrer une communauté d’une vingtaine d’expert·e·s tous passionné·e·s. À leurs côtés, tu pourras évoluer rapidement et développer de nouvelles compétences.\n🎯Mission proposée (longue durée), en régie chez notre client, grande entreprise du secteur de l’énergie (petite couronne – proche Paris) :\nFreelance accepté.\nEn tant que\nData Engineer H/F,\ntu seras rattaché(e) au Responsable Data.\n💡 Enjeux du Poste\nAssurer le lead technique sur l’ingestion, le chargement et la transformation des données dans la data plaform (Datalake, Datawarehouse)\nModéliser et implémenter les différents flux de données\nParticiper à l’urbanisation de l’architecture de la plateforme avec le Tech Lead\n👩‍💻 La répartition des tâches :\nPipelines de données :\nMise en place et optimisation des processus.\nRéférent technique pour l'ingestion, le loading et la transformation de données.\nDéfinition technique du modèle de données.\nDéveloppement :\nPréparation des environnements techniques.\nDéveloppement sur les assets selon les normes.\nCollaboration avec le support de la plateforme pour des expos en API.\nIngénierie de données :\nCompétences en modélisation et création de scripts.\nRéflexions sur l'échelle, la disponibilité et l'optimisation des opérations.\n💻 Stack technique sur le projet :\nCloud : AWS (idéalement) ou Azure\nDEV : Python, API, Git, Docker,\nDevOps (Terraform, CI/CD),\nDATA (SQL, NoSQL, DBT), BIGDATA (HDFS, SPARK).\nData Science (MLOPS),\nNotions : Scalabilité, Clustering, Data Mesh, Data Virt, Data Qualité, Analytique (Tableau, Dataiku).\n✨ Avantages de la mission :\nEn acceptant cette mission, tu bénéficieras d’un environnement de travail\nmotivant et coopératif\n, où tu auras l’occasion de t’impliquer dans\ndes projets diversifiés et challengeants.\nDe plus, vous aurez des\npossibilités d’évolution de carrière.\n🔎 Profil recherché :\nPassionné(e) de nouvelles technologies\net\nproactif(ve)\n, tu excelles en équipe dans un environnement dynamique. Ton engagement envers les meilleures pratiques et tes compétences en communication font de toi la personne idéale pour cette mission.\nTu as\nminimum 5 ans d’expériences\n, une\nsolide expérience\ndans le domaine du data engineering en environnement cloud.\nUne expérience sur le cloud AWS ou équivalent (Azure, GCP) est un plus.\nTon esprit d'équipe, ta pro activité font de toi un(e) candidat(e) idéal(e) pour renforcer l’équipe.\nRejoins-nous ! 🌟\nAvant d’être en mission chez le client, tu es avant tout un(e) salarié(e) d’Atawiz 😊\n✨ La vie interne chez Atawiz:\nChez Atawiz, les opportunités sont nombreuses, nous valorisons ton expertise et ton savoir-faire. Tu auras la possibilité de :\n✅ Développer tes compétences et pour ça, tu bénéficieras d'une formation approfondie, de coaching managérial et technique, et d'un accompagnement pour obtenir des certifications. 📚\n✅ Participer activement à la vie interne de l'entreprise, en partageant tes connaissances et ton expertise au travers d'articles techniques publiés sur notre blog et à notre page LinkedIn.\n👉 notre blog : https://blog.atawiz.fr/\n👉 page LinkedIn : https://www.linkedin.com/company/atawiz/\n✅ Suivre en mission et être garant de l’évolution de carrière de consultants juniors.\n✅ Participer au process de recrutement, pour tester techniquement les candidats ou proposer des cooptations de personnes de ton réseau.\n✅ Profiter d'avantages attractifs, tels que des événements Microsoft, des bootcamps et des workshops & tech coffees réguliers, un matériel performant, et bien plus encore ! 🎉\nBenefits:\nSalaire fixe entre 60k € et 65k € en fonction du profil pour un CDI\nPour Freelance TJM entre 600 et 650 € / jour\nStatut cadre\nRTT\nTitre de transport pris en charge à 100%\nMutuelle Alan pris en charge à 60 %\nLa prévoyance prise en charge à 100 %\n7 € de panier repas par jour travaillé\n2 jours de Télétravail /semaine\nBudget télétravail\nDe beaux locaux à Paris 9e, proche de la gare Saint-Lazare\nRejoins-nous dès maintenant dans nos bureaux situés en plein cœur de Paris 💼 pour faire partie d'une équipe de passionnés, relever de nouveaux défis et booster ta carrière. 🚀\nIntéressé(e) ? Postule dès à présent et découvre notre processus de recrutement interactif. Tu auras l'occasion de rencontrer notre équipe, d'échanger avec nos expert·e·s techniques, et même de discuter avec notre CEO et notre Sales Director.\n🤝 Notre processus de recrutement :\n1 - Postulez en ligne ! L’équipe Recrutement étudie avec attention ta candidature et te répond dans les plus brefs délais.\n2 - Premier échange ! Tu échanges avec notre Talent Acquisition Manager sur ton parcours, tes aspirations professionnelles ainsi que sur Atawiz et les opportunités que nous proposons.\n3 - Un entretien technique avec l’un·e de nos expert·e·s pour te challenger techniquement. C’est également l’occasion pour toi d’avoir son retour d’expérience.\n4 - Dernier entretien : pour finir, tu rencontreras notre CEO. Tu pourras par ailleurs échanger avec notre Sales Director.\nPendant tout le processus de recrutement, tu as la possibilité de participer à des événements organisés par Atawiz et d’échanger avec des collaborateurs afin d’en apprendre plus sur nous !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Junior"
            ],
            "TypeContract": "CDI",
            "Salary": "60k",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-talend-f-h-at-orange-business-3916552363?position=3&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=1c8SaftDClzn7NknZ4szxw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "L’ambition d’Orange Business est de devenir l’intégrateur réseau et numérique de référence en Europe, en nous appuyant sur nos forces autour des solutions de connectivité nouvelle génération, du cloud et de la cybersécurité.\nNos 30 000 femmes et hommes présents dans 65 pays, dont chaque voix compte, sont tous animés par la même détermination et le même esprit d’équipe, pour construire les solutions digitales d’aujourd’hui et de demain et créer un impact positif pour nos clients, pour leurs salariés et pour la planète.\nNous offrons des opportunités passionnantes grâce à des projets innovants dans la data et le digital, le cloud, l’IA, la cybersécurité, l’IoT, ou encore le digital workspace et le big data.\nVenez vivre cette aventure avec nous !\nAfin de développer notre équipe lilloise, nous recherchons aujourd'hui, un Ingénieur DATA à même d’accompagner nos clients dans la structuration de leurs SI autour de la donnée.\nVos principales missions seront les suivantes\n:\n- Concevoir des solutions de traitement et collecter des volumes importants de données.\n- Participer à des études de cadrage pour collecter le besoin métier et concevoir les solutions qui répondent au besoin du client.\n- Apporter son expertise sur des problématiques précises rencontrées chez les clients.\n- Participer à la veille technologique\n- Réaliser les\ndéveloppements TALEND\n- Rester informé et former sur les nouvelles solutions DATA\n- Contribuer aux phases d'avant-vente et au développement business.\n- Participer à la conception, l'évolution et la présentation de nos offres DATA.\nVous\n:\n- Êtes issu(e) de formation bac+5 ?\n- Vous justifiez d'au moins 3 ans d'expériences en qualité d'Ingénieur DATA sur la solution TALEND Enterprise (Data Integration) et avez idéalement une connaissance des solutions Cloud d'AWS et d'AZURE ?\n- Vous êtes intervenu sur des projets intégrant des pratiques DevOps et AGILE ?\nAlors postulez, ce poste est fait pour vous !\nVos compétences clés\n:\n- Expertise sur l'outil\nETL TALEND\nEnterprise (administration et développement)\n- Fortes connaissances des solutions de bases de données (SQL, NoSQL…)\n- Connaissances en langages objets ou scripts (notamment Java mais aussi Javascript, Scala, Python…)\n- Divers systèmes d'exploitation : UNIX, Windows\nAutonomie, rigueur, curiosité, dynamisme et sens du service sont des qualités nécessaires pour ce poste.\nLes compétences complémentaires qui seraient appréciées :\n- Connaissances d'autres modules Talend (MDM, ESB, Data Quality, Cloud…)\n- Maîtrise des technologies du Big Data (Hadoop, Spark, Kafka…)\n- Expertise sur d'autres outils ETL (Informatica, SSIS, DataStage...)\n- Notions en architecture des Systèmes d'Information\n- Maîtrise de l'anglais (oral et écrit)\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "OS": [
                "Windows"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-palantir-toulouse-at-capgemini-3889716121?position=4&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=G8hCyH5UBuERAOY7nMXYqQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini en quelques mots\nChoisir Capgemini, c'est choisir une entreprise où vous serez en mesure de façonner votre carrière selon vos aspirations, où vous serez soutenu et inspiré par une communauté d’experts dans le monde entier, où vous pourrez réécrire votre futur. Rejoignez-nous pour redéfinir les limites de ce qui est possible, contribuer à libérer la valeur de la technologie pour les plus grandes organisations et participez à la construction d’un monde plus durable et inclusif.\nVos missions\nVous maitrisez au minimum un langage de programmation appliqué à l’analyse de données\n(Java, Python, Scala et les environnements Spark et / ou Hadoop)\net vous avez pu intervenir sur la\nsolution Palantir.\nVous êtes passionné par le Big Data et le Machine Learning et l’analyse de données\nVous concevez et mettez en œuvre des stratégies sécurisées d'acquisition et d'intégration de données\nVous configurez des référentiels de données à la pointe de la technologie dans des environnements distribués\nVous construisez des pipelines de données pour collecter, transformer et traiter des données en collaboration avec des scientifiques de données afin de répondre aux exigences de la modélisation de données d'analyse avancée\nVotre profil\nDiplômé(e) de Bac+5 en informatique\n4 ans d’expérience\n(au sein d’une ESN ou chez un intégrateur) en conseil clientèle\nUne solide culture technologique\nUn bon niveau d’anglais\n3 raisons de nous rejoindre\nQualité de vie au travail :\naccord de télétravail en France et à l’international, accord sur l’égalité\nprofessionnelle, la parentalité, l’équilibre des temps et la mobilité durable.\nApprentissage en continu :\ncertifications et formations en libre accès, accompagnement sur mesure avec\nvotre carreer manager, parcours d’intégration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, activités à tarifs préférentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel\nNos engagements et priorités\nLe groupe Capgemini encourage une\nculture inclusive dans un cadre multiculturel et handi-accueillant.\nEn nous rejoignant, vous intégrez un collectif qui valorise la diversité, développe le potentiel de ses talents, s’engage dans des\ninitiatives solidaires avec ses partenaires, et se mobilise pour réduire son impact environnemental sur tous ses sites et auprès de ses clients.\nCapgemini\nest un\nleader mondial\n, responsable et multiculturel, regroupant près de 350 000 personnes dans plus de 50 pays. Fort de\n55 ans d’expérience,\nnous sommes un partenaire stratégique des entreprises pour la transformation de leurs activités en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perpétuelle évolution tels que le\ncloud, la data, l’Intelligence Artificielle, la connectivité, les logiciels, l’ingénierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Action for Market Transformation - A4MT",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-action-for-market-transformation-a4mt-3910049004?position=5&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=Q8pyx%2BdNkBGihv0fWQrksw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "A4MT – Action pour la Transformation des Marchés\nA4MT conçoit et implémente des programmes d’engagement et de « Market Transformation » qui visent à généraliser des pratiques vertueuses – au sens environnemental et sociétal – en modifiant la donne du marché, en reconfigurant le jeu d’acteurs, généralement via des actions collectives.\nCes programmes agissent sur la demande en suscitant de nouvelles pratiques individuelles et collectives. A4MT assure le rôle de pilote, orchestrant les plans d’action des parties prenantes grâce à une équipe de qualité à caractère international, un savoir-faire sur la mise en œuvre des programmes, une connaissance technico-économique experte des sujets traités, et une capacité à interpeller les décideurs à bon niveau.\nChampionnat de France des économies d’énergie\nA4MT avec ses partenaires opère l’ensemble des concours CUBE en France (Championnat de France des Economies d’Energies) et assure son développement international (Europe, Asie, etc.). CUBE est un concours original d’économies d’énergie et de CO2 pour les bâtiments tertiaires et résidentiels qui accélère fortement l’action de terrain grâce à une intelligence collective sur le terrain.\nLe concours est aujourd’hui présent dans 8 pays et se développe encore. Au-delà des économies les plus faciles, il s’agit de mettre en œuvre la trajectoire de gestion immobilière et d’investissement qui permettra, au-delà des avancées dans ce programme à faible investissement, de progresser sur la trajectoire de la neutralité carbone.\nhttps://championnatdefrancedeseconomiesdenergie.org/\nMISSION\nRendant compte au directeur d’A4MT et en étroite collaboration avec le directeur technique A4MT, vous êtes Data Engineer, vous serez responsable de la conception, du développement et de la maintenance des bases de données, et des outils de reporting. Vous travaillerez en étroite collaboration avec l'équipe de développement (prestataire externe) et vous participez à la structuration d’une équipe IT interne pour créer des solutions innovantes répondant aux besoins de l'entreprise.\nVotre mission s'articule autours des 3 axes ci-dessous:\n1/ Pilotage et et développement\ndévelopper et déployer des reporting robustes et évolutifs.\nle planning de développement et le budget alloué.\navec les équipes d’animation et le back office technique du programme et avec les prestataires externes pour comprendre les exigences et les spécifications du projet.\nà la conception de l'architecture des bases de données et à la prise de décisions techniques.\nla qualité des données en effectuant des contrôles qualité.\nles performances des applications pour garantir une expérience utilisateur fluide.\nla maintenance et les mises à jour régulières des applications existantes.\nà l'affût des tendances et des technologies émergentes.\nVous serez responsable du process, de la maîtrise d’ouvrage liée à la Data et garant(e) de la qualité de service.\n2/ Implication des équipes et de la sous-traitance\nVous serez impliqué dans une équipe informatique naissante et dans une équipe projet avec les différentes fonctions métiers. Vous devrez faire le suivi de votre implication avec le responsable de programme et le directeur technique d’ A4MT :\n3/ Gestion de projet\nVous tiendrez le tableau de bord des outils : budgets, engagements, planning, résultats, développements.\nPROFIL\nVous avez une expérience significative d’au moins 3 années dans l’écosystème de big data, des serveurs et bases de données dans des contextes de projets, d’exploitation de migration.\nCOMPETENCES\nBac +5 diplômé(e) d’une grande école d’ingénieur ou équivalent, vous êtes :\n+5 diplômé (e) d’une école d’ingénieurs ou équivalent, en Data science, Informatique, génie logiciel ou domaine connexe.\nprofessionnelle démontrée de 3 ans ou plus en tant que Data Engineer\ndes langages structurés (JavaScript, Scala, Python…),\navec les bases de données relationnelles (MySQL, PostgreSQL) et non relationnelles (MongoDB, Firebase).\nau moins un outil de reporting (Power BI, Tableau …)\ndes services de déploiement et d'hébergement cloud comme AWS, Azure ou Google Cloud Platform.\ncompétences en développement back-end avec des technologies comme Node.js, Python, Ruby on Rails, ou Java. et notamment en PHP sont recommandées\ndes langages de programmation front-end tels que HTML5, CSS3 et JavaScript (notamment frameworks: comme React, Angular ou Vue.js).\nà travailler en équipe, à communiquer efficacement et à résoudre les problèmes de manière autonome.\ndes principes de sécurité des applications web et des meilleures pratiques en matière de développement sécurisé ainsi que le respect du RGPD.\nDate d’entrée et conditions\nLe poste est à pourvoir immédiatement; il est basé au 54, rue de Clichy, Paris (IXème). Niveau de rémunération selon expérience.\nContact\nMerci d’adresser votre candidature complète (CV, lettre de motivation, présentation du cursus en cours de conclusions et références éventuelles) à l’attention de M. Adrien Brunella sur le mail elisabeth.clement@a4mt.com\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "MongoDB"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "CloudComputing": [
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "PostgreSQL",
                "MySQL"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ADVANCED Schema",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-advanced-schema-3539059697?position=6&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=jTTsHcD39zslHD4GFXcERg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ADVANCED SCHEMA\nest une société de services informatiques\nspécialisée dans la donnée.\nDepuis 20 ans, nous créons des plateformes data sur mesure pour nos clients, orientées usages et alliant qualité, performance, sécurité et gouvernance.\nADVANCED SCHEMA\na développé de nouvelles activités pour réaliser l'ambition du groupe : devenir\nune entreprise end-to-end,\nen proposant une offre à 360° à nos clients pour les\naccompagner à chaque étape de leurs projets.\nÀ ce jour, nous sommes près de 220 passionnés répartis entre Paris, Lille, Nantes, Lyon mettant à profit leur expertises aussi bien dans le domaine du retail, de la finance/assurance, du luxe, des médias, de la santé et de l'industrie.\nAujourd’hui, nous souhaitons intégrer de nouveaux renforts dans nos équipes Lilloises.\nEn tant que Data Engineer, vous aurez les missions suivantes :\nConcevoir des modélisations physiques\nConstruire des mappings techniques et rédaction de spécifications d’alimentation.\nDévelopper des flux des données\nContribuer au pilotage de projets, de proof of concepts\nParticiper à des missions d’expertise\nCompétences professionnelles & niveau d'études requis :\nVous êtes titulaire d'un diplôme Bac +3 minimum dans le domaine de la data\nVous possédez minimum 2 ans d'expérience dans le métier\nPositif(ve), curieux(se), rigoureux(se) et doté(e) d'une bonne aisance relationnelle\nÊtre enthousiaste à l'idée d'apprendre de nouvelles technologies\nExpérience de la méthodologie Agile / Scrum\nCapacité à planifier et à prioriser les tâches et les activités confiées en autonomie\nMaîtrise de l’anglais oral et technique obligatoire\nExpérience avérée dans l'écriture de code propre avec 2 ou plusieurs des technologies suivantes : BASH, SQL, Java, Python, NoSQL\nNotre proposition :\nTemps plein en\nCDI\navec un\nsalaire attractif\n+ participation aux bénéfices + prime(s) sur investissement personnel\nMode de\ntravail hybride\n(agence, site, télétravail selon projets/clients)\nTicket restaurant (Sodexo)\nMutuelle financée à 50%\nPrévoyance\nComité entreprise\n5 jours d’onboarding plein temps via la\nADVANCED SCHEMA Academy\nNotre investissement :\nChez\nADVANCED SCHEMA\n, nous t’offrons un environnement de travail stimulant et collaboratif ainsi que des possibilités de croissance et de développement professionnel. Également un\naccompagnement/support au quotidien\npour te faire grandir et monter en compétences, sur des projets qui répondent à de\nvrais enjeux pour nos clients\n. Si tu es passionné(e) par les données et prêt(e) à relever de nouveaux défis, alors nous aussi nous aimerions te rencontrer\nProcess de recrutement :\nSi ta candidature retient notre attention, nous te proposons :\nUn premier échange téléphonique/visio\nUn entretien physique (+questionnaire d’évaluation) avec un senior manager\nUn entretien final à notre siège Parisien afin de rencontrer le DG\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Temps plein",
                "Senior"
            ],
            "TypeContract": "CDI",
            "Salary": "50",
            "Level": "",
            "Experience": "20 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Bash",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Goaheadspace",
        "location": "Pantin, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-x-at-goaheadspace-3915359112?position=7&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=btBBe8MUw%2FU67f8zq5gtgA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "MFG Labs est une société de conseil et réalisation experte en data, qui aide les entreprises à améliorer leurs prises de décision, à automatiser leurs processus et à créer de nouveaux services grâce à la data science, au design et à l'utilisation des dernières technologies.\nMFG Labs intervient à toutes les étapes de votre transformation data : de la création d'une feuille de route de projets data, à la découverte d'insights, à la modélisation de problématiques complexes, de la création d'un modèle prédictif à l'implémentation technique d'une solution data sur-mesure\nMFG Labs accompagne ses clients de différentes manières :\nStratégie\nSolutions\nFondations\nMFG Labs déploie une approche holistique pluridisciplinaire, en mêlant des data scientists, des designers, des data engineers et des consultants, afin d'apporter des solutions complètes de bout en bout à des problématiques complexes.\nDans le cadre du développement de l’équipe, nous recherchons un.e Data Engineer à\nPantin (magasins généraux).\nAu sein de l’équipe Data Technology, vous aurez pour mission de travailler sur des problématiques de collecte de la donnée sur tout type de support digital : web, mobile, application, voire IoT.\nVotre rôle au sein de l’équipe :\nFaire partie d’une équipe pluridisciplinaire avec des talents en Design de Service, Consulting et Data science.\nDévelopper des applications de production intégrant différents outils : des Mathématiques Appliqués, Machine (Deep) Learning, Recherche Opérationnelle, Statistiques.\nDévelopper des pipelines de traitement de données avec l’équipe de Data Science pour : ingérer, transformer et délivrer des données et modèles à nos applications.\nDéployer des applications utilisant les derniers outils mis à disposition par les différents Clouds publics.\nÀ propos de vous :\nVous êtes titulaire d'un niveau Bac +4/Bac +5 d'une école d'ingénieur\nVous avez au minimum deux ans d'expérience hors stage ou alternance\nVous êtes rigoureux·se vis-à-vis de vous-même et des autres quant à la qualité du code.\nVous avez quelques connaissances et compétences solides en développement et en en Data Ingénierie au sens large.\nEn\ndéveloppement\nPython 3 et SQL\nFramework de traitement de données (Spark ou équivalent)\nDocker\nGIT\nEn +\nFramework permettant de déployer des APIs (Flask ou équivalent)\nCI/CD\nLa pratique d'au moins un cloud (AWS, GCP ou Azure) est appréciée\nEn Data Ingénierie\nDatawarehouse ou Datalake\nData Pipelines Batch et/ou Straming\nEn +\nOutils de BI (Tableau, Power BI…)\nOutils MLOps (Sagemaker, VertexAI, etc.)\nSi vous vous reconnaissez dans cette annonce, n'hésitez pas à postuler !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "Containers": [
                "Docker"
            ],
            "Other": [
                "Statistiques",
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Equativ",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-internship-at-equativ-3821045783?position=8&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=aPcC2%2FvBFSIXODHnsxj2aw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "👫 About the team\nAt Equativ, we’re on a mission to develop advertising technologies that empower our customers to reach their digital business goals. This means that we rely on massively scalable, widely distributed, highly available, and efficient software systems; the platform deals with over 3 millions requests per second managed by 3,000 servers.\nOur innovation team based in Paris, Nantes, Limoges, Krakow and Berlin is composed of 100+ straightforward and energetic engineers working in an Agile environment and ready to tackle the most complex technical challenges.\nOur data engineering team is composed of 8 skilled engineers and is based in Paris. We are part of the R&D department which is composed of 120+ engineers spread across Paris, Nantes, Limoges, Krakow and Berlin all working in an Agile environment and ready to tackle the most complex technical challenges.\nThe data engineers are split in two sub-teams working in close collaboration:\nPipeline team: Maintaining and enhancing the operationality of our on-premise and cloud data pipelines which feed our warehouses and APIs\nFeature team: Apply best in class data modeling and orchestrating data transformations in our warehouses, leading the day-to-day management of these warehouses\nOur Mission 👇\nOur Data Engineering team is central to Equativ’s data centric business and is responsible to ingest, transform, model and redistribute all data coming from our ad tech platform.\nWe aim at building scalable and robust Big Data platforms from ingestion to business actionable consumption. Our Big Data ecosystem must handle huge volumes (15 Tb per day), short & long term data storage, complex data modeling, real-time and batch ELT as well as providing external access through dedicated APIs.\nWe enhance and deliver Equativ data directly to our customers and throughout the company whether it is for BI analysis, data science models feeding, customer reporting, invoicing and more.\nWe rely on a top tier on-premise & cloud stack (Kafka, Flink, ClickHouse, Bigquery, Dataflow, Airflow, DBT…) and work hard to increase reporting capabilities, lower maintenance time, improve performances and simplify the access to our raw data.\nWhat you'll do ✏️\nAs a data engineer intern, you will be supporting one of the two sub-team (pipeline or feature) in their tasks and projects:\nTake a leading part on a data engineering project such as but not limited to:\nProof of Concept of Clickhouse Cloud\nImprovement of the data transformation process with DBT, BigQuery and Airflow\nDevelopment of new functionalities on our internal tools (APIs, software applications)\nSetup a data lineage application (castor doc)\nSupport the data engineering team in their day-to-day activities:\nEnhance our DevOps process with CI/CD and testing framework\nMonitor performances and workflow of our applications using reporting tool (Grafana)\nTake part in improving and deploying data engineering standards, documentation and operational guidelines around data usage at Equativ\n💪 About you\nMaster degree in Computer Science or similar technical field of study\nPrior experience in data or software development related environment is desired\nExperience with a cloud datawarehouse (BigQuery, Snowflake, Databrick,..) is a plus\nGood knowledge of SQL and one other data programming language (Java preferred, Python, Scala..).\nKnowledge on the software development process (Git, CI/CD, test, scrum)\nWorking proficiency and communication skills in verbal and written English\nStrong interest in big data and cloud computing technologies.\n👋 About us\nEquativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus — four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.\nHeadquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.\nThe company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times’ FT 1000: Europe’s Fastest-Growing Companies.\nEquativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.\nCome and lead the charge with us in building a transparent ecosystem based on quality!\nEquativ is an equal opportunity employer. Equal access to employment, services, and programs are available to everyone, regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you require reasonable accommodation throughout the application and/or interview process, please contact the recruitment team at ta-team@equativ.com\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Flink"
            ],
            "DevTools": [
                "Git"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Mirakl",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-engineer-at-mirakl-3904074867?position=9&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=fJPMHHthjSFBbS6NfY0iYA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Mirakl, leader et pionnier de l’économie de plateforme, propose aux entreprises une suite unique de solutions leur permettant de transformer significativement leur e-commerce afin d'accélérer de façon durable et rentable leur croissance. Depuis 2012, Mirakl accompagne les entreprises B2C et B2B avec la technologie la plus avancée, sécurisée et évolutive leur permettant de digitaliser leur activité et d'élargir leur offre via la marketplace ou le dropship, faciliter la gestion des catalogues et des paiements de leurs fournisseurs pour plus d'efficacité, offrir une expérience d'achat personnalisée à leurs clients, et augmenter leurs profits grâce au retail media. Basée à Paris et Boston, Mirakl est certifiée Great Place to Work.\nA propos de Mirakl Labs\nNos équipes techniques et produits, nommées Mirakl Labs, sont principalement réparties entre nos 2 hubs situés à Paris et à Bordeaux. Elles collaborent au quotidien afin d'adresser les problématiques de nos clients et utilisateurs en répondant à différents challenges liés aux nouvelles fonctionnalités, à la scalabilité, la sécurité et l’ergonomie…\nElles opèrent en mode agile et s'organisent en Squads composées d'un Squad Lead, de 5 développeurs, d'un Product Manager et d'un QA. Chaque Squad est spécialisée sur un scope fonctionnel afin de concevoir et réaliser de nouvelles features, leurs évolutions et des APIs (avec un découpage en micro-services). Nos équipes Infrastructure, Architecture, Sécurité, Documentation, Product Design, Data et Support opèrent en transverse en apportant leur expertise et de la cohérence sur l’ensemble des produits.\nToutes les équipes sont responsables de leur périmètre et chacun des collaborateurs apporte son expérience et ses idées. Innovation, feedback et implication dans les prises de décision sont au cœur de notre philosophie.\nEt pour favoriser ce partage avec d’autres passionnés, nous sommes sponsors, speakers, et hôtes de différents événements, meetups, et associations de la scène Tech en France. Au cours des dernières années, nous avons participé à des événements tels que Devoxx, ReactEurope, ProductConf et Flupa UX Days.\nA propos du job\nLa solution SaaS Mirakl est le moteur des marketplaces des plus importants e-commerçants à travers le monde. Cette solution gère et produit de gros volumes de données qui présentent des challenges extrêmement intéressants pour les spécialistes de la donnée (produits, commandes, clients, niveaux de stock, prix, messages, appels API, données de navigation, séries temporelles, données géolocalisées etc.).\nEn tant que (Senior) Data Engineer au sein de l’équipe Data Mirakl, vos principales missions seront de :\ncontribuer à l'enrichissement de la Data Platform (ETL)\naméliorer la robustesse de nos pipelines de production pour nos applications Machine Learning (inférence real time etc.)\nIntégré(e) dans une équipe de spécialistes de la donnée (data engineers, machine learning engineers, data scientists, data analysts), vous êtes un des acteurs clés pour garantir la place de Mirakl comme solution dominante sur son marché.\nNotre stack et nos outils\nApache Spark, Kafka, AWS, Databricks, Python, Airflow, Mlflow, Tensorflow, Delta lake, Superset, Kubernetes, Redshift, SQL, Terraform, Ansible\nAu quotidien, vous allez :\nParticiper à la définition et à l’implémentation d’une architecture performante, robuste, scalable et aux coûts maîtrisés pour nos applications Spark ainsi que pour nos pipelines de production de Machine Learning (évaluation des feature stores, refactoring de DAG Airflow)\nAccompagner les Data Scientists lors de leur mise en production (relecture de code, pair programming) et mettre en place les best practices\nOptimiser et améliorer la CI/CD de l’équipe en collaboration avec l’équipe SRE\nAssurer la montée en compétence des membres de l’équipe sur les sujets de MLOps et Data Engineering\nRéfléchir à la meilleure façon d'intégrer les données Google Analytics dans la data platform\nPartager ses connaissances et présenter les travaux devant toutes les équipes Labs\nCe qu’on peut vous apporter :\nDes projets data driven, divers et variés (traitements massifs d’images, de textes, time series etc.) pour des produits différents de Mirakl\nUne culture orientée sur la veille technologique\nDes projets qui ont un vrai impact business devant être déployés sur des centaines de clients dans un contexte multilingue\nQuelques exemples de sujets en cours :\nEnrichissement des données produit à partir des images et des descriptions\nModération automatique des produits\nMapping automatique des données produit\nIdentification des produits à fort potentiels\nDétection de comportements frauduleux\nSentiment analysis sur les messages échangés entre clients et vendeurs et dans les évaluations\nDétermination de prix optimaux\nMonitoring de la qualité de service des vendeurs\nDes applications d’inférence en synchrone de nos modèles de ML\nVous aimerez ce job si :\nVous êtes passionné(e) par la data et les technologies modernes permettant d'en tirer partie\nVous vous intéressez à la data science et avez des connaissances générales sur les algorithmes de Machine Learning\nVous avez un background en développement et avez évolué dans un environnement Data\nVous avez a minima 4 ans d’expérience en environnement Machine Learning et/ou Data\nVous avez mis en production avec succès des applications Big Data faisant appel à du Machine Learning, du NLP, du traitement d’images dans des projets d'envergure, à fort volume de données\nVotre maîtrisez Python, êtes un pro des frameworks data de la fondation Apache et êtes à l'aise dans un environnement AWS\nVous maîtrisez au moins un outil d’orchestration (Airflow, Data Pipeline ou tout autre outil similaire)\nVous présentez vos travaux de manière simple et accessible\nVous faîtes preuve d'un bon relationnel et vous aimez mentorer des collaborateurs\nVous parlez couramment anglais et français\nLes plus pour le poste :\nVous avez une expérience significative dans le domaine du e-commerce\nVous avez déjà mis en place un Data Lake, Data Warehouse ou une Data Platform\nVous avez déployé des applicatifs en environnement Kubernetes\nVous avez mis en place des pipelines d'ingestion de données avec une approche CDC à l'aide de Debezium ou autre\nVous maîtrisez Java/Scala\nMirakl est engagée en faveur de la diversité, de l’égalité des chances et de l’inclusion. Nous célébrons nos différences car nous sommes convaincus que les qualités visibles et invisibles de chaque Mirakl Worker sont une source de force et d’innovation. Dans le cadre de cet engagement, nous étudions toutes les candidatures sans distinction de : genre, ethnicité, religion, orientation sexuelle, handicap, âge ou toute autre caractéristique protégée par la loi.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes",
                "Ansible"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "ML",
                "Machine Learning",
                "Big Data",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "relevanC",
        "location": "Paris, Île-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-engineer-h-f-at-relevanc-3845776902?position=10&pageNum=10&refId=s3VlrhtbUrMVZp4kM4F1Ew%3D%3D&trackingId=ElXPjoEQ7Xk%2B0Qlsl75rVA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "relevanC est une filiale du Groupe Casino et a été fondée en 2017.\nNous avons des bureaux en France, au Brésil et en Colombie et opérons à l'échelle mondiale.\nNos solutions de Retail Media permettent à nos clients de générer de nouvelles sources de revenus publicitaires grâce à des annonces pertinentes et personnalisées.\nEn tant que Data Engineer tu auras accès aux données de nos clients internes (enseignes du groupe Casino) et externes à traiter au sein de notre data warehouse. Tes missions seront les suivantes :\ntravailler en étroite collaboration avec tous les autres membres de la squad\nécrire / relire du code en respectant les bonnes pratiques de développement ainsi que les tests unitaires et participer\nassurer la co-responsabilité du déroulement des déploiements, des mises en production et du bon fonctionnement des applications avec les autres membres de la squad\nrédiger la documentation technique quand cela est nécessaire\nmettre en œuvre les bonnes pratiques relatives au RGPD telles que définies par le tech lead\nCe CDI basé à Paris centre (1er arrondissement) débutera dès que possible.\nFaire partie de relevanC, qu’est-ce que ça signifie ?\nTravailler sur une stack technologique de pointe (Python, PySpark, Google BigQuery, Apache, Airflow…)\nÊtre membre à part entière d’une équipe dynamique et passionnée aux profils très variés (chefs de projets, développeurs, designers, animations commerciales)\nTravailler dans un environnement stimulant et relever des nouveaux défis chaque jour\nRejoindre une entreprise en pleine expansion avec des opportunités fortes de développements et d’innovation\nProfil recherché\nDiplômé(e) d’une grande école d’ingénieur ou profil universitaire spécialisé en Data / Informatique / Math / Stats.\n5 ans (et plus) d’expérience en Data Engineering\nAppétence forte pour le marketing digital et le retail, force de proposition, business oriented et moteur d’innovation\nUne maitrise parfaites des bonnes pratiques de développement\nSolides compétences en Python, Spark et SQL\nUne expérience sur Google Cloud Platform est un plus\nLien vers notre politique de traitement des données : https://relevanc.com/fr/politique-de-protection-des-donn%C3%A9es-recrutemen\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    }
]